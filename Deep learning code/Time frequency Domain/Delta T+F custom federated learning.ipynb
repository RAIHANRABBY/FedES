{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Mbfw8uvdftFM","executionInfo":{"status":"ok","timestamp":1717504498176,"user_tz":-360,"elapsed":1563,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sb"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"INiFJfLjgOkx","executionInfo":{"status":"ok","timestamp":1717504498177,"user_tz":-360,"elapsed":5,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"outputs":[],"source":["from sklearn import metrics\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, cohen_kappa_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"lYo2Uq77gQSH","executionInfo":{"status":"ok","timestamp":1717504505111,"user_tz":-360,"elapsed":6938,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import ExtraTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score, cross_val_predict"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"g66575_xgVzz","executionInfo":{"status":"ok","timestamp":1717504513536,"user_tz":-360,"elapsed":8430,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout,LSTM"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24614,"status":"ok","timestamp":1717504538144,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"},"user_tz":-360},"id":"MOCNWlamfr3v","outputId":"f15c915f-02fa-4588-b6d5-684f7cf7c630"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"iNy9eOGMf2qO","executionInfo":{"status":"ok","timestamp":1717504538145,"user_tz":-360,"elapsed":6,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"outputs":[],"source":["# Raw_data = '/content/drive/MyDrive/EEG Signal /Epileptic seizure/data/only data/raw_data.npz'\n","\n","Theta_data = '/content/drive/MyDrive/EEG Signal /Epileptic seizure/data/time frequency domain/RAW/Delta_merged_features.npz'\n","\n","\n","\n","# RAW = np.load(Raw_data, allow_pickle=True)  # Allow loading pickled object arrays\n","# X = RAW['X'].astype('float64')\n","# Y = RAW['Y'].astype('float64')\n","# group = RAW['Group'].astype('float64')"]},{"cell_type":"markdown","metadata":{"id":"PzeABzSyHgKg"},"source":["This is a good performing model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PTmHq1-S6XWL"},"outputs":[],"source":["# %%capture\n","# !pip install wandb"]},{"cell_type":"markdown","metadata":{"id":"6DhAYwXUSE9z"},"source":["# CNN-LSTM"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"LcgfTajmHWSD","executionInfo":{"status":"ok","timestamp":1717504548287,"user_tz":-360,"elapsed":10148,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"outputs":[],"source":["%%capture\n","!pip install tensorflow_addons"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1717506995238,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"},"user_tz":-360},"id":"VvjC2xCQNHLP"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, LSTM, Dropout\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","import tensorflow_addons as tfa\n","\n","class FederatedData:\n","    def __init__(self, data_path, num_clients, scaler_type='MinMax', test_size=0.2):\n","        self.data_path = data_path\n","        self.num_clients = num_clients\n","        self.scaler_type = scaler_type\n","        self.test_size = test_size\n","        self.partitions = []  # Initialize partitions attribute\n","        self.load_data()\n","        self.scale_data()\n","        self.create_partitions()\n","\n","    def load_data(self):\n","        try:\n","            data = np.load(self.data_path, allow_pickle=True)\n","            self.X = np.moveaxis(data['X'], 1, 2)  # Move axis here\n","            self.Y = data['Y']\n","            self.groups = data['Group']  # Assuming 'Group' contains group identifiers\n","        except KeyError as e:\n","            raise ValueError(f\"Missing expected data field: {e}\")\n","        except FileNotFoundError as e:\n","            raise ValueError(f\"Data file not found: {e}\")\n","\n","    def scale_data(self):\n","        # Reshape data to 2D array for scaling\n","        X_reshaped = self.X.reshape(-1, self.X.shape[-1])\n","\n","        # Select scaler based on input\n","        if self.scaler_type == 'Standard':\n","            scaler = StandardScaler()\n","        elif self.scaler_type == 'MinMax':\n","            scaler = MinMaxScaler(feature_range=(0, 1))\n","        else:\n","            raise ValueError(\"Unsupported scaler type. Choose either 'Standard' or 'MinMax'.\")\n","\n","        # Fit and transform the data\n","        scaled_data_reshaped = scaler.fit_transform(X_reshaped)\n","\n","        # Reshape back to original shape\n","        self.X = scaled_data_reshaped.reshape(self.X.shape)\n","\n","    def create_partitions(self):\n","        gss = GroupShuffleSplit(n_splits=self.num_clients, test_size=self.test_size, random_state=42)\n","        for train_index, test_index in gss.split(self.X, self.Y, self.groups):\n","            X_train, X_test = self.X[train_index], self.X[test_index]\n","            Y_train, Y_test = self.Y[train_index], self.Y[test_index]\n","            self.partitions.append((X_train, Y_train, X_test, Y_test))\n","            print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n","\n","    def get_training_and_validation_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        partition_X_train, partition_Y_train, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        X_train, X_val, Y_train, Y_val = train_test_split(partition_X_train, partition_Y_train, test_size=0.2, random_state=42)\n","        return X_train, X_val, Y_train, Y_val, partition_X_test, partition_Y_test\n","\n","    def get_testing_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        _, _, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        return partition_X_test, partition_Y_test\n","\n","def build_sequential_model(input_shape):\n","    clear_session()\n","    model = Sequential()\n","\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\", input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(512, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(filters=1024, kernel_size=3, strides=1, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(LSTM(256, return_sequences=True))\n","    model.add(LSTM(256, return_sequences=True))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(256, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n","\n","    # opt = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","    opt = tfa.optimizers.RectifiedAdam(learning_rate=0.00009)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = recall\n","\n","    return accuracy, precision, recall, f1, sensitivity, specificity, kappa\n","\n","def federated_learning(data_path):\n","    federated_data = FederatedData(data_path, num_clients=3, scaler_type='MinMax')\n","    federated_data.create_partitions()\n","\n","    # Get the input shape from the data\n","    input_shape = federated_data.X.shape[1:]\n","    global_model = build_sequential_model(input_shape)\n","\n","    num_clients = 3\n","    local_epochs = 5\n","    global_optimizer = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","\n","    # Initialize m and v for Adam optimizer\n","    m = [np.zeros_like(w) for w in global_model.get_weights()]\n","    v = [np.zeros_like(w) for w in global_model.get_weights()]\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 1e-7\n","    t = 0\n","\n","    client_data = []\n","    for client_idx in range(num_clients):\n","        x_train, x_val, y_train, y_val, x_test, y_test = federated_data.get_training_and_validation_data(client_idx)\n","        client_data.append((x_train, x_val, y_train, y_val, x_test, y_test))\n","\n","    metrics_list = []\n","\n","    for epoch in range(local_epochs):\n","        client_models = []\n","\n","        for client in range(num_clients):\n","            x_train, x_val, y_train, y_val, x_test, y_test = client_data[client]\n","            client_model = build_sequential_model(input_shape)\n","            client_model.set_weights(global_model.get_weights())  # Initialize with global weights\n","            opt = Adam(learning_rate=0.000009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","            client_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","            # Set up callbacks\n","            run_name = f\"epoch_{epoch}_client_{client}\"\n","            checkpoint_filepath = f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time+Frequency/Delta/CNN_LSTM/best_model_{run_name}.h5'\n","            checkpoint_callback = ModelCheckpoint(\n","                checkpoint_filepath,\n","                monitor='val_accuracy',\n","                save_best_only=True,\n","                mode='max'\n","            )\n","            csv_logger = CSVLogger(f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time+Frequency/Delta/CNN_LSTM/training_log_{run_name}.csv', append=True, separator=';')\n","\n","            history = client_model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_val, y_val), callbacks=[checkpoint_callback, csv_logger])\n","            client_models.append(client_model)\n","\n","            print(history.history)\n","\n","            # Load the best model before evaluation\n","            best_model = load_model(checkpoint_filepath)\n","\n","            # Evaluate client model\n","            y_pred = (best_model.predict(x_test) > 0.5).astype(\"int32\")\n","            accuracy, precision, recall, f1, sensitivity, specificity, kappa = compute_metrics(y_test, y_pred)\n","            metrics_list.append({\n","                'Client': client,\n","                'Accuracy': accuracy,\n","                'Precision': precision,\n","                'Recall': recall,\n","                'F1': f1,\n","                'Sensitivity': sensitivity,\n","                'Specificity': specificity,\n","                'Kappa': kappa\n","            })\n","\n","        global_weights = global_model.get_weights()\n","        layer_start_idx = 0\n","        for layer in global_model.layers:\n","            layer_weights = []\n","            layer_biases = []\n","            num_params = len(layer.get_weights())\n","            for i in range(num_clients):\n","                layer_params = client_models[i].get_weights()[layer_start_idx:layer_start_idx + num_params]\n","                if num_params > 0:\n","                    layer_weights.append(layer_params[0])\n","                if num_params > 1:\n","                    layer_biases.append(layer_params[1])\n","\n","            if len(layer_weights) > 0:\n","                averaged_layer_weights = np.mean(layer_weights, axis=0)\n","                if averaged_layer_weights.shape == global_weights[layer_start_idx].shape:\n","                    global_weights[layer_start_idx] = averaged_layer_weights\n","                else:\n","                    print(f\"Warning: Shape mismatch for weights at layer {layer.name}, expected {global_weights[layer_start_idx].shape} but got {averaged_layer_weights.shape}\")\n","            if len(layer_biases) > 0:\n","                averaged_layer_biases = np.mean(layer_biases, axis=0)\n","                if averaged_layer_biases.shape == global_weights[layer_start_idx + 1].shape:\n","                    global_weights[layer_start_idx + 1] = averaged_layer_biases\n","                else:\n","                    print(f\"Warning: Shape mismatch for biases at layer {layer.name}, expected {global_weights[layer_start_idx + 1].shape} but got {averaged_layer_biases.shape}\")\n","\n","            layer_start_idx += num_params\n","\n","        # Apply FedOpt (Adam) update to global weights\n","        t += 1\n","        for i in range(len(global_weights)):\n","            g_t = global_weights[i] - global_model.get_weights()[i]  # Gradient\n","            m[i] = beta1 * m[i] + (1 - beta1) * g_t\n","            v[i] = beta2 * v[i] + (1 - beta2) * (g_t ** 2)\n","            m_hat = m[i] / (1 - beta1 ** t)\n","            v_hat = v[i] / (1 - beta2 ** t)\n","            global_weights[i] -= global_optimizer.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n","\n","        global_model.set_weights(global_weights)\n","\n","    metrics_df = pd.DataFrame(metrics_list)\n","    return global_model, metrics_df\n","\n","# Example usage\n","# data_path = 'path/to/your/data.npz'\n","# global_model, metrics_df = federated_learning(data_path)\n","# print(metrics_df)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57664,"status":"ok","timestamp":1717508209119,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"},"user_tz":-360},"id":"ZVURUnmYNNNg","outputId":"7da379dc-5a7f-45d7-a95b-0e09b67443bf"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Train shape: (4640, 37, 29), Test shape: (1194, 37, 29)\n","Train shape: (4418, 37, 29), Test shape: (1416, 37, 29)\n","Train shape: (4838, 37, 29), Test shape: (996, 37, 29)\n","Train shape: (4640, 37, 29), Test shape: (1194, 37, 29)\n","Train shape: (4418, 37, 29), Test shape: (1416, 37, 29)\n","Train shape: (4838, 37, 29), Test shape: (996, 37, 29)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","26/29 [=========================>....] - ETA: 0s - loss: 1.7021 - accuracy: 0.4946"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["29/29 [==============================] - 9s 59ms/step - loss: 1.7015 - accuracy: 0.4954 - val_loss: 1.6963 - val_accuracy: 0.4849\n","Epoch 2/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.6913 - accuracy: 0.5038 - val_loss: 1.6861 - val_accuracy: 0.4849\n","Epoch 3/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.6811 - accuracy: 0.5038 - val_loss: 1.6760 - val_accuracy: 0.4849\n","Epoch 4/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.6710 - accuracy: 0.5038 - val_loss: 1.6660 - val_accuracy: 0.4849\n","Epoch 5/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.6610 - accuracy: 0.5038 - val_loss: 1.6561 - val_accuracy: 0.4849\n","Epoch 6/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.6511 - accuracy: 0.5038 - val_loss: 1.6463 - val_accuracy: 0.4849\n","Epoch 7/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.6412 - accuracy: 0.5038 - val_loss: 1.6366 - val_accuracy: 0.4849\n","Epoch 8/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.6315 - accuracy: 0.5038 - val_loss: 1.6269 - val_accuracy: 0.4849\n","Epoch 9/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.6218 - accuracy: 0.5038 - val_loss: 1.6174 - val_accuracy: 0.4849\n","Epoch 10/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.6122 - accuracy: 0.5038 - val_loss: 1.6080 - val_accuracy: 0.4849\n","Epoch 11/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.6028 - accuracy: 0.5038 - val_loss: 1.5986 - val_accuracy: 0.4784\n","Epoch 12/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.5934 - accuracy: 0.5038 - val_loss: 1.5893 - val_accuracy: 0.4849\n","Epoch 13/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.5842 - accuracy: 0.5043 - val_loss: 1.5801 - val_accuracy: 0.5011\n","Epoch 14/100\n","29/29 [==============================] - 1s 29ms/step - loss: 1.5749 - accuracy: 0.5057 - val_loss: 1.5709 - val_accuracy: 0.5065\n","Epoch 15/100\n","29/29 [==============================] - 1s 26ms/step - loss: 1.5658 - accuracy: 0.5032 - val_loss: 1.5618 - val_accuracy: 0.5162\n","Epoch 16/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.5569 - accuracy: 0.5000 - val_loss: 1.5527 - val_accuracy: 0.5075\n","Epoch 17/100\n","29/29 [==============================] - 1s 28ms/step - loss: 1.5479 - accuracy: 0.5078 - val_loss: 1.5438 - val_accuracy: 0.5453\n","Epoch 18/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.5391 - accuracy: 0.5102 - val_loss: 1.5350 - val_accuracy: 0.5377\n","Epoch 19/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.5304 - accuracy: 0.5119 - val_loss: 1.5262 - val_accuracy: 0.5420\n","Epoch 20/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.5217 - accuracy: 0.5121 - val_loss: 1.5175 - val_accuracy: 0.5269\n","Epoch 21/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.5131 - accuracy: 0.5140 - val_loss: 1.5088 - val_accuracy: 0.5248\n","Epoch 22/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.5047 - accuracy: 0.5162 - val_loss: 1.5003 - val_accuracy: 0.5119\n","Epoch 23/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.4961 - accuracy: 0.5170 - val_loss: 1.4919 - val_accuracy: 0.5226\n","Epoch 24/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.4877 - accuracy: 0.5175 - val_loss: 1.4835 - val_accuracy: 0.5108\n","Epoch 25/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.4794 - accuracy: 0.5218 - val_loss: 1.4752 - val_accuracy: 0.5108\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.4711 - accuracy: 0.5259 - val_loss: 1.4670 - val_accuracy: 0.5097\n","Epoch 27/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.4630 - accuracy: 0.5178 - val_loss: 1.4589 - val_accuracy: 0.5129\n","Epoch 28/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.4549 - accuracy: 0.5253 - val_loss: 1.4508 - val_accuracy: 0.5119\n","Epoch 29/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.4466 - accuracy: 0.5216 - val_loss: 1.4429 - val_accuracy: 0.5119\n","Epoch 30/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.4384 - accuracy: 0.5256 - val_loss: 1.4350 - val_accuracy: 0.5054\n","Epoch 31/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.4305 - accuracy: 0.5259 - val_loss: 1.4273 - val_accuracy: 0.5054\n","Epoch 32/100\n","29/29 [==============================] - 1s 17ms/step - loss: 1.4226 - accuracy: 0.5353 - val_loss: 1.4195 - val_accuracy: 0.5119\n","Epoch 33/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.4148 - accuracy: 0.5361 - val_loss: 1.4120 - val_accuracy: 0.5172\n","Epoch 34/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.4069 - accuracy: 0.5326 - val_loss: 1.4044 - val_accuracy: 0.5151\n","Epoch 35/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3994 - accuracy: 0.5348 - val_loss: 1.3971 - val_accuracy: 0.5162\n","Epoch 36/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3918 - accuracy: 0.5356 - val_loss: 1.3896 - val_accuracy: 0.5205\n","Epoch 37/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3841 - accuracy: 0.5393 - val_loss: 1.3823 - val_accuracy: 0.5226\n","Epoch 38/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3767 - accuracy: 0.5420 - val_loss: 1.3751 - val_accuracy: 0.5259\n","Epoch 39/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3692 - accuracy: 0.5423 - val_loss: 1.3679 - val_accuracy: 0.5366\n","Epoch 40/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3618 - accuracy: 0.5474 - val_loss: 1.3610 - val_accuracy: 0.5366\n","Epoch 41/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3543 - accuracy: 0.5445 - val_loss: 1.3538 - val_accuracy: 0.5323\n","Epoch 42/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3467 - accuracy: 0.5504 - val_loss: 1.3469 - val_accuracy: 0.5323\n","Epoch 43/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3396 - accuracy: 0.5477 - val_loss: 1.3400 - val_accuracy: 0.5409\n","Epoch 44/100\n","29/29 [==============================] - 2s 66ms/step - loss: 1.3324 - accuracy: 0.5509 - val_loss: 1.3331 - val_accuracy: 0.5485\n","Epoch 45/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.3254 - accuracy: 0.5496 - val_loss: 1.3264 - val_accuracy: 0.5409\n","Epoch 46/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.3180 - accuracy: 0.5544 - val_loss: 1.3202 - val_accuracy: 0.5388\n","Epoch 47/100\n","29/29 [==============================] - 1s 31ms/step - loss: 1.3110 - accuracy: 0.5482 - val_loss: 1.3135 - val_accuracy: 0.5614\n","Epoch 48/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.3037 - accuracy: 0.5598 - val_loss: 1.3073 - val_accuracy: 0.5366\n","Epoch 49/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2971 - accuracy: 0.5595 - val_loss: 1.3010 - val_accuracy: 0.5431\n","Epoch 50/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2904 - accuracy: 0.5531 - val_loss: 1.2942 - val_accuracy: 0.5517\n","Epoch 51/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2829 - accuracy: 0.5574 - val_loss: 1.2889 - val_accuracy: 0.5345\n","Epoch 52/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2762 - accuracy: 0.5568 - val_loss: 1.2826 - val_accuracy: 0.5388\n","Epoch 53/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.2698 - accuracy: 0.5558 - val_loss: 1.2764 - val_accuracy: 0.5539\n","Epoch 54/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.2624 - accuracy: 0.5649 - val_loss: 1.2701 - val_accuracy: 0.5517\n","Epoch 55/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.2559 - accuracy: 0.5647 - val_loss: 1.2648 - val_accuracy: 0.5474\n","Epoch 56/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.2487 - accuracy: 0.5665 - val_loss: 1.2595 - val_accuracy: 0.5442\n","Epoch 57/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2422 - accuracy: 0.5663 - val_loss: 1.2526 - val_accuracy: 0.5582\n","Epoch 58/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2367 - accuracy: 0.5663 - val_loss: 1.2471 - val_accuracy: 0.5506\n","Epoch 59/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2305 - accuracy: 0.5638 - val_loss: 1.2422 - val_accuracy: 0.5517\n","Epoch 60/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2233 - accuracy: 0.5698 - val_loss: 1.2369 - val_accuracy: 0.5614\n","Epoch 61/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2164 - accuracy: 0.5735 - val_loss: 1.2313 - val_accuracy: 0.5431\n","Epoch 62/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.2088 - accuracy: 0.5805 - val_loss: 1.2262 - val_accuracy: 0.5614\n","Epoch 63/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2034 - accuracy: 0.5727 - val_loss: 1.2220 - val_accuracy: 0.5442\n","Epoch 64/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.1966 - accuracy: 0.5727 - val_loss: 1.2156 - val_accuracy: 0.5690\n","Epoch 65/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.1911 - accuracy: 0.5768 - val_loss: 1.2106 - val_accuracy: 0.5733\n","Epoch 66/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1839 - accuracy: 0.5800 - val_loss: 1.2059 - val_accuracy: 0.5668\n","Epoch 67/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1771 - accuracy: 0.5762 - val_loss: 1.2026 - val_accuracy: 0.5625\n","Epoch 68/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1715 - accuracy: 0.5854 - val_loss: 1.1973 - val_accuracy: 0.5668\n","Epoch 69/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1675 - accuracy: 0.5768 - val_loss: 1.1930 - val_accuracy: 0.5625\n","Epoch 70/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1603 - accuracy: 0.5803 - val_loss: 1.1875 - val_accuracy: 0.5593\n","Epoch 71/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1528 - accuracy: 0.5873 - val_loss: 1.1831 - val_accuracy: 0.5614\n","Epoch 72/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1488 - accuracy: 0.5822 - val_loss: 1.1785 - val_accuracy: 0.5711\n","Epoch 73/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1426 - accuracy: 0.5886 - val_loss: 1.1769 - val_accuracy: 0.5647\n","Epoch 74/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.1374 - accuracy: 0.5846 - val_loss: 1.1711 - val_accuracy: 0.5636\n","Epoch 75/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.1297 - accuracy: 0.5862 - val_loss: 1.1661 - val_accuracy: 0.5711\n","Epoch 76/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.1255 - accuracy: 0.5911 - val_loss: 1.1641 - val_accuracy: 0.5550\n","Epoch 77/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.1200 - accuracy: 0.5973 - val_loss: 1.1587 - val_accuracy: 0.5668\n","Epoch 78/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.1144 - accuracy: 0.5921 - val_loss: 1.1562 - val_accuracy: 0.5550\n","Epoch 79/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.1084 - accuracy: 0.5916 - val_loss: 1.1521 - val_accuracy: 0.5636\n","Epoch 80/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.1034 - accuracy: 0.5951 - val_loss: 1.1470 - val_accuracy: 0.5700\n","Epoch 81/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.0972 - accuracy: 0.5935 - val_loss: 1.1436 - val_accuracy: 0.5722\n","Epoch 82/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0915 - accuracy: 0.5924 - val_loss: 1.1409 - val_accuracy: 0.5603\n","Epoch 83/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0863 - accuracy: 0.5981 - val_loss: 1.1376 - val_accuracy: 0.5711\n","Epoch 84/100\n","29/29 [==============================] - 2s 69ms/step - loss: 1.0827 - accuracy: 0.6018 - val_loss: 1.1337 - val_accuracy: 0.5787\n","Epoch 85/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0762 - accuracy: 0.5978 - val_loss: 1.1355 - val_accuracy: 0.5614\n","Epoch 86/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0698 - accuracy: 0.6051 - val_loss: 1.1329 - val_accuracy: 0.5420\n","Epoch 87/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0681 - accuracy: 0.6126 - val_loss: 1.1251 - val_accuracy: 0.5754\n","Epoch 88/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0611 - accuracy: 0.6018 - val_loss: 1.1224 - val_accuracy: 0.5711\n","Epoch 89/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0569 - accuracy: 0.6037 - val_loss: 1.1190 - val_accuracy: 0.5690\n","Epoch 90/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0510 - accuracy: 0.6129 - val_loss: 1.1155 - val_accuracy: 0.5657\n","Epoch 91/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0478 - accuracy: 0.6094 - val_loss: 1.1143 - val_accuracy: 0.5593\n","Epoch 92/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0408 - accuracy: 0.6123 - val_loss: 1.1120 - val_accuracy: 0.5560\n","Epoch 93/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0372 - accuracy: 0.6067 - val_loss: 1.1093 - val_accuracy: 0.5668\n","Epoch 94/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0321 - accuracy: 0.6183 - val_loss: 1.1062 - val_accuracy: 0.5657\n","Epoch 95/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0280 - accuracy: 0.6094 - val_loss: 1.1044 - val_accuracy: 0.5571\n","Epoch 96/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.0215 - accuracy: 0.6148 - val_loss: 1.1009 - val_accuracy: 0.5668\n","Epoch 97/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0185 - accuracy: 0.6086 - val_loss: 1.1020 - val_accuracy: 0.5679\n","Epoch 98/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.0151 - accuracy: 0.6123 - val_loss: 1.0986 - val_accuracy: 0.5625\n","Epoch 99/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0102 - accuracy: 0.6094 - val_loss: 1.0956 - val_accuracy: 0.5593\n","Epoch 100/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.0028 - accuracy: 0.6234 - val_loss: 1.0927 - val_accuracy: 0.5625\n","{'loss': [1.7015358209609985, 1.6912572383880615, 1.6810756921768188, 1.6709868907928467, 1.6609736680984497, 1.6510710716247559, 1.6412361860275269, 1.631498098373413, 1.6218340396881104, 1.6122498512268066, 1.6027576923370361, 1.5934218168258667, 1.5841511487960815, 1.574920892715454, 1.5658297538757324, 1.5568596124649048, 1.5478734970092773, 1.5390686988830566, 1.5303895473480225, 1.5217021703720093, 1.513108253479004, 1.5046560764312744, 1.4961026906967163, 1.4877125024795532, 1.479383111000061, 1.4711077213287354, 1.4629507064819336, 1.4549360275268555, 1.4466124773025513, 1.438441514968872, 1.4305475950241089, 1.42263925075531, 1.4147872924804688, 1.4069154262542725, 1.3993573188781738, 1.391766905784607, 1.3840664625167847, 1.376694917678833, 1.3692357540130615, 1.3617676496505737, 1.3542598485946655, 1.3467059135437012, 1.339604377746582, 1.332385540008545, 1.3254179954528809, 1.3180341720581055, 1.3109962940216064, 1.30365788936615, 1.297149658203125, 1.2903902530670166, 1.2828880548477173, 1.2761746644973755, 1.2697653770446777, 1.262429118156433, 1.2559317350387573, 1.2486602067947388, 1.2421939373016357, 1.2367364168167114, 1.2304513454437256, 1.2232511043548584, 1.2163738012313843, 1.2088122367858887, 1.2034173011779785, 1.1965527534484863, 1.1911035776138306, 1.1839001178741455, 1.177138090133667, 1.1715147495269775, 1.1675254106521606, 1.1602675914764404, 1.1527694463729858, 1.1487983465194702, 1.1426228284835815, 1.1374341249465942, 1.1296688318252563, 1.1254706382751465, 1.1199800968170166, 1.1143923997879028, 1.1084387302398682, 1.103442907333374, 1.0971792936325073, 1.091545820236206, 1.0863256454467773, 1.0826747417449951, 1.076175332069397, 1.0698161125183105, 1.0681147575378418, 1.0610551834106445, 1.0568684339523315, 1.051049828529358, 1.0477607250213623, 1.0408284664154053, 1.0372401475906372, 1.0321422815322876, 1.0280399322509766, 1.0215474367141724, 1.0184528827667236, 1.015146017074585, 1.0101978778839111, 1.0028351545333862], 'accuracy': [0.4954202473163605, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5043103694915771, 0.5056573152542114, 0.5032327771186829, 0.5, 0.5078125, 0.5102370977401733, 0.5118534564971924, 0.5121228694915771, 0.514008641242981, 0.5161637663841248, 0.516972005367279, 0.5175107717514038, 0.521821141242981, 0.5258620977401733, 0.5177801847457886, 0.5253232717514038, 0.5215517282485962, 0.5255926847457886, 0.5258620977401733, 0.5352909564971924, 0.5360991358757019, 0.532597005367279, 0.5347521305084229, 0.5355603694915771, 0.5393319129943848, 0.5420258641242981, 0.5422952771186829, 0.5474137663841248, 0.5444504022598267, 0.5503771305084229, 0.5476831793785095, 0.5509159564971924, 0.5495689511299133, 0.5544180870056152, 0.548222005367279, 0.5598060488700867, 0.5595366358757019, 0.553071141242981, 0.5573814511299133, 0.5568426847457886, 0.5557650923728943, 0.5649245977401733, 0.5646551847457886, 0.5665409564971924, 0.5662715435028076, 0.5662715435028076, 0.563847005367279, 0.5697737336158752, 0.5735452771186829, 0.5805495977401733, 0.5727370977401733, 0.5727370977401733, 0.576777994632721, 0.5800107717514038, 0.5762392282485962, 0.5853987336158752, 0.576777994632721, 0.5802801847457886, 0.587284505367279, 0.5821659564971924, 0.5886314511299133, 0.584590494632721, 0.5862069129943848, 0.5910560488700867, 0.5972521305084229, 0.592133641242981, 0.5915948152542114, 0.595097005367279, 0.5934805870056152, 0.592402994632721, 0.5980603694915771, 0.6018319129943848, 0.5977909564971924, 0.6050646305084229, 0.6126077771186829, 0.6018319129943848, 0.6037176847457886, 0.6128771305084229, 0.609375, 0.6123383641242981, 0.6066810488700867, 0.6182650923728943, 0.609375, 0.6147629022598267, 0.6085668206214905, 0.6123383641242981, 0.609375, 0.623383641242981], 'val_loss': [1.6962800025939941, 1.686079978942871, 1.675981044769287, 1.6659820079803467, 1.6560848951339722, 1.6462771892547607, 1.636566162109375, 1.626941204071045, 1.6174153089523315, 1.6079617738723755, 1.5985809564590454, 1.5892730951309204, 1.5800750255584717, 1.5709173679351807, 1.5618146657943726, 1.5527368783950806, 1.543838620185852, 1.5349823236465454, 1.526243805885315, 1.5174542665481567, 1.5088351964950562, 1.500264048576355, 1.49185049533844, 1.4834942817687988, 1.4751965999603271, 1.4669899940490723, 1.4588651657104492, 1.4508211612701416, 1.442875623703003, 1.4350069761276245, 1.4272559881210327, 1.419481635093689, 1.4120081663131714, 1.404422640800476, 1.397096872329712, 1.3896456956863403, 1.3823332786560059, 1.3750683069229126, 1.3679298162460327, 1.3609620332717896, 1.3537704944610596, 1.346875548362732, 1.3399661779403687, 1.3331122398376465, 1.3264096975326538, 1.3201597929000854, 1.3134534358978271, 1.3073254823684692, 1.3010129928588867, 1.29422128200531, 1.2888810634613037, 1.282625436782837, 1.2764300107955933, 1.2701359987258911, 1.2647641897201538, 1.2594552040100098, 1.2525595426559448, 1.247055172920227, 1.2421784400939941, 1.2368500232696533, 1.2313451766967773, 1.2262035608291626, 1.221982717514038, 1.2155548334121704, 1.2106056213378906, 1.2058711051940918, 1.2025669813156128, 1.1972721815109253, 1.1930233240127563, 1.1875444650650024, 1.1831151247024536, 1.1785283088684082, 1.1768540143966675, 1.1711070537567139, 1.166064739227295, 1.1641322374343872, 1.1587179899215698, 1.1562498807907104, 1.1520525217056274, 1.1469526290893555, 1.143585205078125, 1.1409118175506592, 1.1376208066940308, 1.1337132453918457, 1.135506510734558, 1.1328625679016113, 1.1251378059387207, 1.1224100589752197, 1.1190001964569092, 1.1155377626419067, 1.11426842212677, 1.111968994140625, 1.1092541217803955, 1.1062148809432983, 1.1044312715530396, 1.1008731126785278, 1.1019597053527832, 1.0985814332962036, 1.0956103801727295, 1.0927355289459229], 'val_accuracy': [0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.4784482717514038, 0.48491379618644714, 0.5010775923728943, 0.506465494632721, 0.5161637663841248, 0.5075430870056152, 0.545258641242981, 0.537715494632721, 0.5420258641242981, 0.5269396305084229, 0.524784505367279, 0.5118534564971924, 0.5226293206214905, 0.5107758641242981, 0.5107758641242981, 0.5096982717514038, 0.5129310488700867, 0.5118534564971924, 0.5118534564971924, 0.5053879022598267, 0.5053879022598267, 0.5118534564971924, 0.517241358757019, 0.5150862336158752, 0.5161637663841248, 0.5204741358757019, 0.5226293206214905, 0.5258620977401733, 0.5366379022598267, 0.5366379022598267, 0.5323275923728943, 0.5323275923728943, 0.5409482717514038, 0.548491358757019, 0.5409482717514038, 0.5387930870056152, 0.5614224076271057, 0.5366379022598267, 0.5431034564971924, 0.5517241358757019, 0.5344827771186829, 0.5387930870056152, 0.5538793206214905, 0.5517241358757019, 0.5474137663841248, 0.5441810488700867, 0.5581896305084229, 0.5506465435028076, 0.5517241358757019, 0.5614224076271057, 0.5431034564971924, 0.5614224076271057, 0.5441810488700867, 0.568965494632721, 0.5732758641242981, 0.5668103694915771, 0.5625, 0.5668103694915771, 0.5625, 0.5592672228813171, 0.5614224076271057, 0.5711206793785095, 0.5646551847457886, 0.5635775923728943, 0.5711206793785095, 0.5549569129943848, 0.5668103694915771, 0.5549569129943848, 0.5635775923728943, 0.5700430870056152, 0.5721982717514038, 0.5603448152542114, 0.5711206793785095, 0.5786637663841248, 0.5614224076271057, 0.5420258641242981, 0.5754310488700867, 0.5711206793785095, 0.568965494632721, 0.5657327771186829, 0.5592672228813171, 0.556034505367279, 0.5668103694915771, 0.5657327771186829, 0.5571120977401733, 0.5668103694915771, 0.5678879022598267, 0.5625, 0.5592672228813171, 0.5625]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","25/28 [=========================>....] - ETA: 0s - loss: 1.7022 - accuracy: 0.5050"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["28/28 [==============================] - 8s 74ms/step - loss: 1.7018 - accuracy: 0.5011 - val_loss: 1.6966 - val_accuracy: 0.4955\n","Epoch 2/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.6918 - accuracy: 0.5011 - val_loss: 1.6868 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.6819 - accuracy: 0.5011 - val_loss: 1.6770 - val_accuracy: 0.4955\n","Epoch 4/100\n","28/28 [==============================] - 1s 19ms/step - loss: 1.6722 - accuracy: 0.5011 - val_loss: 1.6674 - val_accuracy: 0.4955\n","Epoch 5/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.6624 - accuracy: 0.5011 - val_loss: 1.6578 - val_accuracy: 0.4955\n","Epoch 6/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.6528 - accuracy: 0.5011 - val_loss: 1.6483 - val_accuracy: 0.4955\n","Epoch 7/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.6432 - accuracy: 0.5011 - val_loss: 1.6389 - val_accuracy: 0.4955\n","Epoch 8/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.6337 - accuracy: 0.5011 - val_loss: 1.6296 - val_accuracy: 0.4955\n","Epoch 9/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.6242 - accuracy: 0.5011 - val_loss: 1.6204 - val_accuracy: 0.4955\n","Epoch 10/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.6149 - accuracy: 0.5011 - val_loss: 1.6113 - val_accuracy: 0.4955\n","Epoch 11/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.6056 - accuracy: 0.4997 - val_loss: 1.6022 - val_accuracy: 0.5339\n","Epoch 12/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.5965 - accuracy: 0.5014 - val_loss: 1.5932 - val_accuracy: 0.5362\n","Epoch 13/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.5874 - accuracy: 0.5125 - val_loss: 1.5843 - val_accuracy: 0.5373\n","Epoch 14/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.5784 - accuracy: 0.5158 - val_loss: 1.5755 - val_accuracy: 0.5351\n","Epoch 15/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.5696 - accuracy: 0.5153 - val_loss: 1.5667 - val_accuracy: 0.5441\n","Epoch 16/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.5609 - accuracy: 0.5167 - val_loss: 1.5580 - val_accuracy: 0.5441\n","Epoch 17/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.5522 - accuracy: 0.5150 - val_loss: 1.5494 - val_accuracy: 0.5430\n","Epoch 18/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.5435 - accuracy: 0.5204 - val_loss: 1.5410 - val_accuracy: 0.5419\n","Epoch 19/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.5351 - accuracy: 0.5198 - val_loss: 1.5326 - val_accuracy: 0.5385\n","Epoch 20/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.5267 - accuracy: 0.5190 - val_loss: 1.5244 - val_accuracy: 0.5396\n","Epoch 21/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.5184 - accuracy: 0.5167 - val_loss: 1.5163 - val_accuracy: 0.5260\n","Epoch 22/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.5101 - accuracy: 0.5201 - val_loss: 1.5082 - val_accuracy: 0.5385\n","Epoch 23/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.5019 - accuracy: 0.5207 - val_loss: 1.5003 - val_accuracy: 0.5305\n","Epoch 24/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.4938 - accuracy: 0.5175 - val_loss: 1.4924 - val_accuracy: 0.5249\n","Epoch 25/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.4856 - accuracy: 0.5257 - val_loss: 1.4847 - val_accuracy: 0.5271\n","Epoch 26/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.4776 - accuracy: 0.5280 - val_loss: 1.4771 - val_accuracy: 0.5271\n","Epoch 27/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.4695 - accuracy: 0.5283 - val_loss: 1.4694 - val_accuracy: 0.5317\n","Epoch 28/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.4618 - accuracy: 0.5283 - val_loss: 1.4621 - val_accuracy: 0.5283\n","Epoch 29/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.4536 - accuracy: 0.5294 - val_loss: 1.4545 - val_accuracy: 0.5317\n","Epoch 30/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.4458 - accuracy: 0.5283 - val_loss: 1.4472 - val_accuracy: 0.5385\n","Epoch 31/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.4381 - accuracy: 0.5351 - val_loss: 1.4399 - val_accuracy: 0.5339\n","Epoch 32/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.4301 - accuracy: 0.5345 - val_loss: 1.4331 - val_accuracy: 0.5419\n","Epoch 33/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.4223 - accuracy: 0.5371 - val_loss: 1.4260 - val_accuracy: 0.5419\n","Epoch 34/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.4149 - accuracy: 0.5357 - val_loss: 1.4184 - val_accuracy: 0.5430\n","Epoch 35/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.4072 - accuracy: 0.5342 - val_loss: 1.4125 - val_accuracy: 0.5396\n","Epoch 36/100\n","28/28 [==============================] - 2s 69ms/step - loss: 1.3998 - accuracy: 0.5441 - val_loss: 1.4046 - val_accuracy: 0.5452\n","Epoch 37/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.3920 - accuracy: 0.5433 - val_loss: 1.3983 - val_accuracy: 0.5396\n","Epoch 38/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3851 - accuracy: 0.5410 - val_loss: 1.3914 - val_accuracy: 0.5373\n","Epoch 39/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3777 - accuracy: 0.5512 - val_loss: 1.3847 - val_accuracy: 0.5430\n","Epoch 40/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3704 - accuracy: 0.5458 - val_loss: 1.3775 - val_accuracy: 0.5452\n","Epoch 41/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.3630 - accuracy: 0.5461 - val_loss: 1.3708 - val_accuracy: 0.5430\n","Epoch 42/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.3557 - accuracy: 0.5498 - val_loss: 1.3643 - val_accuracy: 0.5498\n","Epoch 43/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3490 - accuracy: 0.5552 - val_loss: 1.3574 - val_accuracy: 0.5452\n","Epoch 44/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3419 - accuracy: 0.5521 - val_loss: 1.3505 - val_accuracy: 0.5475\n","Epoch 45/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3350 - accuracy: 0.5515 - val_loss: 1.3448 - val_accuracy: 0.5486\n","Epoch 46/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3282 - accuracy: 0.5569 - val_loss: 1.3367 - val_accuracy: 0.5486\n","Epoch 47/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3207 - accuracy: 0.5521 - val_loss: 1.3314 - val_accuracy: 0.5498\n","Epoch 48/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.3138 - accuracy: 0.5589 - val_loss: 1.3240 - val_accuracy: 0.5452\n","Epoch 49/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3070 - accuracy: 0.5560 - val_loss: 1.3171 - val_accuracy: 0.5464\n","Epoch 50/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.3004 - accuracy: 0.5642 - val_loss: 1.3106 - val_accuracy: 0.5554\n","Epoch 51/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.2934 - accuracy: 0.5628 - val_loss: 1.3048 - val_accuracy: 0.5554\n","Epoch 52/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.2869 - accuracy: 0.5628 - val_loss: 1.2978 - val_accuracy: 0.5532\n","Epoch 53/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.2805 - accuracy: 0.5625 - val_loss: 1.2915 - val_accuracy: 0.5509\n","Epoch 54/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.2743 - accuracy: 0.5634 - val_loss: 1.2848 - val_accuracy: 0.5543\n","Epoch 55/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.2676 - accuracy: 0.5665 - val_loss: 1.2821 - val_accuracy: 0.5543\n","Epoch 56/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.2615 - accuracy: 0.5705 - val_loss: 1.2754 - val_accuracy: 0.5520\n","Epoch 57/100\n","28/28 [==============================] - 1s 32ms/step - loss: 1.2547 - accuracy: 0.5668 - val_loss: 1.2665 - val_accuracy: 0.5600\n","Epoch 58/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.2486 - accuracy: 0.5679 - val_loss: 1.2621 - val_accuracy: 0.5588\n","Epoch 59/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.2425 - accuracy: 0.5733 - val_loss: 1.2579 - val_accuracy: 0.5543\n","Epoch 60/100\n","28/28 [==============================] - 1s 27ms/step - loss: 1.2357 - accuracy: 0.5739 - val_loss: 1.2510 - val_accuracy: 0.5633\n","Epoch 61/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.2298 - accuracy: 0.5747 - val_loss: 1.2453 - val_accuracy: 0.5577\n","Epoch 62/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2243 - accuracy: 0.5741 - val_loss: 1.2358 - val_accuracy: 0.5600\n","Epoch 63/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2182 - accuracy: 0.5727 - val_loss: 1.2353 - val_accuracy: 0.5611\n","Epoch 64/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2121 - accuracy: 0.5741 - val_loss: 1.2303 - val_accuracy: 0.5600\n","Epoch 65/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2061 - accuracy: 0.5722 - val_loss: 1.2235 - val_accuracy: 0.5588\n","Epoch 66/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1997 - accuracy: 0.5778 - val_loss: 1.2212 - val_accuracy: 0.5577\n","Epoch 67/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1933 - accuracy: 0.5775 - val_loss: 1.2107 - val_accuracy: 0.5588\n","Epoch 68/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1879 - accuracy: 0.5821 - val_loss: 1.2105 - val_accuracy: 0.5554\n","Epoch 69/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1823 - accuracy: 0.5789 - val_loss: 1.2025 - val_accuracy: 0.5577\n","Epoch 70/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.1765 - accuracy: 0.5756 - val_loss: 1.1944 - val_accuracy: 0.5566\n","Epoch 71/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1696 - accuracy: 0.5821 - val_loss: 1.1910 - val_accuracy: 0.5622\n","Epoch 72/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1649 - accuracy: 0.5852 - val_loss: 1.1881 - val_accuracy: 0.5622\n","Epoch 73/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1593 - accuracy: 0.5823 - val_loss: 1.1869 - val_accuracy: 0.5577\n","Epoch 74/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.1542 - accuracy: 0.5872 - val_loss: 1.1745 - val_accuracy: 0.5645\n","Epoch 75/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.1480 - accuracy: 0.5835 - val_loss: 1.1698 - val_accuracy: 0.5667\n","Epoch 76/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1431 - accuracy: 0.5843 - val_loss: 1.1624 - val_accuracy: 0.5588\n","Epoch 77/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1384 - accuracy: 0.5809 - val_loss: 1.1643 - val_accuracy: 0.5667\n","Epoch 78/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1311 - accuracy: 0.5897 - val_loss: 1.1622 - val_accuracy: 0.5600\n","Epoch 79/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.1244 - accuracy: 0.5928 - val_loss: 1.1546 - val_accuracy: 0.5600\n","Epoch 80/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.1200 - accuracy: 0.5886 - val_loss: 1.1487 - val_accuracy: 0.5667\n","Epoch 81/100\n","28/28 [==============================] - 1s 23ms/step - loss: 1.1154 - accuracy: 0.5835 - val_loss: 1.1488 - val_accuracy: 0.5622\n","Epoch 82/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.1093 - accuracy: 0.5880 - val_loss: 1.1390 - val_accuracy: 0.5633\n","Epoch 83/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.1043 - accuracy: 0.5934 - val_loss: 1.1411 - val_accuracy: 0.5622\n","Epoch 84/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.0987 - accuracy: 0.5891 - val_loss: 1.1426 - val_accuracy: 0.5611\n","Epoch 85/100\n","28/28 [==============================] - 1s 34ms/step - loss: 1.0938 - accuracy: 0.5903 - val_loss: 1.1303 - val_accuracy: 0.5690\n","Epoch 86/100\n","28/28 [==============================] - 1s 17ms/step - loss: 1.0888 - accuracy: 0.5908 - val_loss: 1.1330 - val_accuracy: 0.5645\n","Epoch 87/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0835 - accuracy: 0.5937 - val_loss: 1.1217 - val_accuracy: 0.5656\n","Epoch 88/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0788 - accuracy: 0.5979 - val_loss: 1.1219 - val_accuracy: 0.5633\n","Epoch 89/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0762 - accuracy: 0.5917 - val_loss: 1.1242 - val_accuracy: 0.5588\n","Epoch 90/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0690 - accuracy: 0.5996 - val_loss: 1.1137 - val_accuracy: 0.5667\n","Epoch 91/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0663 - accuracy: 0.5948 - val_loss: 1.1049 - val_accuracy: 0.5690\n","Epoch 92/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0587 - accuracy: 0.6047 - val_loss: 1.1059 - val_accuracy: 0.5656\n","Epoch 93/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0542 - accuracy: 0.6005 - val_loss: 1.1026 - val_accuracy: 0.5645\n","Epoch 94/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0503 - accuracy: 0.6010 - val_loss: 1.1059 - val_accuracy: 0.5588\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0470 - accuracy: 0.5990 - val_loss: 1.1040 - val_accuracy: 0.5566\n","Epoch 96/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0413 - accuracy: 0.6027 - val_loss: 1.0987 - val_accuracy: 0.5622\n","Epoch 97/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.0367 - accuracy: 0.6061 - val_loss: 1.0882 - val_accuracy: 0.5724\n","Epoch 98/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.0294 - accuracy: 0.6044 - val_loss: 1.0842 - val_accuracy: 0.5781\n","Epoch 99/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0279 - accuracy: 0.6038 - val_loss: 1.0854 - val_accuracy: 0.5679\n","Epoch 100/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0203 - accuracy: 0.6058 - val_loss: 1.0770 - val_accuracy: 0.5554\n","{'loss': [1.701772928237915, 1.6918045282363892, 1.6819463968276978, 1.6721667051315308, 1.662428379058838, 1.6527841091156006, 1.6432112455368042, 1.6336872577667236, 1.6242496967315674, 1.6148964166641235, 1.6056157350540161, 1.5965077877044678, 1.5873782634735107, 1.5784200429916382, 1.569583535194397, 1.5608597993850708, 1.5522184371948242, 1.543489933013916, 1.5351142883300781, 1.5267391204833984, 1.518356442451477, 1.5101386308670044, 1.5018714666366577, 1.4937868118286133, 1.4856288433074951, 1.4775880575180054, 1.4695185422897339, 1.4618301391601562, 1.4536494016647339, 1.4458304643630981, 1.4380943775177002, 1.4301115274429321, 1.4222654104232788, 1.4149112701416016, 1.4072186946868896, 1.3997631072998047, 1.391953945159912, 1.3850665092468262, 1.377663254737854, 1.370448350906372, 1.3630093336105347, 1.3557387590408325, 1.3490405082702637, 1.3418571949005127, 1.3350063562393188, 1.3281511068344116, 1.3207286596298218, 1.3137716054916382, 1.3070436716079712, 1.300447702407837, 1.2933690547943115, 1.2868547439575195, 1.2805039882659912, 1.2743123769760132, 1.267645239830017, 1.2615360021591187, 1.254673719406128, 1.2485522031784058, 1.2424901723861694, 1.2357313632965088, 1.2298251390457153, 1.2243434190750122, 1.2182313203811646, 1.2121031284332275, 1.2061234712600708, 1.1997483968734741, 1.193267583847046, 1.187883973121643, 1.1823198795318604, 1.1765029430389404, 1.1695644855499268, 1.164882779121399, 1.159279704093933, 1.154213786125183, 1.1480262279510498, 1.1431273221969604, 1.1384228467941284, 1.1311137676239014, 1.1243559122085571, 1.120018482208252, 1.1154249906539917, 1.1093038320541382, 1.1042982339859009, 1.0987352132797241, 1.093758225440979, 1.0888221263885498, 1.083544135093689, 1.0787845849990845, 1.0761945247650146, 1.0689693689346313, 1.0663416385650635, 1.0586624145507812, 1.0542343854904175, 1.0503324270248413, 1.0470404624938965, 1.0413098335266113, 1.0366860628128052, 1.0294134616851807, 1.027908205986023, 1.0202828645706177], 'accuracy': [0.5011318325996399, 0.5011318325996399, 0.5011318325996399, 0.5011318325996399, 0.5011318325996399, 0.5011318325996399, 0.5011318325996399, 0.5011318325996399, 0.5011318325996399, 0.5011318325996399, 0.49971702694892883, 0.5014148354530334, 0.5124504566192627, 0.5158460736274719, 0.5152801275253296, 0.516694962978363, 0.5149971842765808, 0.520373523235321, 0.5198075771331787, 0.5189586877822876, 0.516694962978363, 0.5200905203819275, 0.5206564664840698, 0.5175438523292542, 0.5257498621940613, 0.5280135869979858, 0.5282965302467346, 0.5282965302467346, 0.5294284224510193, 0.5282965302467346, 0.5350877046585083, 0.534521758556366, 0.5370684862136841, 0.5356536507606506, 0.5342388153076172, 0.5441426038742065, 0.5432937145233154, 0.5410299897193909, 0.5512167811393738, 0.5458403825759888, 0.5461233854293823, 0.5498019456863403, 0.5551782846450806, 0.5520656704902649, 0.5514997243881226, 0.5568760633468628, 0.5520656704902649, 0.5588568449020386, 0.5560271739959717, 0.5642331838607788, 0.5628183484077454, 0.5628183484077454, 0.5625353455543518, 0.5633842945098877, 0.5664969086647034, 0.5704584121704102, 0.5667798519134521, 0.5679117441177368, 0.573288083076477, 0.5738539695739746, 0.5747028589248657, 0.5741369724273682, 0.5727221369743347, 0.5741369724273682, 0.5721561908721924, 0.5778155326843262, 0.5775325298309326, 0.5820599794387817, 0.5789473652839661, 0.5755518078804016, 0.5820599794387817, 0.5851725935935974, 0.5823429822921753, 0.5871533751487732, 0.5834748148918152, 0.5843237042427063, 0.5809281468391418, 0.5897000432014465, 0.5928126573562622, 0.5885682106018066, 0.5834748148918152, 0.5880022644996643, 0.5933786034584045, 0.5891340970993042, 0.5902659893035889, 0.5908319354057312, 0.5936615467071533, 0.5979060530662537, 0.5916808247566223, 0.5996038317680359, 0.594793438911438, 0.6046972274780273, 0.600452721118927, 0.6010186672210693, 0.5990379452705383, 0.6027164459228516, 0.6061120629310608, 0.6044142842292786, 0.6038483381271362, 0.6058290600776672], 'val_loss': [1.6966300010681152, 1.6867766380310059, 1.6770182847976685, 1.667352318763733, 1.65778386592865, 1.648298740386963, 1.6389142274856567, 1.6296159029006958, 1.6203961372375488, 1.6112511157989502, 1.6021931171417236, 1.5932047367095947, 1.584269404411316, 1.5754612684249878, 1.5666940212249756, 1.5580087900161743, 1.5494351387023926, 1.5409971475601196, 1.5326441526412964, 1.5244033336639404, 1.5162534713745117, 1.508217692375183, 1.5002782344818115, 1.4924370050430298, 1.4846889972686768, 1.4771499633789062, 1.4694232940673828, 1.4621235132217407, 1.454541563987732, 1.4472383260726929, 1.4399082660675049, 1.4330646991729736, 1.4260499477386475, 1.4184308052062988, 1.4124783277511597, 1.4045741558074951, 1.3982820510864258, 1.3914451599121094, 1.3846862316131592, 1.3774579763412476, 1.3708100318908691, 1.3642747402191162, 1.357407808303833, 1.350494384765625, 1.3447622060775757, 1.3367403745651245, 1.3314040899276733, 1.3240269422531128, 1.3171418905258179, 1.3105651140213013, 1.3047778606414795, 1.297773838043213, 1.2915003299713135, 1.2847709655761719, 1.2820580005645752, 1.275436282157898, 1.2665022611618042, 1.2620526552200317, 1.2578632831573486, 1.2510125637054443, 1.2452561855316162, 1.2358269691467285, 1.2352851629257202, 1.2302955389022827, 1.223480224609375, 1.2211668491363525, 1.2106717824935913, 1.210510015487671, 1.202510118484497, 1.194353699684143, 1.1909617185592651, 1.1881200075149536, 1.1868720054626465, 1.1744815111160278, 1.1697571277618408, 1.162430763244629, 1.164346694946289, 1.1621803045272827, 1.1545878648757935, 1.1486726999282837, 1.148772120475769, 1.139024019241333, 1.141116738319397, 1.142577052116394, 1.13032066822052, 1.132979393005371, 1.1216611862182617, 1.1218502521514893, 1.1241792440414429, 1.1136661767959595, 1.1049002408981323, 1.1059125661849976, 1.1026371717453003, 1.1059141159057617, 1.103975772857666, 1.0987231731414795, 1.088232398033142, 1.0841960906982422, 1.085364818572998, 1.0769864320755005], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.5339366793632507, 0.5361990928649902, 0.5373303294181824, 0.5350678563117981, 0.5441176295280457, 0.5441176295280457, 0.5429864525794983, 0.5418552160263062, 0.5384615659713745, 0.5395927429199219, 0.5260180830955505, 0.5384615659713745, 0.5305429697036743, 0.5248869061470032, 0.5271493196487427, 0.5271493196487427, 0.5316742062568665, 0.5282805562019348, 0.5316742062568665, 0.5384615659713745, 0.5339366793632507, 0.5418552160263062, 0.5418552160263062, 0.5429864525794983, 0.5395927429199219, 0.5452488660812378, 0.5395927429199219, 0.5373303294181824, 0.5429864525794983, 0.5452488660812378, 0.5429864525794983, 0.5497737526893616, 0.5452488660812378, 0.5475113391876221, 0.5486425161361694, 0.5486425161361694, 0.5497737526893616, 0.5452488660812378, 0.5463801026344299, 0.5554298758506775, 0.5554298758506775, 0.5531674027442932, 0.5509049892425537, 0.5542986392974854, 0.5542986392974854, 0.5520362257957458, 0.5599547624588013, 0.5588235259056091, 0.5542986392974854, 0.5633484125137329, 0.557692289352417, 0.5599547624588013, 0.5610859990119934, 0.5599547624588013, 0.5588235259056091, 0.557692289352417, 0.5588235259056091, 0.5554298758506775, 0.557692289352417, 0.5565611124038696, 0.5622171759605408, 0.5622171759605408, 0.557692289352417, 0.564479649066925, 0.5667420625686646, 0.5588235259056091, 0.5667420625686646, 0.5599547624588013, 0.5599547624588013, 0.5667420625686646, 0.5622171759605408, 0.5633484125137329, 0.5622171759605408, 0.5610859990119934, 0.5690045356750488, 0.564479649066925, 0.5656108856201172, 0.5633484125137329, 0.5588235259056091, 0.5667420625686646, 0.5690045356750488, 0.5656108856201172, 0.564479649066925, 0.5588235259056091, 0.5565611124038696, 0.5622171759605408, 0.5723981857299805, 0.5780543088912964, 0.5678732991218567, 0.5554298758506775]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/31 [==========================>...] - ETA: 0s - loss: 1.7017 - accuracy: 0.5031"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["31/31 [==============================] - 9s 56ms/step - loss: 1.7013 - accuracy: 0.5036 - val_loss: 1.6956 - val_accuracy: 0.4855\n","Epoch 2/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.6902 - accuracy: 0.5036 - val_loss: 1.6847 - val_accuracy: 0.4855\n","Epoch 3/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.6792 - accuracy: 0.5039 - val_loss: 1.6739 - val_accuracy: 0.5300\n","Epoch 4/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.6683 - accuracy: 0.5111 - val_loss: 1.6633 - val_accuracy: 0.5207\n","Epoch 5/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.6574 - accuracy: 0.5191 - val_loss: 1.6527 - val_accuracy: 0.5269\n","Epoch 6/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.6467 - accuracy: 0.5168 - val_loss: 1.6423 - val_accuracy: 0.5258\n","Epoch 7/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.6359 - accuracy: 0.5202 - val_loss: 1.6320 - val_accuracy: 0.5227\n","Epoch 8/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.6253 - accuracy: 0.5194 - val_loss: 1.6219 - val_accuracy: 0.5238\n","Epoch 9/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.6147 - accuracy: 0.5240 - val_loss: 1.6118 - val_accuracy: 0.5248\n","Epoch 10/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.6044 - accuracy: 0.5276 - val_loss: 1.6019 - val_accuracy: 0.5269\n","Epoch 11/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.5942 - accuracy: 0.5289 - val_loss: 1.5920 - val_accuracy: 0.5258\n","Epoch 12/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.5842 - accuracy: 0.5271 - val_loss: 1.5824 - val_accuracy: 0.5248\n","Epoch 13/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.5744 - accuracy: 0.5292 - val_loss: 1.5728 - val_accuracy: 0.5207\n","Epoch 14/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.5648 - accuracy: 0.5297 - val_loss: 1.5634 - val_accuracy: 0.5238\n","Epoch 15/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.5552 - accuracy: 0.5258 - val_loss: 1.5541 - val_accuracy: 0.5227\n","Epoch 16/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.5458 - accuracy: 0.5300 - val_loss: 1.5449 - val_accuracy: 0.5227\n","Epoch 17/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.5365 - accuracy: 0.5300 - val_loss: 1.5358 - val_accuracy: 0.5269\n","Epoch 18/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.5273 - accuracy: 0.5274 - val_loss: 1.5270 - val_accuracy: 0.5258\n","Epoch 19/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.5181 - accuracy: 0.5333 - val_loss: 1.5182 - val_accuracy: 0.5258\n","Epoch 20/100\n","31/31 [==============================] - 1s 25ms/step - loss: 1.5090 - accuracy: 0.5336 - val_loss: 1.5096 - val_accuracy: 0.5310\n","Epoch 21/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.5002 - accuracy: 0.5367 - val_loss: 1.5010 - val_accuracy: 0.5300\n","Epoch 22/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4912 - accuracy: 0.5349 - val_loss: 1.4926 - val_accuracy: 0.5258\n","Epoch 23/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4825 - accuracy: 0.5344 - val_loss: 1.4843 - val_accuracy: 0.5248\n","Epoch 24/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4739 - accuracy: 0.5398 - val_loss: 1.4760 - val_accuracy: 0.5227\n","Epoch 25/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.4653 - accuracy: 0.5362 - val_loss: 1.4679 - val_accuracy: 0.5196\n","Epoch 26/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4567 - accuracy: 0.5460 - val_loss: 1.4596 - val_accuracy: 0.5217\n","Epoch 27/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.4484 - accuracy: 0.5421 - val_loss: 1.4519 - val_accuracy: 0.5248\n","Epoch 28/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.4399 - accuracy: 0.5450 - val_loss: 1.4438 - val_accuracy: 0.5227\n","Epoch 29/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.4315 - accuracy: 0.5463 - val_loss: 1.4358 - val_accuracy: 0.5238\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4232 - accuracy: 0.5452 - val_loss: 1.4280 - val_accuracy: 0.5207\n","Epoch 31/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4150 - accuracy: 0.5455 - val_loss: 1.4196 - val_accuracy: 0.5186\n","Epoch 32/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.4068 - accuracy: 0.5377 - val_loss: 1.4121 - val_accuracy: 0.5227\n","Epoch 33/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.3988 - accuracy: 0.5388 - val_loss: 1.4045 - val_accuracy: 0.5176\n","Epoch 34/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.3906 - accuracy: 0.5475 - val_loss: 1.3971 - val_accuracy: 0.5196\n","Epoch 35/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.3827 - accuracy: 0.5491 - val_loss: 1.3895 - val_accuracy: 0.5227\n","Epoch 36/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.3747 - accuracy: 0.5496 - val_loss: 1.3825 - val_accuracy: 0.5238\n","Epoch 37/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.3670 - accuracy: 0.5473 - val_loss: 1.3750 - val_accuracy: 0.5217\n","Epoch 38/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.3592 - accuracy: 0.5491 - val_loss: 1.3671 - val_accuracy: 0.5217\n","Epoch 39/100\n","31/31 [==============================] - 1s 24ms/step - loss: 1.3516 - accuracy: 0.5540 - val_loss: 1.3604 - val_accuracy: 0.5227\n","Epoch 40/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.3440 - accuracy: 0.5522 - val_loss: 1.3538 - val_accuracy: 0.5238\n","Epoch 41/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.3362 - accuracy: 0.5579 - val_loss: 1.3461 - val_accuracy: 0.5269\n","Epoch 42/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.3290 - accuracy: 0.5597 - val_loss: 1.3387 - val_accuracy: 0.5238\n","Epoch 43/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.3217 - accuracy: 0.5525 - val_loss: 1.3328 - val_accuracy: 0.5227\n","Epoch 44/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.3144 - accuracy: 0.5574 - val_loss: 1.3255 - val_accuracy: 0.5269\n","Epoch 45/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.3070 - accuracy: 0.5623 - val_loss: 1.3190 - val_accuracy: 0.5258\n","Epoch 46/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.3002 - accuracy: 0.5592 - val_loss: 1.3121 - val_accuracy: 0.5279\n","Epoch 47/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2926 - accuracy: 0.5605 - val_loss: 1.3055 - val_accuracy: 0.5227\n","Epoch 48/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.2855 - accuracy: 0.5610 - val_loss: 1.2994 - val_accuracy: 0.5207\n","Epoch 49/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2788 - accuracy: 0.5592 - val_loss: 1.2922 - val_accuracy: 0.5217\n","Epoch 50/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2719 - accuracy: 0.5618 - val_loss: 1.2858 - val_accuracy: 0.5227\n","Epoch 51/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.2651 - accuracy: 0.5641 - val_loss: 1.2792 - val_accuracy: 0.5248\n","Epoch 52/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.2580 - accuracy: 0.5687 - val_loss: 1.2733 - val_accuracy: 0.5238\n","Epoch 53/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.2516 - accuracy: 0.5656 - val_loss: 1.2668 - val_accuracy: 0.5227\n","Epoch 54/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2450 - accuracy: 0.5656 - val_loss: 1.2601 - val_accuracy: 0.5269\n","Epoch 55/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2384 - accuracy: 0.5651 - val_loss: 1.2534 - val_accuracy: 0.5300\n","Epoch 56/100\n","31/31 [==============================] - 2s 62ms/step - loss: 1.2319 - accuracy: 0.5680 - val_loss: 1.2469 - val_accuracy: 0.5320\n","Epoch 57/100\n","31/31 [==============================] - 1s 26ms/step - loss: 1.2251 - accuracy: 0.5680 - val_loss: 1.2413 - val_accuracy: 0.5351\n","Epoch 58/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2184 - accuracy: 0.5716 - val_loss: 1.2360 - val_accuracy: 0.5310\n","Epoch 59/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2120 - accuracy: 0.5685 - val_loss: 1.2302 - val_accuracy: 0.5341\n","Epoch 60/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2056 - accuracy: 0.5752 - val_loss: 1.2236 - val_accuracy: 0.5351\n","Epoch 61/100\n","31/31 [==============================] - 1s 24ms/step - loss: 1.1990 - accuracy: 0.5736 - val_loss: 1.2176 - val_accuracy: 0.5362\n","Epoch 62/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.1926 - accuracy: 0.5703 - val_loss: 1.2150 - val_accuracy: 0.5362\n","Epoch 63/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.1869 - accuracy: 0.5705 - val_loss: 1.2059 - val_accuracy: 0.5279\n","Epoch 64/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1804 - accuracy: 0.5731 - val_loss: 1.2005 - val_accuracy: 0.5238\n","Epoch 65/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.1739 - accuracy: 0.5791 - val_loss: 1.1965 - val_accuracy: 0.5331\n","Epoch 66/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.1678 - accuracy: 0.5824 - val_loss: 1.1923 - val_accuracy: 0.5258\n","Epoch 67/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1618 - accuracy: 0.5817 - val_loss: 1.1851 - val_accuracy: 0.5248\n","Epoch 68/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1565 - accuracy: 0.5814 - val_loss: 1.1789 - val_accuracy: 0.5269\n","Epoch 69/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.1504 - accuracy: 0.5817 - val_loss: 1.1725 - val_accuracy: 0.5310\n","Epoch 70/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.1447 - accuracy: 0.5806 - val_loss: 1.1670 - val_accuracy: 0.5258\n","Epoch 71/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.1384 - accuracy: 0.5871 - val_loss: 1.1692 - val_accuracy: 0.5331\n","Epoch 72/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.1327 - accuracy: 0.5832 - val_loss: 1.1620 - val_accuracy: 0.5310\n","Epoch 73/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.1267 - accuracy: 0.5855 - val_loss: 1.1586 - val_accuracy: 0.5300\n","Epoch 74/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.1211 - accuracy: 0.5876 - val_loss: 1.1512 - val_accuracy: 0.5300\n","Epoch 75/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.1147 - accuracy: 0.5855 - val_loss: 1.1456 - val_accuracy: 0.5320\n","Epoch 76/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.1093 - accuracy: 0.5943 - val_loss: 1.1484 - val_accuracy: 0.5362\n","Epoch 77/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.1036 - accuracy: 0.5891 - val_loss: 1.1335 - val_accuracy: 0.5217\n","Epoch 78/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0981 - accuracy: 0.5860 - val_loss: 1.1318 - val_accuracy: 0.5258\n","Epoch 79/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0926 - accuracy: 0.5953 - val_loss: 1.1268 - val_accuracy: 0.5269\n","Epoch 80/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0868 - accuracy: 0.5925 - val_loss: 1.1257 - val_accuracy: 0.5227\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0820 - accuracy: 0.5876 - val_loss: 1.1236 - val_accuracy: 0.5217\n","Epoch 82/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0761 - accuracy: 0.5941 - val_loss: 1.1138 - val_accuracy: 0.5238\n","Epoch 83/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0709 - accuracy: 0.5943 - val_loss: 1.1086 - val_accuracy: 0.5196\n","Epoch 84/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0652 - accuracy: 0.5928 - val_loss: 1.1082 - val_accuracy: 0.5248\n","Epoch 85/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0600 - accuracy: 0.5941 - val_loss: 1.1065 - val_accuracy: 0.5217\n","Epoch 86/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0541 - accuracy: 0.5990 - val_loss: 1.1017 - val_accuracy: 0.5176\n","Epoch 87/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0495 - accuracy: 0.6005 - val_loss: 1.0984 - val_accuracy: 0.5155\n","Epoch 88/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0441 - accuracy: 0.6054 - val_loss: 1.0939 - val_accuracy: 0.5258\n","Epoch 89/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.0382 - accuracy: 0.6044 - val_loss: 1.0949 - val_accuracy: 0.5238\n","Epoch 90/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0354 - accuracy: 0.6016 - val_loss: 1.0862 - val_accuracy: 0.5258\n","Epoch 91/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0301 - accuracy: 0.6052 - val_loss: 1.0839 - val_accuracy: 0.5227\n","Epoch 92/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.0249 - accuracy: 0.6016 - val_loss: 1.0854 - val_accuracy: 0.5269\n","Epoch 93/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.0208 - accuracy: 0.6039 - val_loss: 1.0801 - val_accuracy: 0.5269\n","Epoch 94/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.0143 - accuracy: 0.6088 - val_loss: 1.0747 - val_accuracy: 0.5217\n","Epoch 95/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0099 - accuracy: 0.6090 - val_loss: 1.0702 - val_accuracy: 0.5279\n","Epoch 96/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.0039 - accuracy: 0.6150 - val_loss: 1.0708 - val_accuracy: 0.5269\n","Epoch 97/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0001 - accuracy: 0.6057 - val_loss: 1.0646 - val_accuracy: 0.5186\n","Epoch 98/100\n","31/31 [==============================] - 2s 58ms/step - loss: 0.9957 - accuracy: 0.6111 - val_loss: 1.0645 - val_accuracy: 0.5382\n","Epoch 99/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9895 - accuracy: 0.6116 - val_loss: 1.0579 - val_accuracy: 0.5300\n","Epoch 100/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9852 - accuracy: 0.6155 - val_loss: 1.0565 - val_accuracy: 0.5279\n","{'loss': [1.7012592554092407, 1.6902059316635132, 1.6792471408843994, 1.6683183908462524, 1.6574496030807495, 1.646682858467102, 1.635938286781311, 1.6252669095993042, 1.6146692037582397, 1.6043837070465088, 1.5941952466964722, 1.584249496459961, 1.574390172958374, 1.564849853515625, 1.5552066564559937, 1.5457934141159058, 1.5364956855773926, 1.5273154973983765, 1.5180590152740479, 1.509017825126648, 1.500157356262207, 1.4912289381027222, 1.4824720621109009, 1.4739210605621338, 1.4653210639953613, 1.4567197561264038, 1.4483548402786255, 1.4398983716964722, 1.4314680099487305, 1.4232401847839355, 1.4150269031524658, 1.4068129062652588, 1.3987609148025513, 1.390619158744812, 1.3827282190322876, 1.3747122287750244, 1.367009162902832, 1.3591759204864502, 1.351637363433838, 1.3440419435501099, 1.3362091779708862, 1.3290259838104248, 1.3216760158538818, 1.314366340637207, 1.3069812059402466, 1.300195574760437, 1.292634129524231, 1.2854845523834229, 1.2787619829177856, 1.2719389200210571, 1.265055537223816, 1.2579504251480103, 1.2515642642974854, 1.2449941635131836, 1.2383582592010498, 1.2318521738052368, 1.2250702381134033, 1.2184147834777832, 1.2120109796524048, 1.2055624723434448, 1.1989669799804688, 1.1926110982894897, 1.1869102716445923, 1.1804255247116089, 1.1739128828048706, 1.167771816253662, 1.161792516708374, 1.1565446853637695, 1.1504497528076172, 1.1447207927703857, 1.1384267807006836, 1.1326789855957031, 1.1266794204711914, 1.1210886240005493, 1.1147490739822388, 1.109256386756897, 1.1035770177841187, 1.0981396436691284, 1.0926482677459717, 1.0867723226547241, 1.0819933414459229, 1.0760791301727295, 1.070884108543396, 1.0652387142181396, 1.0600004196166992, 1.0540822744369507, 1.049520492553711, 1.044103741645813, 1.0382150411605835, 1.0353983640670776, 1.0301305055618286, 1.024869441986084, 1.0208439826965332, 1.014335036277771, 1.0099194049835205, 1.0039024353027344, 1.000132441520691, 0.9957303404808044, 0.9895161390304565, 0.985226571559906], 'accuracy': [0.5036175847053528, 0.5036175847053528, 0.5038759708404541, 0.5111111402511597, 0.5191214680671692, 0.5167958736419678, 0.5201550126075745, 0.5193798542022705, 0.5240309834480286, 0.5276485681533813, 0.5289405584335327, 0.5271317958831787, 0.529198944568634, 0.5297157764434814, 0.5258398056030273, 0.5299741625785828, 0.5299741625785828, 0.52739018201828, 0.5333333611488342, 0.5335917472839355, 0.5366925001144409, 0.5348837375640869, 0.5343669056892395, 0.5397932529449463, 0.5361757278442383, 0.5459948182106018, 0.5421188473701477, 0.5449612140655518, 0.5462532043457031, 0.5452196598052979, 0.5454780459403992, 0.537726104259491, 0.538759708404541, 0.5475451946258545, 0.549095630645752, 0.5496124029159546, 0.5472868084907532, 0.549095630645752, 0.5540051460266113, 0.5521963834762573, 0.5578811168670654, 0.5596899390220642, 0.5524547696113586, 0.5573643445968628, 0.5622739195823669, 0.5591731071472168, 0.5604650974273682, 0.5609819293022156, 0.5591731071472168, 0.5617570877075195, 0.564082682132721, 0.5687338709831238, 0.5656330585479736, 0.5656330585479736, 0.565116286277771, 0.567958652973175, 0.567958652973175, 0.5715762376785278, 0.5684754252433777, 0.5751938223838806, 0.5736433863639832, 0.5702842473983765, 0.5705426335334778, 0.5731266140937805, 0.5790697932243347, 0.5824289321899414, 0.5816537737846375, 0.5813953280448914, 0.5816537737846375, 0.5806201696395874, 0.5870801210403442, 0.5832041501998901, 0.5855297446250916, 0.5875968933105469, 0.5855297446250916, 0.594315230846405, 0.5891472697257996, 0.5860465168952942, 0.5953488349914551, 0.592506468296051, 0.5875968933105469, 0.5940568447113037, 0.594315230846405, 0.5927648544311523, 0.5940568447113037, 0.5989664196968079, 0.6005167961120605, 0.6054263710975647, 0.6043927669525146, 0.6015504002571106, 0.6051679849624634, 0.6015504002571106, 0.603875994682312, 0.6087855100631714, 0.6090439558029175, 0.6149870753288269, 0.605684757232666, 0.6111111044883728, 0.6116279363632202, 0.6155038475990295], 'val_loss': [1.6955764293670654, 1.6846890449523926, 1.673915147781372, 1.6632626056671143, 1.6527278423309326, 1.6423207521438599, 1.632028579711914, 1.6218608617782593, 1.611801266670227, 1.6018648147583008, 1.5920462608337402, 1.582358479499817, 1.5727887153625488, 1.5633609294891357, 1.5540691614151, 1.5448566675186157, 1.535805583000183, 1.5270358324050903, 1.5182228088378906, 1.5095795392990112, 1.5010448694229126, 1.4926012754440308, 1.484334111213684, 1.476025938987732, 1.4679324626922607, 1.459635853767395, 1.451870083808899, 1.4438230991363525, 1.4357630014419556, 1.4280226230621338, 1.419609785079956, 1.4120798110961914, 1.404547929763794, 1.3970540761947632, 1.3894964456558228, 1.3824858665466309, 1.374951720237732, 1.3670618534088135, 1.360370397567749, 1.3537914752960205, 1.3461298942565918, 1.3387067317962646, 1.3328076601028442, 1.3254978656768799, 1.318969488143921, 1.3121333122253418, 1.305530071258545, 1.2994095087051392, 1.2921679019927979, 1.2858450412750244, 1.2791528701782227, 1.2732690572738647, 1.266824722290039, 1.260115146636963, 1.2533557415008545, 1.246925711631775, 1.2413418292999268, 1.2360283136367798, 1.2301909923553467, 1.2235952615737915, 1.2175664901733398, 1.2149850130081177, 1.2059072256088257, 1.2004793882369995, 1.1965086460113525, 1.1922677755355835, 1.1850862503051758, 1.1789377927780151, 1.1725428104400635, 1.1669707298278809, 1.169183611869812, 1.1619716882705688, 1.158603549003601, 1.1511512994766235, 1.1456180810928345, 1.148410439491272, 1.1335419416427612, 1.131760597229004, 1.1268254518508911, 1.1257396936416626, 1.123647689819336, 1.1137529611587524, 1.1085789203643799, 1.1082457304000854, 1.1065465211868286, 1.101697325706482, 1.0983877182006836, 1.0938576459884644, 1.0949169397354126, 1.0861705541610718, 1.0839383602142334, 1.085381269454956, 1.0800747871398926, 1.0747132301330566, 1.0702483654022217, 1.0707851648330688, 1.064552903175354, 1.0644772052764893, 1.0578891038894653, 1.056524395942688], 'val_accuracy': [0.48553720116615295, 0.48553720116615295, 0.5299586653709412, 0.5206611752510071, 0.5268595218658447, 0.5258264541625977, 0.5227272510528564, 0.5237603187561035, 0.5247933864593506, 0.5268595218658447, 0.5258264541625977, 0.5247933864593506, 0.5206611752510071, 0.5237603187561035, 0.5227272510528564, 0.5227272510528564, 0.5268595218658447, 0.5258264541625977, 0.5258264541625977, 0.5309917330741882, 0.5299586653709412, 0.5258264541625977, 0.5247933864593506, 0.5227272510528564, 0.51962810754776, 0.5216942429542542, 0.5247933864593506, 0.5227272510528564, 0.5237603187561035, 0.5206611752510071, 0.5185950398445129, 0.5227272510528564, 0.5175619721412659, 0.51962810754776, 0.5227272510528564, 0.5237603187561035, 0.5216942429542542, 0.5216942429542542, 0.5227272510528564, 0.5237603187561035, 0.5268595218658447, 0.5237603187561035, 0.5227272510528564, 0.5268595218658447, 0.5258264541625977, 0.5278925895690918, 0.5227272510528564, 0.5206611752510071, 0.5216942429542542, 0.5227272510528564, 0.5247933864593506, 0.5237603187561035, 0.5227272510528564, 0.5268595218658447, 0.5299586653709412, 0.5320248007774353, 0.5351239442825317, 0.5309917330741882, 0.5340909361839294, 0.5351239442825317, 0.5361570119857788, 0.5361570119857788, 0.5278925895690918, 0.5237603187561035, 0.5330578684806824, 0.5258264541625977, 0.5247933864593506, 0.5268595218658447, 0.5309917330741882, 0.5258264541625977, 0.5330578684806824, 0.5309917330741882, 0.5299586653709412, 0.5299586653709412, 0.5320248007774353, 0.5361570119857788, 0.5216942429542542, 0.5258264541625977, 0.5268595218658447, 0.5227272510528564, 0.5216942429542542, 0.5237603187561035, 0.51962810754776, 0.5247933864593506, 0.5216942429542542, 0.5175619721412659, 0.5154958963394165, 0.5258264541625977, 0.5237603187561035, 0.5258264541625977, 0.5227272510528564, 0.5268595218658447, 0.5268595218658447, 0.5216942429542542, 0.5278925895690918, 0.5268595218658447, 0.5185950398445129, 0.538223147392273, 0.5299586653709412, 0.5278925895690918]}\n","32/32 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/29 [===========================>..] - ETA: 0s - loss: 1.0345 - accuracy: 0.5709"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["29/29 [==============================] - 10s 60ms/step - loss: 1.0342 - accuracy: 0.5725 - val_loss: 1.0527 - val_accuracy: 0.5151\n","Epoch 2/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0250 - accuracy: 0.5805 - val_loss: 1.0487 - val_accuracy: 0.5151\n","Epoch 3/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.0194 - accuracy: 0.5884 - val_loss: 1.0436 - val_accuracy: 0.5172\n","Epoch 4/100\n","29/29 [==============================] - 1s 25ms/step - loss: 1.0127 - accuracy: 0.5886 - val_loss: 1.0384 - val_accuracy: 0.5248\n","Epoch 5/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.0073 - accuracy: 0.5846 - val_loss: 1.0363 - val_accuracy: 0.5172\n","Epoch 6/100\n","29/29 [==============================] - 1s 28ms/step - loss: 1.0023 - accuracy: 0.5954 - val_loss: 1.0301 - val_accuracy: 0.5280\n","Epoch 7/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.9983 - accuracy: 0.5951 - val_loss: 1.0268 - val_accuracy: 0.5259\n","Epoch 8/100\n","29/29 [==============================] - 1s 32ms/step - loss: 0.9925 - accuracy: 0.5905 - val_loss: 1.0225 - val_accuracy: 0.5291\n","Epoch 9/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.9868 - accuracy: 0.6043 - val_loss: 1.0195 - val_accuracy: 0.5248\n","Epoch 10/100\n","29/29 [==============================] - 1s 30ms/step - loss: 0.9845 - accuracy: 0.5897 - val_loss: 1.0121 - val_accuracy: 0.5571\n","Epoch 11/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.9795 - accuracy: 0.5919 - val_loss: 1.0084 - val_accuracy: 0.5496\n","Epoch 12/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9734 - accuracy: 0.5989 - val_loss: 1.0073 - val_accuracy: 0.5345\n","Epoch 13/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.9670 - accuracy: 0.5970 - val_loss: 1.0039 - val_accuracy: 0.5366\n","Epoch 14/100\n","29/29 [==============================] - 1s 29ms/step - loss: 0.9652 - accuracy: 0.6008 - val_loss: 0.9952 - val_accuracy: 0.5711\n","Epoch 15/100\n","29/29 [==============================] - 1s 33ms/step - loss: 0.9588 - accuracy: 0.5989 - val_loss: 0.9914 - val_accuracy: 0.5787\n","Epoch 16/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.9541 - accuracy: 0.6018 - val_loss: 0.9899 - val_accuracy: 0.5722\n","Epoch 17/100\n","29/29 [==============================] - 1s 30ms/step - loss: 0.9489 - accuracy: 0.6013 - val_loss: 0.9845 - val_accuracy: 0.5808\n","Epoch 18/100\n","29/29 [==============================] - 1s 34ms/step - loss: 0.9471 - accuracy: 0.6053 - val_loss: 0.9791 - val_accuracy: 0.5841\n","Epoch 19/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.9401 - accuracy: 0.6088 - val_loss: 0.9799 - val_accuracy: 0.5754\n","Epoch 20/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9351 - accuracy: 0.6061 - val_loss: 0.9753 - val_accuracy: 0.5830\n","Epoch 21/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9316 - accuracy: 0.6102 - val_loss: 0.9719 - val_accuracy: 0.5841\n","Epoch 22/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9304 - accuracy: 0.6048 - val_loss: 0.9663 - val_accuracy: 0.5841\n","Epoch 23/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.9246 - accuracy: 0.6059 - val_loss: 0.9658 - val_accuracy: 0.5787\n","Epoch 24/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9197 - accuracy: 0.6123 - val_loss: 0.9643 - val_accuracy: 0.5787\n","Epoch 25/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.9153 - accuracy: 0.6134 - val_loss: 0.9666 - val_accuracy: 0.5776\n","Epoch 26/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.9152 - accuracy: 0.6056 - val_loss: 0.9626 - val_accuracy: 0.5862\n","Epoch 27/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9094 - accuracy: 0.6121 - val_loss: 0.9618 - val_accuracy: 0.5808\n","Epoch 28/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.9032 - accuracy: 0.6145 - val_loss: 0.9627 - val_accuracy: 0.5776\n","Epoch 29/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9015 - accuracy: 0.6078 - val_loss: 0.9607 - val_accuracy: 0.5808\n","Epoch 30/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8986 - accuracy: 0.6150 - val_loss: 0.9606 - val_accuracy: 0.5754\n","Epoch 31/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8936 - accuracy: 0.6196 - val_loss: 0.9553 - val_accuracy: 0.5787\n","Epoch 32/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8924 - accuracy: 0.6177 - val_loss: 0.9532 - val_accuracy: 0.5787\n","Epoch 33/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8907 - accuracy: 0.6115 - val_loss: 0.9612 - val_accuracy: 0.5776\n","Epoch 34/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8838 - accuracy: 0.6207 - val_loss: 0.9544 - val_accuracy: 0.5733\n","Epoch 35/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8810 - accuracy: 0.6218 - val_loss: 0.9498 - val_accuracy: 0.5733\n","Epoch 36/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8771 - accuracy: 0.6199 - val_loss: 0.9550 - val_accuracy: 0.5668\n","Epoch 37/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8737 - accuracy: 0.6231 - val_loss: 0.9443 - val_accuracy: 0.5819\n","Epoch 38/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8693 - accuracy: 0.6231 - val_loss: 0.9550 - val_accuracy: 0.5765\n","Epoch 39/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8668 - accuracy: 0.6158 - val_loss: 0.9558 - val_accuracy: 0.5722\n","Epoch 40/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8602 - accuracy: 0.6320 - val_loss: 0.9473 - val_accuracy: 0.5722\n","Epoch 41/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8632 - accuracy: 0.6158 - val_loss: 0.9446 - val_accuracy: 0.5862\n","Epoch 42/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8585 - accuracy: 0.6258 - val_loss: 0.9478 - val_accuracy: 0.5711\n","Epoch 43/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8550 - accuracy: 0.6250 - val_loss: 0.9469 - val_accuracy: 0.5744\n","Epoch 44/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8512 - accuracy: 0.6247 - val_loss: 0.9411 - val_accuracy: 0.5819\n","Epoch 45/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.8473 - accuracy: 0.6263 - val_loss: 0.9428 - val_accuracy: 0.5711\n","Epoch 46/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.8473 - accuracy: 0.6253 - val_loss: 0.9411 - val_accuracy: 0.5797\n","Epoch 47/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.8442 - accuracy: 0.6323 - val_loss: 0.9353 - val_accuracy: 0.5700\n","Epoch 48/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8422 - accuracy: 0.6277 - val_loss: 0.9403 - val_accuracy: 0.5744\n","Epoch 49/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8348 - accuracy: 0.6352 - val_loss: 0.9362 - val_accuracy: 0.5776\n","Epoch 50/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8309 - accuracy: 0.6328 - val_loss: 0.9362 - val_accuracy: 0.5744\n","Epoch 51/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8279 - accuracy: 0.6304 - val_loss: 0.9378 - val_accuracy: 0.5668\n","Epoch 52/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8306 - accuracy: 0.6334 - val_loss: 0.9355 - val_accuracy: 0.5722\n","Epoch 53/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8261 - accuracy: 0.6374 - val_loss: 0.9475 - val_accuracy: 0.5442\n","Epoch 54/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8225 - accuracy: 0.6404 - val_loss: 0.9361 - val_accuracy: 0.5711\n","Epoch 55/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8182 - accuracy: 0.6352 - val_loss: 0.9337 - val_accuracy: 0.5700\n","Epoch 56/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8169 - accuracy: 0.6374 - val_loss: 0.9345 - val_accuracy: 0.5711\n","Epoch 57/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.8151 - accuracy: 0.6360 - val_loss: 0.9397 - val_accuracy: 0.5377\n","Epoch 58/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8154 - accuracy: 0.6379 - val_loss: 0.9319 - val_accuracy: 0.5722\n","Epoch 59/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8063 - accuracy: 0.6412 - val_loss: 0.9322 - val_accuracy: 0.5668\n","Epoch 60/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8049 - accuracy: 0.6417 - val_loss: 0.9293 - val_accuracy: 0.5711\n","Epoch 61/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8029 - accuracy: 0.6428 - val_loss: 0.9274 - val_accuracy: 0.5722\n","Epoch 62/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8030 - accuracy: 0.6474 - val_loss: 0.9293 - val_accuracy: 0.5744\n","Epoch 63/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8001 - accuracy: 0.6395 - val_loss: 0.9286 - val_accuracy: 0.5711\n","Epoch 64/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7958 - accuracy: 0.6433 - val_loss: 0.9251 - val_accuracy: 0.5754\n","Epoch 65/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.7900 - accuracy: 0.6541 - val_loss: 0.9270 - val_accuracy: 0.5657\n","Epoch 66/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7944 - accuracy: 0.6371 - val_loss: 0.9268 - val_accuracy: 0.5668\n","Epoch 67/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7899 - accuracy: 0.6571 - val_loss: 0.9311 - val_accuracy: 0.5711\n","Epoch 68/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7854 - accuracy: 0.6484 - val_loss: 0.9329 - val_accuracy: 0.5722\n","Epoch 69/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7849 - accuracy: 0.6546 - val_loss: 0.9252 - val_accuracy: 0.5560\n","Epoch 70/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.7805 - accuracy: 0.6530 - val_loss: 0.9242 - val_accuracy: 0.5776\n","Epoch 71/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7794 - accuracy: 0.6433 - val_loss: 0.9233 - val_accuracy: 0.5657\n","Epoch 72/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7776 - accuracy: 0.6436 - val_loss: 0.9265 - val_accuracy: 0.5690\n","Epoch 73/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7709 - accuracy: 0.6501 - val_loss: 0.9335 - val_accuracy: 0.5614\n","Epoch 74/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.7759 - accuracy: 0.6492 - val_loss: 0.9252 - val_accuracy: 0.5787\n","Epoch 75/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7723 - accuracy: 0.6490 - val_loss: 0.9278 - val_accuracy: 0.5690\n","Epoch 76/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7682 - accuracy: 0.6544 - val_loss: 0.9211 - val_accuracy: 0.5819\n","Epoch 77/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7639 - accuracy: 0.6592 - val_loss: 0.9312 - val_accuracy: 0.5539\n","Epoch 78/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7664 - accuracy: 0.6509 - val_loss: 0.9257 - val_accuracy: 0.5668\n","Epoch 79/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7625 - accuracy: 0.6487 - val_loss: 0.9184 - val_accuracy: 0.5700\n","Epoch 80/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7535 - accuracy: 0.6598 - val_loss: 0.9270 - val_accuracy: 0.5625\n","Epoch 81/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7614 - accuracy: 0.6552 - val_loss: 0.9235 - val_accuracy: 0.5797\n","Epoch 82/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7585 - accuracy: 0.6525 - val_loss: 0.9244 - val_accuracy: 0.5711\n","Epoch 83/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7544 - accuracy: 0.6668 - val_loss: 0.9210 - val_accuracy: 0.5797\n","Epoch 84/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7462 - accuracy: 0.6589 - val_loss: 0.9235 - val_accuracy: 0.5636\n","Epoch 85/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7424 - accuracy: 0.6665 - val_loss: 0.9388 - val_accuracy: 0.5453\n","Epoch 86/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7444 - accuracy: 0.6616 - val_loss: 0.9300 - val_accuracy: 0.5539\n","Epoch 87/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7417 - accuracy: 0.6676 - val_loss: 0.9260 - val_accuracy: 0.5571\n","Epoch 88/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7420 - accuracy: 0.6562 - val_loss: 0.9316 - val_accuracy: 0.5614\n","Epoch 89/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7394 - accuracy: 0.6713 - val_loss: 0.9326 - val_accuracy: 0.5668\n","Epoch 90/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7383 - accuracy: 0.6692 - val_loss: 0.9267 - val_accuracy: 0.5776\n","Epoch 91/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7374 - accuracy: 0.6662 - val_loss: 0.9446 - val_accuracy: 0.5754\n","Epoch 92/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7355 - accuracy: 0.6579 - val_loss: 0.9461 - val_accuracy: 0.5334\n","Epoch 93/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7309 - accuracy: 0.6740 - val_loss: 0.9310 - val_accuracy: 0.5700\n","Epoch 94/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7282 - accuracy: 0.6692 - val_loss: 0.9208 - val_accuracy: 0.5690\n","Epoch 95/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7246 - accuracy: 0.6700 - val_loss: 0.9297 - val_accuracy: 0.5851\n","Epoch 96/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7275 - accuracy: 0.6638 - val_loss: 0.9870 - val_accuracy: 0.5172\n","Epoch 97/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7269 - accuracy: 0.6770 - val_loss: 0.9202 - val_accuracy: 0.5862\n","Epoch 98/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7203 - accuracy: 0.6659 - val_loss: 0.9292 - val_accuracy: 0.5787\n","Epoch 99/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7199 - accuracy: 0.6767 - val_loss: 0.9262 - val_accuracy: 0.5614\n","Epoch 100/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7222 - accuracy: 0.6662 - val_loss: 0.9250 - val_accuracy: 0.5571\n","{'loss': [1.0341612100601196, 1.0250022411346436, 1.019356608390808, 1.0126832723617554, 1.0073482990264893, 1.0023341178894043, 0.9982962012290955, 0.992474377155304, 0.9868188500404358, 0.9844791889190674, 0.9795347452163696, 0.9733875393867493, 0.9669556021690369, 0.9652166962623596, 0.9588221907615662, 0.9541374444961548, 0.9489265084266663, 0.9471098780632019, 0.9401448369026184, 0.9351345300674438, 0.931637167930603, 0.9304288029670715, 0.9246023893356323, 0.9196706414222717, 0.9153366684913635, 0.9151742458343506, 0.9093824028968811, 0.9032383561134338, 0.9015413522720337, 0.8985725045204163, 0.8936182260513306, 0.8923547863960266, 0.890707790851593, 0.8837858438491821, 0.8809925317764282, 0.8771320581436157, 0.8737467527389526, 0.8692606091499329, 0.866818904876709, 0.8601697683334351, 0.8631500005722046, 0.8585422039031982, 0.8550309538841248, 0.8511937856674194, 0.8473169803619385, 0.8472676277160645, 0.844221830368042, 0.8421977758407593, 0.8348103761672974, 0.8308862447738647, 0.8279271721839905, 0.8306400775909424, 0.8260778188705444, 0.8224968910217285, 0.8182121515274048, 0.8169042468070984, 0.8150604367256165, 0.815432071685791, 0.8062964081764221, 0.8048855066299438, 0.8029299378395081, 0.8030069470405579, 0.8000516295433044, 0.7957515120506287, 0.7900373935699463, 0.7944271564483643, 0.7899099588394165, 0.7853880524635315, 0.7849022150039673, 0.7805029153823853, 0.7793514728546143, 0.7776355147361755, 0.7708920240402222, 0.7758711576461792, 0.7722828984260559, 0.7681793570518494, 0.7638538479804993, 0.7663692235946655, 0.7624791264533997, 0.7534717321395874, 0.7614083886146545, 0.7585009336471558, 0.754390299320221, 0.7461711168289185, 0.7424323558807373, 0.7443941235542297, 0.7416708469390869, 0.7419835925102234, 0.7394430637359619, 0.7383437752723694, 0.7374342679977417, 0.7354766130447388, 0.7308832406997681, 0.728216826915741, 0.7245801091194153, 0.7274613380432129, 0.7268728613853455, 0.7203291654586792, 0.7198939919471741, 0.7222313284873962], 'accuracy': [0.5724676847457886, 0.5805495977401733, 0.5883620977401733, 0.5886314511299133, 0.584590494632721, 0.595366358757019, 0.595097005367279, 0.5905172228813171, 0.6042564511299133, 0.5897090435028076, 0.5918642282485962, 0.5988685488700867, 0.5969827771186829, 0.6007543206214905, 0.5988685488700867, 0.6018319129943848, 0.6012930870056152, 0.6053340435028076, 0.6088362336158752, 0.6061422228813171, 0.6101831793785095, 0.6047952771186829, 0.6058728694915771, 0.6123383641242981, 0.6134159564971924, 0.6056034564971924, 0.6120689511299133, 0.6144935488700867, 0.607758641242981, 0.6150323152542114, 0.6196120977401733, 0.6177262663841248, 0.6115301847457886, 0.6206896305084229, 0.6217672228813171, 0.6198814511299133, 0.6231142282485962, 0.6231142282485962, 0.615840494632721, 0.6320043206214905, 0.615840494632721, 0.6258081793785095, 0.625, 0.6247305870056152, 0.626347005367279, 0.6252694129943848, 0.6322737336158752, 0.6276939511299133, 0.6352370977401733, 0.6328125, 0.6303879022598267, 0.6333512663841248, 0.6373922228813171, 0.6403555870056152, 0.6352370977401733, 0.6373922228813171, 0.6360452771186829, 0.6379310488700867, 0.6411637663841248, 0.6417025923728943, 0.6427801847457886, 0.6473599076271057, 0.6395474076271057, 0.6433189511299133, 0.6540948152542114, 0.6371228694915771, 0.6570581793785095, 0.6484375, 0.654633641242981, 0.6530172228813171, 0.6433189511299133, 0.6435883641242981, 0.650053858757019, 0.6492456793785095, 0.6489762663841248, 0.6543642282485962, 0.6592133641242981, 0.6508620977401733, 0.6487069129943848, 0.6597521305084229, 0.6551724076271057, 0.6524784564971924, 0.6667564511299133, 0.6589439511299133, 0.6664870977401733, 0.6616379022598267, 0.6675646305084229, 0.65625, 0.6713362336158752, 0.6691810488700867, 0.6662176847457886, 0.657866358757019, 0.6740301847457886, 0.6691810488700867, 0.6699892282485962, 0.6637930870056152, 0.6769935488700867, 0.6659482717514038, 0.6767241358757019, 0.6662176847457886], 'val_loss': [1.0527478456497192, 1.0487412214279175, 1.043638825416565, 1.038445234298706, 1.0362629890441895, 1.0301352739334106, 1.0267739295959473, 1.0225398540496826, 1.0195467472076416, 1.0120991468429565, 1.0084102153778076, 1.0072582960128784, 1.0039454698562622, 0.9952276945114136, 0.9914113283157349, 0.9899359345436096, 0.9844719171524048, 0.9790704846382141, 0.9799304008483887, 0.9753010272979736, 0.9718909859657288, 0.9663454294204712, 0.965762197971344, 0.9643133282661438, 0.9665870070457458, 0.9626440405845642, 0.961762011051178, 0.9627444744110107, 0.9607459306716919, 0.9606192111968994, 0.9553467035293579, 0.953200101852417, 0.9611523151397705, 0.954379141330719, 0.9497984647750854, 0.9549573659896851, 0.9442833662033081, 0.9549714922904968, 0.9557698965072632, 0.9473196864128113, 0.9446063041687012, 0.9477896690368652, 0.9468891024589539, 0.9410744309425354, 0.9427947998046875, 0.9411389827728271, 0.9352606534957886, 0.9403268694877625, 0.9362359046936035, 0.9361717700958252, 0.9377689957618713, 0.935482919216156, 0.9475461840629578, 0.9360904693603516, 0.9337115287780762, 0.9345047473907471, 0.9397254586219788, 0.931914746761322, 0.9321556687355042, 0.9293208718299866, 0.9274366497993469, 0.9292685985565186, 0.928582489490509, 0.9250677227973938, 0.927010715007782, 0.9268023371696472, 0.9311169385910034, 0.9329193830490112, 0.9252053499221802, 0.9241917729377747, 0.9232586026191711, 0.9264723062515259, 0.9334573149681091, 0.9252185821533203, 0.9278163909912109, 0.9210952520370483, 0.9311729669570923, 0.9257237911224365, 0.918379008769989, 0.9269551038742065, 0.9234532713890076, 0.9243854880332947, 0.9209539890289307, 0.9234647154808044, 0.9387996196746826, 0.9299691319465637, 0.9259698390960693, 0.9315747022628784, 0.9325816035270691, 0.9267200231552124, 0.9446260333061218, 0.9461184740066528, 0.9309960603713989, 0.92076575756073, 0.9297357797622681, 0.9870327115058899, 0.9202486276626587, 0.9291530251502991, 0.9262456297874451, 0.9249588251113892], 'val_accuracy': [0.5150862336158752, 0.5150862336158752, 0.517241358757019, 0.524784505367279, 0.517241358757019, 0.5280172228813171, 0.5258620977401733, 0.5290948152542114, 0.524784505367279, 0.5571120977401733, 0.5495689511299133, 0.5344827771186829, 0.5366379022598267, 0.5711206793785095, 0.5786637663841248, 0.5721982717514038, 0.5808189511299133, 0.5840517282485962, 0.5754310488700867, 0.5829741358757019, 0.5840517282485962, 0.5840517282485962, 0.5786637663841248, 0.5786637663841248, 0.5775862336158752, 0.5862069129943848, 0.5808189511299133, 0.5775862336158752, 0.5808189511299133, 0.5754310488700867, 0.5786637663841248, 0.5786637663841248, 0.5775862336158752, 0.5732758641242981, 0.5732758641242981, 0.5668103694915771, 0.5818965435028076, 0.576508641242981, 0.5721982717514038, 0.5721982717514038, 0.5862069129943848, 0.5711206793785095, 0.5743534564971924, 0.5818965435028076, 0.5711206793785095, 0.579741358757019, 0.5700430870056152, 0.5743534564971924, 0.5775862336158752, 0.5743534564971924, 0.5668103694915771, 0.5721982717514038, 0.5441810488700867, 0.5711206793785095, 0.5700430870056152, 0.5711206793785095, 0.537715494632721, 0.5721982717514038, 0.5668103694915771, 0.5711206793785095, 0.5721982717514038, 0.5743534564971924, 0.5711206793785095, 0.5754310488700867, 0.5657327771186829, 0.5668103694915771, 0.5711206793785095, 0.5721982717514038, 0.556034505367279, 0.5775862336158752, 0.5657327771186829, 0.568965494632721, 0.5614224076271057, 0.5786637663841248, 0.568965494632721, 0.5818965435028076, 0.5538793206214905, 0.5668103694915771, 0.5700430870056152, 0.5625, 0.579741358757019, 0.5711206793785095, 0.579741358757019, 0.5635775923728943, 0.545258641242981, 0.5538793206214905, 0.5571120977401733, 0.5614224076271057, 0.5668103694915771, 0.5775862336158752, 0.5754310488700867, 0.5334051847457886, 0.5700430870056152, 0.568965494632721, 0.5851293206214905, 0.517241358757019, 0.5862069129943848, 0.5786637663841248, 0.5614224076271057, 0.5571120977401733]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","27/28 [===========================>..] - ETA: 0s - loss: 1.0279 - accuracy: 0.5723"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["28/28 [==============================] - 8s 84ms/step - loss: 1.0279 - accuracy: 0.5722 - val_loss: 1.0539 - val_accuracy: 0.5045\n","Epoch 2/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.0203 - accuracy: 0.5823 - val_loss: 1.0498 - val_accuracy: 0.5045\n","Epoch 3/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.0144 - accuracy: 0.5809 - val_loss: 1.0462 - val_accuracy: 0.5045\n","Epoch 4/100\n","28/28 [==============================] - 1s 33ms/step - loss: 1.0101 - accuracy: 0.5925 - val_loss: 1.0418 - val_accuracy: 0.5057\n","Epoch 5/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.0032 - accuracy: 0.5869 - val_loss: 1.0390 - val_accuracy: 0.5045\n","Epoch 6/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0009 - accuracy: 0.5897 - val_loss: 1.0351 - val_accuracy: 0.5045\n","Epoch 7/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9939 - accuracy: 0.5920 - val_loss: 1.0302 - val_accuracy: 0.5057\n","Epoch 8/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9902 - accuracy: 0.5869 - val_loss: 1.0269 - val_accuracy: 0.5057\n","Epoch 9/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9844 - accuracy: 0.5948 - val_loss: 1.0234 - val_accuracy: 0.5057\n","Epoch 10/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.9807 - accuracy: 0.5985 - val_loss: 1.0177 - val_accuracy: 0.5226\n","Epoch 11/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9758 - accuracy: 0.5951 - val_loss: 1.0145 - val_accuracy: 0.5204\n","Epoch 12/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.9704 - accuracy: 0.5985 - val_loss: 1.0107 - val_accuracy: 0.5271\n","Epoch 13/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.9649 - accuracy: 0.6036 - val_loss: 1.0065 - val_accuracy: 0.5294\n","Epoch 14/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.9618 - accuracy: 0.6067 - val_loss: 1.0035 - val_accuracy: 0.5294\n","Epoch 15/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.9591 - accuracy: 0.5982 - val_loss: 0.9934 - val_accuracy: 0.5441\n","Epoch 16/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.9541 - accuracy: 0.6002 - val_loss: 0.9899 - val_accuracy: 0.5419\n","Epoch 17/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9490 - accuracy: 0.6050 - val_loss: 0.9897 - val_accuracy: 0.5362\n","Epoch 18/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.9464 - accuracy: 0.6013 - val_loss: 0.9776 - val_accuracy: 0.5622\n","Epoch 19/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.9403 - accuracy: 0.6038 - val_loss: 0.9735 - val_accuracy: 0.5645\n","Epoch 20/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9388 - accuracy: 0.6067 - val_loss: 0.9698 - val_accuracy: 0.5633\n","Epoch 21/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.9346 - accuracy: 0.6010 - val_loss: 0.9645 - val_accuracy: 0.5667\n","Epoch 22/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.9313 - accuracy: 0.6064 - val_loss: 0.9596 - val_accuracy: 0.5690\n","Epoch 23/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.9259 - accuracy: 0.6050 - val_loss: 0.9585 - val_accuracy: 0.5701\n","Epoch 24/100\n","28/28 [==============================] - 3s 107ms/step - loss: 0.9233 - accuracy: 0.6106 - val_loss: 0.9534 - val_accuracy: 0.5713\n","Epoch 25/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.9174 - accuracy: 0.6123 - val_loss: 0.9530 - val_accuracy: 0.5758\n","Epoch 26/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9148 - accuracy: 0.6087 - val_loss: 0.9503 - val_accuracy: 0.5758\n","Epoch 27/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.9088 - accuracy: 0.6135 - val_loss: 0.9475 - val_accuracy: 0.5769\n","Epoch 28/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9068 - accuracy: 0.6041 - val_loss: 0.9490 - val_accuracy: 0.5769\n","Epoch 29/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.9042 - accuracy: 0.6104 - val_loss: 0.9453 - val_accuracy: 0.5905\n","Epoch 30/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8999 - accuracy: 0.6101 - val_loss: 0.9424 - val_accuracy: 0.5769\n","Epoch 31/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8957 - accuracy: 0.6143 - val_loss: 0.9428 - val_accuracy: 0.5814\n","Epoch 32/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8944 - accuracy: 0.6135 - val_loss: 0.9447 - val_accuracy: 0.5679\n","Epoch 33/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8921 - accuracy: 0.6146 - val_loss: 0.9365 - val_accuracy: 0.5837\n","Epoch 34/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.8841 - accuracy: 0.6208 - val_loss: 0.9378 - val_accuracy: 0.6018\n","Epoch 35/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8828 - accuracy: 0.6188 - val_loss: 0.9312 - val_accuracy: 0.5916\n","Epoch 36/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8792 - accuracy: 0.6152 - val_loss: 0.9379 - val_accuracy: 0.5882\n","Epoch 37/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8755 - accuracy: 0.6200 - val_loss: 0.9337 - val_accuracy: 0.5939\n","Epoch 38/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8736 - accuracy: 0.6222 - val_loss: 0.9328 - val_accuracy: 0.5984\n","Epoch 39/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.8700 - accuracy: 0.6265 - val_loss: 0.9299 - val_accuracy: 0.5848\n","Epoch 40/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8676 - accuracy: 0.6222 - val_loss: 0.9249 - val_accuracy: 0.5950\n","Epoch 41/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.8680 - accuracy: 0.6251 - val_loss: 0.9273 - val_accuracy: 0.5769\n","Epoch 42/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.8641 - accuracy: 0.6239 - val_loss: 0.9257 - val_accuracy: 0.5792\n","Epoch 43/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.8576 - accuracy: 0.6248 - val_loss: 0.9221 - val_accuracy: 0.5871\n","Epoch 44/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.8558 - accuracy: 0.6234 - val_loss: 0.9244 - val_accuracy: 0.5928\n","Epoch 45/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.8535 - accuracy: 0.6197 - val_loss: 0.9260 - val_accuracy: 0.5814\n","Epoch 46/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.8503 - accuracy: 0.6191 - val_loss: 0.9193 - val_accuracy: 0.5939\n","Epoch 47/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.8480 - accuracy: 0.6336 - val_loss: 0.9176 - val_accuracy: 0.5792\n","Epoch 48/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.8483 - accuracy: 0.6228 - val_loss: 0.9262 - val_accuracy: 0.5962\n","Epoch 49/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.8415 - accuracy: 0.6251 - val_loss: 0.9307 - val_accuracy: 0.5803\n","Epoch 50/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.8365 - accuracy: 0.6313 - val_loss: 0.9216 - val_accuracy: 0.5860\n","Epoch 51/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8335 - accuracy: 0.6330 - val_loss: 0.9151 - val_accuracy: 0.5781\n","Epoch 52/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8301 - accuracy: 0.6364 - val_loss: 0.9130 - val_accuracy: 0.5848\n","Epoch 53/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8349 - accuracy: 0.6220 - val_loss: 0.9074 - val_accuracy: 0.5860\n","Epoch 54/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8277 - accuracy: 0.6285 - val_loss: 0.9079 - val_accuracy: 0.5747\n","Epoch 55/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8308 - accuracy: 0.6265 - val_loss: 0.9041 - val_accuracy: 0.5803\n","Epoch 56/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8251 - accuracy: 0.6372 - val_loss: 0.9147 - val_accuracy: 0.5837\n","Epoch 57/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8232 - accuracy: 0.6389 - val_loss: 0.9160 - val_accuracy: 0.5781\n","Epoch 58/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8159 - accuracy: 0.6420 - val_loss: 0.9080 - val_accuracy: 0.5916\n","Epoch 59/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8160 - accuracy: 0.6319 - val_loss: 0.9135 - val_accuracy: 0.5860\n","Epoch 60/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8129 - accuracy: 0.6355 - val_loss: 0.9086 - val_accuracy: 0.5871\n","Epoch 61/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8109 - accuracy: 0.6355 - val_loss: 0.9109 - val_accuracy: 0.5769\n","Epoch 62/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8080 - accuracy: 0.6381 - val_loss: 0.9049 - val_accuracy: 0.5871\n","Epoch 63/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8049 - accuracy: 0.6398 - val_loss: 0.9088 - val_accuracy: 0.5905\n","Epoch 64/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8047 - accuracy: 0.6460 - val_loss: 0.9232 - val_accuracy: 0.5667\n","Epoch 65/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7993 - accuracy: 0.6398 - val_loss: 0.9060 - val_accuracy: 0.5905\n","Epoch 66/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7953 - accuracy: 0.6415 - val_loss: 0.9069 - val_accuracy: 0.5916\n","Epoch 67/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7935 - accuracy: 0.6460 - val_loss: 0.9001 - val_accuracy: 0.5814\n","Epoch 68/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7954 - accuracy: 0.6418 - val_loss: 0.9029 - val_accuracy: 0.5577\n","Epoch 69/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7904 - accuracy: 0.6454 - val_loss: 0.9038 - val_accuracy: 0.5656\n","Epoch 70/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7886 - accuracy: 0.6486 - val_loss: 0.9035 - val_accuracy: 0.5916\n","Epoch 71/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7852 - accuracy: 0.6491 - val_loss: 0.9027 - val_accuracy: 0.5928\n","Epoch 72/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7853 - accuracy: 0.6435 - val_loss: 0.9009 - val_accuracy: 0.5871\n","Epoch 73/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.7829 - accuracy: 0.6460 - val_loss: 0.9128 - val_accuracy: 0.5701\n","Epoch 74/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7775 - accuracy: 0.6542 - val_loss: 0.8983 - val_accuracy: 0.5769\n","Epoch 75/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.7730 - accuracy: 0.6488 - val_loss: 0.9023 - val_accuracy: 0.5916\n","Epoch 76/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7742 - accuracy: 0.6466 - val_loss: 0.9075 - val_accuracy: 0.5600\n","Epoch 77/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.7740 - accuracy: 0.6483 - val_loss: 0.8994 - val_accuracy: 0.5611\n","Epoch 78/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7706 - accuracy: 0.6520 - val_loss: 0.8938 - val_accuracy: 0.5905\n","Epoch 79/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7648 - accuracy: 0.6522 - val_loss: 0.9014 - val_accuracy: 0.5724\n","Epoch 80/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7672 - accuracy: 0.6460 - val_loss: 0.9030 - val_accuracy: 0.5713\n","Epoch 81/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7643 - accuracy: 0.6522 - val_loss: 0.9042 - val_accuracy: 0.5769\n","Epoch 82/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7617 - accuracy: 0.6590 - val_loss: 0.9185 - val_accuracy: 0.5905\n","Epoch 83/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7629 - accuracy: 0.6505 - val_loss: 0.9027 - val_accuracy: 0.5758\n","Epoch 84/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7621 - accuracy: 0.6443 - val_loss: 0.9076 - val_accuracy: 0.5667\n","Epoch 85/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7540 - accuracy: 0.6570 - val_loss: 0.9107 - val_accuracy: 0.5758\n","Epoch 86/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7510 - accuracy: 0.6585 - val_loss: 0.9097 - val_accuracy: 0.5781\n","Epoch 87/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7509 - accuracy: 0.6517 - val_loss: 0.8972 - val_accuracy: 0.5905\n","Epoch 88/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7485 - accuracy: 0.6559 - val_loss: 0.9103 - val_accuracy: 0.5690\n","Epoch 89/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7475 - accuracy: 0.6596 - val_loss: 0.9048 - val_accuracy: 0.5769\n","Epoch 90/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7469 - accuracy: 0.6539 - val_loss: 0.9201 - val_accuracy: 0.5701\n","Epoch 91/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7435 - accuracy: 0.6658 - val_loss: 0.9252 - val_accuracy: 0.5781\n","Epoch 92/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7490 - accuracy: 0.6619 - val_loss: 0.9043 - val_accuracy: 0.5724\n","Epoch 93/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7424 - accuracy: 0.6633 - val_loss: 0.9283 - val_accuracy: 0.5769\n","Epoch 94/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7399 - accuracy: 0.6590 - val_loss: 0.9124 - val_accuracy: 0.5667\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7321 - accuracy: 0.6706 - val_loss: 0.9044 - val_accuracy: 0.5679\n","Epoch 96/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7333 - accuracy: 0.6709 - val_loss: 0.9245 - val_accuracy: 0.5860\n","Epoch 97/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7327 - accuracy: 0.6607 - val_loss: 0.9451 - val_accuracy: 0.5419\n","Epoch 98/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7331 - accuracy: 0.6644 - val_loss: 0.9449 - val_accuracy: 0.5554\n","Epoch 99/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7272 - accuracy: 0.6616 - val_loss: 0.9674 - val_accuracy: 0.5351\n","Epoch 100/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7280 - accuracy: 0.6664 - val_loss: 0.9111 - val_accuracy: 0.5882\n","{'loss': [1.0279290676116943, 1.0203016996383667, 1.0143589973449707, 1.010074496269226, 1.0031929016113281, 1.0008689165115356, 0.9939166307449341, 0.9902347326278687, 0.9843919277191162, 0.9806827902793884, 0.9757798314094543, 0.9703693389892578, 0.9648873209953308, 0.9617714881896973, 0.9591019749641418, 0.9541251063346863, 0.9489958882331848, 0.9463756084442139, 0.9403141140937805, 0.9387584328651428, 0.9345662593841553, 0.9312597513198853, 0.9259049892425537, 0.9232749342918396, 0.9174125790596008, 0.9148150682449341, 0.9088065028190613, 0.9067617058753967, 0.9042485952377319, 0.8999333381652832, 0.8956857919692993, 0.8944355845451355, 0.8921348452568054, 0.8841357827186584, 0.8828498125076294, 0.8791731595993042, 0.8755401372909546, 0.8736408948898315, 0.8699865937232971, 0.8675774335861206, 0.867996096611023, 0.8640732765197754, 0.8575679659843445, 0.8558213710784912, 0.8534504175186157, 0.8503340482711792, 0.8480278253555298, 0.8483273983001709, 0.8414657711982727, 0.8364577889442444, 0.8334757089614868, 0.8301374912261963, 0.834923267364502, 0.8276755213737488, 0.8307759761810303, 0.8250894546508789, 0.8232302069664001, 0.8158571124076843, 0.815989077091217, 0.8128809332847595, 0.8109058737754822, 0.8080089092254639, 0.8048974275588989, 0.8047134876251221, 0.7992529273033142, 0.795270562171936, 0.7935051918029785, 0.7954230904579163, 0.7903881669044495, 0.7886191010475159, 0.7852426767349243, 0.7852844595909119, 0.7829249501228333, 0.7774950265884399, 0.7730435132980347, 0.7742093801498413, 0.7739935517311096, 0.7706421613693237, 0.7648175358772278, 0.7672349810600281, 0.7643036842346191, 0.7616773843765259, 0.7628682255744934, 0.7620766162872314, 0.753959596157074, 0.7510492205619812, 0.7508678436279297, 0.74853515625, 0.7475058436393738, 0.7468587160110474, 0.7435455322265625, 0.7489942312240601, 0.7423755526542664, 0.7398738861083984, 0.7321285009384155, 0.7333225607872009, 0.7326826453208923, 0.7331483960151672, 0.7271501421928406, 0.7279516458511353], 'accuracy': [0.5721561908721924, 0.5823429822921753, 0.5809281468391418, 0.5925297141075134, 0.5868703722953796, 0.5897000432014465, 0.5919637680053711, 0.5868703722953796, 0.594793438911438, 0.598471999168396, 0.5950763821601868, 0.598471999168396, 0.6035653352737427, 0.6066780090332031, 0.5981889963150024, 0.6001697778701782, 0.6049801707267761, 0.6013016700744629, 0.6038483381271362, 0.6066780090332031, 0.6010186672210693, 0.6063950061798096, 0.6049801707267761, 0.6106395125389099, 0.6123372912406921, 0.6086587309837341, 0.6134691834449768, 0.604131281375885, 0.6103565096855164, 0.6100735664367676, 0.6143180727958679, 0.6134691834449768, 0.6146010160446167, 0.620826244354248, 0.618845522403717, 0.615166962146759, 0.6199773550033569, 0.6222410798072815, 0.6264855861663818, 0.6222410798072815, 0.6250707507133484, 0.6239388585090637, 0.6247877478599548, 0.6233729720115662, 0.6196944117546082, 0.6191284656524658, 0.6335597038269043, 0.6228070259094238, 0.6250707507133484, 0.6312959790229797, 0.632993757724762, 0.6363893747329712, 0.6219581365585327, 0.6284663081169128, 0.6264855861663818, 0.6372382640838623, 0.6389360427856445, 0.6420486569404602, 0.6318619251251221, 0.6355404853820801, 0.6355404853820801, 0.6380871534347534, 0.6397849321365356, 0.646010160446167, 0.6397849321365356, 0.6414827108383179, 0.646010160446167, 0.6417657136917114, 0.6454442739486694, 0.6485568881034851, 0.6491228342056274, 0.6434634923934937, 0.646010160446167, 0.6542161703109741, 0.6488398313522339, 0.6465761065483093, 0.6482738852500916, 0.6519524455070496, 0.6522354483604431, 0.646010160446167, 0.6522354483604431, 0.6590266227722168, 0.6505376100540161, 0.6443123817443848, 0.657045841217041, 0.6584606766700745, 0.6516695022583008, 0.6559139490127563, 0.6595925092697144, 0.6539332270622253, 0.6658177971839905, 0.6618562340736389, 0.6632710695266724, 0.6590266227722168, 0.6706281900405884, 0.6709111332893372, 0.660724401473999, 0.664402961730957, 0.6615732908248901, 0.666383683681488], 'val_loss': [1.0539186000823975, 1.0498385429382324, 1.046189308166504, 1.041832447052002, 1.0389713048934937, 1.0350985527038574, 1.0302478075027466, 1.0269111394882202, 1.0233975648880005, 1.0176950693130493, 1.0144851207733154, 1.0106613636016846, 1.0065410137176514, 1.0035014152526855, 0.9934497475624084, 0.9899338483810425, 0.989732027053833, 0.9775987267494202, 0.9734894633293152, 0.9698370099067688, 0.9644976258277893, 0.959553599357605, 0.9585337042808533, 0.9534419178962708, 0.9529834985733032, 0.9502645134925842, 0.9475416541099548, 0.949015736579895, 0.9452528357505798, 0.9423671364784241, 0.9428383708000183, 0.9447017312049866, 0.9365113973617554, 0.9378037452697754, 0.9312024712562561, 0.9378791451454163, 0.9337183833122253, 0.9327548742294312, 0.9299015998840332, 0.9249259829521179, 0.9272599816322327, 0.9256963729858398, 0.9221414923667908, 0.9244036674499512, 0.9259927272796631, 0.9192854166030884, 0.9176112413406372, 0.9262127876281738, 0.9306856989860535, 0.9215951561927795, 0.915076732635498, 0.913044810295105, 0.9073910117149353, 0.9079469442367554, 0.9040887355804443, 0.9147494435310364, 0.9160085320472717, 0.9080104231834412, 0.9134607315063477, 0.9085506200790405, 0.9108611941337585, 0.9048526287078857, 0.9088045954704285, 0.9231964945793152, 0.9059785604476929, 0.9068687558174133, 0.9000594019889832, 0.9029358625411987, 0.9037566184997559, 0.9034673571586609, 0.9026750326156616, 0.9008819460868835, 0.9127886295318604, 0.8983228206634521, 0.9022765159606934, 0.9075186848640442, 0.8993639349937439, 0.893760621547699, 0.9013664722442627, 0.9029878377914429, 0.9042468667030334, 0.9185454249382019, 0.9027400016784668, 0.9075521230697632, 0.9106752276420593, 0.9097161889076233, 0.8971818089485168, 0.9102762341499329, 0.9047600030899048, 0.9200868010520935, 0.9252275228500366, 0.9043082594871521, 0.9282680749893188, 0.9124234318733215, 0.9043763875961304, 0.9245495200157166, 0.9450675249099731, 0.9449424147605896, 0.9673579931259155, 0.9111273288726807], 'val_accuracy': [0.5045248866081238, 0.5045248866081238, 0.5045248866081238, 0.5056561231613159, 0.5045248866081238, 0.5045248866081238, 0.5056561231613159, 0.5056561231613159, 0.5056561231613159, 0.5226244330406189, 0.5203620195388794, 0.5271493196487427, 0.529411792755127, 0.529411792755127, 0.5441176295280457, 0.5418552160263062, 0.5361990928649902, 0.5622171759605408, 0.564479649066925, 0.5633484125137329, 0.5667420625686646, 0.5690045356750488, 0.570135772228241, 0.5712669491767883, 0.5757918357849121, 0.5757918357849121, 0.5769230723381042, 0.5769230723381042, 0.5904977321624756, 0.5769230723381042, 0.581447958946228, 0.5678732991218567, 0.5837104320526123, 0.6018099784851074, 0.5916289687156677, 0.5882353186607361, 0.5938913822174072, 0.598416268825531, 0.5848416090011597, 0.5950226187705994, 0.5769230723381042, 0.5791855454444885, 0.587104082107544, 0.5927602052688599, 0.581447958946228, 0.5938913822174072, 0.5791855454444885, 0.5961538553237915, 0.5803167223930359, 0.5859728455543518, 0.5780543088912964, 0.5848416090011597, 0.5859728455543518, 0.5746606588363647, 0.5803167223930359, 0.5837104320526123, 0.5780543088912964, 0.5916289687156677, 0.5859728455543518, 0.587104082107544, 0.5769230723381042, 0.587104082107544, 0.5904977321624756, 0.5667420625686646, 0.5904977321624756, 0.5916289687156677, 0.581447958946228, 0.557692289352417, 0.5656108856201172, 0.5916289687156677, 0.5927602052688599, 0.587104082107544, 0.570135772228241, 0.5769230723381042, 0.5916289687156677, 0.5599547624588013, 0.5610859990119934, 0.5904977321624756, 0.5723981857299805, 0.5712669491767883, 0.5769230723381042, 0.5904977321624756, 0.5757918357849121, 0.5667420625686646, 0.5757918357849121, 0.5780543088912964, 0.5904977321624756, 0.5690045356750488, 0.5769230723381042, 0.570135772228241, 0.5780543088912964, 0.5723981857299805, 0.5769230723381042, 0.5667420625686646, 0.5678732991218567, 0.5859728455543518, 0.5418552160263062, 0.5554298758506775, 0.5350678563117981, 0.5882353186607361]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","30/31 [============================>.] - ETA: 0s - loss: 1.0292 - accuracy: 0.5734"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["31/31 [==============================] - 8s 78ms/step - loss: 1.0292 - accuracy: 0.5718 - val_loss: 1.0523 - val_accuracy: 0.5145\n","Epoch 2/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.0228 - accuracy: 0.5809 - val_loss: 1.0481 - val_accuracy: 0.5145\n","Epoch 3/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0160 - accuracy: 0.5853 - val_loss: 1.0439 - val_accuracy: 0.5145\n","Epoch 4/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0101 - accuracy: 0.5928 - val_loss: 1.0394 - val_accuracy: 0.5134\n","Epoch 5/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0061 - accuracy: 0.5863 - val_loss: 1.0359 - val_accuracy: 0.5134\n","Epoch 6/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.0005 - accuracy: 0.5928 - val_loss: 1.0312 - val_accuracy: 0.5145\n","Epoch 7/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.9940 - accuracy: 0.5990 - val_loss: 1.0271 - val_accuracy: 0.5145\n","Epoch 8/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.9891 - accuracy: 0.5910 - val_loss: 1.0224 - val_accuracy: 0.5238\n","Epoch 9/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.9840 - accuracy: 0.5969 - val_loss: 1.0188 - val_accuracy: 0.5227\n","Epoch 10/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.9790 - accuracy: 0.6036 - val_loss: 1.0141 - val_accuracy: 0.5300\n","Epoch 11/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9760 - accuracy: 0.5964 - val_loss: 1.0111 - val_accuracy: 0.5269\n","Epoch 12/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.9702 - accuracy: 0.5964 - val_loss: 1.0054 - val_accuracy: 0.5362\n","Epoch 13/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.9665 - accuracy: 0.6044 - val_loss: 1.0015 - val_accuracy: 0.5413\n","Epoch 14/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.9624 - accuracy: 0.6028 - val_loss: 0.9980 - val_accuracy: 0.5403\n","Epoch 15/100\n","31/31 [==============================] - 1s 42ms/step - loss: 0.9587 - accuracy: 0.5990 - val_loss: 0.9937 - val_accuracy: 0.5444\n","Epoch 16/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.9518 - accuracy: 0.6005 - val_loss: 0.9886 - val_accuracy: 0.5486\n","Epoch 17/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9476 - accuracy: 0.6067 - val_loss: 0.9851 - val_accuracy: 0.5455\n","Epoch 18/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.9462 - accuracy: 0.6000 - val_loss: 0.9809 - val_accuracy: 0.5496\n","Epoch 19/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9409 - accuracy: 0.6054 - val_loss: 0.9775 - val_accuracy: 0.5444\n","Epoch 20/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.9369 - accuracy: 0.6065 - val_loss: 0.9746 - val_accuracy: 0.5475\n","Epoch 21/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.9298 - accuracy: 0.6116 - val_loss: 0.9717 - val_accuracy: 0.5444\n","Epoch 22/100\n","31/31 [==============================] - 1s 36ms/step - loss: 0.9273 - accuracy: 0.6155 - val_loss: 0.9703 - val_accuracy: 0.5527\n","Epoch 23/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.9233 - accuracy: 0.6096 - val_loss: 0.9670 - val_accuracy: 0.5486\n","Epoch 24/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.9184 - accuracy: 0.6183 - val_loss: 0.9721 - val_accuracy: 0.5444\n","Epoch 25/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.9147 - accuracy: 0.6178 - val_loss: 0.9690 - val_accuracy: 0.5413\n","Epoch 26/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9103 - accuracy: 0.6129 - val_loss: 0.9626 - val_accuracy: 0.5465\n","Epoch 27/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.9070 - accuracy: 0.6070 - val_loss: 0.9671 - val_accuracy: 0.5486\n","Epoch 28/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9055 - accuracy: 0.6171 - val_loss: 0.9679 - val_accuracy: 0.5444\n","Epoch 29/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9018 - accuracy: 0.6106 - val_loss: 0.9517 - val_accuracy: 0.5527\n","Epoch 30/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.8971 - accuracy: 0.6140 - val_loss: 0.9530 - val_accuracy: 0.5548\n","Epoch 31/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8902 - accuracy: 0.6207 - val_loss: 0.9597 - val_accuracy: 0.5434\n","Epoch 32/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8887 - accuracy: 0.6106 - val_loss: 0.9559 - val_accuracy: 0.5475\n","Epoch 33/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8844 - accuracy: 0.6186 - val_loss: 0.9560 - val_accuracy: 0.5506\n","Epoch 34/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8817 - accuracy: 0.6145 - val_loss: 0.9709 - val_accuracy: 0.5465\n","Epoch 35/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8795 - accuracy: 0.6194 - val_loss: 0.9506 - val_accuracy: 0.5517\n","Epoch 36/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8777 - accuracy: 0.6171 - val_loss: 0.9622 - val_accuracy: 0.5434\n","Epoch 37/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8722 - accuracy: 0.6191 - val_loss: 0.9568 - val_accuracy: 0.5475\n","Epoch 38/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8686 - accuracy: 0.6196 - val_loss: 0.9446 - val_accuracy: 0.5475\n","Epoch 39/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8660 - accuracy: 0.6204 - val_loss: 0.9497 - val_accuracy: 0.5486\n","Epoch 40/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.8605 - accuracy: 0.6261 - val_loss: 0.9575 - val_accuracy: 0.5568\n","Epoch 41/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8570 - accuracy: 0.6279 - val_loss: 0.9621 - val_accuracy: 0.5506\n","Epoch 42/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8561 - accuracy: 0.6269 - val_loss: 0.9451 - val_accuracy: 0.5517\n","Epoch 43/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8570 - accuracy: 0.6145 - val_loss: 0.9595 - val_accuracy: 0.5465\n","Epoch 44/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8471 - accuracy: 0.6276 - val_loss: 0.9463 - val_accuracy: 0.5517\n","Epoch 45/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.8442 - accuracy: 0.6284 - val_loss: 0.9406 - val_accuracy: 0.5537\n","Epoch 46/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.8457 - accuracy: 0.6202 - val_loss: 0.9612 - val_accuracy: 0.5475\n","Epoch 47/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.8399 - accuracy: 0.6289 - val_loss: 0.9520 - val_accuracy: 0.5486\n","Epoch 48/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.8363 - accuracy: 0.6364 - val_loss: 0.9508 - val_accuracy: 0.5258\n","Epoch 49/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.8364 - accuracy: 0.6300 - val_loss: 0.9433 - val_accuracy: 0.5444\n","Epoch 50/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.8318 - accuracy: 0.6271 - val_loss: 0.9329 - val_accuracy: 0.5424\n","Epoch 51/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.8306 - accuracy: 0.6310 - val_loss: 0.9421 - val_accuracy: 0.5537\n","Epoch 52/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.8283 - accuracy: 0.6269 - val_loss: 0.9351 - val_accuracy: 0.5300\n","Epoch 53/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8235 - accuracy: 0.6320 - val_loss: 0.9374 - val_accuracy: 0.5486\n","Epoch 54/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.8193 - accuracy: 0.6271 - val_loss: 0.9408 - val_accuracy: 0.5579\n","Epoch 55/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.8152 - accuracy: 0.6346 - val_loss: 0.9492 - val_accuracy: 0.5475\n","Epoch 56/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8159 - accuracy: 0.6395 - val_loss: 0.9353 - val_accuracy: 0.5537\n","Epoch 57/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8099 - accuracy: 0.6351 - val_loss: 0.9381 - val_accuracy: 0.5537\n","Epoch 58/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.8089 - accuracy: 0.6269 - val_loss: 0.9290 - val_accuracy: 0.5382\n","Epoch 59/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8064 - accuracy: 0.6380 - val_loss: 0.9409 - val_accuracy: 0.5341\n","Epoch 60/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8018 - accuracy: 0.6339 - val_loss: 0.9570 - val_accuracy: 0.5300\n","Epoch 61/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8016 - accuracy: 0.6292 - val_loss: 0.9241 - val_accuracy: 0.5413\n","Epoch 62/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8031 - accuracy: 0.6349 - val_loss: 0.9543 - val_accuracy: 0.5165\n","Epoch 63/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7973 - accuracy: 0.6318 - val_loss: 0.9197 - val_accuracy: 0.5496\n","Epoch 64/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7903 - accuracy: 0.6346 - val_loss: 0.9285 - val_accuracy: 0.5434\n","Epoch 65/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7899 - accuracy: 0.6411 - val_loss: 0.9302 - val_accuracy: 0.5548\n","Epoch 66/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7890 - accuracy: 0.6339 - val_loss: 0.9370 - val_accuracy: 0.5372\n","Epoch 67/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7836 - accuracy: 0.6426 - val_loss: 0.9196 - val_accuracy: 0.5341\n","Epoch 68/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7836 - accuracy: 0.6367 - val_loss: 0.9474 - val_accuracy: 0.5424\n","Epoch 69/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7841 - accuracy: 0.6351 - val_loss: 0.9426 - val_accuracy: 0.5207\n","Epoch 70/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7780 - accuracy: 0.6473 - val_loss: 0.9413 - val_accuracy: 0.5444\n","Epoch 71/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7763 - accuracy: 0.6403 - val_loss: 0.9045 - val_accuracy: 0.5455\n","Epoch 72/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7725 - accuracy: 0.6463 - val_loss: 0.9280 - val_accuracy: 0.5434\n","Epoch 73/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7723 - accuracy: 0.6429 - val_loss: 0.9355 - val_accuracy: 0.5444\n","Epoch 74/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7695 - accuracy: 0.6455 - val_loss: 0.9664 - val_accuracy: 0.5289\n","Epoch 75/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7707 - accuracy: 0.6375 - val_loss: 0.9382 - val_accuracy: 0.5289\n","Epoch 76/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7666 - accuracy: 0.6537 - val_loss: 0.9163 - val_accuracy: 0.5434\n","Epoch 77/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7586 - accuracy: 0.6463 - val_loss: 0.9409 - val_accuracy: 0.5444\n","Epoch 78/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7570 - accuracy: 0.6486 - val_loss: 0.9157 - val_accuracy: 0.5424\n","Epoch 79/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7630 - accuracy: 0.6512 - val_loss: 0.9721 - val_accuracy: 0.5176\n","Epoch 80/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7524 - accuracy: 0.6594 - val_loss: 0.9288 - val_accuracy: 0.5444\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7589 - accuracy: 0.6434 - val_loss: 0.9446 - val_accuracy: 0.5393\n","Epoch 82/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7538 - accuracy: 0.6532 - val_loss: 0.9096 - val_accuracy: 0.5486\n","Epoch 83/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7492 - accuracy: 0.6561 - val_loss: 0.9226 - val_accuracy: 0.5372\n","Epoch 84/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7488 - accuracy: 0.6481 - val_loss: 0.9464 - val_accuracy: 0.5455\n","Epoch 85/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7519 - accuracy: 0.6496 - val_loss: 0.9215 - val_accuracy: 0.5341\n","Epoch 86/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7453 - accuracy: 0.6574 - val_loss: 0.9415 - val_accuracy: 0.5362\n","Epoch 87/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7441 - accuracy: 0.6522 - val_loss: 0.9007 - val_accuracy: 0.5341\n","Epoch 88/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7442 - accuracy: 0.6447 - val_loss: 0.9328 - val_accuracy: 0.5351\n","Epoch 89/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7399 - accuracy: 0.6517 - val_loss: 0.9283 - val_accuracy: 0.5496\n","Epoch 90/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7352 - accuracy: 0.6563 - val_loss: 0.9563 - val_accuracy: 0.5517\n","Epoch 91/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7360 - accuracy: 0.6599 - val_loss: 0.9452 - val_accuracy: 0.5537\n","Epoch 92/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7371 - accuracy: 0.6514 - val_loss: 0.9727 - val_accuracy: 0.5134\n","Epoch 93/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7335 - accuracy: 0.6494 - val_loss: 0.9504 - val_accuracy: 0.5289\n","Epoch 94/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7269 - accuracy: 0.6597 - val_loss: 0.9528 - val_accuracy: 0.5372\n","Epoch 95/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7342 - accuracy: 0.6434 - val_loss: 0.9471 - val_accuracy: 0.5403\n","Epoch 96/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7198 - accuracy: 0.6649 - val_loss: 0.9162 - val_accuracy: 0.5403\n","Epoch 97/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7297 - accuracy: 0.6561 - val_loss: 0.9081 - val_accuracy: 0.5362\n","Epoch 98/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7248 - accuracy: 0.6589 - val_loss: 0.9402 - val_accuracy: 0.5434\n","Epoch 99/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7205 - accuracy: 0.6517 - val_loss: 0.9209 - val_accuracy: 0.5444\n","Epoch 100/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7170 - accuracy: 0.6698 - val_loss: 0.9201 - val_accuracy: 0.5444\n","{'loss': [1.0292431116104126, 1.0228053331375122, 1.0159525871276855, 1.0101401805877686, 1.006085991859436, 1.0004864931106567, 0.993985116481781, 0.9890613555908203, 0.984015166759491, 0.9790350794792175, 0.976033627986908, 0.9701552987098694, 0.9665481448173523, 0.9624003767967224, 0.958683431148529, 0.9518375992774963, 0.9476208090782166, 0.946201741695404, 0.9409343600273132, 0.9369071125984192, 0.9298181533813477, 0.9273059368133545, 0.9232960343360901, 0.9183546900749207, 0.9147117733955383, 0.9103162288665771, 0.9069749116897583, 0.9054879546165466, 0.9017839431762695, 0.8970608711242676, 0.8902068734169006, 0.8886591792106628, 0.884378969669342, 0.8816547393798828, 0.8794880509376526, 0.8777281641960144, 0.8722048997879028, 0.8685764670372009, 0.8659544587135315, 0.8605012893676758, 0.8569876551628113, 0.8560543060302734, 0.8569806218147278, 0.8471363186836243, 0.8442363142967224, 0.8456858396530151, 0.8399384617805481, 0.8362737894058228, 0.8363780975341797, 0.8317877650260925, 0.8306330442428589, 0.8283270597457886, 0.8235400319099426, 0.8192643523216248, 0.8152458071708679, 0.8158931732177734, 0.8098635077476501, 0.8088752031326294, 0.8063563108444214, 0.8018218874931335, 0.8015955686569214, 0.8031469583511353, 0.7972909212112427, 0.7903143167495728, 0.7898893356323242, 0.7889792919158936, 0.783596932888031, 0.7835929989814758, 0.7840757369995117, 0.777972936630249, 0.7762578129768372, 0.7724967002868652, 0.7723408937454224, 0.769534170627594, 0.7706643342971802, 0.7666260600090027, 0.7585799694061279, 0.757049560546875, 0.762974739074707, 0.7524426579475403, 0.7589284777641296, 0.7537730932235718, 0.7491791844367981, 0.7488332986831665, 0.7519077658653259, 0.745310366153717, 0.7440730333328247, 0.7441821098327637, 0.7398681044578552, 0.7352377772331238, 0.7360340356826782, 0.7370519042015076, 0.7334577441215515, 0.7268686890602112, 0.7341707944869995, 0.7198269367218018, 0.7296639084815979, 0.7247663140296936, 0.720510721206665, 0.7170263528823853], 'accuracy': [0.5718346238136292, 0.5808785557746887, 0.5852712988853455, 0.5927648544311523, 0.5863049030303955, 0.5927648544311523, 0.5989664196968079, 0.5909560918807983, 0.5968992114067078, 0.6036175489425659, 0.5963824391365051, 0.5963824391365051, 0.6043927669525146, 0.602842390537262, 0.5989664196968079, 0.6005167961120605, 0.6067183613777161, 0.6000000238418579, 0.6054263710975647, 0.6064599752426147, 0.6116279363632202, 0.6155038475990295, 0.6095607280731201, 0.6183462738990784, 0.617829442024231, 0.6129198670387268, 0.6069767475128174, 0.617054283618927, 0.6105943322181702, 0.6139534711837769, 0.620671808719635, 0.6105943322181702, 0.6186046600341797, 0.6144703030586243, 0.6193798184394836, 0.617054283618927, 0.6191214323043823, 0.6196382641792297, 0.6204134225845337, 0.6260982155799866, 0.6279069781303406, 0.6268733739852905, 0.6144703030586243, 0.6276485919952393, 0.6284237504005432, 0.6201550364494324, 0.6289405822753906, 0.6364341378211975, 0.6299741864204407, 0.6271317601203918, 0.631007730960846, 0.6268733739852905, 0.632041335105896, 0.6271317601203918, 0.6346253156661987, 0.6395348906517029, 0.6351421475410461, 0.6268733739852905, 0.6379845142364502, 0.6338501572608948, 0.6291989684104919, 0.6348837018013, 0.6317829489707947, 0.6346253156661987, 0.6410852670669556, 0.6338501572608948, 0.6426356434822083, 0.6366925239562988, 0.6351421475410461, 0.6472868323326111, 0.6403100490570068, 0.646253228187561, 0.6428940296173096, 0.6454780101776123, 0.6374676823616028, 0.6537467837333679, 0.646253228187561, 0.6485788226127625, 0.6511628031730652, 0.659431517124176, 0.643410861492157, 0.6532299518585205, 0.6560723781585693, 0.648061990737915, 0.6496124267578125, 0.6573643684387207, 0.6521964073181152, 0.6447028517723083, 0.6516795754432678, 0.6563307642936707, 0.6599483489990234, 0.6514211893081665, 0.6493539810180664, 0.6596899032592773, 0.643410861492157, 0.6648578643798828, 0.6560723781585693, 0.6589147448539734, 0.6516795754432678, 0.669767439365387], 'val_loss': [1.0523221492767334, 1.0481297969818115, 1.0438830852508545, 1.0393784046173096, 1.03587806224823, 1.0311833620071411, 1.0270851850509644, 1.0223783254623413, 1.0188394784927368, 1.0141105651855469, 1.0111074447631836, 1.0053813457489014, 1.0014538764953613, 0.9979695677757263, 0.9936897158622742, 0.9885632991790771, 0.9850825667381287, 0.9809455275535583, 0.9775068163871765, 0.9745839834213257, 0.9716514348983765, 0.9702540636062622, 0.9670264720916748, 0.9721253514289856, 0.9689686298370361, 0.9626477956771851, 0.9670565128326416, 0.9679064750671387, 0.9517408609390259, 0.953048050403595, 0.9597101807594299, 0.9559356570243835, 0.9560075998306274, 0.9709320664405823, 0.9505996108055115, 0.9622205495834351, 0.9567989706993103, 0.9445587396621704, 0.9497494697570801, 0.957546591758728, 0.962138831615448, 0.9450709819793701, 0.9595215916633606, 0.9463047981262207, 0.9405574202537537, 0.9611531496047974, 0.9519715309143066, 0.9508184194564819, 0.943270206451416, 0.9329177141189575, 0.9421056509017944, 0.935071587562561, 0.9373794794082642, 0.9408459067344666, 0.9492254853248596, 0.9353177547454834, 0.938054084777832, 0.9290356040000916, 0.9409332275390625, 0.9569541811943054, 0.9240590333938599, 0.9542883634567261, 0.919675350189209, 0.9285334348678589, 0.9301700592041016, 0.9370266199111938, 0.9196233153343201, 0.9473686814308167, 0.9425968527793884, 0.9412865042686462, 0.9045085310935974, 0.9279530644416809, 0.9354649186134338, 0.9663506150245667, 0.9381607174873352, 0.9163496494293213, 0.9409095048904419, 0.9156767725944519, 0.9721192717552185, 0.9287868142127991, 0.9446322917938232, 0.9096124768257141, 0.9225687980651855, 0.9464102983474731, 0.9214590191841125, 0.9414886832237244, 0.9006639719009399, 0.9328392744064331, 0.9283432364463806, 0.9563390016555786, 0.9452328085899353, 0.9727321267127991, 0.950367271900177, 0.9527663588523865, 0.9470531940460205, 0.9161974787712097, 0.9080955386161804, 0.940240740776062, 0.9209424257278442, 0.9201114773750305], 'val_accuracy': [0.5144628286361694, 0.5144628286361694, 0.5144628286361694, 0.5134297609329224, 0.5134297609329224, 0.5144628286361694, 0.5144628286361694, 0.5237603187561035, 0.5227272510528564, 0.5299586653709412, 0.5268595218658447, 0.5361570119857788, 0.5413222908973694, 0.5402892827987671, 0.5444214940071106, 0.5485537052154541, 0.5454545617103577, 0.5495867729187012, 0.5444214940071106, 0.547520637512207, 0.5444214940071106, 0.5526859760284424, 0.5485537052154541, 0.5444214940071106, 0.5413222908973694, 0.5464876294136047, 0.5485537052154541, 0.5444214940071106, 0.5526859760284424, 0.5547520518302917, 0.5433884263038635, 0.547520637512207, 0.5506198406219482, 0.5464876294136047, 0.5516529083251953, 0.5433884263038635, 0.547520637512207, 0.547520637512207, 0.5485537052154541, 0.5568181872367859, 0.5506198406219482, 0.5516529083251953, 0.5464876294136047, 0.5516529083251953, 0.5537189841270447, 0.547520637512207, 0.5485537052154541, 0.5258264541625977, 0.5444214940071106, 0.5423553586006165, 0.5537189841270447, 0.5299586653709412, 0.5485537052154541, 0.557851254940033, 0.547520637512207, 0.5537189841270447, 0.5537189841270447, 0.538223147392273, 0.5340909361839294, 0.5299586653709412, 0.5413222908973694, 0.5165289044380188, 0.5495867729187012, 0.5433884263038635, 0.5547520518302917, 0.5371900796890259, 0.5340909361839294, 0.5423553586006165, 0.5206611752510071, 0.5444214940071106, 0.5454545617103577, 0.5433884263038635, 0.5444214940071106, 0.5289255976676941, 0.5289255976676941, 0.5433884263038635, 0.5444214940071106, 0.5423553586006165, 0.5175619721412659, 0.5444214940071106, 0.53925621509552, 0.5485537052154541, 0.5371900796890259, 0.5454545617103577, 0.5340909361839294, 0.5361570119857788, 0.5340909361839294, 0.5351239442825317, 0.5495867729187012, 0.5516529083251953, 0.5537189841270447, 0.5134297609329224, 0.5289255976676941, 0.5371900796890259, 0.5402892827987671, 0.5402892827987671, 0.5361570119857788, 0.5433884263038635, 0.5444214940071106, 0.5444214940071106]}\n","32/32 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","26/29 [=========================>....] - ETA: 0s - loss: 0.7543 - accuracy: 0.6355"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["29/29 [==============================] - 8s 61ms/step - loss: 0.7552 - accuracy: 0.6358 - val_loss: 0.8507 - val_accuracy: 0.5151\n","Epoch 2/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7456 - accuracy: 0.6449 - val_loss: 0.8631 - val_accuracy: 0.5151\n","Epoch 3/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7478 - accuracy: 0.6406 - val_loss: 0.8458 - val_accuracy: 0.5151\n","Epoch 4/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.7361 - accuracy: 0.6549 - val_loss: 0.8418 - val_accuracy: 0.5172\n","Epoch 5/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7355 - accuracy: 0.6546 - val_loss: 0.8466 - val_accuracy: 0.5162\n","Epoch 6/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7373 - accuracy: 0.6541 - val_loss: 0.8464 - val_accuracy: 0.5162\n","Epoch 7/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7304 - accuracy: 0.6482 - val_loss: 0.8461 - val_accuracy: 0.5162\n","Epoch 8/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.7287 - accuracy: 0.6600 - val_loss: 0.8352 - val_accuracy: 0.5248\n","Epoch 9/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.7261 - accuracy: 0.6562 - val_loss: 0.8327 - val_accuracy: 0.5291\n","Epoch 10/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.7257 - accuracy: 0.6544 - val_loss: 0.8308 - val_accuracy: 0.5377\n","Epoch 11/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.7212 - accuracy: 0.6700 - val_loss: 0.8239 - val_accuracy: 0.5496\n","Epoch 12/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7256 - accuracy: 0.6598 - val_loss: 0.8439 - val_accuracy: 0.5312\n","Epoch 13/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.7188 - accuracy: 0.6614 - val_loss: 0.8201 - val_accuracy: 0.5614\n","Epoch 14/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7190 - accuracy: 0.6584 - val_loss: 0.8349 - val_accuracy: 0.5496\n","Epoch 15/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7143 - accuracy: 0.6624 - val_loss: 0.8351 - val_accuracy: 0.5550\n","Epoch 16/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.7166 - accuracy: 0.6514 - val_loss: 0.8307 - val_accuracy: 0.5571\n","Epoch 17/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.7149 - accuracy: 0.6611 - val_loss: 0.8141 - val_accuracy: 0.5948\n","Epoch 18/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7144 - accuracy: 0.6576 - val_loss: 0.8112 - val_accuracy: 0.5841\n","Epoch 19/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7078 - accuracy: 0.6708 - val_loss: 0.8281 - val_accuracy: 0.5711\n","Epoch 20/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.7107 - accuracy: 0.6695 - val_loss: 0.8115 - val_accuracy: 0.6013\n","Epoch 21/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.7078 - accuracy: 0.6681 - val_loss: 0.8167 - val_accuracy: 0.5927\n","Epoch 22/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.7065 - accuracy: 0.6622 - val_loss: 0.8134 - val_accuracy: 0.6056\n","Epoch 23/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.7024 - accuracy: 0.6756 - val_loss: 0.8129 - val_accuracy: 0.6088\n","Epoch 24/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6961 - accuracy: 0.6751 - val_loss: 0.8115 - val_accuracy: 0.6002\n","Epoch 25/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6959 - accuracy: 0.6740 - val_loss: 0.8164 - val_accuracy: 0.5991\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6943 - accuracy: 0.6716 - val_loss: 0.8200 - val_accuracy: 0.5970\n","Epoch 27/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6963 - accuracy: 0.6633 - val_loss: 0.8189 - val_accuracy: 0.6056\n","Epoch 28/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6975 - accuracy: 0.6638 - val_loss: 0.8342 - val_accuracy: 0.5916\n","Epoch 29/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6910 - accuracy: 0.6751 - val_loss: 0.8262 - val_accuracy: 0.5981\n","Epoch 30/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6912 - accuracy: 0.6721 - val_loss: 0.8366 - val_accuracy: 0.6067\n","Epoch 31/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6939 - accuracy: 0.6735 - val_loss: 0.8418 - val_accuracy: 0.5819\n","Epoch 32/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6845 - accuracy: 0.6800 - val_loss: 0.8303 - val_accuracy: 0.5905\n","Epoch 33/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.6860 - accuracy: 0.6751 - val_loss: 0.8300 - val_accuracy: 0.6099\n","Epoch 34/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6815 - accuracy: 0.6816 - val_loss: 0.8465 - val_accuracy: 0.6024\n","Epoch 35/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6816 - accuracy: 0.6751 - val_loss: 0.8300 - val_accuracy: 0.6045\n","Epoch 36/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6811 - accuracy: 0.6821 - val_loss: 0.8435 - val_accuracy: 0.5991\n","Epoch 37/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6808 - accuracy: 0.6721 - val_loss: 0.8316 - val_accuracy: 0.6056\n","Epoch 38/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6803 - accuracy: 0.6727 - val_loss: 0.8408 - val_accuracy: 0.6034\n","Epoch 39/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6819 - accuracy: 0.6686 - val_loss: 0.8509 - val_accuracy: 0.5819\n","Epoch 40/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6729 - accuracy: 0.6832 - val_loss: 0.8979 - val_accuracy: 0.5463\n","Epoch 41/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6722 - accuracy: 0.6837 - val_loss: 0.8500 - val_accuracy: 0.5991\n","Epoch 42/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6694 - accuracy: 0.6802 - val_loss: 0.8490 - val_accuracy: 0.6088\n","Epoch 43/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.6691 - accuracy: 0.6816 - val_loss: 0.8420 - val_accuracy: 0.6121\n","Epoch 44/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6738 - accuracy: 0.6754 - val_loss: 0.8447 - val_accuracy: 0.6034\n","Epoch 45/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6697 - accuracy: 0.6827 - val_loss: 0.8587 - val_accuracy: 0.5744\n","Epoch 46/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.6738 - accuracy: 0.6808 - val_loss: 0.8516 - val_accuracy: 0.5797\n","Epoch 47/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6628 - accuracy: 0.6878 - val_loss: 0.8414 - val_accuracy: 0.6078\n","Epoch 48/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6645 - accuracy: 0.6821 - val_loss: 0.8583 - val_accuracy: 0.6002\n","Epoch 49/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6661 - accuracy: 0.6786 - val_loss: 0.8667 - val_accuracy: 0.6034\n","Epoch 50/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6577 - accuracy: 0.6894 - val_loss: 0.8590 - val_accuracy: 0.5991\n","Epoch 51/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6573 - accuracy: 0.6878 - val_loss: 0.8501 - val_accuracy: 0.6002\n","Epoch 52/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6599 - accuracy: 0.6835 - val_loss: 0.8697 - val_accuracy: 0.6024\n","Epoch 53/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6593 - accuracy: 0.6816 - val_loss: 0.8782 - val_accuracy: 0.5657\n","Epoch 54/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6526 - accuracy: 0.6945 - val_loss: 0.8846 - val_accuracy: 0.5679\n","Epoch 55/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6512 - accuracy: 0.6859 - val_loss: 0.8603 - val_accuracy: 0.6078\n","Epoch 56/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6530 - accuracy: 0.6940 - val_loss: 0.8765 - val_accuracy: 0.5668\n","Epoch 57/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6503 - accuracy: 0.6940 - val_loss: 0.8481 - val_accuracy: 0.5959\n","Epoch 58/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6485 - accuracy: 0.6910 - val_loss: 0.8524 - val_accuracy: 0.5959\n","Epoch 59/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6477 - accuracy: 0.6945 - val_loss: 0.8745 - val_accuracy: 0.6045\n","Epoch 60/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6525 - accuracy: 0.6810 - val_loss: 0.8753 - val_accuracy: 0.5744\n","Epoch 61/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6414 - accuracy: 0.6969 - val_loss: 0.8532 - val_accuracy: 0.6002\n","Epoch 62/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6491 - accuracy: 0.6959 - val_loss: 0.8564 - val_accuracy: 0.5991\n","Epoch 63/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6463 - accuracy: 0.6897 - val_loss: 0.8874 - val_accuracy: 0.5819\n","Epoch 64/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6548 - accuracy: 0.6859 - val_loss: 0.9387 - val_accuracy: 0.5485\n","Epoch 65/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6458 - accuracy: 0.6905 - val_loss: 0.8599 - val_accuracy: 0.5841\n","Epoch 66/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6423 - accuracy: 0.6983 - val_loss: 0.8900 - val_accuracy: 0.5614\n","Epoch 67/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6366 - accuracy: 0.6980 - val_loss: 0.8982 - val_accuracy: 0.5625\n","Epoch 68/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6374 - accuracy: 0.7007 - val_loss: 0.8631 - val_accuracy: 0.6088\n","Epoch 69/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6352 - accuracy: 0.6988 - val_loss: 0.8553 - val_accuracy: 0.6013\n","Epoch 70/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6312 - accuracy: 0.7002 - val_loss: 0.8743 - val_accuracy: 0.5787\n","Epoch 71/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6301 - accuracy: 0.7055 - val_loss: 0.8621 - val_accuracy: 0.6013\n","Epoch 72/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6252 - accuracy: 0.7055 - val_loss: 0.8569 - val_accuracy: 0.5916\n","Epoch 73/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6241 - accuracy: 0.7069 - val_loss: 0.8710 - val_accuracy: 0.6024\n","Epoch 74/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6303 - accuracy: 0.6969 - val_loss: 0.8666 - val_accuracy: 0.6088\n","Epoch 75/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6187 - accuracy: 0.7152 - val_loss: 0.8662 - val_accuracy: 0.6110\n","Epoch 76/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6290 - accuracy: 0.7023 - val_loss: 0.8719 - val_accuracy: 0.6024\n","Epoch 77/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6214 - accuracy: 0.7077 - val_loss: 0.9202 - val_accuracy: 0.5593\n","Epoch 78/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6192 - accuracy: 0.7069 - val_loss: 0.8789 - val_accuracy: 0.6067\n","Epoch 79/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6174 - accuracy: 0.7082 - val_loss: 0.8980 - val_accuracy: 0.5711\n","Epoch 80/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6204 - accuracy: 0.7091 - val_loss: 0.9691 - val_accuracy: 0.5560\n","Epoch 81/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6238 - accuracy: 0.7034 - val_loss: 0.8680 - val_accuracy: 0.5981\n","Epoch 82/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6188 - accuracy: 0.7109 - val_loss: 0.8890 - val_accuracy: 0.5948\n","Epoch 83/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6179 - accuracy: 0.7120 - val_loss: 0.8773 - val_accuracy: 0.5991\n","Epoch 84/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6106 - accuracy: 0.7080 - val_loss: 0.8795 - val_accuracy: 0.6045\n","Epoch 85/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6116 - accuracy: 0.7206 - val_loss: 0.8839 - val_accuracy: 0.6002\n","Epoch 86/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6146 - accuracy: 0.7058 - val_loss: 0.8929 - val_accuracy: 0.5916\n","Epoch 87/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6115 - accuracy: 0.7107 - val_loss: 0.8862 - val_accuracy: 0.5970\n","Epoch 88/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6105 - accuracy: 0.7161 - val_loss: 0.9462 - val_accuracy: 0.5528\n","Epoch 89/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6079 - accuracy: 0.7201 - val_loss: 0.8989 - val_accuracy: 0.6002\n","Epoch 90/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6045 - accuracy: 0.7150 - val_loss: 0.8966 - val_accuracy: 0.6078\n","Epoch 91/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6114 - accuracy: 0.7031 - val_loss: 0.8924 - val_accuracy: 0.5948\n","Epoch 92/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6076 - accuracy: 0.7212 - val_loss: 0.9206 - val_accuracy: 0.5808\n","Epoch 93/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6008 - accuracy: 0.7115 - val_loss: 0.8901 - val_accuracy: 0.6034\n","Epoch 94/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6031 - accuracy: 0.7201 - val_loss: 0.8996 - val_accuracy: 0.6045\n","Epoch 95/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5958 - accuracy: 0.7206 - val_loss: 0.9030 - val_accuracy: 0.6078\n","Epoch 96/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6013 - accuracy: 0.7169 - val_loss: 0.9027 - val_accuracy: 0.6078\n","Epoch 97/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6002 - accuracy: 0.7201 - val_loss: 0.9039 - val_accuracy: 0.5938\n","Epoch 98/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5961 - accuracy: 0.7193 - val_loss: 0.9109 - val_accuracy: 0.6078\n","Epoch 99/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5990 - accuracy: 0.7155 - val_loss: 0.9189 - val_accuracy: 0.6067\n","Epoch 100/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5980 - accuracy: 0.7276 - val_loss: 0.9117 - val_accuracy: 0.5981\n","{'loss': [0.7551542520523071, 0.7456457614898682, 0.7477822303771973, 0.7361032366752625, 0.7354834079742432, 0.7372932434082031, 0.7303541898727417, 0.7286891341209412, 0.7260722517967224, 0.7256872653961182, 0.721154510974884, 0.7256060242652893, 0.7188102006912231, 0.7189711332321167, 0.7142550349235535, 0.7166462540626526, 0.7149490118026733, 0.7144275307655334, 0.7077932953834534, 0.7107164859771729, 0.70775306224823, 0.706500768661499, 0.7023723721504211, 0.696124255657196, 0.6958868503570557, 0.6942962408065796, 0.6963162422180176, 0.6974791884422302, 0.69101881980896, 0.691157877445221, 0.6939197182655334, 0.6844508647918701, 0.6859526038169861, 0.6815094947814941, 0.6816368103027344, 0.6811040043830872, 0.6807623505592346, 0.6803156733512878, 0.6819469332695007, 0.6729112267494202, 0.6722022891044617, 0.6694130301475525, 0.6691257953643799, 0.6738129258155823, 0.6696804761886597, 0.6738153696060181, 0.6628343462944031, 0.6645115613937378, 0.6660847067832947, 0.6576833128929138, 0.657280683517456, 0.6598740816116333, 0.6593461036682129, 0.6526031494140625, 0.6511871218681335, 0.653025209903717, 0.6503123641014099, 0.6484970450401306, 0.6477059125900269, 0.6525471210479736, 0.6414415836334229, 0.6491066217422485, 0.6462849974632263, 0.6547937989234924, 0.6457567811012268, 0.642326831817627, 0.6365770101547241, 0.6374000906944275, 0.6352019309997559, 0.6311920881271362, 0.6300575137138367, 0.6251682043075562, 0.6241366863250732, 0.6303183436393738, 0.6187402009963989, 0.6289715766906738, 0.6214488744735718, 0.6192211508750916, 0.617416501045227, 0.6203741431236267, 0.6237972378730774, 0.6187769770622253, 0.6179009079933167, 0.6106422543525696, 0.6116045117378235, 0.6145862936973572, 0.6115329265594482, 0.6104766130447388, 0.6079084277153015, 0.6044780611991882, 0.6114413738250732, 0.6076251268386841, 0.6007893681526184, 0.6030632257461548, 0.5957795977592468, 0.6013083457946777, 0.6001993417739868, 0.5961026549339294, 0.5990006923675537, 0.5979560613632202], 'accuracy': [0.6357758641242981, 0.6449353694915771, 0.640625, 0.654902994632721, 0.654633641242981, 0.6540948152542114, 0.6481680870056152, 0.6600215435028076, 0.65625, 0.6543642282485962, 0.6699892282485962, 0.6597521305084229, 0.6613685488700867, 0.6584051847457886, 0.662446141242981, 0.6514008641242981, 0.6610991358757019, 0.657597005367279, 0.6707974076271057, 0.6694504022598267, 0.6681034564971924, 0.6621767282485962, 0.6756465435028076, 0.6751077771186829, 0.6740301847457886, 0.6716055870056152, 0.6632543206214905, 0.6637930870056152, 0.6751077771186829, 0.6721444129943848, 0.673491358757019, 0.6799569129943848, 0.6751077771186829, 0.6815732717514038, 0.6751077771186829, 0.6821120977401733, 0.6721444129943848, 0.6726831793785095, 0.6686422228813171, 0.6831896305084229, 0.6837284564971924, 0.6802262663841248, 0.6815732717514038, 0.6753771305084229, 0.6826508641242981, 0.6807650923728943, 0.6877694129943848, 0.6821120977401733, 0.6786099076271057, 0.6893857717514038, 0.6877694129943848, 0.6834590435028076, 0.6815732717514038, 0.6945043206214905, 0.685883641242981, 0.693965494632721, 0.693965494632721, 0.6910021305084229, 0.6945043206214905, 0.681034505367279, 0.696928858757019, 0.6958512663841248, 0.6896551847457886, 0.685883641242981, 0.6904633641242981, 0.6982758641242981, 0.6980064511299133, 0.7007004022598267, 0.6988146305084229, 0.7001616358757019, 0.7055495977401733, 0.7055495977401733, 0.7068965435028076, 0.696928858757019, 0.7152478694915771, 0.7023168206214905, 0.7077047228813171, 0.7068965435028076, 0.7082435488700867, 0.7090517282485962, 0.7033944129943848, 0.7109375, 0.7120150923728943, 0.7079741358757019, 0.7206357717514038, 0.7058189511299133, 0.7106680870056152, 0.7160560488700867, 0.720097005367279, 0.7149784564971924, 0.703125, 0.7211745977401733, 0.7114762663841248, 0.720097005367279, 0.7206357717514038, 0.7168642282485962, 0.720097005367279, 0.7192887663841248, 0.7155172228813171, 0.7276400923728943], 'val_loss': [0.8506861925125122, 0.8631083965301514, 0.8457995653152466, 0.8418095111846924, 0.8466337323188782, 0.8463882207870483, 0.8461381793022156, 0.8351547718048096, 0.832661509513855, 0.830786406993866, 0.8238519430160522, 0.8438500165939331, 0.8200624585151672, 0.8348553776741028, 0.8350703120231628, 0.8307020664215088, 0.8141279816627502, 0.8111680746078491, 0.8281117081642151, 0.8114572763442993, 0.8167033195495605, 0.8133570551872253, 0.8128690123558044, 0.8115430474281311, 0.8163553476333618, 0.819960355758667, 0.8189029693603516, 0.834230899810791, 0.8262213468551636, 0.8366256952285767, 0.8417930603027344, 0.8302523493766785, 0.8300028443336487, 0.8465327024459839, 0.8299968838691711, 0.8435317873954773, 0.8315861821174622, 0.8408024311065674, 0.8508979082107544, 0.8978593945503235, 0.8500082492828369, 0.8490319848060608, 0.841979444026947, 0.8447086811065674, 0.858704686164856, 0.851648211479187, 0.8414164185523987, 0.8582808971405029, 0.8667325377464294, 0.8590439558029175, 0.8501270413398743, 0.8696534633636475, 0.8782432675361633, 0.8845608830451965, 0.8602757453918457, 0.8764888644218445, 0.8480668663978577, 0.852368175983429, 0.8744744658470154, 0.8753358721733093, 0.8531690835952759, 0.8564205169677734, 0.8874027132987976, 0.9386647343635559, 0.8599192500114441, 0.889970600605011, 0.8981980085372925, 0.8631353974342346, 0.8552861213684082, 0.8743012547492981, 0.8620831966400146, 0.8569281101226807, 0.8709607720375061, 0.8665925860404968, 0.8661691546440125, 0.8719038367271423, 0.9201900362968445, 0.8789246082305908, 0.8979749083518982, 0.9691368341445923, 0.8679834008216858, 0.8890261650085449, 0.8772580623626709, 0.8794543743133545, 0.8839257955551147, 0.8928686380386353, 0.8862268924713135, 0.9462108612060547, 0.8988724946975708, 0.8965650796890259, 0.892413318157196, 0.9205683469772339, 0.8900697827339172, 0.8995957374572754, 0.9029994010925293, 0.9027125835418701, 0.903876543045044, 0.9108790159225464, 0.9189163446426392, 0.9116898775100708], 'val_accuracy': [0.5150862336158752, 0.5150862336158752, 0.5150862336158752, 0.517241358757019, 0.5161637663841248, 0.5161637663841248, 0.5161637663841248, 0.524784505367279, 0.5290948152542114, 0.537715494632721, 0.5495689511299133, 0.53125, 0.5614224076271057, 0.5495689511299133, 0.5549569129943848, 0.5571120977401733, 0.5948275923728943, 0.5840517282485962, 0.5711206793785095, 0.6012930870056152, 0.5926724076271057, 0.6056034564971924, 0.6088362336158752, 0.600215494632721, 0.5991379022598267, 0.5969827771186829, 0.6056034564971924, 0.5915948152542114, 0.5980603694915771, 0.6066810488700867, 0.5818965435028076, 0.5905172228813171, 0.6099137663841248, 0.6023706793785095, 0.6045258641242981, 0.5991379022598267, 0.6056034564971924, 0.6034482717514038, 0.5818965435028076, 0.5463362336158752, 0.5991379022598267, 0.6088362336158752, 0.6120689511299133, 0.6034482717514038, 0.5743534564971924, 0.579741358757019, 0.607758641242981, 0.600215494632721, 0.6034482717514038, 0.5991379022598267, 0.600215494632721, 0.6023706793785095, 0.5657327771186829, 0.5678879022598267, 0.607758641242981, 0.5668103694915771, 0.5959051847457886, 0.5959051847457886, 0.6045258641242981, 0.5743534564971924, 0.600215494632721, 0.5991379022598267, 0.5818965435028076, 0.548491358757019, 0.5840517282485962, 0.5614224076271057, 0.5625, 0.6088362336158752, 0.6012930870056152, 0.5786637663841248, 0.6012930870056152, 0.5915948152542114, 0.6023706793785095, 0.6088362336158752, 0.610991358757019, 0.6023706793785095, 0.5592672228813171, 0.6066810488700867, 0.5711206793785095, 0.556034505367279, 0.5980603694915771, 0.5948275923728943, 0.5991379022598267, 0.6045258641242981, 0.600215494632721, 0.5915948152542114, 0.5969827771186829, 0.5528017282485962, 0.600215494632721, 0.607758641242981, 0.5948275923728943, 0.5808189511299133, 0.6034482717514038, 0.6045258641242981, 0.607758641242981, 0.607758641242981, 0.59375, 0.607758641242981, 0.6066810488700867, 0.5980603694915771]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","27/28 [===========================>..] - ETA: 0s - loss: 0.7500 - accuracy: 0.6415"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["28/28 [==============================] - 9s 98ms/step - loss: 0.7502 - accuracy: 0.6420 - val_loss: 0.8734 - val_accuracy: 0.5045\n","Epoch 2/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.7476 - accuracy: 0.6454 - val_loss: 0.8569 - val_accuracy: 0.5045\n","Epoch 3/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.7406 - accuracy: 0.6505 - val_loss: 0.8699 - val_accuracy: 0.5045\n","Epoch 4/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.7507 - accuracy: 0.6273 - val_loss: 0.8659 - val_accuracy: 0.5045\n","Epoch 5/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.7356 - accuracy: 0.6514 - val_loss: 0.8599 - val_accuracy: 0.5045\n","Epoch 6/100\n","28/28 [==============================] - 1s 34ms/step - loss: 0.7349 - accuracy: 0.6525 - val_loss: 0.8418 - val_accuracy: 0.5079\n","Epoch 7/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.7291 - accuracy: 0.6539 - val_loss: 0.8436 - val_accuracy: 0.5090\n","Epoch 8/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7275 - accuracy: 0.6585 - val_loss: 0.8428 - val_accuracy: 0.5090\n","Epoch 9/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.7276 - accuracy: 0.6497 - val_loss: 0.8385 - val_accuracy: 0.5113\n","Epoch 10/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.7234 - accuracy: 0.6610 - val_loss: 0.8292 - val_accuracy: 0.5226\n","Epoch 11/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7247 - accuracy: 0.6573 - val_loss: 0.8362 - val_accuracy: 0.5158\n","Epoch 12/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.7216 - accuracy: 0.6539 - val_loss: 0.8245 - val_accuracy: 0.5339\n","Epoch 13/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7214 - accuracy: 0.6616 - val_loss: 0.8311 - val_accuracy: 0.5260\n","Epoch 14/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7214 - accuracy: 0.6522 - val_loss: 0.8289 - val_accuracy: 0.5215\n","Epoch 15/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.7139 - accuracy: 0.6653 - val_loss: 0.8231 - val_accuracy: 0.5373\n","Epoch 16/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7127 - accuracy: 0.6672 - val_loss: 0.8171 - val_accuracy: 0.5362\n","Epoch 17/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.7097 - accuracy: 0.6573 - val_loss: 0.7941 - val_accuracy: 0.5939\n","Epoch 18/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7097 - accuracy: 0.6740 - val_loss: 0.8085 - val_accuracy: 0.5566\n","Epoch 19/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7086 - accuracy: 0.6667 - val_loss: 0.8103 - val_accuracy: 0.5520\n","Epoch 20/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7037 - accuracy: 0.6712 - val_loss: 0.8028 - val_accuracy: 0.5690\n","Epoch 21/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7108 - accuracy: 0.6621 - val_loss: 0.7932 - val_accuracy: 0.5848\n","Epoch 22/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7040 - accuracy: 0.6686 - val_loss: 0.8020 - val_accuracy: 0.5814\n","Epoch 23/100\n","28/28 [==============================] - 1s 31ms/step - loss: 0.7043 - accuracy: 0.6627 - val_loss: 0.7919 - val_accuracy: 0.6109\n","Epoch 24/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7029 - accuracy: 0.6669 - val_loss: 0.7927 - val_accuracy: 0.6097\n","Epoch 25/100\n","28/28 [==============================] - 1s 34ms/step - loss: 0.7007 - accuracy: 0.6658 - val_loss: 0.7967 - val_accuracy: 0.6120\n","Epoch 26/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6953 - accuracy: 0.6709 - val_loss: 0.8027 - val_accuracy: 0.6109\n","Epoch 27/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6916 - accuracy: 0.6678 - val_loss: 0.8123 - val_accuracy: 0.6041\n","Epoch 28/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.6896 - accuracy: 0.6737 - val_loss: 0.8097 - val_accuracy: 0.6143\n","Epoch 29/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.6954 - accuracy: 0.6650 - val_loss: 0.8131 - val_accuracy: 0.6041\n","Epoch 30/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6928 - accuracy: 0.6715 - val_loss: 0.8057 - val_accuracy: 0.6075\n","Epoch 31/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6910 - accuracy: 0.6703 - val_loss: 0.8256 - val_accuracy: 0.6052\n","Epoch 32/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6831 - accuracy: 0.6746 - val_loss: 0.8165 - val_accuracy: 0.5950\n","Epoch 33/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6869 - accuracy: 0.6675 - val_loss: 0.8170 - val_accuracy: 0.6052\n","Epoch 34/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6837 - accuracy: 0.6712 - val_loss: 0.8305 - val_accuracy: 0.6143\n","Epoch 35/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.6851 - accuracy: 0.6701 - val_loss: 0.8182 - val_accuracy: 0.6165\n","Epoch 36/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6873 - accuracy: 0.6678 - val_loss: 0.8286 - val_accuracy: 0.6075\n","Epoch 37/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6805 - accuracy: 0.6760 - val_loss: 0.8301 - val_accuracy: 0.6075\n","Epoch 38/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6806 - accuracy: 0.6825 - val_loss: 0.8184 - val_accuracy: 0.6120\n","Epoch 39/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6789 - accuracy: 0.6729 - val_loss: 0.8444 - val_accuracy: 0.5848\n","Epoch 40/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6803 - accuracy: 0.6712 - val_loss: 0.8313 - val_accuracy: 0.6097\n","Epoch 41/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6775 - accuracy: 0.6842 - val_loss: 0.8437 - val_accuracy: 0.5701\n","Epoch 42/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6709 - accuracy: 0.6814 - val_loss: 0.8383 - val_accuracy: 0.6154\n","Epoch 43/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6725 - accuracy: 0.6794 - val_loss: 0.8295 - val_accuracy: 0.6007\n","Epoch 44/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6721 - accuracy: 0.6769 - val_loss: 0.8350 - val_accuracy: 0.5962\n","Epoch 45/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6693 - accuracy: 0.6760 - val_loss: 0.8315 - val_accuracy: 0.6041\n","Epoch 46/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6726 - accuracy: 0.6712 - val_loss: 0.9058 - val_accuracy: 0.5600\n","Epoch 47/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6718 - accuracy: 0.6771 - val_loss: 0.8352 - val_accuracy: 0.5962\n","Epoch 48/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6649 - accuracy: 0.6819 - val_loss: 0.8490 - val_accuracy: 0.5701\n","Epoch 49/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6621 - accuracy: 0.6788 - val_loss: 0.8356 - val_accuracy: 0.6052\n","Epoch 50/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6594 - accuracy: 0.6890 - val_loss: 0.8602 - val_accuracy: 0.5701\n","Epoch 51/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6594 - accuracy: 0.6856 - val_loss: 0.9110 - val_accuracy: 0.5633\n","Epoch 52/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6680 - accuracy: 0.6856 - val_loss: 0.8428 - val_accuracy: 0.6075\n","Epoch 53/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6608 - accuracy: 0.6831 - val_loss: 0.8296 - val_accuracy: 0.5882\n","Epoch 54/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.6643 - accuracy: 0.6904 - val_loss: 0.8412 - val_accuracy: 0.6063\n","Epoch 55/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.6532 - accuracy: 0.6921 - val_loss: 0.8371 - val_accuracy: 0.5950\n","Epoch 56/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6561 - accuracy: 0.6848 - val_loss: 0.8363 - val_accuracy: 0.6029\n","Epoch 57/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6510 - accuracy: 0.6887 - val_loss: 0.8615 - val_accuracy: 0.6097\n","Epoch 58/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6531 - accuracy: 0.6975 - val_loss: 0.8992 - val_accuracy: 0.5769\n","Epoch 59/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6472 - accuracy: 0.6969 - val_loss: 0.8500 - val_accuracy: 0.6086\n","Epoch 60/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6486 - accuracy: 0.6952 - val_loss: 0.8731 - val_accuracy: 0.5645\n","Epoch 61/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6534 - accuracy: 0.6817 - val_loss: 0.8709 - val_accuracy: 0.6075\n","Epoch 62/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6525 - accuracy: 0.6819 - val_loss: 0.8673 - val_accuracy: 0.6120\n","Epoch 63/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6502 - accuracy: 0.6947 - val_loss: 0.8553 - val_accuracy: 0.6086\n","Epoch 64/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6405 - accuracy: 0.6978 - val_loss: 0.8741 - val_accuracy: 0.5973\n","Epoch 65/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6471 - accuracy: 0.6935 - val_loss: 0.8560 - val_accuracy: 0.5962\n","Epoch 66/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6424 - accuracy: 0.6950 - val_loss: 0.8585 - val_accuracy: 0.6063\n","Epoch 67/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6362 - accuracy: 0.6955 - val_loss: 0.8722 - val_accuracy: 0.5950\n","Epoch 68/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6428 - accuracy: 0.6930 - val_loss: 0.9237 - val_accuracy: 0.5656\n","Epoch 69/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6408 - accuracy: 0.6967 - val_loss: 0.8705 - val_accuracy: 0.5701\n","Epoch 70/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6384 - accuracy: 0.6989 - val_loss: 0.9656 - val_accuracy: 0.5577\n","Epoch 71/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.6307 - accuracy: 0.7060 - val_loss: 0.9376 - val_accuracy: 0.5633\n","Epoch 72/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6278 - accuracy: 0.7111 - val_loss: 0.8741 - val_accuracy: 0.6075\n","Epoch 73/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6263 - accuracy: 0.7032 - val_loss: 0.8838 - val_accuracy: 0.6097\n","Epoch 74/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6317 - accuracy: 0.6950 - val_loss: 0.8848 - val_accuracy: 0.6154\n","Epoch 75/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6344 - accuracy: 0.6958 - val_loss: 0.9171 - val_accuracy: 0.5600\n","Epoch 76/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6240 - accuracy: 0.7077 - val_loss: 0.8920 - val_accuracy: 0.6063\n","Epoch 77/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6297 - accuracy: 0.6986 - val_loss: 0.9453 - val_accuracy: 0.5656\n","Epoch 78/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.6263 - accuracy: 0.7029 - val_loss: 0.8902 - val_accuracy: 0.5701\n","Epoch 79/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6220 - accuracy: 0.7029 - val_loss: 0.8935 - val_accuracy: 0.6029\n","Epoch 80/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.6251 - accuracy: 0.7026 - val_loss: 0.9135 - val_accuracy: 0.5747\n","Epoch 81/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6226 - accuracy: 0.7074 - val_loss: 0.9141 - val_accuracy: 0.5826\n","Epoch 82/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6202 - accuracy: 0.7060 - val_loss: 0.8994 - val_accuracy: 0.6075\n","Epoch 83/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6166 - accuracy: 0.7043 - val_loss: 0.9076 - val_accuracy: 0.5633\n","Epoch 84/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6153 - accuracy: 0.7057 - val_loss: 0.8819 - val_accuracy: 0.6109\n","Epoch 85/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6203 - accuracy: 0.7063 - val_loss: 0.9021 - val_accuracy: 0.5860\n","Epoch 86/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6139 - accuracy: 0.7151 - val_loss: 0.8941 - val_accuracy: 0.5995\n","Epoch 87/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6163 - accuracy: 0.7114 - val_loss: 0.9681 - val_accuracy: 0.5600\n","Epoch 88/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6195 - accuracy: 0.7108 - val_loss: 0.8795 - val_accuracy: 0.6029\n","Epoch 89/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6107 - accuracy: 0.7119 - val_loss: 0.9170 - val_accuracy: 0.5950\n","Epoch 90/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6085 - accuracy: 0.7193 - val_loss: 0.9320 - val_accuracy: 0.5984\n","Epoch 91/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6118 - accuracy: 0.7066 - val_loss: 0.9045 - val_accuracy: 0.6041\n","Epoch 92/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6308 - accuracy: 0.6972 - val_loss: 0.9279 - val_accuracy: 0.5871\n","Epoch 93/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6080 - accuracy: 0.7196 - val_loss: 0.9272 - val_accuracy: 0.5928\n","Epoch 94/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6052 - accuracy: 0.7159 - val_loss: 0.9387 - val_accuracy: 0.5588\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6047 - accuracy: 0.7221 - val_loss: 0.8958 - val_accuracy: 0.5769\n","Epoch 96/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6038 - accuracy: 0.7131 - val_loss: 0.9509 - val_accuracy: 0.6041\n","Epoch 97/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6060 - accuracy: 0.7250 - val_loss: 0.9515 - val_accuracy: 0.5894\n","Epoch 98/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6024 - accuracy: 0.7139 - val_loss: 0.9213 - val_accuracy: 0.5962\n","Epoch 99/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6005 - accuracy: 0.7151 - val_loss: 0.9084 - val_accuracy: 0.5667\n","Epoch 100/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6103 - accuracy: 0.7196 - val_loss: 0.9264 - val_accuracy: 0.5622\n","{'loss': [0.7501686811447144, 0.7475681304931641, 0.7405568361282349, 0.7506610155105591, 0.7356104254722595, 0.7348728775978088, 0.7291167974472046, 0.727527916431427, 0.7275963425636292, 0.7234472036361694, 0.7247027158737183, 0.7216233611106873, 0.7213895916938782, 0.721440851688385, 0.7139378190040588, 0.7127354145050049, 0.7097495198249817, 0.7097170948982239, 0.7086219787597656, 0.7037419676780701, 0.7107802629470825, 0.7040455341339111, 0.7042744159698486, 0.7029409408569336, 0.7007241249084473, 0.695349931716919, 0.6916443705558777, 0.6896078586578369, 0.6954498291015625, 0.6927576065063477, 0.6910260319709778, 0.6830620169639587, 0.6868948936462402, 0.6837313771247864, 0.6851043105125427, 0.6872907280921936, 0.6804829239845276, 0.6806113123893738, 0.678862988948822, 0.6803243160247803, 0.6774589419364929, 0.6708696484565735, 0.6724632978439331, 0.672063410282135, 0.6693314909934998, 0.6725801229476929, 0.6718183755874634, 0.6648682951927185, 0.6621339917182922, 0.6593998670578003, 0.6594001054763794, 0.6679779887199402, 0.6608365178108215, 0.6642735600471497, 0.6531861424446106, 0.6560760736465454, 0.6509712934494019, 0.6530961394309998, 0.647236704826355, 0.6485992074012756, 0.6534292697906494, 0.6525290608406067, 0.6502082943916321, 0.6404537558555603, 0.6470770835876465, 0.6424047350883484, 0.6361696720123291, 0.6427667737007141, 0.6408284306526184, 0.6383521556854248, 0.6307024955749512, 0.6277803182601929, 0.6262789368629456, 0.6317194700241089, 0.6343540549278259, 0.6239843368530273, 0.629687488079071, 0.6262624859809875, 0.6220351457595825, 0.6250882148742676, 0.6226491332054138, 0.6202464699745178, 0.6165831089019775, 0.6152998805046082, 0.6202623844146729, 0.6138602495193481, 0.616326630115509, 0.6194628477096558, 0.6107038259506226, 0.6084778904914856, 0.6117757558822632, 0.6307580471038818, 0.6079595685005188, 0.6052189469337463, 0.6047232747077942, 0.6038190126419067, 0.6059936285018921, 0.6024410128593445, 0.6004812717437744, 0.6103240847587585], 'accuracy': [0.6420486569404602, 0.6454442739486694, 0.6505376100540161, 0.627334475517273, 0.651386559009552, 0.6525183916091919, 0.6539332270622253, 0.6584606766700745, 0.649688720703125, 0.6610073447227478, 0.6573287844657898, 0.6539332270622253, 0.6615732908248901, 0.6522354483604431, 0.6652518510818481, 0.6672325730323792, 0.6573287844657898, 0.6740237474441528, 0.6666666865348816, 0.6711941361427307, 0.6621392369270325, 0.6686474084854126, 0.66270512342453, 0.6669496297836304, 0.6658177971839905, 0.6709111332893372, 0.6677985191345215, 0.673740804195404, 0.6649688482284546, 0.6714770793914795, 0.6703452467918396, 0.6745896935462952, 0.6675155758857727, 0.6711941361427307, 0.670062243938446, 0.6677985191345215, 0.6760045289993286, 0.6825127601623535, 0.6728919148445129, 0.6711941361427307, 0.6842105388641357, 0.6813808679580688, 0.6794000864028931, 0.6768534183502197, 0.6760045289993286, 0.6711941361427307, 0.6771363615989685, 0.6819468140602112, 0.6788341999053955, 0.6890209317207336, 0.6856253743171692, 0.6856253743171692, 0.6830786466598511, 0.6904357671737671, 0.6921335458755493, 0.6847764849662781, 0.6887379884719849, 0.6975098848342896, 0.696943998336792, 0.695246160030365, 0.6816638112068176, 0.6819468140602112, 0.6946802735328674, 0.6977928876876831, 0.6935483813285828, 0.6949632167816162, 0.6955291628837585, 0.6929824352264404, 0.6966609954833984, 0.698924720287323, 0.7059988975524902, 0.7110922336578369, 0.7031692266464233, 0.6949632167816162, 0.6958121061325073, 0.7076966762542725, 0.6986417770385742, 0.7028862237930298, 0.7028862237930298, 0.702603280544281, 0.7074136734008789, 0.7059988975524902, 0.7043010592460632, 0.7057158946990967, 0.706281840801239, 0.7150537371635437, 0.7113752365112305, 0.7108092904090881, 0.711941123008728, 0.719298243522644, 0.7065647840499878, 0.6972269415855408, 0.7195811867713928, 0.7159026861190796, 0.7221279144287109, 0.7130730152130127, 0.7249575257301331, 0.7139219045639038, 0.7150537371635437, 0.7195811867713928], 'val_loss': [0.8734297156333923, 0.8569199442863464, 0.8699089288711548, 0.865879476070404, 0.8598864078521729, 0.8417789340019226, 0.8435519337654114, 0.8427748084068298, 0.8385019898414612, 0.8291980028152466, 0.8361534476280212, 0.8245452046394348, 0.8310920000076294, 0.8289008736610413, 0.8230750560760498, 0.8170825242996216, 0.7941445112228394, 0.8085426092147827, 0.8102644085884094, 0.8027775883674622, 0.7932166457176208, 0.8019539713859558, 0.7919396162033081, 0.792659342288971, 0.7967184782028198, 0.8027015328407288, 0.8122697472572327, 0.8096595406532288, 0.8130821585655212, 0.8057140707969666, 0.8256369233131409, 0.8165371417999268, 0.8169643878936768, 0.8304620981216431, 0.8181610107421875, 0.8286260962486267, 0.8301430940628052, 0.8183571696281433, 0.8444466590881348, 0.8313435912132263, 0.8436595797538757, 0.8383323550224304, 0.8294979929924011, 0.8350247144699097, 0.8314745426177979, 0.9058097004890442, 0.8352027535438538, 0.8490304350852966, 0.8356241583824158, 0.8602102994918823, 0.9109827280044556, 0.8427644371986389, 0.8296433687210083, 0.8412442207336426, 0.8371403217315674, 0.8362859487533569, 0.8614993095397949, 0.8992331027984619, 0.8500498533248901, 0.8731375932693481, 0.870895266532898, 0.8673123121261597, 0.8552951812744141, 0.8741122484207153, 0.856018602848053, 0.8585131168365479, 0.8722276091575623, 0.9236701726913452, 0.8704613447189331, 0.9655541181564331, 0.9376439452171326, 0.8741089105606079, 0.8837636113166809, 0.8848435878753662, 0.917117714881897, 0.892015814781189, 0.9452588558197021, 0.8902367353439331, 0.8935114145278931, 0.9134858250617981, 0.9141170382499695, 0.8993832468986511, 0.9075756669044495, 0.8818579912185669, 0.9021103978157043, 0.8940590620040894, 0.9681390523910522, 0.8795008063316345, 0.917030930519104, 0.9319697618484497, 0.9045283198356628, 0.9278761148452759, 0.9271818995475769, 0.9387317895889282, 0.8957501649856567, 0.9508979320526123, 0.9515278935432434, 0.9212820529937744, 0.9084451794624329, 0.9263923168182373], 'val_accuracy': [0.5045248866081238, 0.5045248866081238, 0.5045248866081238, 0.5045248866081238, 0.5045248866081238, 0.5079185366630554, 0.5090497732162476, 0.5090497732162476, 0.5113122463226318, 0.5226244330406189, 0.5158371329307556, 0.5339366793632507, 0.5260180830955505, 0.5214931964874268, 0.5373303294181824, 0.5361990928649902, 0.5938913822174072, 0.5565611124038696, 0.5520362257957458, 0.5690045356750488, 0.5848416090011597, 0.581447958946228, 0.610859751701355, 0.6097285151481628, 0.6119909286499023, 0.610859751701355, 0.6040723919868469, 0.6142534017562866, 0.6040723919868469, 0.6074660420417786, 0.6052036285400391, 0.5950226187705994, 0.6052036285400391, 0.6142534017562866, 0.6165158152580261, 0.6074660420417786, 0.6074660420417786, 0.6119909286499023, 0.5848416090011597, 0.6097285151481628, 0.570135772228241, 0.6153846383094788, 0.6006787419319153, 0.5961538553237915, 0.6040723919868469, 0.5599547624588013, 0.5961538553237915, 0.570135772228241, 0.6052036285400391, 0.570135772228241, 0.5633484125137329, 0.6074660420417786, 0.5882353186607361, 0.6063348650932312, 0.5950226187705994, 0.6029411554336548, 0.6097285151481628, 0.5769230723381042, 0.6085972785949707, 0.564479649066925, 0.6074660420417786, 0.6119909286499023, 0.6085972785949707, 0.5972850918769836, 0.5961538553237915, 0.6063348650932312, 0.5950226187705994, 0.5656108856201172, 0.570135772228241, 0.557692289352417, 0.5633484125137329, 0.6074660420417786, 0.6097285151481628, 0.6153846383094788, 0.5599547624588013, 0.6063348650932312, 0.5656108856201172, 0.570135772228241, 0.6029411554336548, 0.5746606588363647, 0.5825791954994202, 0.6074660420417786, 0.5633484125137329, 0.610859751701355, 0.5859728455543518, 0.5995475053787231, 0.5599547624588013, 0.6029411554336548, 0.5950226187705994, 0.598416268825531, 0.6040723919868469, 0.587104082107544, 0.5927602052688599, 0.5588235259056091, 0.5769230723381042, 0.6040723919868469, 0.5893664956092834, 0.5961538553237915, 0.5667420625686646, 0.5622171759605408]}\n","45/45 [==============================] - 1s 6ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","30/31 [============================>.] - ETA: 0s - loss: 0.7555 - accuracy: 0.6328"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["31/31 [==============================] - 9s 58ms/step - loss: 0.7557 - accuracy: 0.6328 - val_loss: 0.8604 - val_accuracy: 0.5145\n","Epoch 2/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7484 - accuracy: 0.6432 - val_loss: 0.8593 - val_accuracy: 0.5134\n","Epoch 3/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7436 - accuracy: 0.6488 - val_loss: 0.8487 - val_accuracy: 0.5145\n","Epoch 4/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7431 - accuracy: 0.6468 - val_loss: 0.8497 - val_accuracy: 0.5145\n","Epoch 5/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.7377 - accuracy: 0.6506 - val_loss: 0.8528 - val_accuracy: 0.5155\n","Epoch 6/100\n","31/31 [==============================] - 1s 39ms/step - loss: 0.7382 - accuracy: 0.6413 - val_loss: 0.8522 - val_accuracy: 0.5176\n","Epoch 7/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.7349 - accuracy: 0.6527 - val_loss: 0.8410 - val_accuracy: 0.5186\n","Epoch 8/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7343 - accuracy: 0.6432 - val_loss: 0.8456 - val_accuracy: 0.5176\n","Epoch 9/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.7435 - accuracy: 0.6527 - val_loss: 0.8308 - val_accuracy: 0.5258\n","Epoch 10/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.7292 - accuracy: 0.6512 - val_loss: 0.8230 - val_accuracy: 0.5393\n","Epoch 11/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7290 - accuracy: 0.6452 - val_loss: 0.8321 - val_accuracy: 0.5320\n","Epoch 12/100\n","31/31 [==============================] - 3s 95ms/step - loss: 0.7225 - accuracy: 0.6475 - val_loss: 0.8200 - val_accuracy: 0.5486\n","Epoch 13/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7237 - accuracy: 0.6568 - val_loss: 0.8226 - val_accuracy: 0.5444\n","Epoch 14/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.7247 - accuracy: 0.6491 - val_loss: 0.8104 - val_accuracy: 0.5558\n","Epoch 15/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7265 - accuracy: 0.6527 - val_loss: 0.8212 - val_accuracy: 0.5496\n","Epoch 16/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.7256 - accuracy: 0.6457 - val_loss: 0.8018 - val_accuracy: 0.5682\n","Epoch 17/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7134 - accuracy: 0.6556 - val_loss: 0.8047 - val_accuracy: 0.5610\n","Epoch 18/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7162 - accuracy: 0.6625 - val_loss: 0.8111 - val_accuracy: 0.5661\n","Epoch 19/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7067 - accuracy: 0.6636 - val_loss: 0.8040 - val_accuracy: 0.5599\n","Epoch 20/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7142 - accuracy: 0.6519 - val_loss: 0.8129 - val_accuracy: 0.5651\n","Epoch 21/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7119 - accuracy: 0.6581 - val_loss: 0.8104 - val_accuracy: 0.5506\n","Epoch 22/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7147 - accuracy: 0.6556 - val_loss: 0.8148 - val_accuracy: 0.5599\n","Epoch 23/100\n","31/31 [==============================] - 1s 30ms/step - loss: 0.7074 - accuracy: 0.6561 - val_loss: 0.8177 - val_accuracy: 0.5713\n","Epoch 24/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7019 - accuracy: 0.6618 - val_loss: 0.8245 - val_accuracy: 0.5620\n","Epoch 25/100\n","31/31 [==============================] - 1s 31ms/step - loss: 0.7051 - accuracy: 0.6677 - val_loss: 0.8268 - val_accuracy: 0.5754\n","Epoch 26/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7017 - accuracy: 0.6669 - val_loss: 0.8475 - val_accuracy: 0.5486\n","Epoch 27/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6977 - accuracy: 0.6633 - val_loss: 0.8609 - val_accuracy: 0.5599\n","Epoch 28/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.7028 - accuracy: 0.6587 - val_loss: 0.8594 - val_accuracy: 0.5775\n","Epoch 29/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7005 - accuracy: 0.6646 - val_loss: 0.8633 - val_accuracy: 0.5713\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6942 - accuracy: 0.6700 - val_loss: 0.8515 - val_accuracy: 0.5610\n","Epoch 31/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6912 - accuracy: 0.6633 - val_loss: 0.8640 - val_accuracy: 0.5620\n","Epoch 32/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6904 - accuracy: 0.6651 - val_loss: 0.8492 - val_accuracy: 0.5579\n","Epoch 33/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6869 - accuracy: 0.6579 - val_loss: 0.8801 - val_accuracy: 0.5610\n","Epoch 34/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6923 - accuracy: 0.6607 - val_loss: 0.8635 - val_accuracy: 0.5775\n","Epoch 35/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6943 - accuracy: 0.6680 - val_loss: 0.8542 - val_accuracy: 0.5589\n","Epoch 36/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6871 - accuracy: 0.6602 - val_loss: 0.8513 - val_accuracy: 0.5723\n","Epoch 37/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6827 - accuracy: 0.6729 - val_loss: 0.8711 - val_accuracy: 0.5682\n","Epoch 38/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6840 - accuracy: 0.6677 - val_loss: 0.8595 - val_accuracy: 0.5671\n","Epoch 39/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6868 - accuracy: 0.6620 - val_loss: 0.8799 - val_accuracy: 0.5630\n","Epoch 40/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6842 - accuracy: 0.6607 - val_loss: 0.9087 - val_accuracy: 0.5300\n","Epoch 41/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6763 - accuracy: 0.6713 - val_loss: 0.8948 - val_accuracy: 0.5671\n","Epoch 42/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6832 - accuracy: 0.6747 - val_loss: 0.8793 - val_accuracy: 0.5692\n","Epoch 43/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6811 - accuracy: 0.6630 - val_loss: 0.9160 - val_accuracy: 0.5465\n","Epoch 44/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6779 - accuracy: 0.6695 - val_loss: 0.8636 - val_accuracy: 0.5661\n","Epoch 45/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6802 - accuracy: 0.6661 - val_loss: 0.9053 - val_accuracy: 0.5517\n","Epoch 46/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6744 - accuracy: 0.6778 - val_loss: 0.8731 - val_accuracy: 0.5620\n","Epoch 47/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6695 - accuracy: 0.6744 - val_loss: 0.8759 - val_accuracy: 0.5671\n","Epoch 48/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.6764 - accuracy: 0.6698 - val_loss: 0.9092 - val_accuracy: 0.5640\n","Epoch 49/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6758 - accuracy: 0.6747 - val_loss: 0.9199 - val_accuracy: 0.5496\n","Epoch 50/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6684 - accuracy: 0.6693 - val_loss: 0.9104 - val_accuracy: 0.5527\n","Epoch 51/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.6676 - accuracy: 0.6780 - val_loss: 0.8808 - val_accuracy: 0.5682\n","Epoch 52/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6661 - accuracy: 0.6853 - val_loss: 0.8659 - val_accuracy: 0.5527\n","Epoch 53/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6669 - accuracy: 0.6726 - val_loss: 0.8700 - val_accuracy: 0.5620\n","Epoch 54/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6621 - accuracy: 0.6783 - val_loss: 0.9018 - val_accuracy: 0.5475\n","Epoch 55/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6651 - accuracy: 0.6760 - val_loss: 0.8841 - val_accuracy: 0.5682\n","Epoch 56/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6588 - accuracy: 0.6817 - val_loss: 0.8866 - val_accuracy: 0.5558\n","Epoch 57/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6667 - accuracy: 0.6773 - val_loss: 0.8862 - val_accuracy: 0.5620\n","Epoch 58/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6613 - accuracy: 0.6773 - val_loss: 0.8985 - val_accuracy: 0.5723\n","Epoch 59/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6536 - accuracy: 0.6868 - val_loss: 0.9122 - val_accuracy: 0.5517\n","Epoch 60/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6605 - accuracy: 0.6814 - val_loss: 0.9212 - val_accuracy: 0.5434\n","Epoch 61/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6530 - accuracy: 0.6897 - val_loss: 0.9080 - val_accuracy: 0.5651\n","Epoch 62/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6575 - accuracy: 0.6860 - val_loss: 0.8891 - val_accuracy: 0.5702\n","Epoch 63/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6520 - accuracy: 0.6811 - val_loss: 0.8872 - val_accuracy: 0.5599\n","Epoch 64/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6495 - accuracy: 0.6848 - val_loss: 0.8972 - val_accuracy: 0.5702\n","Epoch 65/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6471 - accuracy: 0.6938 - val_loss: 0.9385 - val_accuracy: 0.5475\n","Epoch 66/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6449 - accuracy: 0.6938 - val_loss: 0.9031 - val_accuracy: 0.5692\n","Epoch 67/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6445 - accuracy: 0.6868 - val_loss: 0.9907 - val_accuracy: 0.5269\n","Epoch 68/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6416 - accuracy: 0.6879 - val_loss: 0.8937 - val_accuracy: 0.5434\n","Epoch 69/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6503 - accuracy: 0.6866 - val_loss: 0.9118 - val_accuracy: 0.5620\n","Epoch 70/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6394 - accuracy: 0.6935 - val_loss: 0.9081 - val_accuracy: 0.5465\n","Epoch 71/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6447 - accuracy: 0.6928 - val_loss: 0.9170 - val_accuracy: 0.5568\n","Epoch 72/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6376 - accuracy: 0.6935 - val_loss: 0.9198 - val_accuracy: 0.5671\n","Epoch 73/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6479 - accuracy: 0.6902 - val_loss: 0.8912 - val_accuracy: 0.5599\n","Epoch 74/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6425 - accuracy: 0.6912 - val_loss: 0.9042 - val_accuracy: 0.5630\n","Epoch 75/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6436 - accuracy: 0.6941 - val_loss: 0.8777 - val_accuracy: 0.5610\n","Epoch 76/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6384 - accuracy: 0.6974 - val_loss: 0.9122 - val_accuracy: 0.5465\n","Epoch 77/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6376 - accuracy: 0.6904 - val_loss: 0.9261 - val_accuracy: 0.5465\n","Epoch 78/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6373 - accuracy: 0.6966 - val_loss: 0.9119 - val_accuracy: 0.5517\n","Epoch 79/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6330 - accuracy: 0.6941 - val_loss: 0.9495 - val_accuracy: 0.5413\n","Epoch 80/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6355 - accuracy: 0.6891 - val_loss: 0.8934 - val_accuracy: 0.5661\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6308 - accuracy: 0.7028 - val_loss: 0.9324 - val_accuracy: 0.5610\n","Epoch 82/100\n","31/31 [==============================] - 2s 64ms/step - loss: 0.6289 - accuracy: 0.7008 - val_loss: 0.9105 - val_accuracy: 0.5806\n","Epoch 83/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6272 - accuracy: 0.7000 - val_loss: 0.9349 - val_accuracy: 0.5393\n","Epoch 84/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6268 - accuracy: 0.6997 - val_loss: 0.9056 - val_accuracy: 0.5682\n","Epoch 85/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6253 - accuracy: 0.6995 - val_loss: 0.9158 - val_accuracy: 0.5568\n","Epoch 86/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6285 - accuracy: 0.6982 - val_loss: 0.9233 - val_accuracy: 0.5589\n","Epoch 87/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6271 - accuracy: 0.6925 - val_loss: 0.9358 - val_accuracy: 0.5640\n","Epoch 88/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6283 - accuracy: 0.7000 - val_loss: 0.9189 - val_accuracy: 0.5599\n","Epoch 89/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6211 - accuracy: 0.7049 - val_loss: 0.9160 - val_accuracy: 0.5620\n","Epoch 90/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6177 - accuracy: 0.7041 - val_loss: 0.9194 - val_accuracy: 0.5651\n","Epoch 91/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6216 - accuracy: 0.7127 - val_loss: 0.9871 - val_accuracy: 0.5444\n","Epoch 92/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6203 - accuracy: 0.7059 - val_loss: 1.0174 - val_accuracy: 0.5341\n","Epoch 93/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6175 - accuracy: 0.7036 - val_loss: 1.0310 - val_accuracy: 0.5289\n","Epoch 94/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6213 - accuracy: 0.7018 - val_loss: 0.9203 - val_accuracy: 0.5599\n","Epoch 95/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6080 - accuracy: 0.7129 - val_loss: 0.9516 - val_accuracy: 0.5599\n","Epoch 96/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6244 - accuracy: 0.7039 - val_loss: 0.9748 - val_accuracy: 0.5465\n","Epoch 97/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6168 - accuracy: 0.7070 - val_loss: 0.9239 - val_accuracy: 0.5599\n","Epoch 98/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.6160 - accuracy: 0.7137 - val_loss: 0.9253 - val_accuracy: 0.5599\n","Epoch 99/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.6162 - accuracy: 0.7083 - val_loss: 0.9217 - val_accuracy: 0.5599\n","Epoch 100/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.6138 - accuracy: 0.7059 - val_loss: 0.9163 - val_accuracy: 0.5568\n","{'loss': [0.7556919455528259, 0.7484126687049866, 0.7435668706893921, 0.7430524826049805, 0.7376604676246643, 0.7381653189659119, 0.7349014282226562, 0.73430997133255, 0.7434598207473755, 0.7291671633720398, 0.7290037274360657, 0.7224544286727905, 0.7237334251403809, 0.7246781587600708, 0.7265047430992126, 0.7256495356559753, 0.7134256362915039, 0.7161960601806641, 0.706701397895813, 0.7141664624214172, 0.7118982076644897, 0.714706301689148, 0.7073628902435303, 0.7019205689430237, 0.7050846219062805, 0.7017468214035034, 0.6976509690284729, 0.7028124928474426, 0.7005284428596497, 0.6941529512405396, 0.6911730170249939, 0.6904444098472595, 0.6868741512298584, 0.6922767758369446, 0.6943003535270691, 0.6870614886283875, 0.6826835870742798, 0.6840183138847351, 0.6868137121200562, 0.6842007040977478, 0.6763368844985962, 0.6832442283630371, 0.681119441986084, 0.6779431700706482, 0.6802177429199219, 0.6744264960289001, 0.6694575548171997, 0.6763773560523987, 0.6757780909538269, 0.6684439182281494, 0.6675688624382019, 0.6660687327384949, 0.6668661236763, 0.6620964407920837, 0.6651054620742798, 0.658818244934082, 0.6666833162307739, 0.6613413095474243, 0.6536337733268738, 0.6604628562927246, 0.6529813408851624, 0.6574999094009399, 0.6520161628723145, 0.649493396282196, 0.6471180319786072, 0.6448621153831482, 0.6444969773292542, 0.6415829062461853, 0.6502729058265686, 0.639356255531311, 0.6446629166603088, 0.6376343965530396, 0.6478649973869324, 0.6424805521965027, 0.6435701251029968, 0.6383928656578064, 0.637584924697876, 0.6373156905174255, 0.6329894065856934, 0.6355317831039429, 0.6308118104934692, 0.6288563013076782, 0.6272123456001282, 0.6268246173858643, 0.6253350377082825, 0.6284716725349426, 0.6271483898162842, 0.6283455491065979, 0.6211275458335876, 0.6176650524139404, 0.6215617060661316, 0.6202946901321411, 0.6175090670585632, 0.6212831139564514, 0.6080190539360046, 0.6244364380836487, 0.6168189644813538, 0.6159594058990479, 0.6161829233169556, 0.6138221621513367], 'accuracy': [0.6328165531158447, 0.6431524753570557, 0.6488372087478638, 0.6467700004577637, 0.6506459712982178, 0.6413436532020569, 0.6527131795883179, 0.6431524753570557, 0.6527131795883179, 0.6511628031730652, 0.645219624042511, 0.6475452184677124, 0.6568475365638733, 0.6490955948829651, 0.6527131795883179, 0.6457364559173584, 0.6555555462837219, 0.6625322699546814, 0.6635658740997314, 0.6519379615783691, 0.6581395268440247, 0.6555555462837219, 0.6560723781585693, 0.6617571115493774, 0.6677002310752869, 0.6669250726699829, 0.6633074879646301, 0.6586563587188721, 0.6645994782447815, 0.6700258255004883, 0.6633074879646301, 0.6651162505149841, 0.6578811407089233, 0.6607235074043274, 0.667958676815033, 0.6602067351341248, 0.6728681921958923, 0.6677002310752869, 0.6620154976844788, 0.6607235074043274, 0.6713178157806396, 0.6746770143508911, 0.6630491018295288, 0.6695090532302856, 0.6661498546600342, 0.6777777671813965, 0.6744186282157898, 0.669767439365387, 0.6746770143508911, 0.6692506670951843, 0.6780361533164978, 0.6852713227272034, 0.672609806060791, 0.6782945990562439, 0.6759690046310425, 0.6816537380218506, 0.6772609949111938, 0.6772609949111938, 0.686821699142456, 0.6813953518867493, 0.6896640658378601, 0.6860465407371521, 0.681136965751648, 0.6847545504570007, 0.6937984228134155, 0.6937984228134155, 0.686821699142456, 0.6878553032875061, 0.6865633130073547, 0.6935400366783142, 0.6927648782730103, 0.6935400366783142, 0.6901808977127075, 0.6912144422531128, 0.6940568685531616, 0.6974160075187683, 0.6904392838478088, 0.6966408491134644, 0.6940568685531616, 0.6891472935676575, 0.7028423547744751, 0.7007752060890198, 0.699999988079071, 0.6997416019439697, 0.6994832158088684, 0.698191225528717, 0.6925064325332642, 0.699999988079071, 0.7049095630645752, 0.7041343450546265, 0.7126615047454834, 0.7059431672096252, 0.7036175727844238, 0.7018088102340698, 0.7129198908805847, 0.7038759589195251, 0.7069767713546753, 0.7136951088905334, 0.7082687616348267, 0.7059431672096252], 'val_loss': [0.8603551983833313, 0.8592514395713806, 0.8487173914909363, 0.8497306704521179, 0.8528128862380981, 0.852225661277771, 0.8410133719444275, 0.8456286191940308, 0.8308360576629639, 0.8229585289955139, 0.8320927023887634, 0.8200398683547974, 0.8225631713867188, 0.8104342818260193, 0.821221113204956, 0.8017916679382324, 0.8046810626983643, 0.8110789060592651, 0.8040103912353516, 0.8129125833511353, 0.8104315996170044, 0.8147541880607605, 0.8176640868186951, 0.8245469927787781, 0.8268482685089111, 0.8474615812301636, 0.8609263896942139, 0.8593602180480957, 0.863335371017456, 0.8515403866767883, 0.8640269637107849, 0.8492304086685181, 0.8801476955413818, 0.8635392189025879, 0.8542059063911438, 0.8513164520263672, 0.8711233735084534, 0.8594849705696106, 0.8799244165420532, 0.9087110757827759, 0.8948294520378113, 0.8792778253555298, 0.9160360097885132, 0.8636494278907776, 0.9052757620811462, 0.8731439709663391, 0.8759160041809082, 0.9091730117797852, 0.91989666223526, 0.9104263186454773, 0.880818247795105, 0.8658511638641357, 0.8700379133224487, 0.9018300771713257, 0.8840841054916382, 0.8865945935249329, 0.8861500024795532, 0.8985360264778137, 0.912155032157898, 0.921244204044342, 0.9080264568328857, 0.8890866637229919, 0.8871505260467529, 0.897159218788147, 0.9385431408882141, 0.903109610080719, 0.9907429814338684, 0.8936517834663391, 0.9118462800979614, 0.9081275463104248, 0.9169530272483826, 0.9198412895202637, 0.8911876082420349, 0.9042157530784607, 0.8776732683181763, 0.9122030138969421, 0.9260873794555664, 0.9119294881820679, 0.9494519233703613, 0.8934372067451477, 0.9323752522468567, 0.9105438590049744, 0.9349050521850586, 0.9055972099304199, 0.9157949686050415, 0.9232662320137024, 0.9358463883399963, 0.9189461469650269, 0.9159564971923828, 0.9193612933158875, 0.9870806336402893, 1.0174262523651123, 1.030971646308899, 0.9203451871871948, 0.9515570998191833, 0.974821150302887, 0.9239222407341003, 0.9253337979316711, 0.9217177033424377, 0.9162959456443787], 'val_accuracy': [0.5144628286361694, 0.5134297609329224, 0.5144628286361694, 0.5144628286361694, 0.5154958963394165, 0.5175619721412659, 0.5185950398445129, 0.5175619721412659, 0.5258264541625977, 0.53925621509552, 0.5320248007774353, 0.5485537052154541, 0.5444214940071106, 0.5557851195335388, 0.5495867729187012, 0.5681818127632141, 0.5609503984451294, 0.56611567735672, 0.5599173307418823, 0.5650826692581177, 0.5506198406219482, 0.5599173307418823, 0.5712810158729553, 0.5619834661483765, 0.5754132270812988, 0.5485537052154541, 0.5599173307418823, 0.577479362487793, 0.5712810158729553, 0.5609503984451294, 0.5619834661483765, 0.557851254940033, 0.5609503984451294, 0.577479362487793, 0.55888432264328, 0.5723140239715576, 0.5681818127632141, 0.567148745059967, 0.5630165338516235, 0.5299586653709412, 0.567148745059967, 0.5692148804664612, 0.5464876294136047, 0.56611567735672, 0.5516529083251953, 0.5619834661483765, 0.567148745059967, 0.5640496015548706, 0.5495867729187012, 0.5526859760284424, 0.5681818127632141, 0.5526859760284424, 0.5619834661483765, 0.547520637512207, 0.5681818127632141, 0.5557851195335388, 0.5619834661483765, 0.5723140239715576, 0.5516529083251953, 0.5433884263038635, 0.5650826692581177, 0.5702479481697083, 0.5599173307418823, 0.5702479481697083, 0.547520637512207, 0.5692148804664612, 0.5268595218658447, 0.5433884263038635, 0.5619834661483765, 0.5464876294136047, 0.5568181872367859, 0.567148745059967, 0.5599173307418823, 0.5630165338516235, 0.5609503984451294, 0.5464876294136047, 0.5464876294136047, 0.5516529083251953, 0.5413222908973694, 0.56611567735672, 0.5609503984451294, 0.5805785059928894, 0.53925621509552, 0.5681818127632141, 0.5568181872367859, 0.55888432264328, 0.5640496015548706, 0.5599173307418823, 0.5619834661483765, 0.5650826692581177, 0.5444214940071106, 0.5340909361839294, 0.5289255976676941, 0.5599173307418823, 0.5599173307418823, 0.5464876294136047, 0.5599173307418823, 0.5599173307418823, 0.5599173307418823, 0.5568181872367859]}\n","32/32 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - ETA: 0s - loss: 0.6413 - accuracy: 0.6948"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["29/29 [==============================] - 8s 60ms/step - loss: 0.6413 - accuracy: 0.6948 - val_loss: 0.8072 - val_accuracy: 0.5151\n","Epoch 2/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6374 - accuracy: 0.6921 - val_loss: 0.8137 - val_accuracy: 0.5151\n","Epoch 3/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.6291 - accuracy: 0.6959 - val_loss: 0.8067 - val_accuracy: 0.5172\n","Epoch 4/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.6229 - accuracy: 0.6980 - val_loss: 0.7841 - val_accuracy: 0.5302\n","Epoch 5/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6189 - accuracy: 0.7112 - val_loss: 0.7924 - val_accuracy: 0.5269\n","Epoch 6/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.6263 - accuracy: 0.7010 - val_loss: 0.7827 - val_accuracy: 0.5323\n","Epoch 7/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6207 - accuracy: 0.7115 - val_loss: 0.8025 - val_accuracy: 0.5280\n","Epoch 8/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.6170 - accuracy: 0.7104 - val_loss: 0.7783 - val_accuracy: 0.5593\n","Epoch 9/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.6178 - accuracy: 0.7004 - val_loss: 0.7783 - val_accuracy: 0.5744\n","Epoch 10/100\n","29/29 [==============================] - 1s 32ms/step - loss: 0.6133 - accuracy: 0.7053 - val_loss: 0.7710 - val_accuracy: 0.6121\n","Epoch 11/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6133 - accuracy: 0.7152 - val_loss: 0.7830 - val_accuracy: 0.5679\n","Epoch 12/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6194 - accuracy: 0.6985 - val_loss: 0.7740 - val_accuracy: 0.5970\n","Epoch 13/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6170 - accuracy: 0.7031 - val_loss: 0.7740 - val_accuracy: 0.6024\n","Epoch 14/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.6238 - accuracy: 0.6940 - val_loss: 0.7840 - val_accuracy: 0.5884\n","Epoch 15/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6067 - accuracy: 0.7185 - val_loss: 0.7790 - val_accuracy: 0.5905\n","Epoch 16/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.6069 - accuracy: 0.7055 - val_loss: 0.7631 - val_accuracy: 0.6207\n","Epoch 17/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6054 - accuracy: 0.7099 - val_loss: 0.7617 - val_accuracy: 0.6175\n","Epoch 18/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6028 - accuracy: 0.7107 - val_loss: 0.7658 - val_accuracy: 0.6164\n","Epoch 19/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6035 - accuracy: 0.7139 - val_loss: 0.7769 - val_accuracy: 0.6185\n","Epoch 20/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6051 - accuracy: 0.7115 - val_loss: 0.7800 - val_accuracy: 0.6207\n","Epoch 21/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6065 - accuracy: 0.7093 - val_loss: 0.8365 - val_accuracy: 0.5894\n","Epoch 22/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6198 - accuracy: 0.7002 - val_loss: 0.8690 - val_accuracy: 0.5797\n","Epoch 23/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6075 - accuracy: 0.7112 - val_loss: 0.8566 - val_accuracy: 0.5970\n","Epoch 24/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5973 - accuracy: 0.7171 - val_loss: 0.7955 - val_accuracy: 0.6218\n","Epoch 25/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.5925 - accuracy: 0.7268 - val_loss: 0.7799 - val_accuracy: 0.6336\n","Epoch 26/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6000 - accuracy: 0.7193 - val_loss: 0.7995 - val_accuracy: 0.6250\n","Epoch 27/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5874 - accuracy: 0.7325 - val_loss: 0.8011 - val_accuracy: 0.6272\n","Epoch 28/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5935 - accuracy: 0.7247 - val_loss: 0.8600 - val_accuracy: 0.6142\n","Epoch 29/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5874 - accuracy: 0.7290 - val_loss: 0.8162 - val_accuracy: 0.6142\n","Epoch 30/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5868 - accuracy: 0.7314 - val_loss: 0.8123 - val_accuracy: 0.6444\n","Epoch 31/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.5868 - accuracy: 0.7244 - val_loss: 0.8039 - val_accuracy: 0.6358\n","Epoch 32/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5902 - accuracy: 0.7206 - val_loss: 0.8365 - val_accuracy: 0.6078\n","Epoch 33/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5845 - accuracy: 0.7271 - val_loss: 0.8399 - val_accuracy: 0.5981\n","Epoch 34/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5857 - accuracy: 0.7174 - val_loss: 0.8414 - val_accuracy: 0.6067\n","Epoch 35/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5829 - accuracy: 0.7311 - val_loss: 0.8269 - val_accuracy: 0.6325\n","Epoch 36/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5837 - accuracy: 0.7249 - val_loss: 0.8349 - val_accuracy: 0.6142\n","Epoch 37/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5854 - accuracy: 0.7201 - val_loss: 0.8365 - val_accuracy: 0.6315\n","Epoch 38/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5873 - accuracy: 0.7228 - val_loss: 0.8519 - val_accuracy: 0.6175\n","Epoch 39/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5750 - accuracy: 0.7363 - val_loss: 0.8627 - val_accuracy: 0.6045\n","Epoch 40/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5789 - accuracy: 0.7276 - val_loss: 0.8440 - val_accuracy: 0.6390\n","Epoch 41/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5782 - accuracy: 0.7355 - val_loss: 0.8402 - val_accuracy: 0.6282\n","Epoch 42/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5842 - accuracy: 0.7228 - val_loss: 0.8395 - val_accuracy: 0.6207\n","Epoch 43/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5717 - accuracy: 0.7376 - val_loss: 0.8438 - val_accuracy: 0.6261\n","Epoch 44/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5821 - accuracy: 0.7258 - val_loss: 0.8495 - val_accuracy: 0.6401\n","Epoch 45/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5732 - accuracy: 0.7301 - val_loss: 0.8427 - val_accuracy: 0.6261\n","Epoch 46/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.5823 - accuracy: 0.7309 - val_loss: 0.8461 - val_accuracy: 0.6218\n","Epoch 47/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5704 - accuracy: 0.7419 - val_loss: 0.8942 - val_accuracy: 0.5948\n","Epoch 48/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.5685 - accuracy: 0.7443 - val_loss: 0.8455 - val_accuracy: 0.6519\n","Epoch 49/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5695 - accuracy: 0.7381 - val_loss: 0.8922 - val_accuracy: 0.5873\n","Epoch 50/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5736 - accuracy: 0.7408 - val_loss: 0.8371 - val_accuracy: 0.6325\n","Epoch 51/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5663 - accuracy: 0.7425 - val_loss: 0.8592 - val_accuracy: 0.6412\n","Epoch 52/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5655 - accuracy: 0.7349 - val_loss: 0.8809 - val_accuracy: 0.6228\n","Epoch 53/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5626 - accuracy: 0.7433 - val_loss: 0.8738 - val_accuracy: 0.6142\n","Epoch 54/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5663 - accuracy: 0.7365 - val_loss: 0.8920 - val_accuracy: 0.6131\n","Epoch 55/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5626 - accuracy: 0.7416 - val_loss: 0.8670 - val_accuracy: 0.6261\n","Epoch 56/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5771 - accuracy: 0.7320 - val_loss: 0.8649 - val_accuracy: 0.6336\n","Epoch 57/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5736 - accuracy: 0.7298 - val_loss: 0.8890 - val_accuracy: 0.6196\n","Epoch 58/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5605 - accuracy: 0.7368 - val_loss: 0.8566 - val_accuracy: 0.6369\n","Epoch 59/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5523 - accuracy: 0.7562 - val_loss: 0.8788 - val_accuracy: 0.6196\n","Epoch 60/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5618 - accuracy: 0.7425 - val_loss: 0.9744 - val_accuracy: 0.5970\n","Epoch 61/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5688 - accuracy: 0.7328 - val_loss: 0.8643 - val_accuracy: 0.6336\n","Epoch 62/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5518 - accuracy: 0.7489 - val_loss: 0.8724 - val_accuracy: 0.6455\n","Epoch 63/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5513 - accuracy: 0.7449 - val_loss: 0.8937 - val_accuracy: 0.6078\n","Epoch 64/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5470 - accuracy: 0.7532 - val_loss: 0.9362 - val_accuracy: 0.5916\n","Epoch 65/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5514 - accuracy: 0.7503 - val_loss: 0.8728 - val_accuracy: 0.6369\n","Epoch 66/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5514 - accuracy: 0.7457 - val_loss: 1.0238 - val_accuracy: 0.5625\n","Epoch 67/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5499 - accuracy: 0.7446 - val_loss: 0.8746 - val_accuracy: 0.6412\n","Epoch 68/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5473 - accuracy: 0.7516 - val_loss: 0.8777 - val_accuracy: 0.6315\n","Epoch 69/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5503 - accuracy: 0.7457 - val_loss: 0.8809 - val_accuracy: 0.6228\n","Epoch 70/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5505 - accuracy: 0.7435 - val_loss: 0.8716 - val_accuracy: 0.6379\n","Epoch 71/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5450 - accuracy: 0.7530 - val_loss: 0.8870 - val_accuracy: 0.6401\n","Epoch 72/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5441 - accuracy: 0.7570 - val_loss: 0.9060 - val_accuracy: 0.6121\n","Epoch 73/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5452 - accuracy: 0.7468 - val_loss: 1.0054 - val_accuracy: 0.5668\n","Epoch 74/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5459 - accuracy: 0.7519 - val_loss: 0.8739 - val_accuracy: 0.6250\n","Epoch 75/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5340 - accuracy: 0.7573 - val_loss: 0.8965 - val_accuracy: 0.6282\n","Epoch 76/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5380 - accuracy: 0.7554 - val_loss: 0.9204 - val_accuracy: 0.5916\n","Epoch 77/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5384 - accuracy: 0.7540 - val_loss: 1.0081 - val_accuracy: 0.5722\n","Epoch 78/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5339 - accuracy: 0.7627 - val_loss: 0.8868 - val_accuracy: 0.6390\n","Epoch 79/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5407 - accuracy: 0.7551 - val_loss: 0.8975 - val_accuracy: 0.6304\n","Epoch 80/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5397 - accuracy: 0.7532 - val_loss: 0.8879 - val_accuracy: 0.6228\n","Epoch 81/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5301 - accuracy: 0.7592 - val_loss: 0.9043 - val_accuracy: 0.6282\n","Epoch 82/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5331 - accuracy: 0.7573 - val_loss: 0.9054 - val_accuracy: 0.6099\n","Epoch 83/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5438 - accuracy: 0.7613 - val_loss: 0.9362 - val_accuracy: 0.6228\n","Epoch 84/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5547 - accuracy: 0.7408 - val_loss: 0.9508 - val_accuracy: 0.5981\n","Epoch 85/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5314 - accuracy: 0.7667 - val_loss: 0.9070 - val_accuracy: 0.6282\n","Epoch 86/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5295 - accuracy: 0.7610 - val_loss: 0.9850 - val_accuracy: 0.5884\n","Epoch 87/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5310 - accuracy: 0.7594 - val_loss: 0.8925 - val_accuracy: 0.6250\n","Epoch 88/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5312 - accuracy: 0.7640 - val_loss: 0.8989 - val_accuracy: 0.6207\n","Epoch 89/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5383 - accuracy: 0.7416 - val_loss: 0.9808 - val_accuracy: 0.5905\n","Epoch 90/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5345 - accuracy: 0.7621 - val_loss: 1.0319 - val_accuracy: 0.5744\n","Epoch 91/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5215 - accuracy: 0.7651 - val_loss: 0.8998 - val_accuracy: 0.6347\n","Epoch 92/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5218 - accuracy: 0.7691 - val_loss: 0.9166 - val_accuracy: 0.6250\n","Epoch 93/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5257 - accuracy: 0.7659 - val_loss: 0.9154 - val_accuracy: 0.6088\n","Epoch 94/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5329 - accuracy: 0.7554 - val_loss: 0.9545 - val_accuracy: 0.6250\n","Epoch 95/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5298 - accuracy: 0.7637 - val_loss: 0.9188 - val_accuracy: 0.6444\n","Epoch 96/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5139 - accuracy: 0.7640 - val_loss: 1.0442 - val_accuracy: 0.5765\n","Epoch 97/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5198 - accuracy: 0.7716 - val_loss: 0.9377 - val_accuracy: 0.6088\n","Epoch 98/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5193 - accuracy: 0.7732 - val_loss: 0.9182 - val_accuracy: 0.6347\n","Epoch 99/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5158 - accuracy: 0.7740 - val_loss: 0.9356 - val_accuracy: 0.6228\n","Epoch 100/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5192 - accuracy: 0.7691 - val_loss: 0.9751 - val_accuracy: 0.6045\n","{'loss': [0.6413330435752869, 0.6373764276504517, 0.6291277408599854, 0.6228675842285156, 0.618929386138916, 0.6263487339019775, 0.6206762194633484, 0.61698317527771, 0.6178460121154785, 0.6132660508155823, 0.6133309602737427, 0.6193671226501465, 0.6169764995574951, 0.6237556338310242, 0.6067167520523071, 0.6068514585494995, 0.6053802371025085, 0.6027716398239136, 0.6034844517707825, 0.6051267385482788, 0.6065235733985901, 0.6197699904441833, 0.6075484156608582, 0.5972514152526855, 0.5925233960151672, 0.600028932094574, 0.5874104499816895, 0.5934914350509644, 0.5873621702194214, 0.5867578387260437, 0.5867823958396912, 0.5902180075645447, 0.5845196843147278, 0.5857448577880859, 0.5828956961631775, 0.5836904644966125, 0.5853570103645325, 0.5872625112533569, 0.5750158429145813, 0.5788763761520386, 0.5782328248023987, 0.5841624736785889, 0.571710467338562, 0.5821395516395569, 0.5732285976409912, 0.582270085811615, 0.5703778266906738, 0.568459153175354, 0.5694711208343506, 0.5736129283905029, 0.5662855505943298, 0.5655148029327393, 0.5626023411750793, 0.566312313079834, 0.5626170039176941, 0.5770878791809082, 0.5735833644866943, 0.5604535937309265, 0.5522980093955994, 0.561784029006958, 0.568800151348114, 0.5518272519111633, 0.5512755513191223, 0.5470048785209656, 0.5514447689056396, 0.5513768196105957, 0.5499065518379211, 0.5472668409347534, 0.5502611398696899, 0.5505278706550598, 0.544965386390686, 0.5440753698348999, 0.5451666116714478, 0.5459474921226501, 0.5339909195899963, 0.5379804372787476, 0.5384032726287842, 0.5338740348815918, 0.5407413840293884, 0.5396943688392639, 0.5301040410995483, 0.5330926179885864, 0.5438491702079773, 0.5547227263450623, 0.5313513875007629, 0.5294779539108276, 0.5309591889381409, 0.5311689972877502, 0.5383318662643433, 0.5345245599746704, 0.5214571356773376, 0.5217885971069336, 0.5256581902503967, 0.5329400897026062, 0.5298298597335815, 0.5138670206069946, 0.5198230743408203, 0.5192839503288269, 0.5157749056816101, 0.5191840529441833], 'accuracy': [0.6947737336158752, 0.6920797228813171, 0.6958512663841248, 0.6980064511299133, 0.7112069129943848, 0.7009698152542114, 0.7114762663841248, 0.7103987336158752, 0.7004310488700867, 0.7052801847457886, 0.7152478694915771, 0.6985452771186829, 0.703125, 0.693965494632721, 0.7184805870056152, 0.7055495977401733, 0.7098599076271057, 0.7106680870056152, 0.7139008641242981, 0.7114762663841248, 0.709321141242981, 0.7001616358757019, 0.7112069129943848, 0.717133641242981, 0.7268319129943848, 0.7192887663841248, 0.7324892282485962, 0.7246767282485962, 0.7289870977401733, 0.7314116358757019, 0.7244073152542114, 0.7206357717514038, 0.7271012663841248, 0.717402994632721, 0.7311422228813171, 0.724946141242981, 0.720097005367279, 0.7227909564971924, 0.7362607717514038, 0.7276400923728943, 0.7354525923728943, 0.7227909564971924, 0.7376077771186829, 0.7257543206214905, 0.7300646305084229, 0.7308728694915771, 0.7419180870056152, 0.7443426847457886, 0.7381465435028076, 0.740840494632721, 0.7424569129943848, 0.7349137663841248, 0.7432650923728943, 0.7365301847457886, 0.7416487336158752, 0.7319504022598267, 0.7297952771186829, 0.7367995977401733, 0.756196141242981, 0.7424569129943848, 0.732758641242981, 0.7489224076271057, 0.7448814511299133, 0.7532327771186829, 0.7502694129943848, 0.7456896305084229, 0.7446120977401733, 0.751616358757019, 0.7456896305084229, 0.743534505367279, 0.7529633641242981, 0.7570043206214905, 0.7467672228813171, 0.7518857717514038, 0.7572737336158752, 0.7553879022598267, 0.7540409564971924, 0.7626616358757019, 0.7551185488700867, 0.7532327771186829, 0.759159505367279, 0.7572737336158752, 0.7613146305084229, 0.740840494632721, 0.7667025923728943, 0.7610452771186829, 0.759428858757019, 0.764008641242981, 0.7416487336158752, 0.7621228694915771, 0.7650862336158752, 0.7691271305084229, 0.7658944129943848, 0.7553879022598267, 0.7637392282485962, 0.764008641242981, 0.7715517282485962, 0.7731680870056152, 0.7739762663841248, 0.7691271305084229], 'val_loss': [0.8071566820144653, 0.8136571645736694, 0.8067482113838196, 0.7841101884841919, 0.7923619747161865, 0.7826652526855469, 0.8024941086769104, 0.778306245803833, 0.7783465385437012, 0.7709596753120422, 0.7830037474632263, 0.7739883661270142, 0.7739919424057007, 0.7839991450309753, 0.7789651155471802, 0.7630778551101685, 0.7617439031600952, 0.7657520174980164, 0.7768685817718506, 0.7800281047821045, 0.8365288376808167, 0.8690171241760254, 0.856631875038147, 0.7954918146133423, 0.7798551917076111, 0.7995446920394897, 0.8010832071304321, 0.8599815964698792, 0.8161582946777344, 0.8123112916946411, 0.8038890361785889, 0.836534321308136, 0.8398982286453247, 0.8414357900619507, 0.8268519043922424, 0.8349384069442749, 0.8364621996879578, 0.8518933653831482, 0.862724244594574, 0.8439648747444153, 0.8401634097099304, 0.8395352363586426, 0.843787431716919, 0.8495145440101624, 0.8426558375358582, 0.8460716009140015, 0.8942093253135681, 0.8455342054367065, 0.89218670129776, 0.8371002674102783, 0.8591695427894592, 0.8808959722518921, 0.8738284707069397, 0.8919565081596375, 0.8670297265052795, 0.8649052381515503, 0.8890177011489868, 0.8566056489944458, 0.8787803649902344, 0.9744048714637756, 0.8642659783363342, 0.8723931908607483, 0.8936880230903625, 0.9362428188323975, 0.8728244304656982, 1.0238382816314697, 0.8746081590652466, 0.8776696920394897, 0.8808658719062805, 0.8716419339179993, 0.8870473504066467, 0.9059584140777588, 1.0053695440292358, 0.8738728165626526, 0.8964585065841675, 0.9203526973724365, 1.008121371269226, 0.8867951035499573, 0.8975033760070801, 0.8878828287124634, 0.9042712450027466, 0.9054037928581238, 0.9362130761146545, 0.950827956199646, 0.9070340991020203, 0.9850279688835144, 0.8924782872200012, 0.8989381194114685, 0.9807973504066467, 1.0318653583526611, 0.8998401761054993, 0.9166244864463806, 0.9153606295585632, 0.9545383453369141, 0.9187830686569214, 1.0442495346069336, 0.9376941919326782, 0.9182118773460388, 0.9355900287628174, 0.9750514030456543], 'val_accuracy': [0.5150862336158752, 0.5150862336158752, 0.517241358757019, 0.5301724076271057, 0.5269396305084229, 0.5323275923728943, 0.5280172228813171, 0.5592672228813171, 0.5743534564971924, 0.6120689511299133, 0.5678879022598267, 0.5969827771186829, 0.6023706793785095, 0.5883620977401733, 0.5905172228813171, 0.6206896305084229, 0.6174569129943848, 0.6163793206214905, 0.618534505367279, 0.6206896305084229, 0.5894396305084229, 0.579741358757019, 0.5969827771186829, 0.6217672228813171, 0.6336206793785095, 0.625, 0.6271551847457886, 0.6142241358757019, 0.6142241358757019, 0.6443965435028076, 0.6357758641242981, 0.607758641242981, 0.5980603694915771, 0.6066810488700867, 0.6325430870056152, 0.6142241358757019, 0.631465494632721, 0.6174569129943848, 0.6045258641242981, 0.639008641242981, 0.6282327771186829, 0.6206896305084229, 0.6260775923728943, 0.6400862336158752, 0.6260775923728943, 0.6217672228813171, 0.5948275923728943, 0.6519396305084229, 0.587284505367279, 0.6325430870056152, 0.6411637663841248, 0.6228448152542114, 0.6142241358757019, 0.6131465435028076, 0.6260775923728943, 0.6336206793785095, 0.6196120977401733, 0.6368534564971924, 0.6196120977401733, 0.5969827771186829, 0.6336206793785095, 0.6454741358757019, 0.607758641242981, 0.5915948152542114, 0.6368534564971924, 0.5625, 0.6411637663841248, 0.631465494632721, 0.6228448152542114, 0.6379310488700867, 0.6400862336158752, 0.6120689511299133, 0.5668103694915771, 0.625, 0.6282327771186829, 0.5915948152542114, 0.5721982717514038, 0.639008641242981, 0.6303879022598267, 0.6228448152542114, 0.6282327771186829, 0.6099137663841248, 0.6228448152542114, 0.5980603694915771, 0.6282327771186829, 0.5883620977401733, 0.625, 0.6206896305084229, 0.5905172228813171, 0.5743534564971924, 0.6346982717514038, 0.625, 0.6088362336158752, 0.625, 0.6443965435028076, 0.576508641242981, 0.6088362336158752, 0.6346982717514038, 0.6228448152542114, 0.6045258641242981]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","27/28 [===========================>..] - ETA: 0s - loss: 0.6503 - accuracy: 0.6855"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["28/28 [==============================] - 9s 67ms/step - loss: 0.6506 - accuracy: 0.6868 - val_loss: 0.8100 - val_accuracy: 0.5045\n","Epoch 2/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.6383 - accuracy: 0.6935 - val_loss: 0.7837 - val_accuracy: 0.5136\n","Epoch 3/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6269 - accuracy: 0.6978 - val_loss: 0.7898 - val_accuracy: 0.5079\n","Epoch 4/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6284 - accuracy: 0.6972 - val_loss: 0.8070 - val_accuracy: 0.5090\n","Epoch 5/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.6250 - accuracy: 0.7100 - val_loss: 0.7684 - val_accuracy: 0.5667\n","Epoch 6/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6384 - accuracy: 0.6992 - val_loss: 0.7858 - val_accuracy: 0.5158\n","Epoch 7/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6215 - accuracy: 0.6989 - val_loss: 0.7828 - val_accuracy: 0.5215\n","Epoch 8/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6221 - accuracy: 0.7023 - val_loss: 0.7887 - val_accuracy: 0.5238\n","Epoch 9/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6237 - accuracy: 0.7043 - val_loss: 0.7734 - val_accuracy: 0.5452\n","Epoch 10/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6171 - accuracy: 0.7032 - val_loss: 0.7607 - val_accuracy: 0.5633\n","Epoch 11/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.6134 - accuracy: 0.7088 - val_loss: 0.7541 - val_accuracy: 0.5962\n","Epoch 12/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6157 - accuracy: 0.7068 - val_loss: 0.7724 - val_accuracy: 0.5498\n","Epoch 13/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6174 - accuracy: 0.7035 - val_loss: 0.7654 - val_accuracy: 0.5577\n","Epoch 14/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.6222 - accuracy: 0.7077 - val_loss: 0.7410 - val_accuracy: 0.6301\n","Epoch 15/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6263 - accuracy: 0.7020 - val_loss: 0.7504 - val_accuracy: 0.6063\n","Epoch 16/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6233 - accuracy: 0.6998 - val_loss: 0.7611 - val_accuracy: 0.5713\n","Epoch 17/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.6105 - accuracy: 0.7083 - val_loss: 0.7369 - val_accuracy: 0.6493\n","Epoch 18/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6084 - accuracy: 0.7179 - val_loss: 0.7430 - val_accuracy: 0.6357\n","Epoch 19/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.6014 - accuracy: 0.7204 - val_loss: 0.7404 - val_accuracy: 0.5995\n","Epoch 20/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6088 - accuracy: 0.7071 - val_loss: 0.7299 - val_accuracy: 0.6493\n","Epoch 21/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6040 - accuracy: 0.7210 - val_loss: 0.7570 - val_accuracy: 0.5939\n","Epoch 22/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6060 - accuracy: 0.7151 - val_loss: 0.7550 - val_accuracy: 0.6493\n","Epoch 23/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6217 - accuracy: 0.7032 - val_loss: 0.7911 - val_accuracy: 0.5916\n","Epoch 24/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.6159 - accuracy: 0.7046 - val_loss: 0.7857 - val_accuracy: 0.5882\n","Epoch 25/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6050 - accuracy: 0.7187 - val_loss: 0.7672 - val_accuracy: 0.6086\n","Epoch 26/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5981 - accuracy: 0.7100 - val_loss: 0.7587 - val_accuracy: 0.6063\n","Epoch 27/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5979 - accuracy: 0.7272 - val_loss: 0.7600 - val_accuracy: 0.6459\n","Epoch 28/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5996 - accuracy: 0.7168 - val_loss: 0.7928 - val_accuracy: 0.6086\n","Epoch 29/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5944 - accuracy: 0.7286 - val_loss: 0.8824 - val_accuracy: 0.5803\n","Epoch 30/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5941 - accuracy: 0.7286 - val_loss: 0.8010 - val_accuracy: 0.6018\n","Epoch 31/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5930 - accuracy: 0.7269 - val_loss: 0.8118 - val_accuracy: 0.6403\n","Epoch 32/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5959 - accuracy: 0.7083 - val_loss: 0.8185 - val_accuracy: 0.6109\n","Epoch 33/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5930 - accuracy: 0.7247 - val_loss: 0.7952 - val_accuracy: 0.6029\n","Epoch 34/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5870 - accuracy: 0.7227 - val_loss: 0.8034 - val_accuracy: 0.5995\n","Epoch 35/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5873 - accuracy: 0.7292 - val_loss: 0.9734 - val_accuracy: 0.5679\n","Epoch 36/100\n","28/28 [==============================] - 2s 69ms/step - loss: 0.5930 - accuracy: 0.7250 - val_loss: 0.7794 - val_accuracy: 0.6505\n","Epoch 37/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5899 - accuracy: 0.7182 - val_loss: 0.8525 - val_accuracy: 0.6143\n","Epoch 38/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5791 - accuracy: 0.7284 - val_loss: 0.8006 - val_accuracy: 0.6414\n","Epoch 39/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5839 - accuracy: 0.7218 - val_loss: 0.8034 - val_accuracy: 0.6425\n","Epoch 40/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5793 - accuracy: 0.7363 - val_loss: 0.8266 - val_accuracy: 0.6290\n","Epoch 41/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5847 - accuracy: 0.7250 - val_loss: 0.8132 - val_accuracy: 0.6448\n","Epoch 42/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5882 - accuracy: 0.7210 - val_loss: 0.8492 - val_accuracy: 0.6301\n","Epoch 43/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5796 - accuracy: 0.7332 - val_loss: 0.8264 - val_accuracy: 0.6448\n","Epoch 44/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5832 - accuracy: 0.7255 - val_loss: 0.8466 - val_accuracy: 0.6380\n","Epoch 45/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5927 - accuracy: 0.7230 - val_loss: 0.8538 - val_accuracy: 0.5984\n","Epoch 46/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5732 - accuracy: 0.7343 - val_loss: 0.8117 - val_accuracy: 0.6324\n","Epoch 47/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5821 - accuracy: 0.7244 - val_loss: 0.8378 - val_accuracy: 0.6109\n","Epoch 48/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5968 - accuracy: 0.7224 - val_loss: 0.8251 - val_accuracy: 0.6335\n","Epoch 49/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5705 - accuracy: 0.7439 - val_loss: 0.8262 - val_accuracy: 0.6459\n","Epoch 50/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5725 - accuracy: 0.7326 - val_loss: 0.9839 - val_accuracy: 0.5735\n","Epoch 51/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5703 - accuracy: 0.7391 - val_loss: 0.8507 - val_accuracy: 0.5995\n","Epoch 52/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5724 - accuracy: 0.7371 - val_loss: 0.8231 - val_accuracy: 0.6380\n","Epoch 53/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5740 - accuracy: 0.7360 - val_loss: 0.8790 - val_accuracy: 0.6075\n","Epoch 54/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5720 - accuracy: 0.7383 - val_loss: 0.8565 - val_accuracy: 0.6414\n","Epoch 55/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5687 - accuracy: 0.7323 - val_loss: 0.8371 - val_accuracy: 0.6290\n","Epoch 56/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5722 - accuracy: 0.7417 - val_loss: 0.8624 - val_accuracy: 0.6267\n","Epoch 57/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5655 - accuracy: 0.7422 - val_loss: 0.8652 - val_accuracy: 0.6380\n","Epoch 58/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5698 - accuracy: 0.7436 - val_loss: 0.9546 - val_accuracy: 0.5894\n","Epoch 59/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5672 - accuracy: 0.7391 - val_loss: 0.8627 - val_accuracy: 0.6210\n","Epoch 60/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5672 - accuracy: 0.7349 - val_loss: 0.8530 - val_accuracy: 0.6369\n","Epoch 61/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5650 - accuracy: 0.7450 - val_loss: 0.9026 - val_accuracy: 0.6346\n","Epoch 62/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5600 - accuracy: 0.7459 - val_loss: 0.8940 - val_accuracy: 0.6154\n","Epoch 63/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5603 - accuracy: 0.7442 - val_loss: 0.9122 - val_accuracy: 0.6256\n","Epoch 64/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5628 - accuracy: 0.7402 - val_loss: 0.8784 - val_accuracy: 0.6097\n","Epoch 65/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5608 - accuracy: 0.7428 - val_loss: 0.8846 - val_accuracy: 0.6380\n","Epoch 66/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5565 - accuracy: 0.7561 - val_loss: 0.9056 - val_accuracy: 0.6312\n","Epoch 67/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5582 - accuracy: 0.7448 - val_loss: 0.9031 - val_accuracy: 0.6199\n","Epoch 68/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5501 - accuracy: 0.7499 - val_loss: 0.8793 - val_accuracy: 0.6324\n","Epoch 69/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5530 - accuracy: 0.7516 - val_loss: 0.8419 - val_accuracy: 0.6188\n","Epoch 70/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5542 - accuracy: 0.7535 - val_loss: 1.0370 - val_accuracy: 0.5781\n","Epoch 71/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5541 - accuracy: 0.7479 - val_loss: 0.9053 - val_accuracy: 0.5973\n","Epoch 72/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5618 - accuracy: 0.7436 - val_loss: 0.9237 - val_accuracy: 0.6052\n","Epoch 73/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5621 - accuracy: 0.7397 - val_loss: 0.9071 - val_accuracy: 0.6097\n","Epoch 74/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5528 - accuracy: 0.7431 - val_loss: 1.0628 - val_accuracy: 0.5724\n","Epoch 75/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5465 - accuracy: 0.7504 - val_loss: 0.8920 - val_accuracy: 0.6301\n","Epoch 76/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.5504 - accuracy: 0.7414 - val_loss: 0.8744 - val_accuracy: 0.6335\n","Epoch 77/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5493 - accuracy: 0.7445 - val_loss: 0.8844 - val_accuracy: 0.6357\n","Epoch 78/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5431 - accuracy: 0.7544 - val_loss: 0.9054 - val_accuracy: 0.6290\n","Epoch 79/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5547 - accuracy: 0.7385 - val_loss: 0.8992 - val_accuracy: 0.6312\n","Epoch 80/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5459 - accuracy: 0.7507 - val_loss: 0.9297 - val_accuracy: 0.6199\n","Epoch 81/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5500 - accuracy: 0.7439 - val_loss: 0.9151 - val_accuracy: 0.6267\n","Epoch 82/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5393 - accuracy: 0.7598 - val_loss: 0.9332 - val_accuracy: 0.6324\n","Epoch 83/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5432 - accuracy: 0.7496 - val_loss: 0.8815 - val_accuracy: 0.6290\n","Epoch 84/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5475 - accuracy: 0.7453 - val_loss: 0.8909 - val_accuracy: 0.6312\n","Epoch 85/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5303 - accuracy: 0.7615 - val_loss: 0.9216 - val_accuracy: 0.6335\n","Epoch 86/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5314 - accuracy: 0.7637 - val_loss: 0.9674 - val_accuracy: 0.6086\n","Epoch 87/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5335 - accuracy: 0.7626 - val_loss: 0.9116 - val_accuracy: 0.6063\n","Epoch 88/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5294 - accuracy: 0.7728 - val_loss: 0.8729 - val_accuracy: 0.6256\n","Epoch 89/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5462 - accuracy: 0.7569 - val_loss: 0.9261 - val_accuracy: 0.6278\n","Epoch 90/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5402 - accuracy: 0.7550 - val_loss: 1.0231 - val_accuracy: 0.5928\n","Epoch 91/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5451 - accuracy: 0.7504 - val_loss: 1.0693 - val_accuracy: 0.5747\n","Epoch 92/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5328 - accuracy: 0.7657 - val_loss: 0.9140 - val_accuracy: 0.6324\n","Epoch 93/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5373 - accuracy: 0.7606 - val_loss: 1.0016 - val_accuracy: 0.5916\n","Epoch 94/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5307 - accuracy: 0.7581 - val_loss: 0.9222 - val_accuracy: 0.6244\n","Epoch 95/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5229 - accuracy: 0.7807 - val_loss: 0.9604 - val_accuracy: 0.6052\n","Epoch 96/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5249 - accuracy: 0.7589 - val_loss: 0.9328 - val_accuracy: 0.6278\n","Epoch 97/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5389 - accuracy: 0.7547 - val_loss: 0.9932 - val_accuracy: 0.5973\n","Epoch 98/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5180 - accuracy: 0.7697 - val_loss: 0.9437 - val_accuracy: 0.6256\n","Epoch 99/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5216 - accuracy: 0.7694 - val_loss: 0.9839 - val_accuracy: 0.6210\n","Epoch 100/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5316 - accuracy: 0.7598 - val_loss: 0.9298 - val_accuracy: 0.6143\n","{'loss': [0.6505801677703857, 0.6382927894592285, 0.6268796324729919, 0.6284260749816895, 0.6249560713768005, 0.6384460926055908, 0.6215197443962097, 0.6221485137939453, 0.6236711144447327, 0.6171482801437378, 0.613385796546936, 0.6157285571098328, 0.6173964738845825, 0.6221654415130615, 0.6262844800949097, 0.6233391165733337, 0.6104720234870911, 0.6083775162696838, 0.6014261245727539, 0.6087678074836731, 0.6039842963218689, 0.6059650182723999, 0.6216821074485779, 0.6159325242042542, 0.6049700379371643, 0.5981050729751587, 0.5979368686676025, 0.5995909571647644, 0.5943726301193237, 0.594092071056366, 0.5930005311965942, 0.5959465503692627, 0.59297776222229, 0.5869773626327515, 0.5873206257820129, 0.5929779410362244, 0.5899171233177185, 0.579119861125946, 0.5838961005210876, 0.5792719125747681, 0.5847101211547852, 0.588178277015686, 0.5796307921409607, 0.5831811428070068, 0.5926573872566223, 0.5731530785560608, 0.5820958614349365, 0.5967804789543152, 0.5705304145812988, 0.5724838376045227, 0.57034832239151, 0.5723699927330017, 0.5740135908126831, 0.5720305442810059, 0.5686746835708618, 0.5721955895423889, 0.5654639005661011, 0.5697829723358154, 0.5671520233154297, 0.5672232508659363, 0.5649846792221069, 0.5600060820579529, 0.5602512955665588, 0.5627791881561279, 0.5607753396034241, 0.5564521551132202, 0.5582084059715271, 0.5500849485397339, 0.5530368089675903, 0.5542455315589905, 0.5541176199913025, 0.5617622137069702, 0.5620946288108826, 0.552800178527832, 0.5465236902236938, 0.5504482984542847, 0.5493204593658447, 0.5430932641029358, 0.5547052025794983, 0.5459002256393433, 0.54999840259552, 0.5393268465995789, 0.5431528091430664, 0.5474798083305359, 0.5303435921669006, 0.5313851237297058, 0.5334829688072205, 0.5294336676597595, 0.5462369322776794, 0.540231466293335, 0.5450950264930725, 0.532803475856781, 0.5372519493103027, 0.5307338237762451, 0.5228824615478516, 0.5249048471450806, 0.5388782024383545, 0.5179626941680908, 0.5215549468994141, 0.5315676927566528], 'accuracy': [0.6867572069168091, 0.6935483813285828, 0.6977928876876831, 0.6972269415855408, 0.709960401058197, 0.6992077231407166, 0.698924720287323, 0.7023203372955322, 0.7043010592460632, 0.7031692266464233, 0.7088285088539124, 0.7068477869033813, 0.7034521698951721, 0.7076966762542725, 0.7020373344421387, 0.6997736096382141, 0.70826256275177, 0.7178834080696106, 0.7204301357269287, 0.7071307301521301, 0.7209960222244263, 0.7150537371635437, 0.7031692266464233, 0.7045840620994568, 0.7187322974205017, 0.709960401058197, 0.7272212505340576, 0.7167515754699707, 0.7286360859870911, 0.7286360859870911, 0.7269383072853088, 0.70826256275177, 0.7246745824813843, 0.7226938605308533, 0.7292020320892334, 0.7249575257301331, 0.7181664109230042, 0.7283531427383423, 0.7218449115753174, 0.7362761497497559, 0.7249575257301331, 0.7209960222244263, 0.7331635355949402, 0.7255234718322754, 0.722976803779602, 0.7342954277992249, 0.7243916392326355, 0.7224108576774597, 0.7439162135124207, 0.7325976490974426, 0.7391058206558228, 0.7371250987052917, 0.7359932065010071, 0.7382569313049316, 0.7323146462440491, 0.7416524887084961, 0.7422184348106384, 0.7436332702636719, 0.7391058206558228, 0.7348613739013672, 0.7450481057167053, 0.7458969950675964, 0.7441992163658142, 0.7402377128601074, 0.7427843809127808, 0.7560837864875793, 0.7447651624679565, 0.7498584985733032, 0.7515563368797302, 0.7535370588302612, 0.7478777766227722, 0.7436332702636719, 0.7396717667579651, 0.7430673241615295, 0.7504244446754456, 0.7413695454597473, 0.744482159614563, 0.7543859481811523, 0.7385398745536804, 0.7507073879241943, 0.7439162135124207, 0.7597622871398926, 0.7495755553245544, 0.7453310489654541, 0.7614601254463196, 0.7637238502502441, 0.7625919580459595, 0.7727787494659424, 0.7569326758384705, 0.7549518942832947, 0.7504244446754456, 0.7657045722007751, 0.7606111764907837, 0.7580645084381104, 0.780701756477356, 0.7589133977890015, 0.7546689510345459, 0.7696660757064819, 0.7693831324577332, 0.7597622871398926], 'val_loss': [0.8099561929702759, 0.7836995720863342, 0.7898467779159546, 0.8070091605186462, 0.7684471011161804, 0.785773515701294, 0.7828498482704163, 0.7887190580368042, 0.7734088897705078, 0.7606877088546753, 0.7541455030441284, 0.7723762392997742, 0.7653622627258301, 0.7410058975219727, 0.750352144241333, 0.7611287832260132, 0.7368883490562439, 0.7430299520492554, 0.7403643131256104, 0.7299281358718872, 0.7569699287414551, 0.7549764513969421, 0.7910762429237366, 0.785728931427002, 0.7671574950218201, 0.7587109804153442, 0.7600266337394714, 0.7927898168563843, 0.8824106454849243, 0.8009758591651917, 0.8118164539337158, 0.8185455799102783, 0.795193076133728, 0.803446888923645, 0.9734209775924683, 0.7794129252433777, 0.8525049090385437, 0.8006317019462585, 0.8034437298774719, 0.8265987038612366, 0.8131697773933411, 0.8491986989974976, 0.826365053653717, 0.8465986847877502, 0.8537984490394592, 0.8117220997810364, 0.8378084897994995, 0.825146496295929, 0.8261504769325256, 0.9838801622390747, 0.8506900668144226, 0.8230886459350586, 0.878965437412262, 0.8565080165863037, 0.8371420502662659, 0.862445592880249, 0.865193784236908, 0.9546098113059998, 0.8627093434333801, 0.8529607653617859, 0.9025611281394958, 0.8940302133560181, 0.9122228026390076, 0.87838214635849, 0.8846265077590942, 0.905633270740509, 0.9031080603599548, 0.8792680501937866, 0.8418786525726318, 1.036969542503357, 0.9053071141242981, 0.9237484931945801, 0.9070754051208496, 1.062798261642456, 0.8919748663902283, 0.8744257092475891, 0.8843870759010315, 0.9054275155067444, 0.8991968631744385, 0.929691731929779, 0.9150965809822083, 0.933220386505127, 0.8815321922302246, 0.8908534049987793, 0.9215708374977112, 0.9674383401870728, 0.9116345643997192, 0.8729076385498047, 0.9260646104812622, 1.023098111152649, 1.0693432092666626, 0.9140358567237854, 1.0015606880187988, 0.9221678972244263, 0.9603639245033264, 0.9328446984291077, 0.9931608438491821, 0.9437130093574524, 0.9838759303092957, 0.9297542572021484], 'val_accuracy': [0.5045248866081238, 0.5135746598243713, 0.5079185366630554, 0.5090497732162476, 0.5667420625686646, 0.5158371329307556, 0.5214931964874268, 0.523755669593811, 0.5452488660812378, 0.5633484125137329, 0.5961538553237915, 0.5497737526893616, 0.557692289352417, 0.6300904750823975, 0.6063348650932312, 0.5712669491767883, 0.6493212580680847, 0.6357465982437134, 0.5995475053787231, 0.6493212580680847, 0.5938913822174072, 0.6493212580680847, 0.5916289687156677, 0.5882353186607361, 0.6085972785949707, 0.6063348650932312, 0.6459276080131531, 0.6085972785949707, 0.5803167223930359, 0.6018099784851074, 0.6402714848518372, 0.610859751701355, 0.6029411554336548, 0.5995475053787231, 0.5678732991218567, 0.6504524946212769, 0.6142534017562866, 0.6414027214050293, 0.6425339579582214, 0.6289592981338501, 0.6447963714599609, 0.6300904750823975, 0.6447963714599609, 0.6380090713500977, 0.598416268825531, 0.6323529481887817, 0.610859751701355, 0.6334841847419739, 0.6459276080131531, 0.5735294222831726, 0.5995475053787231, 0.6380090713500977, 0.6074660420417786, 0.6414027214050293, 0.6289592981338501, 0.6266968250274658, 0.6380090713500977, 0.5893664956092834, 0.6210407018661499, 0.6368778347969055, 0.6346153616905212, 0.6153846383094788, 0.6255655884742737, 0.6097285151481628, 0.6380090713500977, 0.6312217116355896, 0.6199095249176025, 0.6323529481887817, 0.6187782883644104, 0.5780543088912964, 0.5972850918769836, 0.6052036285400391, 0.6097285151481628, 0.5723981857299805, 0.6300904750823975, 0.6334841847419739, 0.6357465982437134, 0.6289592981338501, 0.6312217116355896, 0.6199095249176025, 0.6266968250274658, 0.6323529481887817, 0.6289592981338501, 0.6312217116355896, 0.6334841847419739, 0.6085972785949707, 0.6063348650932312, 0.6255655884742737, 0.627828061580658, 0.5927602052688599, 0.5746606588363647, 0.6323529481887817, 0.5916289687156677, 0.6244344115257263, 0.6052036285400391, 0.627828061580658, 0.5972850918769836, 0.6255655884742737, 0.6210407018661499, 0.6142534017562866]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","30/31 [============================>.] - ETA: 0s - loss: 0.6534 - accuracy: 0.6799"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["31/31 [==============================] - 8s 74ms/step - loss: 0.6552 - accuracy: 0.6786 - val_loss: 0.8027 - val_accuracy: 0.5176\n","Epoch 2/100\n","31/31 [==============================] - 1s 31ms/step - loss: 0.6523 - accuracy: 0.6889 - val_loss: 0.7867 - val_accuracy: 0.5207\n","Epoch 3/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6462 - accuracy: 0.6837 - val_loss: 0.8024 - val_accuracy: 0.5176\n","Epoch 4/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.6433 - accuracy: 0.6928 - val_loss: 0.7815 - val_accuracy: 0.5279\n","Epoch 5/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6383 - accuracy: 0.6951 - val_loss: 0.7944 - val_accuracy: 0.5207\n","Epoch 6/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6309 - accuracy: 0.6987 - val_loss: 0.8094 - val_accuracy: 0.5207\n","Epoch 7/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6331 - accuracy: 0.6902 - val_loss: 0.7989 - val_accuracy: 0.5258\n","Epoch 8/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6342 - accuracy: 0.6956 - val_loss: 0.7982 - val_accuracy: 0.5279\n","Epoch 9/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6321 - accuracy: 0.6946 - val_loss: 0.7947 - val_accuracy: 0.5269\n","Epoch 10/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.6292 - accuracy: 0.6984 - val_loss: 0.7770 - val_accuracy: 0.5465\n","Epoch 11/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.6318 - accuracy: 0.6917 - val_loss: 0.7657 - val_accuracy: 0.5610\n","Epoch 12/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.6258 - accuracy: 0.6997 - val_loss: 0.7661 - val_accuracy: 0.5671\n","Epoch 13/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6287 - accuracy: 0.6902 - val_loss: 0.7917 - val_accuracy: 0.5455\n","Epoch 14/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6284 - accuracy: 0.6902 - val_loss: 0.7833 - val_accuracy: 0.5599\n","Epoch 15/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.6250 - accuracy: 0.6997 - val_loss: 0.7758 - val_accuracy: 0.5702\n","Epoch 16/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.6212 - accuracy: 0.7021 - val_loss: 0.7631 - val_accuracy: 0.5744\n","Epoch 17/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6199 - accuracy: 0.7072 - val_loss: 0.7916 - val_accuracy: 0.5692\n","Epoch 18/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.6262 - accuracy: 0.6930 - val_loss: 0.7620 - val_accuracy: 0.5971\n","Epoch 19/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6221 - accuracy: 0.6992 - val_loss: 0.7659 - val_accuracy: 0.5785\n","Epoch 20/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6266 - accuracy: 0.6995 - val_loss: 0.7941 - val_accuracy: 0.5837\n","Epoch 21/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.6212 - accuracy: 0.6979 - val_loss: 0.7868 - val_accuracy: 0.5992\n","Epoch 22/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6163 - accuracy: 0.7036 - val_loss: 0.8039 - val_accuracy: 0.5930\n","Epoch 23/100\n","31/31 [==============================] - 1s 35ms/step - loss: 0.6129 - accuracy: 0.7054 - val_loss: 0.8007 - val_accuracy: 0.6064\n","Epoch 24/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.6148 - accuracy: 0.7083 - val_loss: 0.7959 - val_accuracy: 0.5950\n","Epoch 25/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6093 - accuracy: 0.7147 - val_loss: 0.8354 - val_accuracy: 0.5775\n","Epoch 26/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6126 - accuracy: 0.7132 - val_loss: 0.8123 - val_accuracy: 0.5847\n","Epoch 27/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6053 - accuracy: 0.7062 - val_loss: 0.8377 - val_accuracy: 0.5826\n","Epoch 28/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6054 - accuracy: 0.7119 - val_loss: 0.8444 - val_accuracy: 0.5888\n","Epoch 29/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6073 - accuracy: 0.7039 - val_loss: 0.8391 - val_accuracy: 0.6043\n","Epoch 30/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5988 - accuracy: 0.7253 - val_loss: 0.8491 - val_accuracy: 0.5971\n","Epoch 31/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6053 - accuracy: 0.7080 - val_loss: 0.8378 - val_accuracy: 0.5899\n","Epoch 32/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6074 - accuracy: 0.7132 - val_loss: 0.9060 - val_accuracy: 0.5702\n","Epoch 33/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6048 - accuracy: 0.7132 - val_loss: 0.8639 - val_accuracy: 0.6064\n","Epoch 34/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5998 - accuracy: 0.7160 - val_loss: 0.8686 - val_accuracy: 0.5961\n","Epoch 35/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6008 - accuracy: 0.7194 - val_loss: 0.8686 - val_accuracy: 0.5847\n","Epoch 36/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6037 - accuracy: 0.7158 - val_loss: 0.9274 - val_accuracy: 0.6054\n","Epoch 37/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6051 - accuracy: 0.7114 - val_loss: 0.8983 - val_accuracy: 0.5837\n","Epoch 38/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5988 - accuracy: 0.7134 - val_loss: 0.8681 - val_accuracy: 0.5888\n","Epoch 39/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5984 - accuracy: 0.7165 - val_loss: 0.9289 - val_accuracy: 0.5651\n","Epoch 40/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6014 - accuracy: 0.7202 - val_loss: 0.8796 - val_accuracy: 0.5888\n","Epoch 41/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5925 - accuracy: 0.7173 - val_loss: 0.8858 - val_accuracy: 0.5919\n","Epoch 42/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5929 - accuracy: 0.7209 - val_loss: 0.8748 - val_accuracy: 0.5950\n","Epoch 43/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5890 - accuracy: 0.7245 - val_loss: 0.9279 - val_accuracy: 0.5692\n","Epoch 44/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5940 - accuracy: 0.7269 - val_loss: 0.8709 - val_accuracy: 0.5826\n","Epoch 45/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5898 - accuracy: 0.7284 - val_loss: 0.8879 - val_accuracy: 0.5940\n","Epoch 46/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5949 - accuracy: 0.7142 - val_loss: 0.9457 - val_accuracy: 0.5826\n","Epoch 47/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5938 - accuracy: 0.7222 - val_loss: 0.9347 - val_accuracy: 0.5909\n","Epoch 48/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5902 - accuracy: 0.7297 - val_loss: 0.9648 - val_accuracy: 0.6002\n","Epoch 49/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5898 - accuracy: 0.7240 - val_loss: 0.8997 - val_accuracy: 0.5816\n","Epoch 50/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5880 - accuracy: 0.7307 - val_loss: 0.8951 - val_accuracy: 0.5878\n","Epoch 51/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5950 - accuracy: 0.7209 - val_loss: 0.8839 - val_accuracy: 0.5826\n","Epoch 52/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5979 - accuracy: 0.7140 - val_loss: 0.9146 - val_accuracy: 0.5971\n","Epoch 53/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5858 - accuracy: 0.7289 - val_loss: 0.8891 - val_accuracy: 0.5816\n","Epoch 54/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5851 - accuracy: 0.7137 - val_loss: 0.9626 - val_accuracy: 0.5682\n","Epoch 55/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5789 - accuracy: 0.7274 - val_loss: 0.9579 - val_accuracy: 0.5723\n","Epoch 56/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5837 - accuracy: 0.7269 - val_loss: 0.9111 - val_accuracy: 0.6012\n","Epoch 57/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5807 - accuracy: 0.7302 - val_loss: 0.9170 - val_accuracy: 0.5785\n","Epoch 58/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5768 - accuracy: 0.7292 - val_loss: 0.9371 - val_accuracy: 0.5826\n","Epoch 59/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5775 - accuracy: 0.7344 - val_loss: 0.9203 - val_accuracy: 0.5888\n","Epoch 60/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5829 - accuracy: 0.7251 - val_loss: 0.9326 - val_accuracy: 0.5847\n","Epoch 61/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5821 - accuracy: 0.7276 - val_loss: 0.9526 - val_accuracy: 0.5671\n","Epoch 62/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5824 - accuracy: 0.7289 - val_loss: 1.0293 - val_accuracy: 0.5506\n","Epoch 63/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5712 - accuracy: 0.7419 - val_loss: 1.0072 - val_accuracy: 0.5620\n","Epoch 64/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5721 - accuracy: 0.7375 - val_loss: 0.9228 - val_accuracy: 0.5671\n","Epoch 65/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5744 - accuracy: 0.7326 - val_loss: 0.8992 - val_accuracy: 0.5785\n","Epoch 66/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5673 - accuracy: 0.7424 - val_loss: 1.0413 - val_accuracy: 0.5589\n","Epoch 67/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5767 - accuracy: 0.7271 - val_loss: 0.9272 - val_accuracy: 0.5816\n","Epoch 68/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5782 - accuracy: 0.7245 - val_loss: 0.9685 - val_accuracy: 0.5795\n","Epoch 69/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5733 - accuracy: 0.7382 - val_loss: 0.9954 - val_accuracy: 0.5888\n","Epoch 70/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5677 - accuracy: 0.7398 - val_loss: 0.9605 - val_accuracy: 0.5888\n","Epoch 71/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5643 - accuracy: 0.7481 - val_loss: 0.9280 - val_accuracy: 0.5878\n","Epoch 72/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5682 - accuracy: 0.7413 - val_loss: 0.8950 - val_accuracy: 0.5930\n","Epoch 73/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5701 - accuracy: 0.7359 - val_loss: 1.0565 - val_accuracy: 0.5558\n","Epoch 74/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5680 - accuracy: 0.7370 - val_loss: 0.9541 - val_accuracy: 0.6012\n","Epoch 75/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5643 - accuracy: 0.7426 - val_loss: 0.9995 - val_accuracy: 0.5713\n","Epoch 76/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5627 - accuracy: 0.7434 - val_loss: 0.9943 - val_accuracy: 0.5744\n","Epoch 77/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5725 - accuracy: 0.7297 - val_loss: 1.0449 - val_accuracy: 0.5661\n","Epoch 78/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5579 - accuracy: 0.7401 - val_loss: 0.9141 - val_accuracy: 0.5857\n","Epoch 79/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5610 - accuracy: 0.7452 - val_loss: 0.9715 - val_accuracy: 0.5868\n","Epoch 80/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5546 - accuracy: 0.7512 - val_loss: 0.9511 - val_accuracy: 0.5733\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5577 - accuracy: 0.7411 - val_loss: 1.0075 - val_accuracy: 0.5630\n","Epoch 82/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5642 - accuracy: 0.7388 - val_loss: 0.9701 - val_accuracy: 0.5806\n","Epoch 83/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5604 - accuracy: 0.7434 - val_loss: 0.9290 - val_accuracy: 0.5837\n","Epoch 84/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5540 - accuracy: 0.7491 - val_loss: 0.9849 - val_accuracy: 0.5795\n","Epoch 85/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5630 - accuracy: 0.7426 - val_loss: 0.9672 - val_accuracy: 0.5868\n","Epoch 86/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5559 - accuracy: 0.7514 - val_loss: 0.9366 - val_accuracy: 0.5795\n","Epoch 87/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5476 - accuracy: 0.7517 - val_loss: 0.9326 - val_accuracy: 0.5868\n","Epoch 88/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5484 - accuracy: 0.7486 - val_loss: 0.9716 - val_accuracy: 0.5950\n","Epoch 89/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5454 - accuracy: 0.7561 - val_loss: 1.0116 - val_accuracy: 0.5733\n","Epoch 90/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5534 - accuracy: 0.7532 - val_loss: 0.9969 - val_accuracy: 0.5878\n","Epoch 91/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5490 - accuracy: 0.7576 - val_loss: 0.9768 - val_accuracy: 0.5785\n","Epoch 92/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5496 - accuracy: 0.7558 - val_loss: 1.0524 - val_accuracy: 0.5548\n","Epoch 93/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5532 - accuracy: 0.7483 - val_loss: 1.1551 - val_accuracy: 0.5424\n","Epoch 94/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5573 - accuracy: 0.7336 - val_loss: 0.9804 - val_accuracy: 0.5899\n","Epoch 95/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5496 - accuracy: 0.7488 - val_loss: 1.0076 - val_accuracy: 0.5847\n","Epoch 96/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5438 - accuracy: 0.7584 - val_loss: 0.9394 - val_accuracy: 0.5930\n","Epoch 97/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5393 - accuracy: 0.7527 - val_loss: 0.9617 - val_accuracy: 0.5785\n","Epoch 98/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5370 - accuracy: 0.7561 - val_loss: 1.0194 - val_accuracy: 0.5888\n","Epoch 99/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5418 - accuracy: 0.7540 - val_loss: 0.9920 - val_accuracy: 0.5806\n","Epoch 100/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5362 - accuracy: 0.7543 - val_loss: 0.9921 - val_accuracy: 0.5795\n","{'loss': [0.6552456617355347, 0.6523003578186035, 0.6462470293045044, 0.6432666182518005, 0.6382908225059509, 0.6308881640434265, 0.6331052780151367, 0.6342125535011292, 0.6321322321891785, 0.6291815638542175, 0.6318347454071045, 0.6258261203765869, 0.6286827921867371, 0.628402054309845, 0.6250159740447998, 0.6211563944816589, 0.619938850402832, 0.6261969208717346, 0.622084379196167, 0.6266152858734131, 0.6212342381477356, 0.6163219809532166, 0.6129478812217712, 0.6148154735565186, 0.6092922687530518, 0.612605631351471, 0.6053135395050049, 0.605426549911499, 0.6073209643363953, 0.598760187625885, 0.6053040623664856, 0.6073797941207886, 0.604820191860199, 0.5998028516769409, 0.6008313894271851, 0.6037107706069946, 0.6051055192947388, 0.5988372564315796, 0.5984366536140442, 0.6013950705528259, 0.5924885869026184, 0.5929085612297058, 0.5889972448348999, 0.5940233469009399, 0.5898067951202393, 0.5948803424835205, 0.5937680602073669, 0.5901591181755066, 0.5898464918136597, 0.5879691243171692, 0.5949706435203552, 0.5979363918304443, 0.5857778191566467, 0.5850574374198914, 0.5789309740066528, 0.5837209820747375, 0.5807315707206726, 0.5767658948898315, 0.5774639844894409, 0.5828962326049805, 0.5821290612220764, 0.5823598504066467, 0.5712369680404663, 0.5721100568771362, 0.5744432210922241, 0.5672938227653503, 0.5767292380332947, 0.5782084465026855, 0.5733456015586853, 0.5676921606063843, 0.5642885565757751, 0.568184494972229, 0.5700908303260803, 0.5679789781570435, 0.5643056035041809, 0.5626704096794128, 0.5725131630897522, 0.5578654408454895, 0.5609562397003174, 0.554568886756897, 0.5576589107513428, 0.5641644597053528, 0.5604154467582703, 0.553984522819519, 0.5630242824554443, 0.555919349193573, 0.5475530028343201, 0.5484480857849121, 0.5454145669937134, 0.5533839464187622, 0.549034059047699, 0.5495529770851135, 0.5532081723213196, 0.5573485493659973, 0.5495598316192627, 0.5438316464424133, 0.5393403768539429, 0.5370064377784729, 0.5417531132698059, 0.536231517791748], 'accuracy': [0.6785529851913452, 0.6888889074325562, 0.6837209463119507, 0.6927648782730103, 0.6950904130935669, 0.6987079977989197, 0.6901808977127075, 0.6956072449684143, 0.6945736408233643, 0.6984496116638184, 0.6917312741279602, 0.6997416019439697, 0.6901808977127075, 0.6901808977127075, 0.6997416019439697, 0.7020671963691711, 0.7072351574897766, 0.6930232644081116, 0.6992248296737671, 0.6994832158088684, 0.6979328393936157, 0.7036175727844238, 0.7054263353347778, 0.7082687616348267, 0.7147286534309387, 0.713178277015686, 0.7062015533447266, 0.7118862867355347, 0.7038759589195251, 0.7253230214118958, 0.7080103158950806, 0.713178277015686, 0.713178277015686, 0.7160206437110901, 0.7193798422813416, 0.7157622575759888, 0.711369514465332, 0.7134366631507874, 0.7165374755859375, 0.7201550602912903, 0.7173126339912415, 0.7209302186965942, 0.724547803401947, 0.7268733978271484, 0.7284237742424011, 0.7142118811607361, 0.7222222089767456, 0.7297157645225525, 0.7240310311317444, 0.7307493686676025, 0.7209302186965942, 0.7139534950256348, 0.7289405465126038, 0.7136951088905334, 0.7273901700973511, 0.7268733978271484, 0.7302325367927551, 0.7291989922523499, 0.7343669533729553, 0.7250645756721497, 0.7276485562324524, 0.7289405465126038, 0.7418604493141174, 0.7374677062034607, 0.7325581312179565, 0.7423772811889648, 0.7271317839622498, 0.724547803401947, 0.7382428646087646, 0.7397933006286621, 0.748062014579773, 0.7413436770439148, 0.735917329788208, 0.7369509339332581, 0.7426356673240662, 0.7434108257293701, 0.7297157645225525, 0.7400516867637634, 0.7452196478843689, 0.7511627674102783, 0.7410852909088135, 0.7387596964836121, 0.7434108257293701, 0.749095618724823, 0.7426356673240662, 0.7514212131500244, 0.7516795992851257, 0.7485787868499756, 0.7560723423957825, 0.7532299757003784, 0.7576227188110352, 0.7558139562606812, 0.7483204007148743, 0.7335917353630066, 0.7488372325897217, 0.7583979368209839, 0.7527132034301758, 0.7560723423957825, 0.7540051937103271, 0.7542635798454285], 'val_loss': [0.8026898503303528, 0.786676824092865, 0.8024241328239441, 0.7815057039260864, 0.7943875193595886, 0.8093649744987488, 0.7988961935043335, 0.7982003092765808, 0.794653058052063, 0.7769655585289001, 0.7657224535942078, 0.7661474943161011, 0.7917178869247437, 0.7833014726638794, 0.7758257985115051, 0.7630673050880432, 0.7915758490562439, 0.7619871497154236, 0.765949547290802, 0.7941156029701233, 0.7868093848228455, 0.803900420665741, 0.8006694912910461, 0.7958801984786987, 0.8353852033615112, 0.8123214244842529, 0.8376625776290894, 0.8443688154220581, 0.8391107320785522, 0.8490942716598511, 0.8377841114997864, 0.9060367941856384, 0.8639463782310486, 0.8686339855194092, 0.8686272501945496, 0.9273887872695923, 0.898274838924408, 0.8680883646011353, 0.928864598274231, 0.8796048164367676, 0.8858140707015991, 0.8748343586921692, 0.9278897643089294, 0.8708508610725403, 0.8878611326217651, 0.9457316994667053, 0.9346898794174194, 0.9648422598838806, 0.8997443914413452, 0.8950657844543457, 0.8838624358177185, 0.9146103858947754, 0.8890711665153503, 0.9626050591468811, 0.9579061269760132, 0.9111170768737793, 0.9170269966125488, 0.9370799660682678, 0.9203298091888428, 0.932555079460144, 0.952606201171875, 1.0293359756469727, 1.0072357654571533, 0.9228001832962036, 0.8992395401000977, 1.0412917137145996, 0.9272380471229553, 0.9685173630714417, 0.9954078197479248, 0.9604703187942505, 0.9280015230178833, 0.8949949145317078, 1.0565438270568848, 0.9540567398071289, 0.9995067715644836, 0.9942735433578491, 1.0448826551437378, 0.9141201972961426, 0.9714741110801697, 0.9510637521743774, 1.0074844360351562, 0.9700838923454285, 0.9289557933807373, 0.9848962426185608, 0.967218279838562, 0.9366037249565125, 0.9325896501541138, 0.9715696573257446, 1.0115933418273926, 0.9969373345375061, 0.976821780204773, 1.0523744821548462, 1.155098795890808, 0.9804025888442993, 1.0076189041137695, 0.9394007921218872, 0.9616919159889221, 1.01944100856781, 0.992006778717041, 0.9920950531959534], 'val_accuracy': [0.5175619721412659, 0.5206611752510071, 0.5175619721412659, 0.5278925895690918, 0.5206611752510071, 0.5206611752510071, 0.5258264541625977, 0.5278925895690918, 0.5268595218658447, 0.5464876294136047, 0.5609503984451294, 0.567148745059967, 0.5454545617103577, 0.5599173307418823, 0.5702479481697083, 0.5743801593780518, 0.5692148804664612, 0.5971074104309082, 0.5785123705863953, 0.5836777091026306, 0.5991735458374023, 0.5929751992225647, 0.6064049601554871, 0.5950413346290588, 0.577479362487793, 0.5847107172012329, 0.5826446413993835, 0.5888429880142212, 0.6043388247489929, 0.5971074104309082, 0.5898760557174683, 0.5702479481697083, 0.6064049601554871, 0.5960744023323059, 0.5847107172012329, 0.60537189245224, 0.5836777091026306, 0.5888429880142212, 0.5650826692581177, 0.5888429880142212, 0.5919421315193176, 0.5950413346290588, 0.5692148804664612, 0.5826446413993835, 0.5940082669258118, 0.5826446413993835, 0.5909090638160706, 0.6002066135406494, 0.5816115736961365, 0.5878099203109741, 0.5826446413993835, 0.5971074104309082, 0.5816115736961365, 0.5681818127632141, 0.5723140239715576, 0.6012396812438965, 0.5785123705863953, 0.5826446413993835, 0.5888429880142212, 0.5847107172012329, 0.567148745059967, 0.5506198406219482, 0.5619834661483765, 0.567148745059967, 0.5785123705863953, 0.55888432264328, 0.5816115736961365, 0.5795454382896423, 0.5888429880142212, 0.5888429880142212, 0.5878099203109741, 0.5929751992225647, 0.5557851195335388, 0.6012396812438965, 0.5712810158729553, 0.5743801593780518, 0.56611567735672, 0.58574378490448, 0.586776852607727, 0.5733470916748047, 0.5630165338516235, 0.5805785059928894, 0.5836777091026306, 0.5795454382896423, 0.586776852607727, 0.5795454382896423, 0.586776852607727, 0.5950413346290588, 0.5733470916748047, 0.5878099203109741, 0.5785123705863953, 0.5547520518302917, 0.5423553586006165, 0.5898760557174683, 0.5847107172012329, 0.5929751992225647, 0.5785123705863953, 0.5888429880142212, 0.5805785059928894, 0.5795454382896423]}\n","32/32 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","26/29 [=========================>....] - ETA: 0s - loss: 0.5702 - accuracy: 0.7416"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["29/29 [==============================] - 8s 62ms/step - loss: 0.5667 - accuracy: 0.7449 - val_loss: 0.7750 - val_accuracy: 0.5280\n","Epoch 2/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5622 - accuracy: 0.7406 - val_loss: 0.7918 - val_accuracy: 0.5226\n","Epoch 3/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.5728 - accuracy: 0.7398 - val_loss: 0.7676 - val_accuracy: 0.5690\n","Epoch 4/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.5475 - accuracy: 0.7505 - val_loss: 0.7656 - val_accuracy: 0.5959\n","Epoch 5/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5527 - accuracy: 0.7355 - val_loss: 0.7723 - val_accuracy: 0.5022\n","Epoch 6/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.5479 - accuracy: 0.7524 - val_loss: 0.7608 - val_accuracy: 0.6369\n","Epoch 7/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5487 - accuracy: 0.7408 - val_loss: 0.7601 - val_accuracy: 0.6131\n","Epoch 8/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5460 - accuracy: 0.7470 - val_loss: 0.7606 - val_accuracy: 0.5948\n","Epoch 9/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5452 - accuracy: 0.7557 - val_loss: 0.7628 - val_accuracy: 0.5851\n","Epoch 10/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5407 - accuracy: 0.7530 - val_loss: 0.7826 - val_accuracy: 0.5657\n","Epoch 11/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5483 - accuracy: 0.7487 - val_loss: 0.7536 - val_accuracy: 0.5970\n","Epoch 12/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5427 - accuracy: 0.7573 - val_loss: 0.7491 - val_accuracy: 0.6164\n","Epoch 13/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5441 - accuracy: 0.7513 - val_loss: 0.7531 - val_accuracy: 0.6412\n","Epoch 14/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.5329 - accuracy: 0.7546 - val_loss: 0.7370 - val_accuracy: 0.6627\n","Epoch 15/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5359 - accuracy: 0.7557 - val_loss: 0.7339 - val_accuracy: 0.6476\n","Epoch 16/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5334 - accuracy: 0.7597 - val_loss: 0.7361 - val_accuracy: 0.6519\n","Epoch 17/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5262 - accuracy: 0.7624 - val_loss: 0.7376 - val_accuracy: 0.6595\n","Epoch 18/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5244 - accuracy: 0.7734 - val_loss: 0.7476 - val_accuracy: 0.6455\n","Epoch 19/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5206 - accuracy: 0.7680 - val_loss: 0.7900 - val_accuracy: 0.6185\n","Epoch 20/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5346 - accuracy: 0.7554 - val_loss: 0.7614 - val_accuracy: 0.6433\n","Epoch 21/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5260 - accuracy: 0.7594 - val_loss: 0.7541 - val_accuracy: 0.6519\n","Epoch 22/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5414 - accuracy: 0.7505 - val_loss: 0.7728 - val_accuracy: 0.6369\n","Epoch 23/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.5246 - accuracy: 0.7600 - val_loss: 0.7531 - val_accuracy: 0.6562\n","Epoch 24/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5253 - accuracy: 0.7627 - val_loss: 0.7664 - val_accuracy: 0.6595\n","Epoch 25/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5276 - accuracy: 0.7675 - val_loss: 0.7822 - val_accuracy: 0.6509\n","Epoch 26/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5202 - accuracy: 0.7640 - val_loss: 0.7948 - val_accuracy: 0.6487\n","Epoch 27/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5189 - accuracy: 0.7729 - val_loss: 0.7968 - val_accuracy: 0.6573\n","Epoch 28/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.5304 - accuracy: 0.7605 - val_loss: 0.7844 - val_accuracy: 0.6713\n","Epoch 29/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5178 - accuracy: 0.7635 - val_loss: 0.8130 - val_accuracy: 0.6595\n","Epoch 30/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5210 - accuracy: 0.7624 - val_loss: 0.8030 - val_accuracy: 0.6552\n","Epoch 31/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5123 - accuracy: 0.7807 - val_loss: 0.7854 - val_accuracy: 0.6552\n","Epoch 32/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5065 - accuracy: 0.7780 - val_loss: 0.8554 - val_accuracy: 0.6498\n","Epoch 33/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5115 - accuracy: 0.7702 - val_loss: 0.8101 - val_accuracy: 0.6649\n","Epoch 34/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5203 - accuracy: 0.7680 - val_loss: 0.8536 - val_accuracy: 0.6487\n","Epoch 35/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5151 - accuracy: 0.7635 - val_loss: 0.9308 - val_accuracy: 0.6369\n","Epoch 36/100\n","29/29 [==============================] - 1s 30ms/step - loss: 0.5079 - accuracy: 0.7710 - val_loss: 0.8047 - val_accuracy: 0.6778\n","Epoch 37/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5095 - accuracy: 0.7737 - val_loss: 0.8183 - val_accuracy: 0.6552\n","Epoch 38/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5030 - accuracy: 0.7734 - val_loss: 0.8176 - val_accuracy: 0.6616\n","Epoch 39/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5039 - accuracy: 0.7818 - val_loss: 0.8124 - val_accuracy: 0.6530\n","Epoch 40/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5178 - accuracy: 0.7667 - val_loss: 0.8436 - val_accuracy: 0.6616\n","Epoch 41/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5054 - accuracy: 0.7775 - val_loss: 0.8146 - val_accuracy: 0.6767\n","Epoch 42/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5124 - accuracy: 0.7718 - val_loss: 0.8473 - val_accuracy: 0.6670\n","Epoch 43/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5212 - accuracy: 0.7729 - val_loss: 0.9308 - val_accuracy: 0.5991\n","Epoch 44/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5169 - accuracy: 0.7777 - val_loss: 0.8696 - val_accuracy: 0.6455\n","Epoch 45/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5083 - accuracy: 0.7772 - val_loss: 0.8711 - val_accuracy: 0.6487\n","Epoch 46/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4962 - accuracy: 0.7907 - val_loss: 0.9699 - val_accuracy: 0.6293\n","Epoch 47/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4987 - accuracy: 0.7756 - val_loss: 0.8214 - val_accuracy: 0.6509\n","Epoch 48/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.4939 - accuracy: 0.7883 - val_loss: 0.8606 - val_accuracy: 0.6390\n","Epoch 49/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4964 - accuracy: 0.7818 - val_loss: 0.8765 - val_accuracy: 0.6444\n","Epoch 50/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.4908 - accuracy: 0.7831 - val_loss: 0.8484 - val_accuracy: 0.6595\n","Epoch 51/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4911 - accuracy: 0.7864 - val_loss: 0.9464 - val_accuracy: 0.6325\n","Epoch 52/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4938 - accuracy: 0.7783 - val_loss: 0.9369 - val_accuracy: 0.6121\n","Epoch 53/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5009 - accuracy: 0.7761 - val_loss: 1.0414 - val_accuracy: 0.5808\n","Epoch 54/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4944 - accuracy: 0.7804 - val_loss: 0.8438 - val_accuracy: 0.6530\n","Epoch 55/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4860 - accuracy: 0.7893 - val_loss: 0.8452 - val_accuracy: 0.6627\n","Epoch 56/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4878 - accuracy: 0.7837 - val_loss: 0.9672 - val_accuracy: 0.6045\n","Epoch 57/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4924 - accuracy: 0.7923 - val_loss: 0.9886 - val_accuracy: 0.5981\n","Epoch 58/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4826 - accuracy: 0.7926 - val_loss: 0.8852 - val_accuracy: 0.6466\n","Epoch 59/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4830 - accuracy: 0.7961 - val_loss: 1.0172 - val_accuracy: 0.5830\n","Epoch 60/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4821 - accuracy: 0.7934 - val_loss: 0.9833 - val_accuracy: 0.5959\n","Epoch 61/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4853 - accuracy: 0.7888 - val_loss: 0.8808 - val_accuracy: 0.6390\n","Epoch 62/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4818 - accuracy: 0.7928 - val_loss: 0.8641 - val_accuracy: 0.6487\n","Epoch 63/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4826 - accuracy: 0.7950 - val_loss: 0.8945 - val_accuracy: 0.6433\n","Epoch 64/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4886 - accuracy: 0.7848 - val_loss: 0.8616 - val_accuracy: 0.6498\n","Epoch 65/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4776 - accuracy: 0.7953 - val_loss: 0.8790 - val_accuracy: 0.6466\n","Epoch 66/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4783 - accuracy: 0.7980 - val_loss: 0.8776 - val_accuracy: 0.6595\n","Epoch 67/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4771 - accuracy: 0.7923 - val_loss: 0.9103 - val_accuracy: 0.6724\n","Epoch 68/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4818 - accuracy: 0.7907 - val_loss: 0.8964 - val_accuracy: 0.6487\n","Epoch 69/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4881 - accuracy: 0.7899 - val_loss: 0.9912 - val_accuracy: 0.6336\n","Epoch 70/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4713 - accuracy: 0.8009 - val_loss: 0.9064 - val_accuracy: 0.6466\n","Epoch 71/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4701 - accuracy: 0.7982 - val_loss: 0.9038 - val_accuracy: 0.6455\n","Epoch 72/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4733 - accuracy: 0.7945 - val_loss: 0.9268 - val_accuracy: 0.6358\n","Epoch 73/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4731 - accuracy: 0.7939 - val_loss: 1.2124 - val_accuracy: 0.5711\n","Epoch 74/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4695 - accuracy: 0.8020 - val_loss: 0.8848 - val_accuracy: 0.6573\n","Epoch 75/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4751 - accuracy: 0.7953 - val_loss: 0.8861 - val_accuracy: 0.6422\n","Epoch 76/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4689 - accuracy: 0.7977 - val_loss: 0.8890 - val_accuracy: 0.6756\n","Epoch 77/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4692 - accuracy: 0.8103 - val_loss: 0.9152 - val_accuracy: 0.6487\n","Epoch 78/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4759 - accuracy: 0.7985 - val_loss: 0.8823 - val_accuracy: 0.6509\n","Epoch 79/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4575 - accuracy: 0.8068 - val_loss: 1.2104 - val_accuracy: 0.5744\n","Epoch 80/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4661 - accuracy: 0.7990 - val_loss: 0.9429 - val_accuracy: 0.6315\n","Epoch 81/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4691 - accuracy: 0.7939 - val_loss: 0.9037 - val_accuracy: 0.6562\n","Epoch 82/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4671 - accuracy: 0.8004 - val_loss: 0.9064 - val_accuracy: 0.6433\n","Epoch 83/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4722 - accuracy: 0.7853 - val_loss: 0.9375 - val_accuracy: 0.6466\n","Epoch 84/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4729 - accuracy: 0.7963 - val_loss: 0.9396 - val_accuracy: 0.6466\n","Epoch 85/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4600 - accuracy: 0.8093 - val_loss: 0.8956 - val_accuracy: 0.6412\n","Epoch 86/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4610 - accuracy: 0.8060 - val_loss: 1.0331 - val_accuracy: 0.6304\n","Epoch 87/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.4625 - accuracy: 0.8025 - val_loss: 0.9765 - val_accuracy: 0.6325\n","Epoch 88/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.4581 - accuracy: 0.8074 - val_loss: 0.9483 - val_accuracy: 0.6412\n","Epoch 89/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.4573 - accuracy: 0.8004 - val_loss: 0.9528 - val_accuracy: 0.6315\n","Epoch 90/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4608 - accuracy: 0.8033 - val_loss: 0.9438 - val_accuracy: 0.6466\n","Epoch 91/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4557 - accuracy: 0.8060 - val_loss: 0.9575 - val_accuracy: 0.6401\n","Epoch 92/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4627 - accuracy: 0.8001 - val_loss: 0.9640 - val_accuracy: 0.6422\n","Epoch 93/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.4625 - accuracy: 0.8050 - val_loss: 1.0716 - val_accuracy: 0.6304\n","Epoch 94/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4552 - accuracy: 0.8060 - val_loss: 0.9161 - val_accuracy: 0.6616\n","Epoch 95/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4430 - accuracy: 0.8176 - val_loss: 1.0423 - val_accuracy: 0.6045\n","Epoch 96/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4452 - accuracy: 0.8133 - val_loss: 0.9314 - val_accuracy: 0.6476\n","Epoch 97/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4455 - accuracy: 0.8152 - val_loss: 1.1881 - val_accuracy: 0.5776\n","Epoch 98/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4542 - accuracy: 0.8103 - val_loss: 0.9617 - val_accuracy: 0.6552\n","Epoch 99/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4607 - accuracy: 0.8015 - val_loss: 1.1136 - val_accuracy: 0.5948\n","Epoch 100/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4455 - accuracy: 0.8087 - val_loss: 1.0027 - val_accuracy: 0.6325\n","{'loss': [0.5667410492897034, 0.5621606111526489, 0.5727567076683044, 0.5475346446037292, 0.5527467131614685, 0.5479232668876648, 0.548747181892395, 0.5459843277931213, 0.5451542139053345, 0.5406731367111206, 0.548302412033081, 0.5427088737487793, 0.5440576076507568, 0.5328710079193115, 0.5359444618225098, 0.5334468483924866, 0.5262162685394287, 0.5244193077087402, 0.5205654501914978, 0.5346424579620361, 0.5259774327278137, 0.541405439376831, 0.5245801210403442, 0.525321900844574, 0.5275698304176331, 0.5201701521873474, 0.5188530683517456, 0.530441403388977, 0.5177816152572632, 0.520973801612854, 0.5122982263565063, 0.5064523220062256, 0.5114811658859253, 0.5202615857124329, 0.5151243805885315, 0.507860004901886, 0.5095221996307373, 0.5029566287994385, 0.5038682818412781, 0.5177518129348755, 0.5053983330726624, 0.5123915076255798, 0.5211818814277649, 0.5168600678443909, 0.5083425045013428, 0.49617698788642883, 0.49866345524787903, 0.49394097924232483, 0.4963764548301697, 0.49078431725502014, 0.4910581111907959, 0.4937945008277893, 0.5009141564369202, 0.4943656027317047, 0.48598185181617737, 0.487758994102478, 0.492352157831192, 0.48256850242614746, 0.4830230176448822, 0.4821462035179138, 0.4853137731552124, 0.48180916905403137, 0.4826005697250366, 0.48858198523521423, 0.477608323097229, 0.47832465171813965, 0.47712379693984985, 0.48178908228874207, 0.4880791902542114, 0.47131413221359253, 0.47006288170814514, 0.4732723534107208, 0.47308093309402466, 0.46949121356010437, 0.475050151348114, 0.46889251470565796, 0.46916186809539795, 0.47592970728874207, 0.45754116773605347, 0.46608150005340576, 0.4691314399242401, 0.4671185612678528, 0.4722253084182739, 0.4729444980621338, 0.4600251019001007, 0.46096453070640564, 0.4624636471271515, 0.4581111967563629, 0.4572950601577759, 0.4607981741428375, 0.45565134286880493, 0.4626723527908325, 0.462540864944458, 0.45517751574516296, 0.4430173337459564, 0.4452325105667114, 0.4454536736011505, 0.4541695713996887, 0.4606649875640869, 0.445549339056015], 'accuracy': [0.7448814511299133, 0.740571141242981, 0.7397629022598267, 0.7505387663841248, 0.7354525923728943, 0.7524245977401733, 0.740840494632721, 0.7470366358757019, 0.7556573152542114, 0.7529633641242981, 0.748652994632721, 0.7572737336158752, 0.751347005367279, 0.7545797228813171, 0.7556573152542114, 0.7596982717514038, 0.7623922228813171, 0.7734375, 0.7680495977401733, 0.7553879022598267, 0.759428858757019, 0.7505387663841248, 0.7599676847457886, 0.7626616358757019, 0.7675107717514038, 0.764008641242981, 0.7728987336158752, 0.7605064511299133, 0.7634698152542114, 0.7623922228813171, 0.7807112336158752, 0.7780172228813171, 0.7702047228813171, 0.7680495977401733, 0.7634698152542114, 0.7710129022598267, 0.7737069129943848, 0.7734375, 0.7817887663841248, 0.7667025923728943, 0.7774784564971924, 0.771821141242981, 0.7728987336158752, 0.7777478694915771, 0.7772090435028076, 0.790678858757019, 0.7755926847457886, 0.7882543206214905, 0.7817887663841248, 0.7831357717514038, 0.7863685488700867, 0.7782866358757019, 0.7761314511299133, 0.7804418206214905, 0.7893319129943848, 0.7836745977401733, 0.7922952771186829, 0.7925646305084229, 0.7960668206214905, 0.7933728694915771, 0.7887930870056152, 0.7928340435028076, 0.7949892282485962, 0.7847521305084229, 0.795258641242981, 0.7979525923728943, 0.7922952771186829, 0.790678858757019, 0.7898706793785095, 0.8009159564971924, 0.798222005367279, 0.7944504022598267, 0.7939116358757019, 0.8019935488700867, 0.795258641242981, 0.7976831793785095, 0.8103448152542114, 0.798491358757019, 0.8068426847457886, 0.7990301847457886, 0.7939116358757019, 0.8003771305084229, 0.7852909564971924, 0.7963362336158752, 0.8092672228813171, 0.806034505367279, 0.8025323152542114, 0.8073814511299133, 0.8003771305084229, 0.803340494632721, 0.806034505367279, 0.8001077771186829, 0.8049569129943848, 0.806034505367279, 0.8176185488700867, 0.8133081793785095, 0.8151939511299133, 0.8103448152542114, 0.8014547228813171, 0.8087284564971924], 'val_loss': [0.774997353553772, 0.791810929775238, 0.7676212787628174, 0.7655938863754272, 0.7722798585891724, 0.7608015537261963, 0.7600504159927368, 0.760556161403656, 0.762819230556488, 0.7825878858566284, 0.7535920739173889, 0.7490583658218384, 0.7531113624572754, 0.736997127532959, 0.7339165210723877, 0.7360725998878479, 0.7375532388687134, 0.7476105690002441, 0.7899748086929321, 0.7614321708679199, 0.754090428352356, 0.7728374004364014, 0.7530556321144104, 0.7663511633872986, 0.7822445034980774, 0.7947737574577332, 0.7968226671218872, 0.7843692898750305, 0.8130414485931396, 0.8030070662498474, 0.7853586077690125, 0.8554072380065918, 0.8100903034210205, 0.8536165952682495, 0.9307742118835449, 0.8047226071357727, 0.8182840943336487, 0.8175621628761292, 0.8123560547828674, 0.8435547947883606, 0.814611554145813, 0.8472805619239807, 0.9307851195335388, 0.8696325421333313, 0.8711263537406921, 0.9698601961135864, 0.8214064240455627, 0.8605916500091553, 0.8765368461608887, 0.8483977913856506, 0.9464020133018494, 0.9368722438812256, 1.0413880348205566, 0.8437504172325134, 0.8452351093292236, 0.9672065377235413, 0.9886088371276855, 0.8852201700210571, 1.0171915292739868, 0.9832617044448853, 0.8807998299598694, 0.8641409277915955, 0.8944739699363708, 0.8616130948066711, 0.8790062069892883, 0.8776208758354187, 0.9102806448936462, 0.896369993686676, 0.991187334060669, 0.9063810706138611, 0.9037766456604004, 0.9268107414245605, 1.2123993635177612, 0.8848479986190796, 0.8860626816749573, 0.8890217542648315, 0.9152051210403442, 0.8823127746582031, 1.2104465961456299, 0.942945122718811, 0.9037309885025024, 0.9063973426818848, 0.9374921321868896, 0.9396342039108276, 0.8956169486045837, 1.033088207244873, 0.9764913320541382, 0.9483439326286316, 0.9528065323829651, 0.9437637329101562, 0.9574711918830872, 0.963989794254303, 1.0715548992156982, 0.9160626530647278, 1.042270541191101, 0.9314214587211609, 1.1880528926849365, 0.9616638422012329, 1.1135681867599487, 1.002737045288086], 'val_accuracy': [0.5280172228813171, 0.5226293206214905, 0.568965494632721, 0.5959051847457886, 0.5021551847457886, 0.6368534564971924, 0.6131465435028076, 0.5948275923728943, 0.5851293206214905, 0.5657327771186829, 0.5969827771186829, 0.6163793206214905, 0.6411637663841248, 0.662715494632721, 0.6476293206214905, 0.6519396305084229, 0.6594827771186829, 0.6454741358757019, 0.618534505367279, 0.6433189511299133, 0.6519396305084229, 0.6368534564971924, 0.65625, 0.6594827771186829, 0.6508620977401733, 0.6487069129943848, 0.6573275923728943, 0.6713362336158752, 0.6594827771186829, 0.6551724076271057, 0.6551724076271057, 0.649784505367279, 0.6648706793785095, 0.6487069129943848, 0.6368534564971924, 0.6778017282485962, 0.6551724076271057, 0.6616379022598267, 0.6530172228813171, 0.6616379022598267, 0.6767241358757019, 0.6670258641242981, 0.5991379022598267, 0.6454741358757019, 0.6487069129943848, 0.6293103694915771, 0.6508620977401733, 0.639008641242981, 0.6443965435028076, 0.6594827771186829, 0.6325430870056152, 0.6120689511299133, 0.5808189511299133, 0.6530172228813171, 0.662715494632721, 0.6045258641242981, 0.5980603694915771, 0.6465517282485962, 0.5829741358757019, 0.5959051847457886, 0.639008641242981, 0.6487069129943848, 0.6433189511299133, 0.649784505367279, 0.6465517282485962, 0.6594827771186829, 0.6724137663841248, 0.6487069129943848, 0.6336206793785095, 0.6465517282485962, 0.6454741358757019, 0.6357758641242981, 0.5711206793785095, 0.6573275923728943, 0.642241358757019, 0.6756465435028076, 0.6487069129943848, 0.6508620977401733, 0.5743534564971924, 0.631465494632721, 0.65625, 0.6433189511299133, 0.6465517282485962, 0.6465517282485962, 0.6411637663841248, 0.6303879022598267, 0.6325430870056152, 0.6411637663841248, 0.631465494632721, 0.6465517282485962, 0.6400862336158752, 0.642241358757019, 0.6303879022598267, 0.6616379022598267, 0.6045258641242981, 0.6476293206214905, 0.5775862336158752, 0.6551724076271057, 0.5948275923728943, 0.6325430870056152]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","25/28 [=========================>....] - ETA: 0s - loss: 0.5666 - accuracy: 0.7378"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["28/28 [==============================] - 7s 66ms/step - loss: 0.5662 - accuracy: 0.7385 - val_loss: 0.7583 - val_accuracy: 0.6199\n","Epoch 2/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5689 - accuracy: 0.7462 - val_loss: 0.7542 - val_accuracy: 0.5860\n","Epoch 3/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5656 - accuracy: 0.7422 - val_loss: 0.7518 - val_accuracy: 0.5430\n","Epoch 4/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5551 - accuracy: 0.7507 - val_loss: 0.7569 - val_accuracy: 0.5475\n","Epoch 5/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5464 - accuracy: 0.7575 - val_loss: 0.7638 - val_accuracy: 0.5385\n","Epoch 6/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5503 - accuracy: 0.7462 - val_loss: 0.7482 - val_accuracy: 0.5633\n","Epoch 7/100\n","28/28 [==============================] - 1s 37ms/step - loss: 0.5462 - accuracy: 0.7581 - val_loss: 0.7379 - val_accuracy: 0.6312\n","Epoch 8/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5517 - accuracy: 0.7490 - val_loss: 0.7611 - val_accuracy: 0.5532\n","Epoch 9/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5390 - accuracy: 0.7603 - val_loss: 0.7726 - val_accuracy: 0.5452\n","Epoch 10/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5427 - accuracy: 0.7473 - val_loss: 0.7316 - val_accuracy: 0.5928\n","Epoch 11/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.5532 - accuracy: 0.7450 - val_loss: 0.7207 - val_accuracy: 0.6369\n","Epoch 12/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5546 - accuracy: 0.7391 - val_loss: 0.7417 - val_accuracy: 0.5758\n","Epoch 13/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5413 - accuracy: 0.7504 - val_loss: 0.7391 - val_accuracy: 0.5826\n","Epoch 14/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5379 - accuracy: 0.7521 - val_loss: 0.7091 - val_accuracy: 0.6357\n","Epoch 15/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5311 - accuracy: 0.7609 - val_loss: 0.7185 - val_accuracy: 0.5995\n","Epoch 16/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5354 - accuracy: 0.7510 - val_loss: 0.7165 - val_accuracy: 0.6244\n","Epoch 17/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5330 - accuracy: 0.7677 - val_loss: 0.7018 - val_accuracy: 0.6369\n","Epoch 18/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5333 - accuracy: 0.7603 - val_loss: 0.7186 - val_accuracy: 0.6154\n","Epoch 19/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.5356 - accuracy: 0.7572 - val_loss: 0.7061 - val_accuracy: 0.6629\n","Epoch 20/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.5310 - accuracy: 0.7550 - val_loss: 0.6997 - val_accuracy: 0.6867\n","Epoch 21/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5272 - accuracy: 0.7592 - val_loss: 0.7143 - val_accuracy: 0.6652\n","Epoch 22/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5310 - accuracy: 0.7600 - val_loss: 0.7248 - val_accuracy: 0.6471\n","Epoch 23/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5240 - accuracy: 0.7646 - val_loss: 0.7347 - val_accuracy: 0.6437\n","Epoch 24/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5312 - accuracy: 0.7702 - val_loss: 0.7345 - val_accuracy: 0.6821\n","Epoch 25/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5259 - accuracy: 0.7668 - val_loss: 0.7445 - val_accuracy: 0.6753\n","Epoch 26/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5303 - accuracy: 0.7637 - val_loss: 0.7607 - val_accuracy: 0.6618\n","Epoch 27/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5269 - accuracy: 0.7663 - val_loss: 0.7702 - val_accuracy: 0.6459\n","Epoch 28/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5302 - accuracy: 0.7651 - val_loss: 0.7256 - val_accuracy: 0.6618\n","Epoch 29/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5259 - accuracy: 0.7685 - val_loss: 0.7587 - val_accuracy: 0.6618\n","Epoch 30/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5226 - accuracy: 0.7654 - val_loss: 0.7732 - val_accuracy: 0.6708\n","Epoch 31/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5204 - accuracy: 0.7680 - val_loss: 0.8608 - val_accuracy: 0.6357\n","Epoch 32/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5202 - accuracy: 0.7731 - val_loss: 0.7478 - val_accuracy: 0.6640\n","Epoch 33/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5206 - accuracy: 0.7708 - val_loss: 0.7422 - val_accuracy: 0.6686\n","Epoch 34/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5230 - accuracy: 0.7646 - val_loss: 0.7829 - val_accuracy: 0.6787\n","Epoch 35/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5170 - accuracy: 0.7705 - val_loss: 0.8157 - val_accuracy: 0.6686\n","Epoch 36/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5238 - accuracy: 0.7646 - val_loss: 0.8325 - val_accuracy: 0.6380\n","Epoch 37/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5104 - accuracy: 0.7827 - val_loss: 0.9698 - val_accuracy: 0.6018\n","Epoch 38/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5274 - accuracy: 0.7606 - val_loss: 0.8894 - val_accuracy: 0.6267\n","Epoch 39/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5117 - accuracy: 0.7736 - val_loss: 0.8164 - val_accuracy: 0.6561\n","Epoch 40/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5136 - accuracy: 0.7722 - val_loss: 0.8204 - val_accuracy: 0.6719\n","Epoch 41/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5101 - accuracy: 0.7657 - val_loss: 0.7907 - val_accuracy: 0.6753\n","Epoch 42/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5062 - accuracy: 0.7725 - val_loss: 0.9227 - val_accuracy: 0.6143\n","Epoch 43/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5024 - accuracy: 0.7807 - val_loss: 0.9341 - val_accuracy: 0.6165\n","Epoch 44/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5023 - accuracy: 0.7801 - val_loss: 0.8159 - val_accuracy: 0.6527\n","Epoch 45/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5043 - accuracy: 0.7790 - val_loss: 0.8500 - val_accuracy: 0.6572\n","Epoch 46/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5054 - accuracy: 0.7858 - val_loss: 0.8367 - val_accuracy: 0.6753\n","Epoch 47/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5046 - accuracy: 0.7748 - val_loss: 0.8819 - val_accuracy: 0.6516\n","Epoch 48/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4963 - accuracy: 0.7872 - val_loss: 0.8387 - val_accuracy: 0.6697\n","Epoch 49/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5010 - accuracy: 0.7869 - val_loss: 0.8527 - val_accuracy: 0.6686\n","Epoch 50/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5092 - accuracy: 0.7736 - val_loss: 0.8454 - val_accuracy: 0.6787\n","Epoch 51/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5087 - accuracy: 0.7748 - val_loss: 0.8158 - val_accuracy: 0.6550\n","Epoch 52/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4954 - accuracy: 0.7847 - val_loss: 1.0716 - val_accuracy: 0.5950\n","Epoch 53/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4922 - accuracy: 0.7886 - val_loss: 0.8240 - val_accuracy: 0.6708\n","Epoch 54/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4912 - accuracy: 0.7917 - val_loss: 0.8420 - val_accuracy: 0.6572\n","Epoch 55/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5048 - accuracy: 0.7714 - val_loss: 0.8511 - val_accuracy: 0.6618\n","Epoch 56/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4905 - accuracy: 0.7881 - val_loss: 0.8267 - val_accuracy: 0.6505\n","Epoch 57/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5009 - accuracy: 0.7818 - val_loss: 0.8088 - val_accuracy: 0.6629\n","Epoch 58/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.4935 - accuracy: 0.7861 - val_loss: 0.8264 - val_accuracy: 0.6606\n","Epoch 59/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4923 - accuracy: 0.7855 - val_loss: 0.8681 - val_accuracy: 0.6527\n","Epoch 60/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5030 - accuracy: 0.7731 - val_loss: 0.9513 - val_accuracy: 0.6335\n","Epoch 61/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4988 - accuracy: 0.7866 - val_loss: 0.8819 - val_accuracy: 0.6425\n","Epoch 62/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4945 - accuracy: 0.7847 - val_loss: 0.9402 - val_accuracy: 0.6391\n","Epoch 63/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4906 - accuracy: 0.7886 - val_loss: 0.9136 - val_accuracy: 0.6357\n","Epoch 64/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4877 - accuracy: 0.7900 - val_loss: 0.8738 - val_accuracy: 0.6527\n","Epoch 65/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4981 - accuracy: 0.7762 - val_loss: 0.9588 - val_accuracy: 0.6256\n","Epoch 66/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4858 - accuracy: 0.7872 - val_loss: 0.8412 - val_accuracy: 0.6448\n","Epoch 67/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4774 - accuracy: 0.7892 - val_loss: 0.8190 - val_accuracy: 0.6606\n","Epoch 68/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4854 - accuracy: 0.7852 - val_loss: 0.9560 - val_accuracy: 0.6199\n","Epoch 69/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4861 - accuracy: 0.7895 - val_loss: 0.8846 - val_accuracy: 0.6561\n","Epoch 70/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4814 - accuracy: 0.7886 - val_loss: 0.9263 - val_accuracy: 0.6425\n","Epoch 71/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4830 - accuracy: 0.7963 - val_loss: 0.8765 - val_accuracy: 0.6765\n","Epoch 72/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4890 - accuracy: 0.7861 - val_loss: 0.8493 - val_accuracy: 0.6471\n","Epoch 73/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4795 - accuracy: 0.7906 - val_loss: 0.8319 - val_accuracy: 0.6482\n","Epoch 74/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4752 - accuracy: 0.7977 - val_loss: 0.9492 - val_accuracy: 0.6471\n","Epoch 75/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4927 - accuracy: 0.7793 - val_loss: 0.8525 - val_accuracy: 0.6572\n","Epoch 76/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4824 - accuracy: 0.7929 - val_loss: 0.8569 - val_accuracy: 0.6459\n","Epoch 77/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.4787 - accuracy: 0.7988 - val_loss: 1.1012 - val_accuracy: 0.6029\n","Epoch 78/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4721 - accuracy: 0.8084 - val_loss: 0.8819 - val_accuracy: 0.6527\n","Epoch 79/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4679 - accuracy: 0.8022 - val_loss: 1.2454 - val_accuracy: 0.5860\n","Epoch 80/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4736 - accuracy: 0.7934 - val_loss: 0.8953 - val_accuracy: 0.6527\n","Epoch 81/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4767 - accuracy: 0.7940 - val_loss: 0.8523 - val_accuracy: 0.6629\n","Epoch 82/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4674 - accuracy: 0.7906 - val_loss: 0.8779 - val_accuracy: 0.6697\n","Epoch 83/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.4700 - accuracy: 0.8019 - val_loss: 0.9586 - val_accuracy: 0.6482\n","Epoch 84/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4644 - accuracy: 0.7971 - val_loss: 0.8331 - val_accuracy: 0.6686\n","Epoch 85/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4700 - accuracy: 0.7974 - val_loss: 0.8695 - val_accuracy: 0.6686\n","Epoch 86/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.4755 - accuracy: 0.7968 - val_loss: 0.9364 - val_accuracy: 0.6425\n","Epoch 87/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4608 - accuracy: 0.8110 - val_loss: 0.8365 - val_accuracy: 0.6606\n","Epoch 88/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4676 - accuracy: 0.8019 - val_loss: 0.8831 - val_accuracy: 0.6606\n","Epoch 89/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4630 - accuracy: 0.8048 - val_loss: 0.8643 - val_accuracy: 0.6584\n","Epoch 90/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4549 - accuracy: 0.8107 - val_loss: 1.0146 - val_accuracy: 0.6256\n","Epoch 91/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.4663 - accuracy: 0.7988 - val_loss: 0.8898 - val_accuracy: 0.6652\n","Epoch 92/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4592 - accuracy: 0.8005 - val_loss: 0.9576 - val_accuracy: 0.6335\n","Epoch 93/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.4544 - accuracy: 0.8008 - val_loss: 0.9127 - val_accuracy: 0.6584\n","Epoch 94/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4550 - accuracy: 0.8079 - val_loss: 1.0590 - val_accuracy: 0.6154\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4513 - accuracy: 0.8096 - val_loss: 0.9249 - val_accuracy: 0.6471\n","Epoch 96/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4504 - accuracy: 0.8067 - val_loss: 0.9405 - val_accuracy: 0.6425\n","Epoch 97/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4576 - accuracy: 0.8110 - val_loss: 1.5146 - val_accuracy: 0.5735\n","Epoch 98/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4601 - accuracy: 0.8090 - val_loss: 0.9569 - val_accuracy: 0.6550\n","Epoch 99/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4646 - accuracy: 0.7963 - val_loss: 1.0280 - val_accuracy: 0.6471\n","Epoch 100/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4539 - accuracy: 0.8005 - val_loss: 1.0137 - val_accuracy: 0.6482\n","{'loss': [0.5661908388137817, 0.5689429640769958, 0.5656182169914246, 0.555124819278717, 0.5463859438896179, 0.5503407120704651, 0.5462431907653809, 0.55165034532547, 0.5390323996543884, 0.5426826477050781, 0.5532127022743225, 0.5545534491539001, 0.5413021445274353, 0.5379122495651245, 0.5311124324798584, 0.5353672504425049, 0.5329962372779846, 0.5333337187767029, 0.5356443524360657, 0.5310074687004089, 0.5271918773651123, 0.5310032963752747, 0.5240381360054016, 0.5312049388885498, 0.5258616805076599, 0.5302613377571106, 0.5269402265548706, 0.5302469730377197, 0.5259096622467041, 0.5226348042488098, 0.5203975439071655, 0.5202431678771973, 0.5206488966941833, 0.5229688882827759, 0.5169857740402222, 0.5237590670585632, 0.510421872138977, 0.5273746252059937, 0.5116637349128723, 0.5135861039161682, 0.5101285576820374, 0.506193995475769, 0.5024193525314331, 0.5023096203804016, 0.5042529702186584, 0.5053501129150391, 0.5046055316925049, 0.49625229835510254, 0.5010417699813843, 0.5092471837997437, 0.5087376832962036, 0.49539852142333984, 0.49222230911254883, 0.49124619364738464, 0.5048026442527771, 0.49047398567199707, 0.5008641481399536, 0.49348554015159607, 0.49227964878082275, 0.5029750466346741, 0.4988349378108978, 0.4944622814655304, 0.49058112502098083, 0.4876834452152252, 0.49809232354164124, 0.48576295375823975, 0.47742897272109985, 0.4854223132133484, 0.486085444688797, 0.48139968514442444, 0.482960045337677, 0.48896023631095886, 0.47951480746269226, 0.47524529695510864, 0.49271005392074585, 0.4824088215827942, 0.4786750078201294, 0.4721168875694275, 0.46787938475608826, 0.4735991060733795, 0.47666135430336, 0.46737316250801086, 0.4699976444244385, 0.4643547236919403, 0.4699932038784027, 0.47547823190689087, 0.46077367663383484, 0.4676215946674347, 0.46300947666168213, 0.454946368932724, 0.4663260281085968, 0.4592050313949585, 0.45444241166114807, 0.45503443479537964, 0.4513075649738312, 0.45037731528282166, 0.4576124846935272, 0.46013230085372925, 0.46462586522102356, 0.4539029002189636], 'accuracy': [0.7385398745536804, 0.7461799383163452, 0.7422184348106384, 0.7507073879241943, 0.757498562335968, 0.7461799383163452, 0.7580645084381104, 0.7490096092224121, 0.7603282332420349, 0.7473118305206299, 0.7450481057167053, 0.7391058206558228, 0.7504244446754456, 0.7521222233772278, 0.7608941793441772, 0.7509903907775879, 0.7676853537559509, 0.7603282332420349, 0.7572156190872192, 0.7549518942832947, 0.759196400642395, 0.7600452899932861, 0.7645727396011353, 0.7702320218086243, 0.7668364644050598, 0.7637238502502441, 0.7662705183029175, 0.7651386260986328, 0.768534243106842, 0.7654216289520264, 0.7679682970046997, 0.7730616927146912, 0.7707979679107666, 0.7645727396011353, 0.7705150246620178, 0.7645727396011353, 0.7826825380325317, 0.7606111764907837, 0.7736276388168335, 0.7722128033638, 0.7657045722007751, 0.7724957466125488, 0.780701756477356, 0.7801358103752136, 0.7790039777755737, 0.7857951521873474, 0.7747594714164734, 0.7872099876403809, 0.7869269847869873, 0.7736276388168335, 0.7747594714164734, 0.7846632599830627, 0.7886247634887695, 0.79173743724823, 0.7713639140129089, 0.788058876991272, 0.7818335890769958, 0.7860780954360962, 0.7855121493339539, 0.7730616927146912, 0.7866440415382385, 0.7846632599830627, 0.7886247634887695, 0.790039598941803, 0.7761743068695068, 0.7872099876403809, 0.7891907095909119, 0.7852292060852051, 0.7894737124443054, 0.7886247634887695, 0.7962648272514343, 0.7860780954360962, 0.7906055450439453, 0.7976796627044678, 0.7792869210243225, 0.7928692698478699, 0.7988115549087524, 0.808432400226593, 0.8022071123123169, 0.7934352159500122, 0.7940011024475098, 0.7906055450439453, 0.8019241690635681, 0.7971137762069702, 0.797396719455719, 0.7968307733535767, 0.8109790682792664, 0.8019241690635681, 0.804753839969635, 0.8106960654258728, 0.7988115549087524, 0.8005093336105347, 0.8007922768592834, 0.8078664541244507, 0.8095642328262329, 0.806734561920166, 0.8109790682792664, 0.8089982867240906, 0.7962648272514343, 0.8005093336105347], 'val_loss': [0.7583214044570923, 0.7542389631271362, 0.7518341541290283, 0.7569150328636169, 0.763797402381897, 0.7482037544250488, 0.7379102110862732, 0.761130690574646, 0.7725508809089661, 0.7316250205039978, 0.7207231521606445, 0.7416954040527344, 0.73912513256073, 0.7091372609138489, 0.7184555530548096, 0.7165374159812927, 0.7017744779586792, 0.7186204791069031, 0.7061429023742676, 0.6997154355049133, 0.7143322229385376, 0.7248045802116394, 0.7346983551979065, 0.7345248460769653, 0.7444667816162109, 0.7607210874557495, 0.7702257037162781, 0.725589394569397, 0.7586966156959534, 0.7731727957725525, 0.8608130812644958, 0.7478141188621521, 0.7421874403953552, 0.7828686833381653, 0.8157244324684143, 0.8324750065803528, 0.969795823097229, 0.8893577456474304, 0.8164341449737549, 0.8203705549240112, 0.7907226085662842, 0.9227452278137207, 0.9340680241584778, 0.8158718943595886, 0.8500173687934875, 0.8366610407829285, 0.8818641304969788, 0.8386780619621277, 0.852739691734314, 0.8454345464706421, 0.8158029317855835, 1.0715910196304321, 0.8240038156509399, 0.8420012593269348, 0.8510618805885315, 0.82666015625, 0.8087719678878784, 0.826424241065979, 0.8680793642997742, 0.951338529586792, 0.881856381893158, 0.940176784992218, 0.9136248230934143, 0.8738364577293396, 0.9588019847869873, 0.8411765098571777, 0.8189513683319092, 0.9560136198997498, 0.8846435546875, 0.9263117909431458, 0.8765004873275757, 0.8492937684059143, 0.8319251537322998, 0.9491952061653137, 0.8524841666221619, 0.8568805456161499, 1.1012189388275146, 0.8818817138671875, 1.2454419136047363, 0.8953405618667603, 0.8523064255714417, 0.8778773546218872, 0.95858234167099, 0.8330501317977905, 0.8695328831672668, 0.9364188313484192, 0.8365393877029419, 0.8831464052200317, 0.8643080592155457, 1.0146149396896362, 0.8897530436515808, 0.9576184153556824, 0.9126929640769958, 1.059007167816162, 0.924946665763855, 0.9404537081718445, 1.5146411657333374, 0.956854522228241, 1.0280052423477173, 1.0137159824371338], 'val_accuracy': [0.6199095249176025, 0.5859728455543518, 0.5429864525794983, 0.5475113391876221, 0.5384615659713745, 0.5633484125137329, 0.6312217116355896, 0.5531674027442932, 0.5452488660812378, 0.5927602052688599, 0.6368778347969055, 0.5757918357849121, 0.5825791954994202, 0.6357465982437134, 0.5995475053787231, 0.6244344115257263, 0.6368778347969055, 0.6153846383094788, 0.662895917892456, 0.6866515874862671, 0.6651583909988403, 0.6470588445663452, 0.6436651349067688, 0.6821267008781433, 0.6753393411636353, 0.6617646813392639, 0.6459276080131531, 0.6617646813392639, 0.6617646813392639, 0.6708144545555115, 0.6357465982437134, 0.6640271544456482, 0.668552041053772, 0.6787330508232117, 0.668552041053772, 0.6380090713500977, 0.6018099784851074, 0.6266968250274658, 0.6561086177825928, 0.6719456911087036, 0.6753393411636353, 0.6142534017562866, 0.6165158152580261, 0.6527149081230164, 0.6572397947311401, 0.6753393411636353, 0.651583731174469, 0.6696832776069641, 0.668552041053772, 0.6787330508232117, 0.6549773812294006, 0.5950226187705994, 0.6708144545555115, 0.6572397947311401, 0.6617646813392639, 0.6504524946212769, 0.662895917892456, 0.6606335043907166, 0.6527149081230164, 0.6334841847419739, 0.6425339579582214, 0.639140248298645, 0.6357465982437134, 0.6527149081230164, 0.6255655884742737, 0.6447963714599609, 0.6606335043907166, 0.6199095249176025, 0.6561086177825928, 0.6425339579582214, 0.6764705777168274, 0.6470588445663452, 0.6481900215148926, 0.6470588445663452, 0.6572397947311401, 0.6459276080131531, 0.6029411554336548, 0.6527149081230164, 0.5859728455543518, 0.6527149081230164, 0.662895917892456, 0.6696832776069641, 0.6481900215148926, 0.668552041053772, 0.668552041053772, 0.6425339579582214, 0.6606335043907166, 0.6606335043907166, 0.6583710312843323, 0.6255655884742737, 0.6651583909988403, 0.6334841847419739, 0.6583710312843323, 0.6153846383094788, 0.6470588445663452, 0.6425339579582214, 0.5735294222831726, 0.6549773812294006, 0.6470588445663452, 0.6481900215148926]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 3, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 3, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4558401 (17.39 MB)\n","Trainable params: 4557889 (17.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","30/31 [============================>.] - ETA: 0s - loss: 0.5903 - accuracy: 0.7302"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 10s 57ms/step - loss: 0.5914 - accuracy: 0.7305 - val_loss: 0.7653 - val_accuracy: 0.5589\n","Epoch 2/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5752 - accuracy: 0.7302 - val_loss: 0.7622 - val_accuracy: 0.5610\n","Epoch 3/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5713 - accuracy: 0.7444 - val_loss: 0.7632 - val_accuracy: 0.5320\n","Epoch 4/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.5838 - accuracy: 0.7230 - val_loss: 0.7604 - val_accuracy: 0.5764\n","Epoch 5/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5750 - accuracy: 0.7271 - val_loss: 0.7646 - val_accuracy: 0.5444\n","Epoch 6/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5656 - accuracy: 0.7401 - val_loss: 0.7550 - val_accuracy: 0.5878\n","Epoch 7/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5662 - accuracy: 0.7411 - val_loss: 0.7525 - val_accuracy: 0.5806\n","Epoch 8/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5640 - accuracy: 0.7439 - val_loss: 0.7506 - val_accuracy: 0.5816\n","Epoch 9/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5636 - accuracy: 0.7380 - val_loss: 0.7475 - val_accuracy: 0.5868\n","Epoch 10/100\n","31/31 [==============================] - 1s 31ms/step - loss: 0.5623 - accuracy: 0.7473 - val_loss: 0.7445 - val_accuracy: 0.5888\n","Epoch 11/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5632 - accuracy: 0.7395 - val_loss: 0.7556 - val_accuracy: 0.5517\n","Epoch 12/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.5574 - accuracy: 0.7408 - val_loss: 0.7430 - val_accuracy: 0.5961\n","Epoch 13/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5587 - accuracy: 0.7393 - val_loss: 0.7448 - val_accuracy: 0.5775\n","Epoch 14/100\n","31/31 [==============================] - 1s 32ms/step - loss: 0.5562 - accuracy: 0.7473 - val_loss: 0.7438 - val_accuracy: 0.6043\n","Epoch 15/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5585 - accuracy: 0.7463 - val_loss: 0.7601 - val_accuracy: 0.5919\n","Epoch 16/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5667 - accuracy: 0.7375 - val_loss: 0.8146 - val_accuracy: 0.5444\n","Epoch 17/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5708 - accuracy: 0.7307 - val_loss: 0.7662 - val_accuracy: 0.5733\n","Epoch 18/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5512 - accuracy: 0.7501 - val_loss: 0.7648 - val_accuracy: 0.6095\n","Epoch 19/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5500 - accuracy: 0.7501 - val_loss: 0.7759 - val_accuracy: 0.6054\n","Epoch 20/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5506 - accuracy: 0.7475 - val_loss: 0.7660 - val_accuracy: 0.6147\n","Epoch 21/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5518 - accuracy: 0.7393 - val_loss: 0.7808 - val_accuracy: 0.6095\n","Epoch 22/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5509 - accuracy: 0.7517 - val_loss: 0.7944 - val_accuracy: 0.6136\n","Epoch 23/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.5457 - accuracy: 0.7478 - val_loss: 0.7875 - val_accuracy: 0.6302\n","Epoch 24/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5531 - accuracy: 0.7452 - val_loss: 0.8605 - val_accuracy: 0.5961\n","Epoch 25/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5503 - accuracy: 0.7532 - val_loss: 0.7930 - val_accuracy: 0.6126\n","Epoch 26/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5435 - accuracy: 0.7589 - val_loss: 0.7984 - val_accuracy: 0.6229\n","Epoch 27/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5431 - accuracy: 0.7527 - val_loss: 0.8152 - val_accuracy: 0.6333\n","Epoch 28/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5471 - accuracy: 0.7607 - val_loss: 0.8192 - val_accuracy: 0.6219\n","Epoch 29/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5429 - accuracy: 0.7535 - val_loss: 0.8228 - val_accuracy: 0.6074\n","Epoch 30/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5831 - accuracy: 0.7307 - val_loss: 0.8432 - val_accuracy: 0.6136\n","Epoch 31/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5494 - accuracy: 0.7587 - val_loss: 0.9044 - val_accuracy: 0.6250\n","Epoch 32/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5453 - accuracy: 0.7612 - val_loss: 0.9435 - val_accuracy: 0.5785\n","Epoch 33/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5351 - accuracy: 0.7589 - val_loss: 0.8259 - val_accuracy: 0.6198\n","Epoch 34/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5308 - accuracy: 0.7680 - val_loss: 0.8518 - val_accuracy: 0.6157\n","Epoch 35/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5322 - accuracy: 0.7677 - val_loss: 0.8712 - val_accuracy: 0.5992\n","Epoch 36/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5323 - accuracy: 0.7607 - val_loss: 0.8933 - val_accuracy: 0.5888\n","Epoch 37/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5380 - accuracy: 0.7527 - val_loss: 0.8623 - val_accuracy: 0.6074\n","Epoch 38/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5467 - accuracy: 0.7390 - val_loss: 0.9132 - val_accuracy: 0.5899\n","Epoch 39/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5341 - accuracy: 0.7574 - val_loss: 0.8916 - val_accuracy: 0.6157\n","Epoch 40/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5297 - accuracy: 0.7654 - val_loss: 0.8774 - val_accuracy: 0.6198\n","Epoch 41/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5273 - accuracy: 0.7602 - val_loss: 0.9310 - val_accuracy: 0.5950\n","Epoch 42/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5225 - accuracy: 0.7651 - val_loss: 0.9347 - val_accuracy: 0.5826\n","Epoch 43/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5222 - accuracy: 0.7664 - val_loss: 0.9092 - val_accuracy: 0.6136\n","Epoch 44/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5243 - accuracy: 0.7633 - val_loss: 0.9291 - val_accuracy: 0.6085\n","Epoch 45/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5190 - accuracy: 0.7739 - val_loss: 0.9230 - val_accuracy: 0.6157\n","Epoch 46/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5329 - accuracy: 0.7620 - val_loss: 0.8910 - val_accuracy: 0.6064\n","Epoch 47/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5200 - accuracy: 0.7674 - val_loss: 0.8555 - val_accuracy: 0.6178\n","Epoch 48/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5215 - accuracy: 0.7638 - val_loss: 1.0769 - val_accuracy: 0.5754\n","Epoch 49/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5119 - accuracy: 0.7796 - val_loss: 0.9492 - val_accuracy: 0.6095\n","Epoch 50/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5198 - accuracy: 0.7695 - val_loss: 0.8819 - val_accuracy: 0.6260\n","Epoch 51/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5147 - accuracy: 0.7739 - val_loss: 0.9769 - val_accuracy: 0.6054\n","Epoch 52/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5349 - accuracy: 0.7597 - val_loss: 0.8980 - val_accuracy: 0.6136\n","Epoch 53/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5190 - accuracy: 0.7685 - val_loss: 1.2740 - val_accuracy: 0.5506\n","Epoch 54/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5246 - accuracy: 0.7664 - val_loss: 0.9139 - val_accuracy: 0.6198\n","Epoch 55/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5256 - accuracy: 0.7628 - val_loss: 1.0558 - val_accuracy: 0.5744\n","Epoch 56/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5060 - accuracy: 0.7798 - val_loss: 0.8825 - val_accuracy: 0.6209\n","Epoch 57/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5124 - accuracy: 0.7724 - val_loss: 0.9844 - val_accuracy: 0.6074\n","Epoch 58/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5234 - accuracy: 0.7602 - val_loss: 0.8954 - val_accuracy: 0.6105\n","Epoch 59/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5103 - accuracy: 0.7762 - val_loss: 0.8936 - val_accuracy: 0.6188\n","Epoch 60/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5180 - accuracy: 0.7669 - val_loss: 0.8746 - val_accuracy: 0.6281\n","Epoch 61/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5209 - accuracy: 0.7693 - val_loss: 0.9173 - val_accuracy: 0.6105\n","Epoch 62/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5071 - accuracy: 0.7752 - val_loss: 0.8975 - val_accuracy: 0.6209\n","Epoch 63/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5136 - accuracy: 0.7703 - val_loss: 0.9283 - val_accuracy: 0.6271\n","Epoch 64/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5102 - accuracy: 0.7734 - val_loss: 0.9031 - val_accuracy: 0.6178\n","Epoch 65/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5090 - accuracy: 0.7806 - val_loss: 0.9508 - val_accuracy: 0.6085\n","Epoch 66/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5244 - accuracy: 0.7537 - val_loss: 0.9000 - val_accuracy: 0.6198\n","Epoch 67/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5052 - accuracy: 0.7760 - val_loss: 0.8981 - val_accuracy: 0.6116\n","Epoch 68/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4993 - accuracy: 0.7829 - val_loss: 0.9324 - val_accuracy: 0.6157\n","Epoch 69/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.4979 - accuracy: 0.7837 - val_loss: 1.0287 - val_accuracy: 0.5806\n","Epoch 70/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5000 - accuracy: 0.7827 - val_loss: 0.8975 - val_accuracy: 0.6281\n","Epoch 71/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5095 - accuracy: 0.7742 - val_loss: 1.0456 - val_accuracy: 0.5795\n","Epoch 72/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5038 - accuracy: 0.7835 - val_loss: 0.9599 - val_accuracy: 0.5919\n","Epoch 73/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5079 - accuracy: 0.7775 - val_loss: 0.9136 - val_accuracy: 0.6157\n","Epoch 74/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4994 - accuracy: 0.7842 - val_loss: 0.9395 - val_accuracy: 0.6064\n","Epoch 75/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5090 - accuracy: 0.7747 - val_loss: 0.9399 - val_accuracy: 0.6126\n","Epoch 76/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4927 - accuracy: 0.7855 - val_loss: 0.9630 - val_accuracy: 0.6064\n","Epoch 77/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4954 - accuracy: 0.7848 - val_loss: 0.9152 - val_accuracy: 0.6167\n","Epoch 78/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4966 - accuracy: 0.7835 - val_loss: 1.0196 - val_accuracy: 0.5806\n","Epoch 79/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4926 - accuracy: 0.7881 - val_loss: 1.1507 - val_accuracy: 0.5723\n","Epoch 80/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4883 - accuracy: 0.7910 - val_loss: 0.9506 - val_accuracy: 0.6240\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4928 - accuracy: 0.7775 - val_loss: 0.9623 - val_accuracy: 0.6116\n","Epoch 82/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5041 - accuracy: 0.7835 - val_loss: 0.9866 - val_accuracy: 0.5930\n","Epoch 83/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4927 - accuracy: 0.7835 - val_loss: 0.9635 - val_accuracy: 0.6167\n","Epoch 84/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4976 - accuracy: 0.7853 - val_loss: 0.9750 - val_accuracy: 0.6085\n","Epoch 85/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4840 - accuracy: 0.7946 - val_loss: 0.9930 - val_accuracy: 0.6043\n","Epoch 86/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4936 - accuracy: 0.7767 - val_loss: 0.9979 - val_accuracy: 0.6054\n","Epoch 87/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4936 - accuracy: 0.7891 - val_loss: 0.9165 - val_accuracy: 0.6116\n","Epoch 88/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.4896 - accuracy: 0.7894 - val_loss: 1.0372 - val_accuracy: 0.5971\n","Epoch 89/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.4859 - accuracy: 0.7868 - val_loss: 0.9375 - val_accuracy: 0.5961\n","Epoch 90/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4891 - accuracy: 0.7811 - val_loss: 1.1214 - val_accuracy: 0.5785\n","Epoch 91/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4930 - accuracy: 0.7817 - val_loss: 1.0825 - val_accuracy: 0.6095\n","Epoch 92/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4902 - accuracy: 0.7891 - val_loss: 1.0100 - val_accuracy: 0.5961\n","Epoch 93/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4836 - accuracy: 0.7876 - val_loss: 0.9906 - val_accuracy: 0.6074\n","Epoch 94/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4702 - accuracy: 0.8016 - val_loss: 0.9986 - val_accuracy: 0.6157\n","Epoch 95/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4805 - accuracy: 0.7961 - val_loss: 1.1387 - val_accuracy: 0.5795\n","Epoch 96/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5016 - accuracy: 0.7832 - val_loss: 0.9874 - val_accuracy: 0.6167\n","Epoch 97/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4707 - accuracy: 0.7964 - val_loss: 1.0497 - val_accuracy: 0.6023\n","Epoch 98/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4745 - accuracy: 0.7948 - val_loss: 0.9467 - val_accuracy: 0.6095\n","Epoch 99/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4743 - accuracy: 0.7948 - val_loss: 1.0746 - val_accuracy: 0.5930\n","Epoch 100/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4792 - accuracy: 0.7910 - val_loss: 0.9770 - val_accuracy: 0.5961\n","{'loss': [0.5914071202278137, 0.575219452381134, 0.5713487863540649, 0.5838395357131958, 0.5749714970588684, 0.5656486749649048, 0.5661510229110718, 0.5640072226524353, 0.5636304020881653, 0.5622693300247192, 0.563227653503418, 0.5573780536651611, 0.5587373375892639, 0.5561754107475281, 0.5584721565246582, 0.5667009949684143, 0.5707985162734985, 0.5511753559112549, 0.5499605536460876, 0.5505852699279785, 0.5518420934677124, 0.5508623123168945, 0.5456820726394653, 0.5531457662582397, 0.5502831339836121, 0.5435361266136169, 0.5430947542190552, 0.5471111536026001, 0.5429072380065918, 0.5830996036529541, 0.549385666847229, 0.5453089475631714, 0.535090982913971, 0.5308007001876831, 0.5321869850158691, 0.5322920680046082, 0.5379670858383179, 0.5467130541801453, 0.534088134765625, 0.529707133769989, 0.527327835559845, 0.5225303173065186, 0.5222002267837524, 0.5242820978164673, 0.519001841545105, 0.5328880548477173, 0.5199540853500366, 0.521497368812561, 0.5119146704673767, 0.5198246240615845, 0.5147461295127869, 0.5349031686782837, 0.5189501047134399, 0.5246180295944214, 0.5255672335624695, 0.5059731602668762, 0.5123781561851501, 0.5234177708625793, 0.5103278160095215, 0.5179958939552307, 0.5209164619445801, 0.5071134567260742, 0.5135782361030579, 0.5102463960647583, 0.5089601278305054, 0.52436363697052, 0.5052042007446289, 0.49933403730392456, 0.4979339838027954, 0.5000349879264832, 0.5094621777534485, 0.5038101077079773, 0.5078750848770142, 0.49936261773109436, 0.5089934468269348, 0.4927281439304352, 0.4954344928264618, 0.49662476778030396, 0.4925914704799652, 0.4882640540599823, 0.49281638860702515, 0.5041133761405945, 0.49267578125, 0.49764326214790344, 0.48399731516838074, 0.49363768100738525, 0.49359309673309326, 0.48959240317344666, 0.48588600754737854, 0.4891327917575836, 0.4929603338241577, 0.4902121126651764, 0.48355337977409363, 0.47020766139030457, 0.48053014278411865, 0.5015562772750854, 0.47067612409591675, 0.47452425956726074, 0.4742881655693054, 0.4792416989803314], 'accuracy': [0.7304909825325012, 0.7302325367927551, 0.7444444298744202, 0.7229974269866943, 0.7271317839622498, 0.7400516867637634, 0.7410852909088135, 0.7439276576042175, 0.7379844784736633, 0.7472867965698242, 0.739534854888916, 0.7408268451690674, 0.7392764687538147, 0.7472867965698242, 0.746253252029419, 0.7374677062034607, 0.7307493686676025, 0.750129222869873, 0.750129222869873, 0.7475452423095703, 0.7392764687538147, 0.7516795992851257, 0.7478036284446716, 0.7452196478843689, 0.7532299757003784, 0.7589147090911865, 0.7527132034301758, 0.7607235312461853, 0.7534883618354797, 0.7307493686676025, 0.7586563229560852, 0.7612403035163879, 0.7589147090911865, 0.7679586410522461, 0.7677002549171448, 0.7607235312461853, 0.7527132034301758, 0.7390180826187134, 0.7573643326759338, 0.7653746604919434, 0.7602066993713379, 0.765116274356842, 0.7664082646369934, 0.763307511806488, 0.7739018201828003, 0.7620155215263367, 0.7674418687820435, 0.7638242840766907, 0.7795865535736084, 0.7695090174674988, 0.7739018201828003, 0.7596899271011353, 0.7684754729270935, 0.7664082646369934, 0.7627906799316406, 0.7798449397087097, 0.7723514437675476, 0.7602066993713379, 0.7762274146080017, 0.766925036907196, 0.7692506313323975, 0.7751938104629517, 0.7702842354774475, 0.7733849883079529, 0.7806201577186584, 0.753746747970581, 0.7759689688682556, 0.7829457521438599, 0.7837209105491638, 0.7826873660087585, 0.7741602063179016, 0.7834625244140625, 0.7775194048881531, 0.7842377424240112, 0.7746769785881042, 0.7855297327041626, 0.7847545146942139, 0.7834625244140625, 0.7881137132644653, 0.7909560799598694, 0.7775194048881531, 0.7834625244140625, 0.7834625244140625, 0.7852713465690613, 0.7945736646652222, 0.7767441868782043, 0.7891472578048706, 0.7894057035446167, 0.786821722984314, 0.7811369299888611, 0.7816537618637085, 0.7891472578048706, 0.7875968813896179, 0.8015503883361816, 0.7961240410804749, 0.7832041382789612, 0.7963824272155762, 0.7948320508003235, 0.7948320508003235, 0.7909560799598694], 'val_loss': [0.7652741074562073, 0.7621831893920898, 0.7631534934043884, 0.7603602409362793, 0.7646083235740662, 0.7550057172775269, 0.7524840831756592, 0.7506020069122314, 0.7475265860557556, 0.7444765567779541, 0.7556062936782837, 0.7430053949356079, 0.7447776198387146, 0.7438023090362549, 0.7601479291915894, 0.8145549893379211, 0.7662159204483032, 0.7648443579673767, 0.7759003043174744, 0.7659652233123779, 0.7807698249816895, 0.7944105863571167, 0.7875409126281738, 0.8605183959007263, 0.7930224537849426, 0.7983617782592773, 0.8152217864990234, 0.8191787600517273, 0.8228158950805664, 0.843196451663971, 0.9043575525283813, 0.9435169696807861, 0.8259317874908447, 0.851772129535675, 0.8711867928504944, 0.8933250308036804, 0.8623170852661133, 0.9131640195846558, 0.8915634155273438, 0.8774335980415344, 0.9309986233711243, 0.9347133040428162, 0.9091609120368958, 0.9291471242904663, 0.9229782819747925, 0.8909637331962585, 0.8554831743240356, 1.076863169670105, 0.949161946773529, 0.8818781971931458, 0.9769461154937744, 0.8979921340942383, 1.2739936113357544, 0.9138858914375305, 1.0558360815048218, 0.8825494647026062, 0.9843562245368958, 0.8954176306724548, 0.893579363822937, 0.8745841383934021, 0.9172577857971191, 0.8974645733833313, 0.9282912015914917, 0.9031188488006592, 0.9508046507835388, 0.89995276927948, 0.8981223702430725, 0.9324012398719788, 1.028703212738037, 0.8974671363830566, 1.0456061363220215, 0.9599108695983887, 0.9135966897010803, 0.9395394921302795, 0.939910888671875, 0.9629982709884644, 0.915187418460846, 1.0196257829666138, 1.1506962776184082, 0.9506070613861084, 0.9623216986656189, 0.9865688681602478, 0.9635185599327087, 0.9749977588653564, 0.9930078387260437, 0.9979441165924072, 0.9164847135543823, 1.0371804237365723, 0.937514066696167, 1.121417760848999, 1.0824909210205078, 1.0099765062332153, 0.9906149506568909, 0.9985622763633728, 1.1387139558792114, 0.9873852133750916, 1.0497246980667114, 0.9467127919197083, 1.0745551586151123, 0.9769937992095947], 'val_accuracy': [0.55888432264328, 0.5609503984451294, 0.5320248007774353, 0.5764462947845459, 0.5444214940071106, 0.5878099203109741, 0.5805785059928894, 0.5816115736961365, 0.586776852607727, 0.5888429880142212, 0.5516529083251953, 0.5960744023323059, 0.577479362487793, 0.6043388247489929, 0.5919421315193176, 0.5444214940071106, 0.5733470916748047, 0.6095041036605835, 0.60537189245224, 0.6146694421768188, 0.6095041036605835, 0.6136363744735718, 0.6301652789115906, 0.5960744023323059, 0.6126033067703247, 0.6229338645935059, 0.6332644820213318, 0.6219007968902588, 0.6074380278587341, 0.6136363744735718, 0.625, 0.5785123705863953, 0.6198347210884094, 0.6157024502754211, 0.5991735458374023, 0.5888429880142212, 0.6074380278587341, 0.5898760557174683, 0.6157024502754211, 0.6198347210884094, 0.5950413346290588, 0.5826446413993835, 0.6136363744735718, 0.6084710955619812, 0.6157024502754211, 0.6064049601554871, 0.6177685856819153, 0.5754132270812988, 0.6095041036605835, 0.6260330677032471, 0.60537189245224, 0.6136363744735718, 0.5506198406219482, 0.6198347210884094, 0.5743801593780518, 0.6208677887916565, 0.6074380278587341, 0.6105371713638306, 0.6188016533851624, 0.6280992031097412, 0.6105371713638306, 0.6208677887916565, 0.6270661354064941, 0.6177685856819153, 0.6084710955619812, 0.6198347210884094, 0.6115702390670776, 0.6157024502754211, 0.5805785059928894, 0.6280992031097412, 0.5795454382896423, 0.5919421315193176, 0.6157024502754211, 0.6064049601554871, 0.6126033067703247, 0.6064049601554871, 0.6167355179786682, 0.5805785059928894, 0.5723140239715576, 0.6239669322967529, 0.6115702390670776, 0.5929751992225647, 0.6167355179786682, 0.6084710955619812, 0.6043388247489929, 0.60537189245224, 0.6115702390670776, 0.5971074104309082, 0.5960744023323059, 0.5785123705863953, 0.6095041036605835, 0.5960744023323059, 0.6074380278587341, 0.6157024502754211, 0.5795454382896423, 0.6167355179786682, 0.6022727489471436, 0.6095041036605835, 0.5929751992225647, 0.5960744023323059]}\n","32/32 [==============================] - 1s 6ms/step\n"]}],"source":["global_model, metrics_df = federated_learning(Theta_data)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1717508209125,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"},"user_tz":-360},"id":"Ik5JoVP2NPvY","outputId":"76144360-6fad-409c-f39d-e08e86292608"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Client  Accuracy  Precision  Recall     F1  Sensitivity  Specificity  \\\n","0        0     0.548      0.623   0.241  0.348        0.241        0.854   \n","1        1     0.557      0.617   0.301  0.405        0.301        0.814   \n","2        2     0.555      0.572   0.438  0.496        0.438        0.673   \n","3        0     0.564      0.708   0.219  0.335        0.219        0.910   \n","4        1     0.572      0.618   0.377  0.468        0.377        0.767   \n","5        2     0.584      0.613   0.458  0.524        0.458        0.711   \n","6        0     0.590      0.715   0.298  0.421        0.298        0.881   \n","7        1     0.585      0.652   0.364  0.467        0.364        0.805   \n","8        2     0.614      0.598   0.701  0.645        0.701        0.528   \n","9        0     0.595      0.596   0.593  0.594        0.593        0.598   \n","10       1     0.589      0.582   0.634  0.607        0.634        0.544   \n","11       2     0.655      0.643   0.695  0.668        0.695        0.614   \n","12       0     0.618      0.616   0.628  0.622        0.628        0.608   \n","13       1     0.627      0.671   0.499  0.572        0.499        0.756   \n","14       2     0.680      0.699   0.631  0.663        0.631        0.729   \n","\n","    Kappa  \n","0   0.095  \n","1   0.114  \n","2   0.110  \n","3   0.129  \n","4   0.144  \n","5   0.169  \n","6   0.179  \n","7   0.169  \n","8   0.229  \n","9   0.191  \n","10  0.178  \n","11  0.309  \n","12  0.236  \n","13  0.254  \n","14  0.359  "],"text/html":["\n","  <div id=\"df-53ccbcac-af51-4d94-bb5c-41f8de3230fd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Client</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Sensitivity</th>\n","      <th>Specificity</th>\n","      <th>Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.548</td>\n","      <td>0.623</td>\n","      <td>0.241</td>\n","      <td>0.348</td>\n","      <td>0.241</td>\n","      <td>0.854</td>\n","      <td>0.095</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.557</td>\n","      <td>0.617</td>\n","      <td>0.301</td>\n","      <td>0.405</td>\n","      <td>0.301</td>\n","      <td>0.814</td>\n","      <td>0.114</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.555</td>\n","      <td>0.572</td>\n","      <td>0.438</td>\n","      <td>0.496</td>\n","      <td>0.438</td>\n","      <td>0.673</td>\n","      <td>0.110</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.564</td>\n","      <td>0.708</td>\n","      <td>0.219</td>\n","      <td>0.335</td>\n","      <td>0.219</td>\n","      <td>0.910</td>\n","      <td>0.129</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.572</td>\n","      <td>0.618</td>\n","      <td>0.377</td>\n","      <td>0.468</td>\n","      <td>0.377</td>\n","      <td>0.767</td>\n","      <td>0.144</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2</td>\n","      <td>0.584</td>\n","      <td>0.613</td>\n","      <td>0.458</td>\n","      <td>0.524</td>\n","      <td>0.458</td>\n","      <td>0.711</td>\n","      <td>0.169</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0.590</td>\n","      <td>0.715</td>\n","      <td>0.298</td>\n","      <td>0.421</td>\n","      <td>0.298</td>\n","      <td>0.881</td>\n","      <td>0.179</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0.585</td>\n","      <td>0.652</td>\n","      <td>0.364</td>\n","      <td>0.467</td>\n","      <td>0.364</td>\n","      <td>0.805</td>\n","      <td>0.169</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>0.614</td>\n","      <td>0.598</td>\n","      <td>0.701</td>\n","      <td>0.645</td>\n","      <td>0.701</td>\n","      <td>0.528</td>\n","      <td>0.229</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>0.595</td>\n","      <td>0.596</td>\n","      <td>0.593</td>\n","      <td>0.594</td>\n","      <td>0.593</td>\n","      <td>0.598</td>\n","      <td>0.191</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0.589</td>\n","      <td>0.582</td>\n","      <td>0.634</td>\n","      <td>0.607</td>\n","      <td>0.634</td>\n","      <td>0.544</td>\n","      <td>0.178</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2</td>\n","      <td>0.655</td>\n","      <td>0.643</td>\n","      <td>0.695</td>\n","      <td>0.668</td>\n","      <td>0.695</td>\n","      <td>0.614</td>\n","      <td>0.309</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>0.618</td>\n","      <td>0.616</td>\n","      <td>0.628</td>\n","      <td>0.622</td>\n","      <td>0.628</td>\n","      <td>0.608</td>\n","      <td>0.236</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0.627</td>\n","      <td>0.671</td>\n","      <td>0.499</td>\n","      <td>0.572</td>\n","      <td>0.499</td>\n","      <td>0.756</td>\n","      <td>0.254</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2</td>\n","      <td>0.680</td>\n","      <td>0.699</td>\n","      <td>0.631</td>\n","      <td>0.663</td>\n","      <td>0.631</td>\n","      <td>0.729</td>\n","      <td>0.359</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53ccbcac-af51-4d94-bb5c-41f8de3230fd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-53ccbcac-af51-4d94-bb5c-41f8de3230fd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-53ccbcac-af51-4d94-bb5c-41f8de3230fd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a95c26f5-2b5e-4480-b667-ba76f610bd27\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a95c26f5-2b5e-4480-b667-ba76f610bd27')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a95c26f5-2b5e-4480-b667-ba76f610bd27 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"metrics_df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Client\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0375687306654021,\n        \"min\": 0.548,\n        \"max\": 0.68,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.595,\n          0.655,\n          0.548\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04531140926437634,\n        \"min\": 0.572,\n        \"max\": 0.715,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.596,\n          0.643,\n          0.623\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16740208225364805,\n        \"min\": 0.219,\n        \"max\": 0.701,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.593,\n          0.695,\n          0.241\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11243707234758134,\n        \"min\": 0.335,\n        \"max\": 0.668,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.594,\n          0.668,\n          0.348\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sensitivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16740208225364805,\n        \"min\": 0.219,\n        \"max\": 0.701,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.593,\n          0.695,\n          0.241\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Specificity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12201104555774955,\n        \"min\": 0.528,\n        \"max\": 0.91,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.598,\n          0.614,\n          0.854\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kappa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0749409291184921,\n        \"min\": 0.095,\n        \"max\": 0.359,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.178,\n          0.236,\n          0.095\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":18}],"source":["\n","metrics_df.round(3)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Ncf_cMAQF6g4","executionInfo":{"status":"ok","timestamp":1717508209126,"user_tz":-360,"elapsed":4,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"outputs":[],"source":["metrics_df.round(4).to_csv('/content/drive/MyDrive/EEG Signal /results/FLresults/TF Domain/CNN_LSTM/Delta_tf_CNN_LSTM.csv', index = False)"]},{"cell_type":"markdown","source":["# CNN"],"metadata":{"id":"QUuWzfMHSNmi"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, LSTM, Dropout\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","import tensorflow_addons as tfa\n","\n","class FederatedData:\n","    def __init__(self, data_path, num_clients, scaler_type='MinMax', test_size=0.2):\n","        self.data_path = data_path\n","        self.num_clients = num_clients\n","        self.scaler_type = scaler_type\n","        self.test_size = test_size\n","        self.partitions = []  # Initialize partitions attribute\n","        self.load_data()\n","        self.scale_data()\n","        self.create_partitions()\n","\n","    def load_data(self):\n","        try:\n","            data = np.load(self.data_path, allow_pickle=True)\n","            self.X = np.moveaxis(data['X'], 1, 2)  # Move axis here\n","            self.Y = data['Y']\n","            self.groups = data['Group']  # Assuming 'Group' contains group identifiers\n","        except KeyError as e:\n","            raise ValueError(f\"Missing expected data field: {e}\")\n","        except FileNotFoundError as e:\n","            raise ValueError(f\"Data file not found: {e}\")\n","\n","    def scale_data(self):\n","        # Reshape data to 2D array for scaling\n","        X_reshaped = self.X.reshape(-1, self.X.shape[-1])\n","\n","        # Select scaler based on input\n","        if self.scaler_type == 'Standard':\n","            scaler = StandardScaler()\n","        elif self.scaler_type == 'MinMax':\n","            scaler = MinMaxScaler(feature_range=(0, 1))\n","        else:\n","            raise ValueError(\"Unsupported scaler type. Choose either 'Standard' or 'MinMax'.\")\n","\n","        # Fit and transform the data\n","        scaled_data_reshaped = scaler.fit_transform(X_reshaped)\n","\n","        # Reshape back to original shape\n","        self.X = scaled_data_reshaped.reshape(self.X.shape)\n","\n","    def create_partitions(self):\n","        gss = GroupShuffleSplit(n_splits=self.num_clients, test_size=self.test_size, random_state=42)\n","        for train_index, test_index in gss.split(self.X, self.Y, self.groups):\n","            X_train, X_test = self.X[train_index], self.X[test_index]\n","            Y_train, Y_test = self.Y[train_index], self.Y[test_index]\n","            self.partitions.append((X_train, Y_train, X_test, Y_test))\n","            print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n","\n","    def get_training_and_validation_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        partition_X_train, partition_Y_train, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        X_train, X_val, Y_train, Y_val = train_test_split(partition_X_train, partition_Y_train, test_size=0.2, random_state=42)\n","        return X_train, X_val, Y_train, Y_val, partition_X_test, partition_Y_test\n","\n","    def get_testing_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        _, _, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        return partition_X_test, partition_Y_test\n","\n","def build_sequential_model(input_shape):\n","    clear_session()\n","    model = Sequential()\n","\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\", input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(512, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(filters=1024, kernel_size=3, strides=1, activation='relu'))\n","    # model.add(Dropout(0.5))\n","    # model.add(LSTM(256, return_sequences=True))\n","    # model.add(LSTM(256, return_sequences=True))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(256, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n","\n","    # opt = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","    opt = tfa.optimizers.RectifiedAdam(learning_rate=0.00009)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = recall\n","\n","    return accuracy, precision, recall, f1, sensitivity, specificity, kappa\n","\n","def federated_learning(data_path):\n","    federated_data = FederatedData(data_path, num_clients=3, scaler_type='MinMax')\n","    federated_data.create_partitions()\n","\n","    # Get the input shape from the data\n","    input_shape = federated_data.X.shape[1:]\n","    global_model = build_sequential_model(input_shape)\n","\n","    num_clients = 3\n","    local_epochs = 5\n","    global_optimizer = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","\n","    # Initialize m and v for Adam optimizer\n","    m = [np.zeros_like(w) for w in global_model.get_weights()]\n","    v = [np.zeros_like(w) for w in global_model.get_weights()]\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 1e-7\n","    t = 0\n","\n","    client_data = []\n","    for client_idx in range(num_clients):\n","        x_train, x_val, y_train, y_val, x_test, y_test = federated_data.get_training_and_validation_data(client_idx)\n","        client_data.append((x_train, x_val, y_train, y_val, x_test, y_test))\n","\n","    metrics_list = []\n","\n","    for epoch in range(local_epochs):\n","        client_models = []\n","\n","        for client in range(num_clients):\n","            x_train, x_val, y_train, y_val, x_test, y_test = client_data[client]\n","            client_model = build_sequential_model(input_shape)\n","            client_model.set_weights(global_model.get_weights())  # Initialize with global weights\n","            opt = Adam(learning_rate=0.000009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","            client_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","            # Set up callbacks\n","            run_name = f\"epoch_{epoch}_client_{client}\"\n","            checkpoint_filepath = f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time+Frequency/Delta/CNN/best_model_{run_name}.h5'\n","            checkpoint_callback = ModelCheckpoint(\n","                checkpoint_filepath,\n","                monitor='val_accuracy',\n","                save_best_only=True,\n","                mode='max'\n","            )\n","            csv_logger = CSVLogger(f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time+Frequency/Delta/CNN/training_log_{run_name}.csv', append=True, separator=';')\n","\n","            history = client_model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_val, y_val), callbacks=[checkpoint_callback, csv_logger])\n","            client_models.append(client_model)\n","\n","            print(history.history)\n","\n","            # Load the best model before evaluation\n","            best_model = load_model(checkpoint_filepath)\n","\n","            # Evaluate client model\n","            y_pred = (best_model.predict(x_test) > 0.5).astype(\"int32\")\n","            accuracy, precision, recall, f1, sensitivity, specificity, kappa = compute_metrics(y_test, y_pred)\n","            metrics_list.append({\n","                'Client': client,\n","                'Accuracy': accuracy,\n","                'Precision': precision,\n","                'Recall': recall,\n","                'F1': f1,\n","                'Sensitivity': sensitivity,\n","                'Specificity': specificity,\n","                'Kappa': kappa\n","            })\n","\n","        global_weights = global_model.get_weights()\n","        layer_start_idx = 0\n","        for layer in global_model.layers:\n","            layer_weights = []\n","            layer_biases = []\n","            num_params = len(layer.get_weights())\n","            for i in range(num_clients):\n","                layer_params = client_models[i].get_weights()[layer_start_idx:layer_start_idx + num_params]\n","                if num_params > 0:\n","                    layer_weights.append(layer_params[0])\n","                if num_params > 1:\n","                    layer_biases.append(layer_params[1])\n","\n","            if len(layer_weights) > 0:\n","                averaged_layer_weights = np.mean(layer_weights, axis=0)\n","                if averaged_layer_weights.shape == global_weights[layer_start_idx].shape:\n","                    global_weights[layer_start_idx] = averaged_layer_weights\n","                else:\n","                    print(f\"Warning: Shape mismatch for weights at layer {layer.name}, expected {global_weights[layer_start_idx].shape} but got {averaged_layer_weights.shape}\")\n","            if len(layer_biases) > 0:\n","                averaged_layer_biases = np.mean(layer_biases, axis=0)\n","                if averaged_layer_biases.shape == global_weights[layer_start_idx + 1].shape:\n","                    global_weights[layer_start_idx + 1] = averaged_layer_biases\n","                else:\n","                    print(f\"Warning: Shape mismatch for biases at layer {layer.name}, expected {global_weights[layer_start_idx + 1].shape} but got {averaged_layer_biases.shape}\")\n","\n","            layer_start_idx += num_params\n","\n","        # Apply FedOpt (Adam) update to global weights\n","        t += 1\n","        for i in range(len(global_weights)):\n","            g_t = global_weights[i] - global_model.get_weights()[i]  # Gradient\n","            m[i] = beta1 * m[i] + (1 - beta1) * g_t\n","            v[i] = beta2 * v[i] + (1 - beta2) * (g_t ** 2)\n","            m_hat = m[i] / (1 - beta1 ** t)\n","            v_hat = v[i] / (1 - beta2 ** t)\n","            global_weights[i] -= global_optimizer.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n","\n","        global_model.set_weights(global_weights)\n","\n","    metrics_df = pd.DataFrame(metrics_list)\n","    return global_model, metrics_df\n","\n","# Example usage\n","# data_path = 'path/to/your/data.npz'\n","# global_model, metrics_df = federated_learning(data_path)\n","# print(metrics_df)\n"],"metadata":{"id":"8xrx_fxBSWGd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717504549314,"user_tz":-360,"elapsed":1034,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}},"outputId":"682de1c2-3e64-4d34-de9d-8c3d8714104f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["global_model_CNN, metrics_df_CNN = federated_learning(Theta_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1TaNdIbSfkq","outputId":"87b361d4-e2a6-45ae-e7ef-e7d589f6c9a1","executionInfo":{"status":"ok","timestamp":1717505645437,"user_tz":-360,"elapsed":1096128,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}},"collapsed":true},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (4640, 37, 29), Test shape: (1194, 37, 29)\n","Train shape: (4418, 37, 29), Test shape: (1416, 37, 29)\n","Train shape: (4838, 37, 29), Test shape: (996, 37, 29)\n","Train shape: (4640, 37, 29), Test shape: (1194, 37, 29)\n","Train shape: (4418, 37, 29), Test shape: (1416, 37, 29)\n","Train shape: (4838, 37, 29), Test shape: (996, 37, 29)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/29 [===========================>..] - ETA: 0s - loss: 1.9609 - accuracy: 0.5042"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 18s 60ms/step - loss: 1.9607 - accuracy: 0.5046 - val_loss: 1.9516 - val_accuracy: 0.4849\n","Epoch 2/100\n","29/29 [==============================] - 1s 39ms/step - loss: 1.9414 - accuracy: 0.5038 - val_loss: 1.9332 - val_accuracy: 0.5409\n","Epoch 3/100\n","29/29 [==============================] - 1s 46ms/step - loss: 1.9228 - accuracy: 0.5054 - val_loss: 1.9151 - val_accuracy: 0.5560\n","Epoch 4/100\n","29/29 [==============================] - 1s 28ms/step - loss: 1.9045 - accuracy: 0.5084 - val_loss: 1.8972 - val_accuracy: 0.5485\n","Epoch 5/100\n","29/29 [==============================] - 1s 29ms/step - loss: 1.8865 - accuracy: 0.5164 - val_loss: 1.8796 - val_accuracy: 0.5474\n","Epoch 6/100\n","29/29 [==============================] - 1s 37ms/step - loss: 1.8688 - accuracy: 0.5242 - val_loss: 1.8623 - val_accuracy: 0.5453\n","Epoch 7/100\n","29/29 [==============================] - 1s 33ms/step - loss: 1.8513 - accuracy: 0.5229 - val_loss: 1.8453 - val_accuracy: 0.5453\n","Epoch 8/100\n","29/29 [==============================] - 1s 32ms/step - loss: 1.8341 - accuracy: 0.5288 - val_loss: 1.8285 - val_accuracy: 0.5399\n","Epoch 9/100\n","29/29 [==============================] - 1s 32ms/step - loss: 1.8173 - accuracy: 0.5315 - val_loss: 1.8120 - val_accuracy: 0.5442\n","Epoch 10/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.8005 - accuracy: 0.5331 - val_loss: 1.7958 - val_accuracy: 0.5431\n","Epoch 11/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.7843 - accuracy: 0.5396 - val_loss: 1.7796 - val_accuracy: 0.5474\n","Epoch 12/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.7680 - accuracy: 0.5345 - val_loss: 1.7639 - val_accuracy: 0.5496\n","Epoch 13/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.7521 - accuracy: 0.5436 - val_loss: 1.7482 - val_accuracy: 0.5496\n","Epoch 14/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.7364 - accuracy: 0.5434 - val_loss: 1.7330 - val_accuracy: 0.5550\n","Epoch 15/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.7209 - accuracy: 0.5480 - val_loss: 1.7176 - val_accuracy: 0.5539\n","Epoch 16/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.7057 - accuracy: 0.5488 - val_loss: 1.7027 - val_accuracy: 0.5517\n","Epoch 17/100\n","29/29 [==============================] - 1s 33ms/step - loss: 1.6908 - accuracy: 0.5480 - val_loss: 1.6880 - val_accuracy: 0.5614\n","Epoch 18/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.6758 - accuracy: 0.5509 - val_loss: 1.6731 - val_accuracy: 0.5603\n","Epoch 19/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.6614 - accuracy: 0.5515 - val_loss: 1.6586 - val_accuracy: 0.5582\n","Epoch 20/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.6472 - accuracy: 0.5512 - val_loss: 1.6444 - val_accuracy: 0.5593\n","Epoch 21/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.6330 - accuracy: 0.5550 - val_loss: 1.6310 - val_accuracy: 0.5571\n","Epoch 22/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.6189 - accuracy: 0.5515 - val_loss: 1.6168 - val_accuracy: 0.5560\n","Epoch 23/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.6053 - accuracy: 0.5587 - val_loss: 1.6036 - val_accuracy: 0.5550\n","Epoch 24/100\n","29/29 [==============================] - 1s 28ms/step - loss: 1.5919 - accuracy: 0.5555 - val_loss: 1.5908 - val_accuracy: 0.5614\n","Epoch 25/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.5782 - accuracy: 0.5571 - val_loss: 1.5774 - val_accuracy: 0.5474\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.5653 - accuracy: 0.5512 - val_loss: 1.5656 - val_accuracy: 0.5582\n","Epoch 27/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.5523 - accuracy: 0.5595 - val_loss: 1.5528 - val_accuracy: 0.5431\n","Epoch 28/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.5394 - accuracy: 0.5633 - val_loss: 1.5400 - val_accuracy: 0.5420\n","Epoch 29/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.5265 - accuracy: 0.5598 - val_loss: 1.5285 - val_accuracy: 0.5409\n","Epoch 30/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.5146 - accuracy: 0.5668 - val_loss: 1.5189 - val_accuracy: 0.5539\n","Epoch 31/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.5019 - accuracy: 0.5657 - val_loss: 1.5052 - val_accuracy: 0.5366\n","Epoch 32/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.4902 - accuracy: 0.5647 - val_loss: 1.4931 - val_accuracy: 0.5312\n","Epoch 33/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.4783 - accuracy: 0.5727 - val_loss: 1.4824 - val_accuracy: 0.5388\n","Epoch 34/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.4657 - accuracy: 0.5706 - val_loss: 1.4766 - val_accuracy: 0.5474\n","Epoch 35/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.4545 - accuracy: 0.5700 - val_loss: 1.4620 - val_accuracy: 0.5539\n","Epoch 36/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.4428 - accuracy: 0.5717 - val_loss: 1.4502 - val_accuracy: 0.5345\n","Epoch 37/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.4321 - accuracy: 0.5773 - val_loss: 1.4413 - val_accuracy: 0.5345\n","Epoch 38/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.4207 - accuracy: 0.5805 - val_loss: 1.4306 - val_accuracy: 0.5366\n","Epoch 39/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.4099 - accuracy: 0.5757 - val_loss: 1.4217 - val_accuracy: 0.5366\n","Epoch 40/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.3995 - accuracy: 0.5789 - val_loss: 1.4126 - val_accuracy: 0.5345\n","Epoch 41/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.3879 - accuracy: 0.5795 - val_loss: 1.4048 - val_accuracy: 0.5496\n","Epoch 42/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.3772 - accuracy: 0.5841 - val_loss: 1.3937 - val_accuracy: 0.5334\n","Epoch 43/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.3671 - accuracy: 0.5819 - val_loss: 1.3897 - val_accuracy: 0.5463\n","Epoch 44/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.3574 - accuracy: 0.5902 - val_loss: 1.3778 - val_accuracy: 0.5399\n","Epoch 45/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.3469 - accuracy: 0.5867 - val_loss: 1.3685 - val_accuracy: 0.5463\n","Epoch 46/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.3371 - accuracy: 0.5921 - val_loss: 1.3584 - val_accuracy: 0.5302\n","Epoch 47/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.3278 - accuracy: 0.5908 - val_loss: 1.3539 - val_accuracy: 0.5474\n","Epoch 48/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.3177 - accuracy: 0.5924 - val_loss: 1.3452 - val_accuracy: 0.5442\n","Epoch 49/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.3086 - accuracy: 0.5943 - val_loss: 1.3355 - val_accuracy: 0.5420\n","Epoch 50/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.2999 - accuracy: 0.5959 - val_loss: 1.3271 - val_accuracy: 0.5420\n","Epoch 51/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.2898 - accuracy: 0.5902 - val_loss: 1.3215 - val_accuracy: 0.5409\n","Epoch 52/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.2802 - accuracy: 0.5943 - val_loss: 1.3146 - val_accuracy: 0.5485\n","Epoch 53/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.2713 - accuracy: 0.5991 - val_loss: 1.3052 - val_accuracy: 0.5442\n","Epoch 54/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.2620 - accuracy: 0.6005 - val_loss: 1.2986 - val_accuracy: 0.5453\n","Epoch 55/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.2536 - accuracy: 0.5951 - val_loss: 1.2912 - val_accuracy: 0.5420\n","Epoch 56/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.2454 - accuracy: 0.6002 - val_loss: 1.2891 - val_accuracy: 0.5496\n","Epoch 57/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.2375 - accuracy: 0.5983 - val_loss: 1.2831 - val_accuracy: 0.5603\n","Epoch 58/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.2289 - accuracy: 0.6040 - val_loss: 1.2771 - val_accuracy: 0.5463\n","Epoch 59/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.2210 - accuracy: 0.6040 - val_loss: 1.2687 - val_accuracy: 0.5517\n","Epoch 60/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.2119 - accuracy: 0.6070 - val_loss: 1.2611 - val_accuracy: 0.5571\n","Epoch 61/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.2040 - accuracy: 0.6078 - val_loss: 1.2555 - val_accuracy: 0.5506\n","Epoch 62/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1954 - accuracy: 0.6080 - val_loss: 1.2463 - val_accuracy: 0.5528\n","Epoch 63/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1888 - accuracy: 0.6026 - val_loss: 1.2394 - val_accuracy: 0.5431\n","Epoch 64/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.1817 - accuracy: 0.6091 - val_loss: 1.2341 - val_accuracy: 0.5550\n","Epoch 65/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1729 - accuracy: 0.6078 - val_loss: 1.2358 - val_accuracy: 0.5463\n","Epoch 66/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1652 - accuracy: 0.6140 - val_loss: 1.2306 - val_accuracy: 0.5399\n","Epoch 67/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1574 - accuracy: 0.6153 - val_loss: 1.2211 - val_accuracy: 0.5453\n","Epoch 68/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1505 - accuracy: 0.6183 - val_loss: 1.2159 - val_accuracy: 0.5420\n","Epoch 69/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1450 - accuracy: 0.6158 - val_loss: 1.2093 - val_accuracy: 0.5603\n","Epoch 70/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1368 - accuracy: 0.6158 - val_loss: 1.2054 - val_accuracy: 0.5517\n","Epoch 71/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.1312 - accuracy: 0.6134 - val_loss: 1.2039 - val_accuracy: 0.5517\n","Epoch 72/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.1238 - accuracy: 0.6215 - val_loss: 1.1958 - val_accuracy: 0.5517\n","Epoch 73/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.1156 - accuracy: 0.6223 - val_loss: 1.1967 - val_accuracy: 0.5496\n","Epoch 74/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.1089 - accuracy: 0.6263 - val_loss: 1.1844 - val_accuracy: 0.5603\n","Epoch 75/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.1019 - accuracy: 0.6242 - val_loss: 1.1813 - val_accuracy: 0.5528\n","Epoch 76/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0962 - accuracy: 0.6272 - val_loss: 1.1802 - val_accuracy: 0.5550\n","Epoch 77/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0890 - accuracy: 0.6301 - val_loss: 1.1814 - val_accuracy: 0.5496\n","Epoch 78/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0830 - accuracy: 0.6258 - val_loss: 1.1712 - val_accuracy: 0.5582\n","Epoch 79/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0784 - accuracy: 0.6202 - val_loss: 1.1681 - val_accuracy: 0.5453\n","Epoch 80/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0729 - accuracy: 0.6309 - val_loss: 1.1718 - val_accuracy: 0.5485\n","Epoch 81/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0646 - accuracy: 0.6320 - val_loss: 1.1579 - val_accuracy: 0.5453\n","Epoch 82/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0586 - accuracy: 0.6369 - val_loss: 1.1546 - val_accuracy: 0.5582\n","Epoch 83/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.0532 - accuracy: 0.6347 - val_loss: 1.1529 - val_accuracy: 0.5485\n","Epoch 84/100\n","29/29 [==============================] - 2s 62ms/step - loss: 1.0477 - accuracy: 0.6374 - val_loss: 1.1460 - val_accuracy: 0.5679\n","Epoch 85/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.0419 - accuracy: 0.6374 - val_loss: 1.1603 - val_accuracy: 0.5539\n","Epoch 86/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0359 - accuracy: 0.6382 - val_loss: 1.1476 - val_accuracy: 0.5614\n","Epoch 87/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0310 - accuracy: 0.6320 - val_loss: 1.1412 - val_accuracy: 0.5409\n","Epoch 88/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0238 - accuracy: 0.6430 - val_loss: 1.1497 - val_accuracy: 0.5582\n","Epoch 89/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0182 - accuracy: 0.6404 - val_loss: 1.1442 - val_accuracy: 0.5571\n","Epoch 90/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0126 - accuracy: 0.6449 - val_loss: 1.1357 - val_accuracy: 0.5539\n","Epoch 91/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0064 - accuracy: 0.6509 - val_loss: 1.1350 - val_accuracy: 0.5571\n","Epoch 92/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0032 - accuracy: 0.6463 - val_loss: 1.1449 - val_accuracy: 0.5582\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.9961 - accuracy: 0.6474 - val_loss: 1.1261 - val_accuracy: 0.5571\n","Epoch 94/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9921 - accuracy: 0.6479 - val_loss: 1.1364 - val_accuracy: 0.5528\n","Epoch 95/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.9843 - accuracy: 0.6560 - val_loss: 1.1238 - val_accuracy: 0.5463\n","Epoch 96/100\n","29/29 [==============================] - 1s 32ms/step - loss: 0.9810 - accuracy: 0.6495 - val_loss: 1.1265 - val_accuracy: 0.5506\n","Epoch 97/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.9746 - accuracy: 0.6538 - val_loss: 1.1274 - val_accuracy: 0.5528\n","Epoch 98/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.9708 - accuracy: 0.6541 - val_loss: 1.1179 - val_accuracy: 0.5571\n","Epoch 99/100\n","29/29 [==============================] - 1s 42ms/step - loss: 0.9651 - accuracy: 0.6598 - val_loss: 1.1319 - val_accuracy: 0.5690\n","Epoch 100/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.9597 - accuracy: 0.6654 - val_loss: 1.1233 - val_accuracy: 0.5560\n","{'loss': [1.9606566429138184, 1.9414315223693848, 1.922824740409851, 1.9044855833053589, 1.8864526748657227, 1.8688009977340698, 1.8513472080230713, 1.8341152667999268, 1.817318320274353, 1.8005353212356567, 1.7842862606048584, 1.7680299282073975, 1.752076268196106, 1.7364466190338135, 1.7209218740463257, 1.705731987953186, 1.6908328533172607, 1.6758294105529785, 1.6614315509796143, 1.6471660137176514, 1.6329959630966187, 1.6189112663269043, 1.6052745580673218, 1.5919171571731567, 1.578155755996704, 1.5653101205825806, 1.5522916316986084, 1.539361596107483, 1.5264768600463867, 1.5146294832229614, 1.5019278526306152, 1.4901598691940308, 1.4783380031585693, 1.4657447338104248, 1.4544706344604492, 1.4428389072418213, 1.432084560394287, 1.4207088947296143, 1.4098681211471558, 1.3995230197906494, 1.3879420757293701, 1.3772163391113281, 1.3671066761016846, 1.35735023021698, 1.346917986869812, 1.3371267318725586, 1.327756643295288, 1.317650318145752, 1.308571696281433, 1.299879789352417, 1.2897592782974243, 1.2802026271820068, 1.27134370803833, 1.2620046138763428, 1.253596305847168, 1.2454160451889038, 1.2374886274337769, 1.2288572788238525, 1.2209630012512207, 1.2119406461715698, 1.204014778137207, 1.1953917741775513, 1.188767433166504, 1.1816904544830322, 1.172898530960083, 1.1652307510375977, 1.157444953918457, 1.150537133216858, 1.144951581954956, 1.1368328332901, 1.1312016248703003, 1.1238380670547485, 1.1156108379364014, 1.108941674232483, 1.1018911600112915, 1.0961689949035645, 1.0889832973480225, 1.0830374956130981, 1.0783920288085938, 1.07293701171875, 1.0646353960037231, 1.058625340461731, 1.0531681776046753, 1.0476912260055542, 1.0418800115585327, 1.0358922481536865, 1.0309830904006958, 1.023805856704712, 1.0181623697280884, 1.0125749111175537, 1.0063817501068115, 1.003220796585083, 0.9961323142051697, 0.9921303987503052, 0.9843021035194397, 0.9809913039207458, 0.974587619304657, 0.9707837104797363, 0.9651262760162354, 0.9596675634384155], 'accuracy': [0.5045797228813171, 0.5037715435028076, 0.5053879022598267, 0.5083512663841248, 0.5164331793785095, 0.5242456793785095, 0.5228987336158752, 0.5288254022598267, 0.5315194129943848, 0.5331357717514038, 0.5396012663841248, 0.5344827771186829, 0.5436422228813171, 0.5433728694915771, 0.5479525923728943, 0.5487607717514038, 0.5479525923728943, 0.5509159564971924, 0.5514547228813171, 0.5511853694915771, 0.5549569129943848, 0.5514547228813171, 0.5587284564971924, 0.5554956793785095, 0.5571120977401733, 0.5511853694915771, 0.5595366358757019, 0.5633081793785095, 0.5598060488700867, 0.5668103694915771, 0.5657327771186829, 0.5646551847457886, 0.5727370977401733, 0.5705819129943848, 0.5700430870056152, 0.571659505367279, 0.5773168206214905, 0.5805495977401733, 0.5757004022598267, 0.5789331793785095, 0.579472005367279, 0.5840517282485962, 0.5818965435028076, 0.5902478694915771, 0.5867456793785095, 0.592133641242981, 0.5907866358757019, 0.592402994632721, 0.5942887663841248, 0.5959051847457886, 0.5902478694915771, 0.5942887663841248, 0.5991379022598267, 0.6004849076271057, 0.595097005367279, 0.600215494632721, 0.5983297228813171, 0.6039870977401733, 0.6039870977401733, 0.6069504022598267, 0.607758641242981, 0.608027994632721, 0.6026400923728943, 0.6091055870056152, 0.607758641242981, 0.6139547228813171, 0.6153017282485962, 0.6182650923728943, 0.615840494632721, 0.615840494632721, 0.6134159564971924, 0.6214978694915771, 0.6223060488700867, 0.626347005367279, 0.6241918206214905, 0.6271551847457886, 0.6301185488700867, 0.6258081793785095, 0.6201508641242981, 0.6309267282485962, 0.6320043206214905, 0.6368534564971924, 0.6346982717514038, 0.6373922228813171, 0.6373922228813171, 0.6382004022598267, 0.6320043206214905, 0.6430495977401733, 0.6403555870056152, 0.6449353694915771, 0.6508620977401733, 0.6462823152542114, 0.6473599076271057, 0.6478987336158752, 0.6559805870056152, 0.6495150923728943, 0.6538254022598267, 0.6540948152542114, 0.6597521305084229, 0.665409505367279], 'val_loss': [1.9516148567199707, 1.9332146644592285, 1.9150819778442383, 1.8972315788269043, 1.8796287775039673, 1.8623486757278442, 1.8453282117843628, 1.8285030126571655, 1.8120028972625732, 1.7957587242126465, 1.7796413898468018, 1.7638548612594604, 1.748193621635437, 1.7329866886138916, 1.7175891399383545, 1.7027357816696167, 1.6880316734313965, 1.6731222867965698, 1.658618688583374, 1.6443713903427124, 1.6309924125671387, 1.6167534589767456, 1.603630781173706, 1.5908112525939941, 1.577444314956665, 1.5656496286392212, 1.552779197692871, 1.5400075912475586, 1.5284790992736816, 1.5189450979232788, 1.505151391029358, 1.49314284324646, 1.4824129343032837, 1.4765764474868774, 1.4620002508163452, 1.4502272605895996, 1.4412882328033447, 1.430603265762329, 1.4217214584350586, 1.412554383277893, 1.4047701358795166, 1.3936536312103271, 1.3897420167922974, 1.3778483867645264, 1.3684993982315063, 1.3584431409835815, 1.353941798210144, 1.3451995849609375, 1.3355095386505127, 1.3271300792694092, 1.321527123451233, 1.3146086931228638, 1.3052486181259155, 1.2986092567443848, 1.2911591529846191, 1.2890982627868652, 1.2830764055252075, 1.2770882844924927, 1.2686631679534912, 1.2610859870910645, 1.2554922103881836, 1.246259093284607, 1.2393988370895386, 1.234118938446045, 1.2358431816101074, 1.2306478023529053, 1.2211253643035889, 1.2158637046813965, 1.209334135055542, 1.2053650617599487, 1.203935980796814, 1.1958034038543701, 1.196710228919983, 1.1843634843826294, 1.1813174486160278, 1.1802102327346802, 1.181381106376648, 1.171154499053955, 1.1681163311004639, 1.1717997789382935, 1.1578634977340698, 1.1545745134353638, 1.1528717279434204, 1.1459541320800781, 1.1602911949157715, 1.147618293762207, 1.1411900520324707, 1.1497299671173096, 1.1442086696624756, 1.1356736421585083, 1.134964942932129, 1.1449460983276367, 1.126104712486267, 1.1363579034805298, 1.1238164901733398, 1.1265259981155396, 1.1274181604385376, 1.1178580522537231, 1.1318827867507935, 1.1233469247817993], 'val_accuracy': [0.48491379618644714, 0.5409482717514038, 0.556034505367279, 0.548491358757019, 0.5474137663841248, 0.545258641242981, 0.545258641242981, 0.5398706793785095, 0.5441810488700867, 0.5431034564971924, 0.5474137663841248, 0.5495689511299133, 0.5495689511299133, 0.5549569129943848, 0.5538793206214905, 0.5517241358757019, 0.5614224076271057, 0.5603448152542114, 0.5581896305084229, 0.5592672228813171, 0.5571120977401733, 0.556034505367279, 0.5549569129943848, 0.5614224076271057, 0.5474137663841248, 0.5581896305084229, 0.5431034564971924, 0.5420258641242981, 0.5409482717514038, 0.5538793206214905, 0.5366379022598267, 0.53125, 0.5387930870056152, 0.5474137663841248, 0.5538793206214905, 0.5344827771186829, 0.5344827771186829, 0.5366379022598267, 0.5366379022598267, 0.5344827771186829, 0.5495689511299133, 0.5334051847457886, 0.5463362336158752, 0.5398706793785095, 0.5463362336158752, 0.5301724076271057, 0.5474137663841248, 0.5441810488700867, 0.5420258641242981, 0.5420258641242981, 0.5409482717514038, 0.548491358757019, 0.5441810488700867, 0.545258641242981, 0.5420258641242981, 0.5495689511299133, 0.5603448152542114, 0.5463362336158752, 0.5517241358757019, 0.5571120977401733, 0.5506465435028076, 0.5528017282485962, 0.5431034564971924, 0.5549569129943848, 0.5463362336158752, 0.5398706793785095, 0.545258641242981, 0.5420258641242981, 0.5603448152542114, 0.5517241358757019, 0.5517241358757019, 0.5517241358757019, 0.5495689511299133, 0.5603448152542114, 0.5528017282485962, 0.5549569129943848, 0.5495689511299133, 0.5581896305084229, 0.545258641242981, 0.548491358757019, 0.545258641242981, 0.5581896305084229, 0.548491358757019, 0.5678879022598267, 0.5538793206214905, 0.5614224076271057, 0.5409482717514038, 0.5581896305084229, 0.5571120977401733, 0.5538793206214905, 0.5571120977401733, 0.5581896305084229, 0.5571120977401733, 0.5528017282485962, 0.5463362336158752, 0.5506465435028076, 0.5528017282485962, 0.5571120977401733, 0.568965494632721, 0.556034505367279]}\n","38/38 [==============================] - 0s 9ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 6s 65ms/step - loss: 1.9610 - accuracy: 0.5014 - val_loss: 1.9522 - val_accuracy: 0.5238\n","Epoch 2/100\n"," 1/28 [>.............................] - ETA: 0s - loss: 1.9492 - accuracy: 0.5312"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["28/28 [==============================] - 1s 20ms/step - loss: 1.9421 - accuracy: 0.5023 - val_loss: 1.9343 - val_accuracy: 0.5362\n","Epoch 3/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.9239 - accuracy: 0.5085 - val_loss: 1.9167 - val_accuracy: 0.5260\n","Epoch 4/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.9060 - accuracy: 0.5170 - val_loss: 1.8994 - val_accuracy: 0.5283\n","Epoch 5/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.8884 - accuracy: 0.5170 - val_loss: 1.8823 - val_accuracy: 0.5226\n","Epoch 6/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.8711 - accuracy: 0.5198 - val_loss: 1.8655 - val_accuracy: 0.5238\n","Epoch 7/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.8542 - accuracy: 0.5255 - val_loss: 1.8490 - val_accuracy: 0.5226\n","Epoch 8/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.8376 - accuracy: 0.5396 - val_loss: 1.8328 - val_accuracy: 0.5181\n","Epoch 9/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.8211 - accuracy: 0.5362 - val_loss: 1.8167 - val_accuracy: 0.5249\n","Epoch 10/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.8047 - accuracy: 0.5427 - val_loss: 1.8009 - val_accuracy: 0.5294\n","Epoch 11/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.7886 - accuracy: 0.5430 - val_loss: 1.7854 - val_accuracy: 0.5283\n","Epoch 12/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.7728 - accuracy: 0.5439 - val_loss: 1.7700 - val_accuracy: 0.5283\n","Epoch 13/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.7572 - accuracy: 0.5473 - val_loss: 1.7548 - val_accuracy: 0.5317\n","Epoch 14/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.7419 - accuracy: 0.5433 - val_loss: 1.7400 - val_accuracy: 0.5351\n","Epoch 15/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.7268 - accuracy: 0.5490 - val_loss: 1.7253 - val_accuracy: 0.5305\n","Epoch 16/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.7117 - accuracy: 0.5566 - val_loss: 1.7106 - val_accuracy: 0.5396\n","Epoch 17/100\n","28/28 [==============================] - 1s 23ms/step - loss: 1.6969 - accuracy: 0.5507 - val_loss: 1.6964 - val_accuracy: 0.5419\n","Epoch 18/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.6824 - accuracy: 0.5509 - val_loss: 1.6824 - val_accuracy: 0.5373\n","Epoch 19/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.6682 - accuracy: 0.5574 - val_loss: 1.6684 - val_accuracy: 0.5520\n","Epoch 20/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.6538 - accuracy: 0.5574 - val_loss: 1.6548 - val_accuracy: 0.5509\n","Epoch 21/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.6400 - accuracy: 0.5625 - val_loss: 1.6413 - val_accuracy: 0.5611\n","Epoch 22/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.6261 - accuracy: 0.5594 - val_loss: 1.6283 - val_accuracy: 0.5554\n","Epoch 23/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.6126 - accuracy: 0.5634 - val_loss: 1.6152 - val_accuracy: 0.5532\n","Epoch 24/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.5992 - accuracy: 0.5637 - val_loss: 1.6024 - val_accuracy: 0.5600\n","Epoch 25/100\n","28/28 [==============================] - 1s 29ms/step - loss: 1.5858 - accuracy: 0.5640 - val_loss: 1.5904 - val_accuracy: 0.5656\n","Epoch 26/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.5730 - accuracy: 0.5642 - val_loss: 1.5779 - val_accuracy: 0.5486\n","Epoch 27/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.5605 - accuracy: 0.5645 - val_loss: 1.5659 - val_accuracy: 0.5656\n","Epoch 28/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.5472 - accuracy: 0.5648 - val_loss: 1.5545 - val_accuracy: 0.5588\n","Epoch 29/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.5347 - accuracy: 0.5756 - val_loss: 1.5427 - val_accuracy: 0.5554\n","Epoch 30/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.5229 - accuracy: 0.5679 - val_loss: 1.5317 - val_accuracy: 0.5611\n","Epoch 31/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.5104 - accuracy: 0.5705 - val_loss: 1.5206 - val_accuracy: 0.5633\n","Epoch 32/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.4984 - accuracy: 0.5679 - val_loss: 1.5093 - val_accuracy: 0.5600\n","Epoch 33/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.4868 - accuracy: 0.5705 - val_loss: 1.4986 - val_accuracy: 0.5611\n","Epoch 34/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.4754 - accuracy: 0.5702 - val_loss: 1.4890 - val_accuracy: 0.5645\n","Epoch 35/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.4643 - accuracy: 0.5727 - val_loss: 1.4778 - val_accuracy: 0.5690\n","Epoch 36/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.4532 - accuracy: 0.5772 - val_loss: 1.4681 - val_accuracy: 0.5679\n","Epoch 37/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.4419 - accuracy: 0.5716 - val_loss: 1.4590 - val_accuracy: 0.5645\n","Epoch 38/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.4318 - accuracy: 0.5744 - val_loss: 1.4480 - val_accuracy: 0.5679\n","Epoch 39/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.4205 - accuracy: 0.5778 - val_loss: 1.4392 - val_accuracy: 0.5667\n","Epoch 40/100\n","28/28 [==============================] - 1s 32ms/step - loss: 1.4098 - accuracy: 0.5818 - val_loss: 1.4305 - val_accuracy: 0.5701\n","Epoch 41/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.3996 - accuracy: 0.5815 - val_loss: 1.4203 - val_accuracy: 0.5735\n","Epoch 42/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.3895 - accuracy: 0.5761 - val_loss: 1.4121 - val_accuracy: 0.5701\n","Epoch 43/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.3797 - accuracy: 0.5829 - val_loss: 1.4021 - val_accuracy: 0.5701\n","Epoch 44/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.3701 - accuracy: 0.5821 - val_loss: 1.3930 - val_accuracy: 0.5724\n","Epoch 45/100\n","28/28 [==============================] - 1s 23ms/step - loss: 1.3600 - accuracy: 0.5835 - val_loss: 1.3850 - val_accuracy: 0.5769\n","Epoch 46/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.3506 - accuracy: 0.5877 - val_loss: 1.3787 - val_accuracy: 0.5679\n","Epoch 47/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.3413 - accuracy: 0.5835 - val_loss: 1.3680 - val_accuracy: 0.5735\n","Epoch 48/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.3321 - accuracy: 0.5866 - val_loss: 1.3618 - val_accuracy: 0.5690\n","Epoch 49/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.3239 - accuracy: 0.5823 - val_loss: 1.3563 - val_accuracy: 0.5622\n","Epoch 50/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.3140 - accuracy: 0.5905 - val_loss: 1.3471 - val_accuracy: 0.5633\n","Epoch 51/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.3057 - accuracy: 0.5860 - val_loss: 1.3383 - val_accuracy: 0.5701\n","Epoch 52/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.2975 - accuracy: 0.5886 - val_loss: 1.3304 - val_accuracy: 0.5769\n","Epoch 53/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2877 - accuracy: 0.5891 - val_loss: 1.3217 - val_accuracy: 0.5758\n","Epoch 54/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2790 - accuracy: 0.5945 - val_loss: 1.3141 - val_accuracy: 0.5724\n","Epoch 55/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2708 - accuracy: 0.5951 - val_loss: 1.3081 - val_accuracy: 0.5724\n","Epoch 56/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.2622 - accuracy: 0.5931 - val_loss: 1.2998 - val_accuracy: 0.5747\n","Epoch 57/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2545 - accuracy: 0.5948 - val_loss: 1.2932 - val_accuracy: 0.5724\n","Epoch 58/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.2467 - accuracy: 0.5973 - val_loss: 1.2864 - val_accuracy: 0.5690\n","Epoch 59/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.2390 - accuracy: 0.5962 - val_loss: 1.2804 - val_accuracy: 0.5656\n","Epoch 60/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.2314 - accuracy: 0.5965 - val_loss: 1.2750 - val_accuracy: 0.5633\n","Epoch 61/100\n","28/28 [==============================] - 1s 19ms/step - loss: 1.2225 - accuracy: 0.6007 - val_loss: 1.2659 - val_accuracy: 0.5690\n","Epoch 62/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.2161 - accuracy: 0.5948 - val_loss: 1.2596 - val_accuracy: 0.5701\n","Epoch 63/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.2083 - accuracy: 0.6010 - val_loss: 1.2546 - val_accuracy: 0.5690\n","Epoch 64/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.2018 - accuracy: 0.6019 - val_loss: 1.2480 - val_accuracy: 0.5747\n","Epoch 65/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.1946 - accuracy: 0.5990 - val_loss: 1.2442 - val_accuracy: 0.5645\n","Epoch 66/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.1867 - accuracy: 0.6033 - val_loss: 1.2375 - val_accuracy: 0.5645\n","Epoch 67/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1788 - accuracy: 0.6038 - val_loss: 1.2322 - val_accuracy: 0.5690\n","Epoch 68/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.1717 - accuracy: 0.6078 - val_loss: 1.2257 - val_accuracy: 0.5724\n","Epoch 69/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1645 - accuracy: 0.6126 - val_loss: 1.2199 - val_accuracy: 0.5667\n","Epoch 70/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1580 - accuracy: 0.6058 - val_loss: 1.2145 - val_accuracy: 0.5679\n","Epoch 71/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.1509 - accuracy: 0.6118 - val_loss: 1.2112 - val_accuracy: 0.5622\n","Epoch 72/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.1441 - accuracy: 0.6115 - val_loss: 1.2066 - val_accuracy: 0.5701\n","Epoch 73/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1384 - accuracy: 0.6132 - val_loss: 1.2007 - val_accuracy: 0.5566\n","Epoch 74/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1316 - accuracy: 0.6135 - val_loss: 1.1965 - val_accuracy: 0.5622\n","Epoch 75/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.1259 - accuracy: 0.6152 - val_loss: 1.1894 - val_accuracy: 0.5690\n","Epoch 76/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.1187 - accuracy: 0.6152 - val_loss: 1.1865 - val_accuracy: 0.5611\n","Epoch 77/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1119 - accuracy: 0.6186 - val_loss: 1.1814 - val_accuracy: 0.5577\n","Epoch 78/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1058 - accuracy: 0.6208 - val_loss: 1.1777 - val_accuracy: 0.5611\n","Epoch 79/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0992 - accuracy: 0.6211 - val_loss: 1.1728 - val_accuracy: 0.5600\n","Epoch 80/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0934 - accuracy: 0.6208 - val_loss: 1.1705 - val_accuracy: 0.5656\n","Epoch 81/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0881 - accuracy: 0.6254 - val_loss: 1.1718 - val_accuracy: 0.5679\n","Epoch 82/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.0811 - accuracy: 0.6234 - val_loss: 1.1608 - val_accuracy: 0.5611\n","Epoch 83/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.0745 - accuracy: 0.6279 - val_loss: 1.1616 - val_accuracy: 0.5566\n","Epoch 84/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0698 - accuracy: 0.6302 - val_loss: 1.1552 - val_accuracy: 0.5690\n","Epoch 85/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0644 - accuracy: 0.6290 - val_loss: 1.1514 - val_accuracy: 0.5622\n","Epoch 86/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0571 - accuracy: 0.6304 - val_loss: 1.1447 - val_accuracy: 0.5690\n","Epoch 87/100\n","28/28 [==============================] - 1s 19ms/step - loss: 1.0523 - accuracy: 0.6327 - val_loss: 1.1427 - val_accuracy: 0.5622\n","Epoch 88/100\n","28/28 [==============================] - 1s 19ms/step - loss: 1.0475 - accuracy: 0.6324 - val_loss: 1.1402 - val_accuracy: 0.5566\n","Epoch 89/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.0422 - accuracy: 0.6361 - val_loss: 1.1362 - val_accuracy: 0.5611\n","Epoch 90/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0361 - accuracy: 0.6378 - val_loss: 1.1380 - val_accuracy: 0.5577\n","Epoch 91/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0308 - accuracy: 0.6375 - val_loss: 1.1332 - val_accuracy: 0.5633\n","Epoch 92/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0253 - accuracy: 0.6367 - val_loss: 1.1295 - val_accuracy: 0.5543\n","Epoch 93/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0184 - accuracy: 0.6415 - val_loss: 1.1225 - val_accuracy: 0.5622\n","Epoch 94/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0139 - accuracy: 0.6418 - val_loss: 1.1190 - val_accuracy: 0.5588\n","Epoch 95/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.0117 - accuracy: 0.6364 - val_loss: 1.1171 - val_accuracy: 0.5611\n","Epoch 96/100\n","28/28 [==============================] - 1s 19ms/step - loss: 1.0035 - accuracy: 0.6443 - val_loss: 1.1124 - val_accuracy: 0.5566\n","Epoch 97/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0002 - accuracy: 0.6395 - val_loss: 1.1094 - val_accuracy: 0.5611\n","Epoch 98/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9947 - accuracy: 0.6449 - val_loss: 1.1073 - val_accuracy: 0.5566\n","Epoch 99/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9893 - accuracy: 0.6551 - val_loss: 1.1128 - val_accuracy: 0.5464\n","Epoch 100/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.9843 - accuracy: 0.6480 - val_loss: 1.1014 - val_accuracy: 0.5520\n","{'loss': [1.961019515991211, 1.9421446323394775, 1.9238759279251099, 1.9059919118881226, 1.8884334564208984, 1.8711248636245728, 1.8542060852050781, 1.837626338005066, 1.8210967779159546, 1.8047031164169312, 1.7886247634887695, 1.7728376388549805, 1.7571990489959717, 1.741927146911621, 1.7267507314682007, 1.7117313146591187, 1.6969337463378906, 1.682399868965149, 1.668177843093872, 1.6537728309631348, 1.6399526596069336, 1.6260793209075928, 1.612562656402588, 1.5991920232772827, 1.5857517719268799, 1.5730077028274536, 1.5605335235595703, 1.547191858291626, 1.5346956253051758, 1.5228830575942993, 1.510359287261963, 1.4983855485916138, 1.4868171215057373, 1.4753564596176147, 1.4643477201461792, 1.4532068967819214, 1.4419292211532593, 1.4317851066589355, 1.4204641580581665, 1.4098052978515625, 1.3996292352676392, 1.389454960823059, 1.379722237586975, 1.3700792789459229, 1.360007643699646, 1.3506407737731934, 1.3412511348724365, 1.3320746421813965, 1.3238816261291504, 1.3140188455581665, 1.3057498931884766, 1.2975082397460938, 1.2877113819122314, 1.2789901494979858, 1.270829200744629, 1.2622376680374146, 1.2544885873794556, 1.2467334270477295, 1.2390162944793701, 1.2313508987426758, 1.2225379943847656, 1.216103434562683, 1.208285927772522, 1.201812505722046, 1.1945741176605225, 1.1867187023162842, 1.1787701845169067, 1.1716892719268799, 1.1644797325134277, 1.1580417156219482, 1.1508965492248535, 1.1441212892532349, 1.1383901834487915, 1.1316490173339844, 1.1259225606918335, 1.1186509132385254, 1.1118923425674438, 1.1057829856872559, 1.0992330312728882, 1.0933550596237183, 1.088075876235962, 1.0810848474502563, 1.0745471715927124, 1.06975519657135, 1.0643644332885742, 1.0570887327194214, 1.0523420572280884, 1.0475342273712158, 1.042235255241394, 1.0361415147781372, 1.030767560005188, 1.0252728462219238, 1.0184324979782104, 1.0138700008392334, 1.0116932392120361, 1.003548264503479, 1.0001567602157593, 0.9946788549423218, 0.9893098473548889, 0.9842828512191772], 'accuracy': [0.5014148354530334, 0.5022637248039246, 0.5084889531135559, 0.5169779062271118, 0.5169779062271118, 0.5198075771331787, 0.5254669189453125, 0.5396151542663574, 0.536219596862793, 0.5427277684211731, 0.5430107712745667, 0.5438596606254578, 0.5472552180290222, 0.5432937145233154, 0.5489530563354492, 0.556593120098114, 0.5506508350372314, 0.5509337782859802, 0.5574420094490051, 0.5574420094490051, 0.5625353455543518, 0.5594227313995361, 0.5633842945098877, 0.5636672377586365, 0.5639501810073853, 0.5642331838607788, 0.5645161271095276, 0.5647990703582764, 0.5755518078804016, 0.5679117441177368, 0.5704584121704102, 0.5679117441177368, 0.5704584121704102, 0.5701754093170166, 0.5727221369743347, 0.5772495865821838, 0.57159024477005, 0.5744199156761169, 0.5778155326843262, 0.581777036190033, 0.5814940333366394, 0.5761176943778992, 0.5829088687896729, 0.5820599794387817, 0.5834748148918152, 0.5877193212509155, 0.5834748148918152, 0.5865874290466309, 0.5823429822921753, 0.5905489325523376, 0.5860214829444885, 0.5885682106018066, 0.5891340970993042, 0.5945104956626892, 0.5950763821601868, 0.5930956602096558, 0.594793438911438, 0.5973401069641113, 0.5962082743644714, 0.5964912176132202, 0.6007357239723206, 0.594793438911438, 0.6010186672210693, 0.6018675565719604, 0.5990379452705383, 0.6032823920249939, 0.6038483381271362, 0.607809841632843, 0.6126202344894409, 0.6058290600776672, 0.6117713451385498, 0.611488401889801, 0.6131861805915833, 0.6134691834449768, 0.615166962146759, 0.615166962146759, 0.6185625195503235, 0.620826244354248, 0.6211092472076416, 0.620826244354248, 0.6253536939620972, 0.6233729720115662, 0.6279004216194153, 0.6301641464233398, 0.6290322542190552, 0.6304470896720886, 0.6327108144760132, 0.6324278712272644, 0.6361063718795776, 0.6378042101860046, 0.6375212073326111, 0.63667231798172, 0.6414827108383179, 0.6417657136917114, 0.6363893747329712, 0.6443123817443848, 0.6395019888877869, 0.6448783278465271, 0.6550650596618652, 0.6479909420013428], 'val_loss': [1.9522370100021362, 1.9343366622924805, 1.9167454242706299, 1.8994117975234985, 1.8823423385620117, 1.8655273914337158, 1.8490170240402222, 1.8327699899673462, 1.816729187965393, 1.8009012937545776, 1.7853657007217407, 1.7699631452560425, 1.7548295259475708, 1.7400014400482178, 1.725277304649353, 1.7106373310089111, 1.6963720321655273, 1.6824233531951904, 1.6684080362319946, 1.6548116207122803, 1.6413391828536987, 1.6282538175582886, 1.6152424812316895, 1.6024385690689087, 1.590439796447754, 1.5779246091842651, 1.5659263134002686, 1.5544989109039307, 1.542709469795227, 1.531691312789917, 1.520578384399414, 1.5093036890029907, 1.498615026473999, 1.489030122756958, 1.4778426885604858, 1.468056082725525, 1.459006905555725, 1.4479804039001465, 1.4391685724258423, 1.430471658706665, 1.4203273057937622, 1.4121079444885254, 1.402143120765686, 1.3929837942123413, 1.3850359916687012, 1.3787442445755005, 1.367956519126892, 1.3617849349975586, 1.3562514781951904, 1.347089171409607, 1.3382618427276611, 1.330445408821106, 1.3216729164123535, 1.3141486644744873, 1.3081018924713135, 1.29984450340271, 1.293211817741394, 1.286447286605835, 1.2803900241851807, 1.2750201225280762, 1.2658711671829224, 1.259607195854187, 1.2545677423477173, 1.247982382774353, 1.2442066669464111, 1.2375112771987915, 1.2321643829345703, 1.2257155179977417, 1.219897747039795, 1.2145164012908936, 1.21122407913208, 1.2065606117248535, 1.200696587562561, 1.1965283155441284, 1.1894042491912842, 1.1865283250808716, 1.1813771724700928, 1.1776708364486694, 1.1727710962295532, 1.1704778671264648, 1.171776294708252, 1.1607712507247925, 1.1615623235702515, 1.1552430391311646, 1.1514480113983154, 1.1446833610534668, 1.1426782608032227, 1.1402074098587036, 1.1361720561981201, 1.1379966735839844, 1.133226752281189, 1.1295002698898315, 1.1224911212921143, 1.1190176010131836, 1.1171069145202637, 1.1124221086502075, 1.1093579530715942, 1.1072766780853271, 1.1127901077270508, 1.1014045476913452], 'val_accuracy': [0.523755669593811, 0.5361990928649902, 0.5260180830955505, 0.5282805562019348, 0.5226244330406189, 0.523755669593811, 0.5226244330406189, 0.5180995464324951, 0.5248869061470032, 0.529411792755127, 0.5282805562019348, 0.5282805562019348, 0.5316742062568665, 0.5350678563117981, 0.5305429697036743, 0.5395927429199219, 0.5418552160263062, 0.5373303294181824, 0.5520362257957458, 0.5509049892425537, 0.5610859990119934, 0.5554298758506775, 0.5531674027442932, 0.5599547624588013, 0.5656108856201172, 0.5486425161361694, 0.5656108856201172, 0.5588235259056091, 0.5554298758506775, 0.5610859990119934, 0.5633484125137329, 0.5599547624588013, 0.5610859990119934, 0.564479649066925, 0.5690045356750488, 0.5678732991218567, 0.564479649066925, 0.5678732991218567, 0.5667420625686646, 0.570135772228241, 0.5735294222831726, 0.570135772228241, 0.570135772228241, 0.5723981857299805, 0.5769230723381042, 0.5678732991218567, 0.5735294222831726, 0.5690045356750488, 0.5622171759605408, 0.5633484125137329, 0.570135772228241, 0.5769230723381042, 0.5757918357849121, 0.5723981857299805, 0.5723981857299805, 0.5746606588363647, 0.5723981857299805, 0.5690045356750488, 0.5656108856201172, 0.5633484125137329, 0.5690045356750488, 0.570135772228241, 0.5690045356750488, 0.5746606588363647, 0.564479649066925, 0.564479649066925, 0.5690045356750488, 0.5723981857299805, 0.5667420625686646, 0.5678732991218567, 0.5622171759605408, 0.570135772228241, 0.5565611124038696, 0.5622171759605408, 0.5690045356750488, 0.5610859990119934, 0.557692289352417, 0.5610859990119934, 0.5599547624588013, 0.5656108856201172, 0.5678732991218567, 0.5610859990119934, 0.5565611124038696, 0.5690045356750488, 0.5622171759605408, 0.5690045356750488, 0.5622171759605408, 0.5565611124038696, 0.5610859990119934, 0.557692289352417, 0.5633484125137329, 0.5542986392974854, 0.5622171759605408, 0.5588235259056091, 0.5610859990119934, 0.5565611124038696, 0.5610859990119934, 0.5565611124038696, 0.5463801026344299, 0.5520362257957458]}\n","45/45 [==============================] - 0s 7ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - ETA: 0s - loss: 1.9598 - accuracy: 0.4982"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 5s 62ms/step - loss: 1.9598 - accuracy: 0.4982 - val_loss: 1.9505 - val_accuracy: 0.4855\n","Epoch 2/100\n","31/31 [==============================] - 1s 25ms/step - loss: 1.9387 - accuracy: 0.5059 - val_loss: 1.9308 - val_accuracy: 0.5248\n","Epoch 3/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.9185 - accuracy: 0.5171 - val_loss: 1.9115 - val_accuracy: 0.5217\n","Epoch 4/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.8990 - accuracy: 0.5235 - val_loss: 1.8925 - val_accuracy: 0.5238\n","Epoch 5/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.8797 - accuracy: 0.5238 - val_loss: 1.8739 - val_accuracy: 0.5196\n","Epoch 6/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.8609 - accuracy: 0.5279 - val_loss: 1.8555 - val_accuracy: 0.5196\n","Epoch 7/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.8423 - accuracy: 0.5310 - val_loss: 1.8375 - val_accuracy: 0.5217\n","Epoch 8/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.8240 - accuracy: 0.5344 - val_loss: 1.8197 - val_accuracy: 0.5196\n","Epoch 9/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.8061 - accuracy: 0.5434 - val_loss: 1.8023 - val_accuracy: 0.5238\n","Epoch 10/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.7884 - accuracy: 0.5473 - val_loss: 1.7851 - val_accuracy: 0.5207\n","Epoch 11/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.7710 - accuracy: 0.5457 - val_loss: 1.7682 - val_accuracy: 0.5227\n","Epoch 12/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.7539 - accuracy: 0.5488 - val_loss: 1.7516 - val_accuracy: 0.5227\n","Epoch 13/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.7371 - accuracy: 0.5481 - val_loss: 1.7353 - val_accuracy: 0.5207\n","Epoch 14/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.7206 - accuracy: 0.5465 - val_loss: 1.7193 - val_accuracy: 0.5258\n","Epoch 15/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.7046 - accuracy: 0.5504 - val_loss: 1.7036 - val_accuracy: 0.5248\n","Epoch 16/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.6884 - accuracy: 0.5506 - val_loss: 1.6881 - val_accuracy: 0.5238\n","Epoch 17/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.6728 - accuracy: 0.5522 - val_loss: 1.6730 - val_accuracy: 0.5248\n","Epoch 18/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.6571 - accuracy: 0.5615 - val_loss: 1.6582 - val_accuracy: 0.5269\n","Epoch 19/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.6419 - accuracy: 0.5574 - val_loss: 1.6438 - val_accuracy: 0.5248\n","Epoch 20/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.6269 - accuracy: 0.5530 - val_loss: 1.6293 - val_accuracy: 0.5248\n","Epoch 21/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.6123 - accuracy: 0.5625 - val_loss: 1.6157 - val_accuracy: 0.5238\n","Epoch 22/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.5977 - accuracy: 0.5602 - val_loss: 1.6021 - val_accuracy: 0.5248\n","Epoch 23/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.5835 - accuracy: 0.5643 - val_loss: 1.5897 - val_accuracy: 0.5331\n","Epoch 24/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.5692 - accuracy: 0.5607 - val_loss: 1.5755 - val_accuracy: 0.5320\n","Epoch 25/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.5557 - accuracy: 0.5656 - val_loss: 1.5629 - val_accuracy: 0.5269\n","Epoch 26/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.5419 - accuracy: 0.5630 - val_loss: 1.5499 - val_accuracy: 0.5351\n","Epoch 27/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.5284 - accuracy: 0.5646 - val_loss: 1.5368 - val_accuracy: 0.5351\n","Epoch 28/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.5161 - accuracy: 0.5659 - val_loss: 1.5255 - val_accuracy: 0.5258\n","Epoch 29/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.5025 - accuracy: 0.5612 - val_loss: 1.5146 - val_accuracy: 0.5310\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4894 - accuracy: 0.5677 - val_loss: 1.5022 - val_accuracy: 0.5300\n","Epoch 31/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4772 - accuracy: 0.5646 - val_loss: 1.4914 - val_accuracy: 0.5289\n","Epoch 32/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.4645 - accuracy: 0.5695 - val_loss: 1.4812 - val_accuracy: 0.5351\n","Epoch 33/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.4524 - accuracy: 0.5682 - val_loss: 1.4688 - val_accuracy: 0.5300\n","Epoch 34/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.4403 - accuracy: 0.5677 - val_loss: 1.4620 - val_accuracy: 0.5258\n","Epoch 35/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.4294 - accuracy: 0.5703 - val_loss: 1.4485 - val_accuracy: 0.5279\n","Epoch 36/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.4174 - accuracy: 0.5680 - val_loss: 1.4389 - val_accuracy: 0.5320\n","Epoch 37/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.4061 - accuracy: 0.5724 - val_loss: 1.4270 - val_accuracy: 0.5341\n","Epoch 38/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.3954 - accuracy: 0.5731 - val_loss: 1.4223 - val_accuracy: 0.5289\n","Epoch 39/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.3838 - accuracy: 0.5729 - val_loss: 1.4070 - val_accuracy: 0.5372\n","Epoch 40/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.3732 - accuracy: 0.5736 - val_loss: 1.3952 - val_accuracy: 0.5310\n","Epoch 41/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.3634 - accuracy: 0.5734 - val_loss: 1.3904 - val_accuracy: 0.5362\n","Epoch 42/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.3522 - accuracy: 0.5773 - val_loss: 1.3782 - val_accuracy: 0.5362\n","Epoch 43/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.3421 - accuracy: 0.5809 - val_loss: 1.3717 - val_accuracy: 0.5341\n","Epoch 44/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.3320 - accuracy: 0.5749 - val_loss: 1.3639 - val_accuracy: 0.5331\n","Epoch 45/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.3218 - accuracy: 0.5819 - val_loss: 1.3549 - val_accuracy: 0.5238\n","Epoch 46/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.3128 - accuracy: 0.5824 - val_loss: 1.3441 - val_accuracy: 0.5310\n","Epoch 47/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.3028 - accuracy: 0.5832 - val_loss: 1.3358 - val_accuracy: 0.5310\n","Epoch 48/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.2935 - accuracy: 0.5827 - val_loss: 1.3282 - val_accuracy: 0.5310\n","Epoch 49/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.2840 - accuracy: 0.5840 - val_loss: 1.3234 - val_accuracy: 0.5258\n","Epoch 50/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.2748 - accuracy: 0.5827 - val_loss: 1.3133 - val_accuracy: 0.5310\n","Epoch 51/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.2656 - accuracy: 0.5894 - val_loss: 1.3099 - val_accuracy: 0.5227\n","Epoch 52/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2576 - accuracy: 0.5842 - val_loss: 1.2967 - val_accuracy: 0.5279\n","Epoch 53/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.2478 - accuracy: 0.5917 - val_loss: 1.2996 - val_accuracy: 0.5279\n","Epoch 54/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.2398 - accuracy: 0.5925 - val_loss: 1.2837 - val_accuracy: 0.5279\n","Epoch 55/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.2307 - accuracy: 0.5956 - val_loss: 1.2773 - val_accuracy: 0.5289\n","Epoch 56/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2225 - accuracy: 0.5956 - val_loss: 1.2650 - val_accuracy: 0.5341\n","Epoch 57/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2143 - accuracy: 0.5966 - val_loss: 1.2591 - val_accuracy: 0.5331\n","Epoch 58/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2060 - accuracy: 0.5969 - val_loss: 1.2599 - val_accuracy: 0.5269\n","Epoch 59/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.1978 - accuracy: 0.5948 - val_loss: 1.2458 - val_accuracy: 0.5310\n","Epoch 60/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.1906 - accuracy: 0.5933 - val_loss: 1.2406 - val_accuracy: 0.5300\n","Epoch 61/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.1831 - accuracy: 0.6039 - val_loss: 1.2445 - val_accuracy: 0.5310\n","Epoch 62/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.1755 - accuracy: 0.5969 - val_loss: 1.2358 - val_accuracy: 0.5269\n","Epoch 63/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.1666 - accuracy: 0.6034 - val_loss: 1.2233 - val_accuracy: 0.5300\n","Epoch 64/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.1591 - accuracy: 0.6003 - val_loss: 1.2192 - val_accuracy: 0.5289\n","Epoch 65/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.1524 - accuracy: 0.6067 - val_loss: 1.2120 - val_accuracy: 0.5258\n","Epoch 66/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.1449 - accuracy: 0.6111 - val_loss: 1.2073 - val_accuracy: 0.5320\n","Epoch 67/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1373 - accuracy: 0.6093 - val_loss: 1.1996 - val_accuracy: 0.5269\n","Epoch 68/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.1300 - accuracy: 0.6155 - val_loss: 1.1953 - val_accuracy: 0.5320\n","Epoch 69/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.1243 - accuracy: 0.6145 - val_loss: 1.1907 - val_accuracy: 0.5269\n","Epoch 70/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1161 - accuracy: 0.6176 - val_loss: 1.1877 - val_accuracy: 0.5310\n","Epoch 71/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.1102 - accuracy: 0.6134 - val_loss: 1.1804 - val_accuracy: 0.5320\n","Epoch 72/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1028 - accuracy: 0.6220 - val_loss: 1.1825 - val_accuracy: 0.5331\n","Epoch 73/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0970 - accuracy: 0.6158 - val_loss: 1.1757 - val_accuracy: 0.5310\n","Epoch 74/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0906 - accuracy: 0.6163 - val_loss: 1.1760 - val_accuracy: 0.5362\n","Epoch 75/100\n","31/31 [==============================] - 2s 55ms/step - loss: 1.0833 - accuracy: 0.6212 - val_loss: 1.1685 - val_accuracy: 0.5403\n","Epoch 76/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.0765 - accuracy: 0.6233 - val_loss: 1.1618 - val_accuracy: 0.5320\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0705 - accuracy: 0.6269 - val_loss: 1.1547 - val_accuracy: 0.5382\n","Epoch 78/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0642 - accuracy: 0.6295 - val_loss: 1.1483 - val_accuracy: 0.5279\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0575 - accuracy: 0.6258 - val_loss: 1.1489 - val_accuracy: 0.5269\n","Epoch 80/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.0519 - accuracy: 0.6289 - val_loss: 1.1444 - val_accuracy: 0.5362\n","Epoch 81/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0458 - accuracy: 0.6276 - val_loss: 1.1367 - val_accuracy: 0.5372\n","Epoch 82/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.0399 - accuracy: 0.6258 - val_loss: 1.1309 - val_accuracy: 0.5362\n","Epoch 83/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0353 - accuracy: 0.6245 - val_loss: 1.1373 - val_accuracy: 0.5310\n","Epoch 84/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0283 - accuracy: 0.6253 - val_loss: 1.1353 - val_accuracy: 0.5320\n","Epoch 85/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0243 - accuracy: 0.6253 - val_loss: 1.1237 - val_accuracy: 0.5300\n","Epoch 86/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0173 - accuracy: 0.6279 - val_loss: 1.1138 - val_accuracy: 0.5362\n","Epoch 87/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0110 - accuracy: 0.6326 - val_loss: 1.1201 - val_accuracy: 0.5372\n","Epoch 88/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0055 - accuracy: 0.6310 - val_loss: 1.1098 - val_accuracy: 0.5331\n","Epoch 89/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0007 - accuracy: 0.6351 - val_loss: 1.1074 - val_accuracy: 0.5403\n","Epoch 90/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.9945 - accuracy: 0.6359 - val_loss: 1.1068 - val_accuracy: 0.5310\n","Epoch 91/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.9895 - accuracy: 0.6331 - val_loss: 1.1049 - val_accuracy: 0.5413\n","Epoch 92/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.9864 - accuracy: 0.6336 - val_loss: 1.1010 - val_accuracy: 0.5393\n","Epoch 93/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9805 - accuracy: 0.6416 - val_loss: 1.1012 - val_accuracy: 0.5362\n","Epoch 94/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9745 - accuracy: 0.6388 - val_loss: 1.0929 - val_accuracy: 0.5351\n","Epoch 95/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9696 - accuracy: 0.6421 - val_loss: 1.0843 - val_accuracy: 0.5413\n","Epoch 96/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9649 - accuracy: 0.6416 - val_loss: 1.0813 - val_accuracy: 0.5424\n","Epoch 97/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9603 - accuracy: 0.6388 - val_loss: 1.0809 - val_accuracy: 0.5341\n","Epoch 98/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9559 - accuracy: 0.6437 - val_loss: 1.0922 - val_accuracy: 0.5382\n","Epoch 99/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9505 - accuracy: 0.6475 - val_loss: 1.0718 - val_accuracy: 0.5372\n","Epoch 100/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9475 - accuracy: 0.6463 - val_loss: 1.0896 - val_accuracy: 0.5351\n","{'loss': [1.9598362445831299, 1.9386948347091675, 1.9184941053390503, 1.8989529609680176, 1.8796623945236206, 1.860860824584961, 1.8423292636871338, 1.8240045309066772, 1.8060708045959473, 1.7883837223052979, 1.7710497379302979, 1.7539206743240356, 1.737133264541626, 1.7205910682678223, 1.7046282291412354, 1.688411831855774, 1.6728047132492065, 1.6570816040039062, 1.6418548822402954, 1.6268601417541504, 1.6123374700546265, 1.597718358039856, 1.583479642868042, 1.5691972970962524, 1.5556730031967163, 1.5418710708618164, 1.5284147262573242, 1.516116738319397, 1.5025354623794556, 1.4893579483032227, 1.4772008657455444, 1.464538812637329, 1.4524405002593994, 1.4403096437454224, 1.4293949604034424, 1.4173598289489746, 1.4061368703842163, 1.3954187631607056, 1.3838269710540771, 1.373215913772583, 1.3634480237960815, 1.3521932363510132, 1.3420926332473755, 1.3320465087890625, 1.3217717409133911, 1.3127741813659668, 1.3028227090835571, 1.2935245037078857, 1.283976435661316, 1.2748005390167236, 1.2656168937683105, 1.2575528621673584, 1.247847318649292, 1.2397745847702026, 1.2306926250457764, 1.2225217819213867, 1.2142667770385742, 1.2060444355010986, 1.197819709777832, 1.1906431913375854, 1.1830650568008423, 1.1754955053329468, 1.166562795639038, 1.1590949296951294, 1.1524038314819336, 1.1448928117752075, 1.137329339981079, 1.1300445795059204, 1.124295949935913, 1.1161164045333862, 1.1102046966552734, 1.1027683019638062, 1.0970443487167358, 1.090630054473877, 1.0832847356796265, 1.076467752456665, 1.0704717636108398, 1.0642410516738892, 1.057453989982605, 1.0518639087677002, 1.04575777053833, 1.0398732423782349, 1.0352782011032104, 1.028334140777588, 1.0242549180984497, 1.0172936916351318, 1.0110152959823608, 1.0055426359176636, 1.0007195472717285, 0.9944669008255005, 0.9894825220108032, 0.9863867163658142, 0.9805058240890503, 0.9745344519615173, 0.9696170687675476, 0.9649056792259216, 0.9602634906768799, 0.9559072852134705, 0.9505392909049988, 0.9474522471427917], 'accuracy': [0.4981912076473236, 0.5059431791305542, 0.5170542597770691, 0.5235142111778259, 0.5237725973129272, 0.5279069542884827, 0.5310077667236328, 0.5343669056892395, 0.5434108376502991, 0.5472868084907532, 0.5457364320755005, 0.5488371849060059, 0.5480620265007019, 0.5465116500854492, 0.5503876209259033, 0.5506460070610046, 0.5521963834762573, 0.5614987015724182, 0.5573643445968628, 0.552971601486206, 0.5625323057174683, 0.5602067112922668, 0.5643410682678223, 0.5607235431671143, 0.5656330585479736, 0.5630490779876709, 0.5645994544029236, 0.565891444683075, 0.5612403154373169, 0.5677002668380737, 0.5645994544029236, 0.5695090293884277, 0.5682170391082764, 0.5677002668380737, 0.5702842473983765, 0.567958652973175, 0.5723513960838318, 0.5731266140937805, 0.5728682279586792, 0.5736433863639832, 0.5733850002288818, 0.5772609710693359, 0.5808785557746887, 0.5749353766441345, 0.5819121599197388, 0.5824289321899414, 0.5832041501998901, 0.5826873183250427, 0.5839793086051941, 0.5826873183250427, 0.5894056558609009, 0.5842377543449402, 0.5917312502861023, 0.592506468296051, 0.5956072211265564, 0.5956072211265564, 0.5966408252716064, 0.5968992114067078, 0.5948320627212524, 0.593281626701355, 0.603875994682312, 0.5968992114067078, 0.6033591628074646, 0.6002584099769592, 0.6067183613777161, 0.6111111044883728, 0.6093023419380188, 0.6155038475990295, 0.6144703030586243, 0.6175710558891296, 0.6134366989135742, 0.6219637989997864, 0.6157622933387756, 0.6162790656089783, 0.6211886405944824, 0.6232557892799377, 0.6268733739852905, 0.6294573545455933, 0.6258397698402405, 0.6289405822753906, 0.6276485919952393, 0.6258397698402405, 0.6245477795600891, 0.6253229975700378, 0.6253229975700378, 0.6279069781303406, 0.6325581669807434, 0.631007730960846, 0.6351421475410461, 0.6359173059463501, 0.633074939250946, 0.6335917115211487, 0.6416020393371582, 0.6387596726417542, 0.6421188712120056, 0.6416020393371582, 0.6387596726417542, 0.6436692476272583, 0.6475452184677124, 0.646253228187561], 'val_loss': [1.9504655599594116, 1.9308284521102905, 1.911478042602539, 1.8924845457077026, 1.873861312866211, 1.8555251359939575, 1.8374741077423096, 1.8197134733200073, 1.8022507429122925, 1.785104513168335, 1.768237829208374, 1.7516459226608276, 1.7352982759475708, 1.719252347946167, 1.703574299812317, 1.688140869140625, 1.6730135679244995, 1.6582398414611816, 1.643764853477478, 1.6292554140090942, 1.615739107131958, 1.6021393537521362, 1.5897419452667236, 1.5755141973495483, 1.5628728866577148, 1.5498517751693726, 1.5367646217346191, 1.5255320072174072, 1.5145888328552246, 1.5021730661392212, 1.4913595914840698, 1.4811933040618896, 1.4688189029693604, 1.461958885192871, 1.4485313892364502, 1.4389431476593018, 1.4269832372665405, 1.4223341941833496, 1.4069968461990356, 1.3951659202575684, 1.3904283046722412, 1.3781791925430298, 1.3716691732406616, 1.3638737201690674, 1.354892611503601, 1.344100832939148, 1.335842490196228, 1.3281865119934082, 1.3233891725540161, 1.3133282661437988, 1.3098868131637573, 1.2967305183410645, 1.2996221780776978, 1.2837406396865845, 1.277336597442627, 1.2650115489959717, 1.2591259479522705, 1.2598613500595093, 1.2457836866378784, 1.2406058311462402, 1.2444617748260498, 1.2358418703079224, 1.2233145236968994, 1.2191660404205322, 1.2120472192764282, 1.2072727680206299, 1.1995824575424194, 1.1953054666519165, 1.1906588077545166, 1.1876955032348633, 1.1804327964782715, 1.1825484037399292, 1.1757346391677856, 1.1759928464889526, 1.1685441732406616, 1.1618330478668213, 1.1547479629516602, 1.1483349800109863, 1.1488865613937378, 1.1444087028503418, 1.1367360353469849, 1.130939245223999, 1.1372809410095215, 1.1352719068527222, 1.1237353086471558, 1.1138408184051514, 1.1201002597808838, 1.1097733974456787, 1.1074168682098389, 1.106785535812378, 1.1048603057861328, 1.1009624004364014, 1.1012446880340576, 1.0928798913955688, 1.0842735767364502, 1.081324577331543, 1.080902338027954, 1.0921756029129028, 1.0718340873718262, 1.0895930528640747], 'val_accuracy': [0.48553720116615295, 0.5247933864593506, 0.5216942429542542, 0.5237603187561035, 0.51962810754776, 0.51962810754776, 0.5216942429542542, 0.51962810754776, 0.5237603187561035, 0.5206611752510071, 0.5227272510528564, 0.5227272510528564, 0.5206611752510071, 0.5258264541625977, 0.5247933864593506, 0.5237603187561035, 0.5247933864593506, 0.5268595218658447, 0.5247933864593506, 0.5247933864593506, 0.5237603187561035, 0.5247933864593506, 0.5330578684806824, 0.5320248007774353, 0.5268595218658447, 0.5351239442825317, 0.5351239442825317, 0.5258264541625977, 0.5309917330741882, 0.5299586653709412, 0.5289255976676941, 0.5351239442825317, 0.5299586653709412, 0.5258264541625977, 0.5278925895690918, 0.5320248007774353, 0.5340909361839294, 0.5289255976676941, 0.5371900796890259, 0.5309917330741882, 0.5361570119857788, 0.5361570119857788, 0.5340909361839294, 0.5330578684806824, 0.5237603187561035, 0.5309917330741882, 0.5309917330741882, 0.5309917330741882, 0.5258264541625977, 0.5309917330741882, 0.5227272510528564, 0.5278925895690918, 0.5278925895690918, 0.5278925895690918, 0.5289255976676941, 0.5340909361839294, 0.5330578684806824, 0.5268595218658447, 0.5309917330741882, 0.5299586653709412, 0.5309917330741882, 0.5268595218658447, 0.5299586653709412, 0.5289255976676941, 0.5258264541625977, 0.5320248007774353, 0.5268595218658447, 0.5320248007774353, 0.5268595218658447, 0.5309917330741882, 0.5320248007774353, 0.5330578684806824, 0.5309917330741882, 0.5361570119857788, 0.5402892827987671, 0.5320248007774353, 0.538223147392273, 0.5278925895690918, 0.5268595218658447, 0.5361570119857788, 0.5371900796890259, 0.5361570119857788, 0.5309917330741882, 0.5320248007774353, 0.5299586653709412, 0.5361570119857788, 0.5371900796890259, 0.5330578684806824, 0.5402892827987671, 0.5309917330741882, 0.5413222908973694, 0.53925621509552, 0.5361570119857788, 0.5351239442825317, 0.5413222908973694, 0.5423553586006165, 0.5340909361839294, 0.538223147392273, 0.5371900796890259, 0.5351239442825317]}\n","32/32 [==============================] - 1s 11ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 4s 33ms/step - loss: 0.9998 - accuracy: 0.6150 - val_loss: 1.0451 - val_accuracy: 0.5183\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 0s 13ms/step - loss: 0.9883 - accuracy: 0.6191 - val_loss: 1.0415 - val_accuracy: 0.5183\n","Epoch 3/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9825 - accuracy: 0.6215 - val_loss: 1.0373 - val_accuracy: 0.5183\n","Epoch 4/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9766 - accuracy: 0.6277 - val_loss: 1.0332 - val_accuracy: 0.5194\n","Epoch 5/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9720 - accuracy: 0.6226 - val_loss: 1.0280 - val_accuracy: 0.5291\n","Epoch 6/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9669 - accuracy: 0.6245 - val_loss: 1.0240 - val_accuracy: 0.5377\n","Epoch 7/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.9618 - accuracy: 0.6226 - val_loss: 1.0219 - val_accuracy: 0.5248\n","Epoch 8/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9579 - accuracy: 0.6307 - val_loss: 1.0163 - val_accuracy: 0.5474\n","Epoch 9/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9528 - accuracy: 0.6293 - val_loss: 1.0128 - val_accuracy: 0.5463\n","Epoch 10/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9480 - accuracy: 0.6290 - val_loss: 1.0080 - val_accuracy: 0.5506\n","Epoch 11/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9433 - accuracy: 0.6360 - val_loss: 1.0027 - val_accuracy: 0.5550\n","Epoch 12/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9390 - accuracy: 0.6344 - val_loss: 0.9996 - val_accuracy: 0.5593\n","Epoch 13/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.9345 - accuracy: 0.6350 - val_loss: 0.9956 - val_accuracy: 0.5679\n","Epoch 14/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.9310 - accuracy: 0.6350 - val_loss: 0.9903 - val_accuracy: 0.5700\n","Epoch 15/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.9260 - accuracy: 0.6425 - val_loss: 0.9854 - val_accuracy: 0.5744\n","Epoch 16/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.9225 - accuracy: 0.6350 - val_loss: 0.9817 - val_accuracy: 0.5776\n","Epoch 17/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.9184 - accuracy: 0.6377 - val_loss: 0.9771 - val_accuracy: 0.5830\n","Epoch 18/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.9137 - accuracy: 0.6404 - val_loss: 0.9797 - val_accuracy: 0.5711\n","Epoch 19/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9110 - accuracy: 0.6433 - val_loss: 0.9736 - val_accuracy: 0.5787\n","Epoch 20/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9064 - accuracy: 0.6476 - val_loss: 0.9718 - val_accuracy: 0.5733\n","Epoch 21/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.9028 - accuracy: 0.6517 - val_loss: 0.9767 - val_accuracy: 0.5797\n","Epoch 22/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.8982 - accuracy: 0.6463 - val_loss: 0.9734 - val_accuracy: 0.5884\n","Epoch 23/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8947 - accuracy: 0.6479 - val_loss: 0.9621 - val_accuracy: 0.5830\n","Epoch 24/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.8904 - accuracy: 0.6552 - val_loss: 0.9634 - val_accuracy: 0.5948\n","Epoch 25/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8867 - accuracy: 0.6511 - val_loss: 0.9712 - val_accuracy: 0.5787\n","Epoch 26/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8834 - accuracy: 0.6471 - val_loss: 0.9767 - val_accuracy: 0.5873\n","Epoch 27/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8814 - accuracy: 0.6530 - val_loss: 0.9626 - val_accuracy: 0.5894\n","Epoch 28/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8790 - accuracy: 0.6506 - val_loss: 0.9766 - val_accuracy: 0.5797\n","Epoch 29/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8730 - accuracy: 0.6552 - val_loss: 0.9714 - val_accuracy: 0.5862\n","Epoch 30/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8689 - accuracy: 0.6630 - val_loss: 0.9698 - val_accuracy: 0.5905\n","Epoch 31/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8648 - accuracy: 0.6657 - val_loss: 0.9654 - val_accuracy: 0.5970\n","Epoch 32/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8622 - accuracy: 0.6684 - val_loss: 0.9650 - val_accuracy: 0.5927\n","Epoch 33/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8599 - accuracy: 0.6646 - val_loss: 0.9699 - val_accuracy: 0.5884\n","Epoch 34/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8560 - accuracy: 0.6678 - val_loss: 0.9643 - val_accuracy: 0.5927\n","Epoch 35/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8526 - accuracy: 0.6635 - val_loss: 0.9676 - val_accuracy: 0.5830\n","Epoch 36/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8491 - accuracy: 0.6662 - val_loss: 0.9702 - val_accuracy: 0.5841\n","Epoch 37/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8456 - accuracy: 0.6751 - val_loss: 0.9741 - val_accuracy: 0.5884\n","Epoch 38/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8447 - accuracy: 0.6697 - val_loss: 0.9813 - val_accuracy: 0.5938\n","Epoch 39/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8407 - accuracy: 0.6697 - val_loss: 0.9733 - val_accuracy: 0.5905\n","Epoch 40/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8417 - accuracy: 0.6686 - val_loss: 0.9816 - val_accuracy: 0.5905\n","Epoch 41/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8463 - accuracy: 0.6595 - val_loss: 0.9842 - val_accuracy: 0.5787\n","Epoch 42/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8326 - accuracy: 0.6751 - val_loss: 0.9662 - val_accuracy: 0.5657\n","Epoch 43/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8262 - accuracy: 0.6840 - val_loss: 0.9703 - val_accuracy: 0.5927\n","Epoch 44/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8233 - accuracy: 0.6840 - val_loss: 0.9623 - val_accuracy: 0.5873\n","Epoch 45/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8233 - accuracy: 0.6756 - val_loss: 0.9651 - val_accuracy: 0.5830\n","Epoch 46/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8218 - accuracy: 0.6816 - val_loss: 0.9690 - val_accuracy: 0.5927\n","Epoch 47/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8163 - accuracy: 0.6816 - val_loss: 0.9695 - val_accuracy: 0.5927\n","Epoch 48/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8172 - accuracy: 0.6770 - val_loss: 0.9836 - val_accuracy: 0.5808\n","Epoch 49/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8132 - accuracy: 0.6773 - val_loss: 0.9716 - val_accuracy: 0.5948\n","Epoch 50/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8078 - accuracy: 0.6837 - val_loss: 0.9653 - val_accuracy: 0.5894\n","Epoch 51/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.8040 - accuracy: 0.6880 - val_loss: 0.9691 - val_accuracy: 0.6034\n","Epoch 52/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8005 - accuracy: 0.6910 - val_loss: 0.9759 - val_accuracy: 0.5981\n","Epoch 53/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7989 - accuracy: 0.6859 - val_loss: 0.9663 - val_accuracy: 0.5657\n","Epoch 54/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7971 - accuracy: 0.6891 - val_loss: 0.9840 - val_accuracy: 0.5894\n","Epoch 55/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7968 - accuracy: 0.6880 - val_loss: 1.0098 - val_accuracy: 0.5916\n","Epoch 56/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7947 - accuracy: 0.6840 - val_loss: 0.9859 - val_accuracy: 0.5797\n","Epoch 57/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.7886 - accuracy: 0.6923 - val_loss: 0.9756 - val_accuracy: 0.6099\n","Epoch 58/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7843 - accuracy: 0.7007 - val_loss: 0.9714 - val_accuracy: 0.5959\n","Epoch 59/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7827 - accuracy: 0.7018 - val_loss: 0.9688 - val_accuracy: 0.5959\n","Epoch 60/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7797 - accuracy: 0.7007 - val_loss: 0.9775 - val_accuracy: 0.5894\n","Epoch 61/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7800 - accuracy: 0.6945 - val_loss: 0.9997 - val_accuracy: 0.5894\n","Epoch 62/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7747 - accuracy: 0.6994 - val_loss: 0.9615 - val_accuracy: 0.5948\n","Epoch 63/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7716 - accuracy: 0.7050 - val_loss: 0.9959 - val_accuracy: 0.5894\n","Epoch 64/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7692 - accuracy: 0.7020 - val_loss: 0.9715 - val_accuracy: 0.5830\n","Epoch 65/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7681 - accuracy: 0.7026 - val_loss: 0.9827 - val_accuracy: 0.5873\n","Epoch 66/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7651 - accuracy: 0.7045 - val_loss: 0.9734 - val_accuracy: 0.6024\n","Epoch 67/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7622 - accuracy: 0.7123 - val_loss: 0.9852 - val_accuracy: 0.5905\n","Epoch 68/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7600 - accuracy: 0.7134 - val_loss: 0.9873 - val_accuracy: 0.5916\n","Epoch 69/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7572 - accuracy: 0.7066 - val_loss: 0.9807 - val_accuracy: 0.5894\n","Epoch 70/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7527 - accuracy: 0.7104 - val_loss: 0.9957 - val_accuracy: 0.5970\n","Epoch 71/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7533 - accuracy: 0.7074 - val_loss: 1.0069 - val_accuracy: 0.5830\n","Epoch 72/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7550 - accuracy: 0.7042 - val_loss: 0.9940 - val_accuracy: 0.5830\n","Epoch 73/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7453 - accuracy: 0.7091 - val_loss: 1.0080 - val_accuracy: 0.5830\n","Epoch 74/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7461 - accuracy: 0.7169 - val_loss: 1.0044 - val_accuracy: 0.6002\n","Epoch 75/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7443 - accuracy: 0.7093 - val_loss: 0.9819 - val_accuracy: 0.5744\n","Epoch 76/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7394 - accuracy: 0.7188 - val_loss: 0.9819 - val_accuracy: 0.5894\n","Epoch 77/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7405 - accuracy: 0.7099 - val_loss: 0.9870 - val_accuracy: 0.5787\n","Epoch 78/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7348 - accuracy: 0.7225 - val_loss: 1.0138 - val_accuracy: 0.5754\n","Epoch 79/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7330 - accuracy: 0.7247 - val_loss: 1.0282 - val_accuracy: 0.5938\n","Epoch 80/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7277 - accuracy: 0.7276 - val_loss: 0.9795 - val_accuracy: 0.5916\n","Epoch 81/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7296 - accuracy: 0.7263 - val_loss: 1.0125 - val_accuracy: 0.5948\n","Epoch 82/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7279 - accuracy: 0.7241 - val_loss: 1.0691 - val_accuracy: 0.5927\n","Epoch 83/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7288 - accuracy: 0.7276 - val_loss: 1.0087 - val_accuracy: 0.5873\n","Epoch 84/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7172 - accuracy: 0.7309 - val_loss: 0.9973 - val_accuracy: 0.5905\n","Epoch 85/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7150 - accuracy: 0.7320 - val_loss: 0.9991 - val_accuracy: 0.5841\n","Epoch 86/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7139 - accuracy: 0.7390 - val_loss: 1.0092 - val_accuracy: 0.5873\n","Epoch 87/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7101 - accuracy: 0.7287 - val_loss: 0.9840 - val_accuracy: 0.5884\n","Epoch 88/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.7089 - accuracy: 0.7338 - val_loss: 1.0130 - val_accuracy: 0.5927\n","Epoch 89/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7089 - accuracy: 0.7320 - val_loss: 1.0875 - val_accuracy: 0.5754\n","Epoch 90/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7081 - accuracy: 0.7360 - val_loss: 1.0845 - val_accuracy: 0.5905\n","Epoch 91/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7082 - accuracy: 0.7276 - val_loss: 1.0158 - val_accuracy: 0.5959\n","Epoch 92/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7056 - accuracy: 0.7373 - val_loss: 1.0130 - val_accuracy: 0.5927\n","Epoch 93/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7045 - accuracy: 0.7330 - val_loss: 1.0328 - val_accuracy: 0.5873\n","Epoch 94/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6983 - accuracy: 0.7346 - val_loss: 1.0059 - val_accuracy: 0.5970\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6974 - accuracy: 0.7363 - val_loss: 1.0389 - val_accuracy: 0.5959\n","Epoch 96/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6887 - accuracy: 0.7446 - val_loss: 1.0087 - val_accuracy: 0.5970\n","Epoch 97/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6851 - accuracy: 0.7487 - val_loss: 1.0770 - val_accuracy: 0.5873\n","Epoch 98/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6881 - accuracy: 0.7433 - val_loss: 0.9947 - val_accuracy: 0.6045\n","Epoch 99/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6854 - accuracy: 0.7468 - val_loss: 1.0327 - val_accuracy: 0.5776\n","Epoch 100/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6800 - accuracy: 0.7519 - val_loss: 1.0228 - val_accuracy: 0.5884\n","{'loss': [0.9998334050178528, 0.9883329272270203, 0.9825135469436646, 0.976600170135498, 0.9719923734664917, 0.966949999332428, 0.9617670774459839, 0.9578537344932556, 0.9528278708457947, 0.9479746222496033, 0.9432640671730042, 0.9390162825584412, 0.9345413446426392, 0.9309672713279724, 0.9259568452835083, 0.9224841594696045, 0.9183562994003296, 0.9136790037155151, 0.911048173904419, 0.9063803553581238, 0.9027695655822754, 0.8981545567512512, 0.8947069048881531, 0.890388011932373, 0.8866790533065796, 0.8834018111228943, 0.8814268708229065, 0.8789626359939575, 0.8730422258377075, 0.8689031004905701, 0.8648480772972107, 0.862237274646759, 0.8598659038543701, 0.8559811115264893, 0.8526284694671631, 0.8491163849830627, 0.8456140756607056, 0.8446943759918213, 0.8407042026519775, 0.841712474822998, 0.8462931513786316, 0.8326375484466553, 0.8261657357215881, 0.8232848644256592, 0.8233144879341125, 0.8217834234237671, 0.816295862197876, 0.8172094225883484, 0.8132497072219849, 0.80781489610672, 0.8040383458137512, 0.8004747629165649, 0.7989102602005005, 0.7971253991127014, 0.7967553734779358, 0.7947192788124084, 0.7885850071907043, 0.7843394875526428, 0.7826735377311707, 0.779670238494873, 0.7799953818321228, 0.7747274041175842, 0.7715619206428528, 0.7691579461097717, 0.7681100368499756, 0.765149712562561, 0.7622289061546326, 0.7599847912788391, 0.7572148442268372, 0.7526728510856628, 0.7532585263252258, 0.7550221681594849, 0.7453204393386841, 0.7460951805114746, 0.7442749738693237, 0.7393810749053955, 0.7405267953872681, 0.7347976565361023, 0.7330211997032166, 0.7276846766471863, 0.7296290993690491, 0.7279452681541443, 0.7288323044776917, 0.7172032594680786, 0.7150033712387085, 0.7139487266540527, 0.7100644707679749, 0.7088966369628906, 0.7089481949806213, 0.7080608010292053, 0.708206832408905, 0.7055831551551819, 0.7045401930809021, 0.6982836723327637, 0.697373628616333, 0.688713014125824, 0.6851295232772827, 0.6880829334259033, 0.6854407787322998, 0.6800183057785034], 'accuracy': [0.6150323152542114, 0.6190732717514038, 0.6214978694915771, 0.6276939511299133, 0.6225754022598267, 0.6244612336158752, 0.6225754022598267, 0.6306573152542114, 0.6293103694915771, 0.6290409564971924, 0.6360452771186829, 0.634428858757019, 0.6349676847457886, 0.6349676847457886, 0.6425107717514038, 0.6349676847457886, 0.6376616358757019, 0.6403555870056152, 0.6433189511299133, 0.6476293206214905, 0.6516702771186829, 0.6462823152542114, 0.6478987336158752, 0.6551724076271057, 0.6511314511299133, 0.647090494632721, 0.6530172228813171, 0.6505926847457886, 0.6551724076271057, 0.6629849076271057, 0.665678858757019, 0.6683728694915771, 0.6646012663841248, 0.6678340435028076, 0.6635237336158752, 0.6662176847457886, 0.6751077771186829, 0.6697198152542114, 0.6697198152542114, 0.6686422228813171, 0.6594827771186829, 0.6751077771186829, 0.6839978694915771, 0.6839978694915771, 0.6756465435028076, 0.6815732717514038, 0.6815732717514038, 0.6769935488700867, 0.6772629022598267, 0.6837284564971924, 0.6880387663841248, 0.6910021305084229, 0.685883641242981, 0.689116358757019, 0.6880387663841248, 0.6839978694915771, 0.6923491358757019, 0.7007004022598267, 0.701777994632721, 0.7007004022598267, 0.6945043206214905, 0.6993534564971924, 0.7050107717514038, 0.7020474076271057, 0.7025862336158752, 0.704472005367279, 0.712284505367279, 0.7133620977401733, 0.7066271305084229, 0.7103987336158752, 0.7074353694915771, 0.7042025923728943, 0.7090517282485962, 0.7168642282485962, 0.709321141242981, 0.71875, 0.7098599076271057, 0.7225215435028076, 0.7246767282485962, 0.7276400923728943, 0.7262930870056152, 0.7241379022598267, 0.7276400923728943, 0.7308728694915771, 0.7319504022598267, 0.7389547228813171, 0.7287176847457886, 0.7338362336158752, 0.7319504022598267, 0.735991358757019, 0.7276400923728943, 0.7373383641242981, 0.733027994632721, 0.7346444129943848, 0.7362607717514038, 0.7446120977401733, 0.748652994632721, 0.7432650923728943, 0.7467672228813171, 0.7518857717514038], 'val_loss': [1.0451421737670898, 1.0415089130401611, 1.0373378992080688, 1.0331743955612183, 1.0280063152313232, 1.0239657163619995, 1.021895408630371, 1.0162951946258545, 1.0127679109573364, 1.007997989654541, 1.0027239322662354, 0.9996079206466675, 0.9956179261207581, 0.9902599453926086, 0.985372006893158, 0.981741726398468, 0.9770545363426208, 0.979712963104248, 0.9736039638519287, 0.9717733860015869, 0.9766684174537659, 0.9733619689941406, 0.962064802646637, 0.9634026288986206, 0.9711753726005554, 0.9766932725906372, 0.9626438617706299, 0.9765534400939941, 0.9713630080223083, 0.9698272347450256, 0.965420126914978, 0.9650266170501709, 0.9699205160140991, 0.9643135666847229, 0.967618465423584, 0.9702056646347046, 0.9741207361221313, 0.9812602400779724, 0.9733279943466187, 0.9815530180931091, 0.984197199344635, 0.9662248492240906, 0.9702763557434082, 0.9623083472251892, 0.9650963544845581, 0.9689593315124512, 0.9694735407829285, 0.9835674166679382, 0.971583902835846, 0.9653286337852478, 0.969116747379303, 0.9759393334388733, 0.9663075804710388, 0.9839870929718018, 1.009826421737671, 0.9859447479248047, 0.9755978584289551, 0.9714166522026062, 0.9688087105751038, 0.977475643157959, 0.9997088313102722, 0.9615041613578796, 0.9958671927452087, 0.9715314507484436, 0.9826678037643433, 0.9734350442886353, 0.9851725101470947, 0.9872530698776245, 0.9807202816009521, 0.9956766366958618, 1.0069068670272827, 0.993955135345459, 1.0080347061157227, 1.0044342279434204, 0.9819362163543701, 0.9818779230117798, 0.9869518280029297, 1.0137817859649658, 1.028229832649231, 0.9795116186141968, 1.0124969482421875, 1.069140911102295, 1.008713960647583, 0.9972915649414062, 0.9991112351417542, 1.0091700553894043, 0.9840474128723145, 1.012976884841919, 1.0874577760696411, 1.0845192670822144, 1.0158343315124512, 1.0130012035369873, 1.032810091972351, 1.005900263786316, 1.0388660430908203, 1.0086697340011597, 1.0769731998443604, 0.9946938753128052, 1.0327122211456299, 1.0228022336959839], 'val_accuracy': [0.5183189511299133, 0.5183189511299133, 0.5183189511299133, 0.5193965435028076, 0.5290948152542114, 0.537715494632721, 0.524784505367279, 0.5474137663841248, 0.5463362336158752, 0.5506465435028076, 0.5549569129943848, 0.5592672228813171, 0.5678879022598267, 0.5700430870056152, 0.5743534564971924, 0.5775862336158752, 0.5829741358757019, 0.5711206793785095, 0.5786637663841248, 0.5732758641242981, 0.579741358757019, 0.5883620977401733, 0.5829741358757019, 0.5948275923728943, 0.5786637663841248, 0.587284505367279, 0.5894396305084229, 0.579741358757019, 0.5862069129943848, 0.5905172228813171, 0.5969827771186829, 0.5926724076271057, 0.5883620977401733, 0.5926724076271057, 0.5829741358757019, 0.5840517282485962, 0.5883620977401733, 0.59375, 0.5905172228813171, 0.5905172228813171, 0.5786637663841248, 0.5657327771186829, 0.5926724076271057, 0.587284505367279, 0.5829741358757019, 0.5926724076271057, 0.5926724076271057, 0.5808189511299133, 0.5948275923728943, 0.5894396305084229, 0.6034482717514038, 0.5980603694915771, 0.5657327771186829, 0.5894396305084229, 0.5915948152542114, 0.579741358757019, 0.6099137663841248, 0.5959051847457886, 0.5959051847457886, 0.5894396305084229, 0.5894396305084229, 0.5948275923728943, 0.5894396305084229, 0.5829741358757019, 0.587284505367279, 0.6023706793785095, 0.5905172228813171, 0.5915948152542114, 0.5894396305084229, 0.5969827771186829, 0.5829741358757019, 0.5829741358757019, 0.5829741358757019, 0.600215494632721, 0.5743534564971924, 0.5894396305084229, 0.5786637663841248, 0.5754310488700867, 0.59375, 0.5915948152542114, 0.5948275923728943, 0.5926724076271057, 0.587284505367279, 0.5905172228813171, 0.5840517282485962, 0.587284505367279, 0.5883620977401733, 0.5926724076271057, 0.5754310488700867, 0.5905172228813171, 0.5959051847457886, 0.5926724076271057, 0.587284505367279, 0.5969827771186829, 0.5959051847457886, 0.5969827771186829, 0.587284505367279, 0.6045258641242981, 0.5775862336158752, 0.5883620977401733]}\n","38/38 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","27/28 [===========================>..] - ETA: 0s - loss: 0.9952 - accuracy: 0.6262"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 5s 46ms/step - loss: 0.9948 - accuracy: 0.6262 - val_loss: 1.0472 - val_accuracy: 0.5057\n","Epoch 2/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.9862 - accuracy: 0.6220 - val_loss: 1.0430 - val_accuracy: 0.5057\n","Epoch 3/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9801 - accuracy: 0.6203 - val_loss: 1.0389 - val_accuracy: 0.5057\n","Epoch 4/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9757 - accuracy: 0.6225 - val_loss: 1.0346 - val_accuracy: 0.5057\n","Epoch 5/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9707 - accuracy: 0.6273 - val_loss: 1.0318 - val_accuracy: 0.5045\n","Epoch 6/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.9673 - accuracy: 0.6251 - val_loss: 1.0276 - val_accuracy: 0.5068\n","Epoch 7/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.9604 - accuracy: 0.6338 - val_loss: 1.0217 - val_accuracy: 0.5136\n","Epoch 8/100\n","28/28 [==============================] - 1s 34ms/step - loss: 0.9555 - accuracy: 0.6299 - val_loss: 1.0157 - val_accuracy: 0.5249\n","Epoch 9/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9506 - accuracy: 0.6321 - val_loss: 1.0134 - val_accuracy: 0.5215\n","Epoch 10/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.9470 - accuracy: 0.6341 - val_loss: 1.0064 - val_accuracy: 0.5407\n","Epoch 11/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9425 - accuracy: 0.6378 - val_loss: 1.0039 - val_accuracy: 0.5317\n","Epoch 12/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9381 - accuracy: 0.6350 - val_loss: 1.0013 - val_accuracy: 0.5317\n","Epoch 13/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.9337 - accuracy: 0.6355 - val_loss: 0.9951 - val_accuracy: 0.5464\n","Epoch 14/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.9296 - accuracy: 0.6392 - val_loss: 0.9882 - val_accuracy: 0.5600\n","Epoch 15/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9265 - accuracy: 0.6384 - val_loss: 0.9844 - val_accuracy: 0.5543\n","Epoch 16/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9225 - accuracy: 0.6387 - val_loss: 0.9787 - val_accuracy: 0.5588\n","Epoch 17/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.9176 - accuracy: 0.6426 - val_loss: 0.9761 - val_accuracy: 0.5656\n","Epoch 18/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.9143 - accuracy: 0.6528 - val_loss: 0.9704 - val_accuracy: 0.5735\n","Epoch 19/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9102 - accuracy: 0.6435 - val_loss: 0.9663 - val_accuracy: 0.5713\n","Epoch 20/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.9066 - accuracy: 0.6443 - val_loss: 0.9652 - val_accuracy: 0.5758\n","Epoch 21/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.9028 - accuracy: 0.6415 - val_loss: 0.9574 - val_accuracy: 0.5826\n","Epoch 22/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8987 - accuracy: 0.6477 - val_loss: 0.9534 - val_accuracy: 0.5792\n","Epoch 23/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8945 - accuracy: 0.6449 - val_loss: 0.9485 - val_accuracy: 0.5826\n","Epoch 24/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8911 - accuracy: 0.6638 - val_loss: 0.9482 - val_accuracy: 0.5758\n","Epoch 25/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.8892 - accuracy: 0.6520 - val_loss: 0.9533 - val_accuracy: 0.6029\n","Epoch 26/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8853 - accuracy: 0.6576 - val_loss: 0.9454 - val_accuracy: 0.5860\n","Epoch 27/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8828 - accuracy: 0.6508 - val_loss: 0.9453 - val_accuracy: 0.5826\n","Epoch 28/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8805 - accuracy: 0.6573 - val_loss: 0.9425 - val_accuracy: 0.5939\n","Epoch 29/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8763 - accuracy: 0.6636 - val_loss: 0.9405 - val_accuracy: 0.5871\n","Epoch 30/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.8707 - accuracy: 0.6613 - val_loss: 0.9481 - val_accuracy: 0.5814\n","Epoch 31/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8707 - accuracy: 0.6599 - val_loss: 0.9482 - val_accuracy: 0.5894\n","Epoch 32/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8673 - accuracy: 0.6661 - val_loss: 0.9392 - val_accuracy: 0.6007\n","Epoch 33/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8616 - accuracy: 0.6678 - val_loss: 0.9364 - val_accuracy: 0.5962\n","Epoch 34/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8589 - accuracy: 0.6678 - val_loss: 0.9389 - val_accuracy: 0.5826\n","Epoch 35/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8548 - accuracy: 0.6706 - val_loss: 0.9347 - val_accuracy: 0.5894\n","Epoch 36/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8529 - accuracy: 0.6658 - val_loss: 0.9370 - val_accuracy: 0.5894\n","Epoch 37/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8504 - accuracy: 0.6692 - val_loss: 0.9367 - val_accuracy: 0.5950\n","Epoch 38/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8465 - accuracy: 0.6797 - val_loss: 0.9337 - val_accuracy: 0.5894\n","Epoch 39/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8444 - accuracy: 0.6718 - val_loss: 0.9341 - val_accuracy: 0.5962\n","Epoch 40/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8393 - accuracy: 0.6737 - val_loss: 0.9337 - val_accuracy: 0.5871\n","Epoch 41/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.8380 - accuracy: 0.6777 - val_loss: 0.9338 - val_accuracy: 0.6052\n","Epoch 42/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8367 - accuracy: 0.6706 - val_loss: 0.9301 - val_accuracy: 0.5962\n","Epoch 43/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8340 - accuracy: 0.6746 - val_loss: 0.9342 - val_accuracy: 0.5939\n","Epoch 44/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8336 - accuracy: 0.6698 - val_loss: 0.9433 - val_accuracy: 0.6007\n","Epoch 45/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8300 - accuracy: 0.6794 - val_loss: 0.9370 - val_accuracy: 0.6052\n","Epoch 46/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8247 - accuracy: 0.6723 - val_loss: 0.9303 - val_accuracy: 0.5894\n","Epoch 47/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8197 - accuracy: 0.6757 - val_loss: 0.9328 - val_accuracy: 0.5916\n","Epoch 48/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8161 - accuracy: 0.6896 - val_loss: 0.9390 - val_accuracy: 0.5882\n","Epoch 49/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8149 - accuracy: 0.6834 - val_loss: 0.9473 - val_accuracy: 0.5758\n","Epoch 50/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8160 - accuracy: 0.6856 - val_loss: 0.9315 - val_accuracy: 0.5905\n","Epoch 51/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8094 - accuracy: 0.6893 - val_loss: 0.9398 - val_accuracy: 0.6007\n","Epoch 52/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8079 - accuracy: 0.6757 - val_loss: 0.9318 - val_accuracy: 0.6052\n","Epoch 53/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8049 - accuracy: 0.6902 - val_loss: 0.9358 - val_accuracy: 0.5826\n","Epoch 54/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7991 - accuracy: 0.6955 - val_loss: 0.9397 - val_accuracy: 0.5860\n","Epoch 55/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.7974 - accuracy: 0.6984 - val_loss: 0.9394 - val_accuracy: 0.5973\n","Epoch 56/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7993 - accuracy: 0.6921 - val_loss: 0.9606 - val_accuracy: 0.5837\n","Epoch 57/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7965 - accuracy: 0.6913 - val_loss: 0.9451 - val_accuracy: 0.5894\n","Epoch 58/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7916 - accuracy: 0.6930 - val_loss: 0.9455 - val_accuracy: 0.5871\n","Epoch 59/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.7894 - accuracy: 0.6893 - val_loss: 0.9363 - val_accuracy: 0.5882\n","Epoch 60/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7860 - accuracy: 0.6961 - val_loss: 0.9486 - val_accuracy: 0.6029\n","Epoch 61/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7847 - accuracy: 0.7012 - val_loss: 0.9467 - val_accuracy: 0.5962\n","Epoch 62/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.7789 - accuracy: 0.6958 - val_loss: 0.9472 - val_accuracy: 0.6018\n","Epoch 63/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7783 - accuracy: 0.7009 - val_loss: 0.9451 - val_accuracy: 0.5871\n","Epoch 64/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7774 - accuracy: 0.7074 - val_loss: 0.9621 - val_accuracy: 0.5814\n","Epoch 65/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7747 - accuracy: 0.7043 - val_loss: 0.9426 - val_accuracy: 0.5916\n","Epoch 66/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7704 - accuracy: 0.7032 - val_loss: 0.9537 - val_accuracy: 0.5894\n","Epoch 67/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7669 - accuracy: 0.7032 - val_loss: 0.9453 - val_accuracy: 0.5928\n","Epoch 68/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7699 - accuracy: 0.7032 - val_loss: 0.9702 - val_accuracy: 0.5894\n","Epoch 69/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7691 - accuracy: 0.6986 - val_loss: 0.9657 - val_accuracy: 0.6018\n","Epoch 70/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7625 - accuracy: 0.7077 - val_loss: 0.9610 - val_accuracy: 0.5803\n","Epoch 71/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7620 - accuracy: 0.7023 - val_loss: 0.9666 - val_accuracy: 0.5894\n","Epoch 72/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7582 - accuracy: 0.7142 - val_loss: 0.9553 - val_accuracy: 0.5950\n","Epoch 73/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7544 - accuracy: 0.7074 - val_loss: 0.9711 - val_accuracy: 0.5814\n","Epoch 74/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7563 - accuracy: 0.7066 - val_loss: 0.9593 - val_accuracy: 0.5962\n","Epoch 75/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7669 - accuracy: 0.7066 - val_loss: 0.9764 - val_accuracy: 0.5803\n","Epoch 76/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7495 - accuracy: 0.7159 - val_loss: 0.9641 - val_accuracy: 0.5905\n","Epoch 77/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7465 - accuracy: 0.7139 - val_loss: 0.9577 - val_accuracy: 0.5950\n","Epoch 78/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7513 - accuracy: 0.7040 - val_loss: 0.9667 - val_accuracy: 0.5848\n","Epoch 79/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7395 - accuracy: 0.7173 - val_loss: 0.9655 - val_accuracy: 0.5860\n","Epoch 80/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7396 - accuracy: 0.7184 - val_loss: 0.9647 - val_accuracy: 0.5860\n","Epoch 81/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7415 - accuracy: 0.7213 - val_loss: 0.9838 - val_accuracy: 0.5781\n","Epoch 82/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7361 - accuracy: 0.7136 - val_loss: 0.9780 - val_accuracy: 0.5928\n","Epoch 83/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7337 - accuracy: 0.7213 - val_loss: 0.9721 - val_accuracy: 0.5814\n","Epoch 84/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7327 - accuracy: 0.7233 - val_loss: 0.9869 - val_accuracy: 0.5769\n","Epoch 85/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7272 - accuracy: 0.7289 - val_loss: 0.9966 - val_accuracy: 0.5837\n","Epoch 86/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7276 - accuracy: 0.7272 - val_loss: 0.9894 - val_accuracy: 0.5837\n","Epoch 87/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7252 - accuracy: 0.7238 - val_loss: 0.9946 - val_accuracy: 0.5701\n","Epoch 88/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7216 - accuracy: 0.7295 - val_loss: 0.9844 - val_accuracy: 0.5882\n","Epoch 89/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7201 - accuracy: 0.7275 - val_loss: 0.9795 - val_accuracy: 0.5792\n","Epoch 90/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7200 - accuracy: 0.7278 - val_loss: 1.0189 - val_accuracy: 0.5826\n","Epoch 91/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7126 - accuracy: 0.7371 - val_loss: 1.0005 - val_accuracy: 0.5826\n","Epoch 92/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7132 - accuracy: 0.7289 - val_loss: 0.9837 - val_accuracy: 0.5826\n","Epoch 93/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7117 - accuracy: 0.7343 - val_loss: 1.0050 - val_accuracy: 0.5747\n","Epoch 94/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.7135 - accuracy: 0.7388 - val_loss: 0.9973 - val_accuracy: 0.5735\n","Epoch 95/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7070 - accuracy: 0.7301 - val_loss: 1.0113 - val_accuracy: 0.5826\n","Epoch 96/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7023 - accuracy: 0.7391 - val_loss: 0.9971 - val_accuracy: 0.5848\n","Epoch 97/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7025 - accuracy: 0.7456 - val_loss: 1.0098 - val_accuracy: 0.5962\n","Epoch 98/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7008 - accuracy: 0.7292 - val_loss: 1.0145 - val_accuracy: 0.5735\n","Epoch 99/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6993 - accuracy: 0.7374 - val_loss: 1.0021 - val_accuracy: 0.5792\n","Epoch 100/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7030 - accuracy: 0.7301 - val_loss: 1.0161 - val_accuracy: 0.5848\n","{'loss': [0.994828462600708, 0.9861562252044678, 0.9800552725791931, 0.97572922706604, 0.9706657528877258, 0.9673439264297485, 0.9604083895683289, 0.9555432200431824, 0.9505693316459656, 0.9469674825668335, 0.942469596862793, 0.938098132610321, 0.9336944222450256, 0.9296207427978516, 0.9264540672302246, 0.9225313067436218, 0.917593777179718, 0.9142746329307556, 0.9102193117141724, 0.9066029191017151, 0.9028063416481018, 0.8987051248550415, 0.8945004940032959, 0.8910945653915405, 0.889240026473999, 0.8852667808532715, 0.882805585861206, 0.8804559707641602, 0.876347005367279, 0.8706820011138916, 0.8706557750701904, 0.8672596216201782, 0.8615895509719849, 0.8589309453964233, 0.8547790050506592, 0.8529113531112671, 0.850395917892456, 0.8465031385421753, 0.8444274067878723, 0.8392559885978699, 0.8380284905433655, 0.8366924524307251, 0.8340199589729309, 0.8336398005485535, 0.8300239443778992, 0.8247382044792175, 0.8197346329689026, 0.8160820007324219, 0.8148614168167114, 0.8159652352333069, 0.8093531131744385, 0.8078528046607971, 0.8049185276031494, 0.7990610599517822, 0.7973779439926147, 0.7992761135101318, 0.796463668346405, 0.7916106581687927, 0.789412260055542, 0.7860066890716553, 0.7846754789352417, 0.7788589000701904, 0.7782538533210754, 0.7773754000663757, 0.7746781706809998, 0.7704101204872131, 0.7669311761856079, 0.7699234485626221, 0.769072949886322, 0.7624772787094116, 0.7619531750679016, 0.7581881880760193, 0.7544352412223816, 0.7562825083732605, 0.7669196128845215, 0.7494581937789917, 0.7464801073074341, 0.7512624263763428, 0.7395492196083069, 0.7396302819252014, 0.7414827346801758, 0.7361224889755249, 0.7336576581001282, 0.732719361782074, 0.727240800857544, 0.7275884747505188, 0.7251922488212585, 0.7215837836265564, 0.720084011554718, 0.7199885249137878, 0.7126177549362183, 0.7132211923599243, 0.711681067943573, 0.7135069966316223, 0.7070107460021973, 0.7023093104362488, 0.7025498747825623, 0.7008196115493774, 0.6992594003677368, 0.7030258774757385], 'accuracy': [0.6262025833129883, 0.6219581365585327, 0.6202603578567505, 0.6225240230560303, 0.627334475517273, 0.6250707507133484, 0.6338426470756531, 0.6298811435699463, 0.6321448683738708, 0.6341256499290466, 0.6378042101860046, 0.6349745392799377, 0.6355404853820801, 0.6392189860343933, 0.6383700966835022, 0.6386530995368958, 0.6426146030426025, 0.6528013348579407, 0.6434634923934937, 0.6443123817443848, 0.6414827108383179, 0.647707998752594, 0.6448783278465271, 0.6638370156288147, 0.6519524455070496, 0.6576117873191833, 0.6508206129074097, 0.6573287844657898, 0.6635540723800659, 0.6612903475761414, 0.6598755121231079, 0.6661007404327393, 0.6677985191345215, 0.6677985191345215, 0.6706281900405884, 0.6658177971839905, 0.6692133545875549, 0.6796830892562866, 0.6717600226402283, 0.673740804195404, 0.6777023077011108, 0.6706281900405884, 0.6745896935462952, 0.6697793006896973, 0.6794000864028931, 0.6723259687423706, 0.6757215857505798, 0.689586877822876, 0.6833616495132446, 0.6856253743171692, 0.6893039345741272, 0.6757215857505798, 0.6901528239250183, 0.6955291628837585, 0.6983587741851807, 0.6921335458755493, 0.6912846565246582, 0.6929824352264404, 0.6893039345741272, 0.6960950493812561, 0.7011884450912476, 0.6958121061325073, 0.7009055018424988, 0.7074136734008789, 0.7043010592460632, 0.7031692266464233, 0.7031692266464233, 0.7031692266464233, 0.6986417770385742, 0.7076966762542725, 0.7023203372955322, 0.7142048478126526, 0.7074136734008789, 0.7065647840499878, 0.7065647840499878, 0.7159026861190796, 0.7139219045639038, 0.7040181159973145, 0.7173174619674683, 0.7184493541717529, 0.7212790250778198, 0.713638961315155, 0.7212790250778198, 0.7232597470283508, 0.7289190888404846, 0.7272212505340576, 0.7238256931304932, 0.7294849753379822, 0.7275042533874512, 0.7277871966362, 0.7371250987052917, 0.7289190888404846, 0.7342954277992249, 0.738822877407074, 0.7300509214401245, 0.7391058206558228, 0.7456140518188477, 0.7292020320892334, 0.7374080419540405, 0.7300509214401245], 'val_loss': [1.0472474098205566, 1.0429657697677612, 1.0388987064361572, 1.0345540046691895, 1.031822681427002, 1.0276143550872803, 1.0217264890670776, 1.0157034397125244, 1.013443112373352, 1.006353497505188, 1.0038552284240723, 1.0012797117233276, 0.9951388835906982, 0.9881997108459473, 0.9843668937683105, 0.9787209630012512, 0.9760929942131042, 0.9704334139823914, 0.9662994742393494, 0.9652408957481384, 0.9574023485183716, 0.9534204602241516, 0.948503315448761, 0.9481750726699829, 0.9533348083496094, 0.9454149603843689, 0.9453325271606445, 0.9424840807914734, 0.9405344724655151, 0.9480755925178528, 0.9482004642486572, 0.9391754269599915, 0.936427652835846, 0.938928484916687, 0.9346662759780884, 0.9369716644287109, 0.9367426633834839, 0.9336934089660645, 0.9340571165084839, 0.9337061643600464, 0.93378746509552, 0.9301166534423828, 0.9342308640480042, 0.94332355260849, 0.9369711875915527, 0.9302924871444702, 0.9327601790428162, 0.9389727115631104, 0.9472677707672119, 0.9314892292022705, 0.9397571682929993, 0.9318045377731323, 0.9357917308807373, 0.9397357106208801, 0.9394281506538391, 0.9606409668922424, 0.9450718760490417, 0.945529580116272, 0.9362844824790955, 0.9486314058303833, 0.946664571762085, 0.9471606612205505, 0.9451085925102234, 0.9620580077171326, 0.9426499605178833, 0.9536704421043396, 0.9453346729278564, 0.9701724052429199, 0.9657448530197144, 0.9609912633895874, 0.9666192531585693, 0.955321729183197, 0.9711023569107056, 0.9593254327774048, 0.9763562083244324, 0.9640900492668152, 0.9576632380485535, 0.9666657447814941, 0.9654956459999084, 0.9647220969200134, 0.9838340878486633, 0.9779669046401978, 0.9720799922943115, 0.9868829846382141, 0.9966461658477783, 0.9894174337387085, 0.9945806264877319, 0.9844071269035339, 0.9795145988464355, 1.0188980102539062, 1.0004631280899048, 0.9836805462837219, 1.005048394203186, 0.9972999691963196, 1.0112941265106201, 0.9970521330833435, 1.0098217725753784, 1.0144541263580322, 1.002132534980774, 1.0161309242248535], 'val_accuracy': [0.5056561231613159, 0.5056561231613159, 0.5056561231613159, 0.5056561231613159, 0.5045248866081238, 0.5067873597145081, 0.5135746598243713, 0.5248869061470032, 0.5214931964874268, 0.540723979473114, 0.5316742062568665, 0.5316742062568665, 0.5463801026344299, 0.5599547624588013, 0.5542986392974854, 0.5588235259056091, 0.5656108856201172, 0.5735294222831726, 0.5712669491767883, 0.5757918357849121, 0.5825791954994202, 0.5791855454444885, 0.5825791954994202, 0.5757918357849121, 0.6029411554336548, 0.5859728455543518, 0.5825791954994202, 0.5938913822174072, 0.587104082107544, 0.581447958946228, 0.5893664956092834, 0.6006787419319153, 0.5961538553237915, 0.5825791954994202, 0.5893664956092834, 0.5893664956092834, 0.5950226187705994, 0.5893664956092834, 0.5961538553237915, 0.587104082107544, 0.6052036285400391, 0.5961538553237915, 0.5938913822174072, 0.6006787419319153, 0.6052036285400391, 0.5893664956092834, 0.5916289687156677, 0.5882353186607361, 0.5757918357849121, 0.5904977321624756, 0.6006787419319153, 0.6052036285400391, 0.5825791954994202, 0.5859728455543518, 0.5972850918769836, 0.5837104320526123, 0.5893664956092834, 0.587104082107544, 0.5882353186607361, 0.6029411554336548, 0.5961538553237915, 0.6018099784851074, 0.587104082107544, 0.581447958946228, 0.5916289687156677, 0.5893664956092834, 0.5927602052688599, 0.5893664956092834, 0.6018099784851074, 0.5803167223930359, 0.5893664956092834, 0.5950226187705994, 0.581447958946228, 0.5961538553237915, 0.5803167223930359, 0.5904977321624756, 0.5950226187705994, 0.5848416090011597, 0.5859728455543518, 0.5859728455543518, 0.5780543088912964, 0.5927602052688599, 0.581447958946228, 0.5769230723381042, 0.5837104320526123, 0.5837104320526123, 0.570135772228241, 0.5882353186607361, 0.5791855454444885, 0.5825791954994202, 0.5825791954994202, 0.5825791954994202, 0.5746606588363647, 0.5735294222831726, 0.5825791954994202, 0.5848416090011597, 0.5961538553237915, 0.5735294222831726, 0.5791855454444885, 0.5848416090011597]}\n","45/45 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 4s 33ms/step - loss: 1.0002 - accuracy: 0.6140 - val_loss: 1.0472 - val_accuracy: 0.5134\n","Epoch 2/100\n"," 1/31 [..............................] - ETA: 0s - loss: 0.9784 - accuracy: 0.6484"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 0s 14ms/step - loss: 0.9899 - accuracy: 0.6140 - val_loss: 1.0420 - val_accuracy: 0.5134\n","Epoch 3/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9842 - accuracy: 0.6217 - val_loss: 1.0380 - val_accuracy: 0.5134\n","Epoch 4/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.9790 - accuracy: 0.6217 - val_loss: 1.0347 - val_accuracy: 0.5134\n","Epoch 5/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.9732 - accuracy: 0.6243 - val_loss: 1.0293 - val_accuracy: 0.5176\n","Epoch 6/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9682 - accuracy: 0.6222 - val_loss: 1.0255 - val_accuracy: 0.5176\n","Epoch 7/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.9645 - accuracy: 0.6271 - val_loss: 1.0220 - val_accuracy: 0.5186\n","Epoch 8/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.9586 - accuracy: 0.6305 - val_loss: 1.0172 - val_accuracy: 0.5248\n","Epoch 9/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.9547 - accuracy: 0.6269 - val_loss: 1.0130 - val_accuracy: 0.5320\n","Epoch 10/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9489 - accuracy: 0.6320 - val_loss: 1.0085 - val_accuracy: 0.5362\n","Epoch 11/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.9445 - accuracy: 0.6315 - val_loss: 1.0050 - val_accuracy: 0.5372\n","Epoch 12/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.9400 - accuracy: 0.6344 - val_loss: 0.9995 - val_accuracy: 0.5434\n","Epoch 13/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9351 - accuracy: 0.6336 - val_loss: 0.9965 - val_accuracy: 0.5403\n","Epoch 14/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.9322 - accuracy: 0.6326 - val_loss: 0.9920 - val_accuracy: 0.5465\n","Epoch 15/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9280 - accuracy: 0.6351 - val_loss: 0.9881 - val_accuracy: 0.5558\n","Epoch 16/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9244 - accuracy: 0.6339 - val_loss: 0.9857 - val_accuracy: 0.5486\n","Epoch 17/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9204 - accuracy: 0.6328 - val_loss: 0.9816 - val_accuracy: 0.5506\n","Epoch 18/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.9148 - accuracy: 0.6395 - val_loss: 0.9826 - val_accuracy: 0.5640\n","Epoch 19/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9144 - accuracy: 0.6320 - val_loss: 0.9755 - val_accuracy: 0.5620\n","Epoch 20/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.9108 - accuracy: 0.6375 - val_loss: 0.9755 - val_accuracy: 0.5692\n","Epoch 21/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9044 - accuracy: 0.6437 - val_loss: 0.9753 - val_accuracy: 0.5620\n","Epoch 22/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.9003 - accuracy: 0.6406 - val_loss: 0.9705 - val_accuracy: 0.5661\n","Epoch 23/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8965 - accuracy: 0.6455 - val_loss: 0.9826 - val_accuracy: 0.5579\n","Epoch 24/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8939 - accuracy: 0.6426 - val_loss: 0.9740 - val_accuracy: 0.5620\n","Epoch 25/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8898 - accuracy: 0.6444 - val_loss: 0.9771 - val_accuracy: 0.5589\n","Epoch 26/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8858 - accuracy: 0.6501 - val_loss: 0.9708 - val_accuracy: 0.5640\n","Epoch 27/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8830 - accuracy: 0.6556 - val_loss: 0.9710 - val_accuracy: 0.5682\n","Epoch 28/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8779 - accuracy: 0.6496 - val_loss: 0.9623 - val_accuracy: 0.5620\n","Epoch 29/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.8777 - accuracy: 0.6496 - val_loss: 0.9651 - val_accuracy: 0.5579\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8734 - accuracy: 0.6450 - val_loss: 0.9563 - val_accuracy: 0.5599\n","Epoch 31/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.8714 - accuracy: 0.6501 - val_loss: 0.9785 - val_accuracy: 0.5610\n","Epoch 32/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.8695 - accuracy: 0.6548 - val_loss: 0.9730 - val_accuracy: 0.5579\n","Epoch 33/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8636 - accuracy: 0.6597 - val_loss: 0.9782 - val_accuracy: 0.5558\n","Epoch 34/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.8634 - accuracy: 0.6581 - val_loss: 0.9676 - val_accuracy: 0.5599\n","Epoch 35/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.8583 - accuracy: 0.6548 - val_loss: 0.9581 - val_accuracy: 0.5733\n","Epoch 36/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.8545 - accuracy: 0.6540 - val_loss: 0.9622 - val_accuracy: 0.5640\n","Epoch 37/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8531 - accuracy: 0.6602 - val_loss: 0.9636 - val_accuracy: 0.5517\n","Epoch 38/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8484 - accuracy: 0.6672 - val_loss: 0.9694 - val_accuracy: 0.5568\n","Epoch 39/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8446 - accuracy: 0.6680 - val_loss: 0.9635 - val_accuracy: 0.5527\n","Epoch 40/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8420 - accuracy: 0.6630 - val_loss: 0.9613 - val_accuracy: 0.5558\n","Epoch 41/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8408 - accuracy: 0.6587 - val_loss: 0.9701 - val_accuracy: 0.5579\n","Epoch 42/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8387 - accuracy: 0.6659 - val_loss: 0.9618 - val_accuracy: 0.5568\n","Epoch 43/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8369 - accuracy: 0.6687 - val_loss: 0.9525 - val_accuracy: 0.5610\n","Epoch 44/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8316 - accuracy: 0.6718 - val_loss: 0.9565 - val_accuracy: 0.5640\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8310 - accuracy: 0.6685 - val_loss: 0.9638 - val_accuracy: 0.5589\n","Epoch 46/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8305 - accuracy: 0.6638 - val_loss: 0.9692 - val_accuracy: 0.5506\n","Epoch 47/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8296 - accuracy: 0.6646 - val_loss: 0.9671 - val_accuracy: 0.5568\n","Epoch 48/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8214 - accuracy: 0.6793 - val_loss: 0.9523 - val_accuracy: 0.5640\n","Epoch 49/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8233 - accuracy: 0.6643 - val_loss: 0.9659 - val_accuracy: 0.5558\n","Epoch 50/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8171 - accuracy: 0.6767 - val_loss: 0.9604 - val_accuracy: 0.5610\n","Epoch 51/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8131 - accuracy: 0.6798 - val_loss: 0.9535 - val_accuracy: 0.5630\n","Epoch 52/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8101 - accuracy: 0.6837 - val_loss: 0.9845 - val_accuracy: 0.5527\n","Epoch 53/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8068 - accuracy: 0.6832 - val_loss: 0.9568 - val_accuracy: 0.5579\n","Epoch 54/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8040 - accuracy: 0.6876 - val_loss: 0.9651 - val_accuracy: 0.5579\n","Epoch 55/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8041 - accuracy: 0.6897 - val_loss: 0.9538 - val_accuracy: 0.5610\n","Epoch 56/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8024 - accuracy: 0.6853 - val_loss: 0.9566 - val_accuracy: 0.5579\n","Epoch 57/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7987 - accuracy: 0.6829 - val_loss: 0.9589 - val_accuracy: 0.5599\n","Epoch 58/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7940 - accuracy: 0.6972 - val_loss: 0.9706 - val_accuracy: 0.5558\n","Epoch 59/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7926 - accuracy: 0.6876 - val_loss: 0.9500 - val_accuracy: 0.5610\n","Epoch 60/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7889 - accuracy: 0.6984 - val_loss: 0.9639 - val_accuracy: 0.5640\n","Epoch 61/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7873 - accuracy: 0.6894 - val_loss: 0.9553 - val_accuracy: 0.5527\n","Epoch 62/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7892 - accuracy: 0.6866 - val_loss: 0.9518 - val_accuracy: 0.5506\n","Epoch 63/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7813 - accuracy: 0.6881 - val_loss: 0.9480 - val_accuracy: 0.5630\n","Epoch 64/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7803 - accuracy: 0.6930 - val_loss: 0.9682 - val_accuracy: 0.5537\n","Epoch 65/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7771 - accuracy: 0.6938 - val_loss: 0.9500 - val_accuracy: 0.5599\n","Epoch 66/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7742 - accuracy: 0.6987 - val_loss: 0.9588 - val_accuracy: 0.5640\n","Epoch 67/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7708 - accuracy: 0.6972 - val_loss: 0.9650 - val_accuracy: 0.5475\n","Epoch 68/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7671 - accuracy: 0.6997 - val_loss: 0.9434 - val_accuracy: 0.5610\n","Epoch 69/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7685 - accuracy: 0.6974 - val_loss: 0.9763 - val_accuracy: 0.5486\n","Epoch 70/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7637 - accuracy: 0.7039 - val_loss: 0.9695 - val_accuracy: 0.5568\n","Epoch 71/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7665 - accuracy: 0.6917 - val_loss: 0.9611 - val_accuracy: 0.5589\n","Epoch 72/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7611 - accuracy: 0.7036 - val_loss: 0.9735 - val_accuracy: 0.5527\n","Epoch 73/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7578 - accuracy: 0.7021 - val_loss: 0.9702 - val_accuracy: 0.5465\n","Epoch 74/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7545 - accuracy: 0.7054 - val_loss: 0.9915 - val_accuracy: 0.5424\n","Epoch 75/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7542 - accuracy: 0.7078 - val_loss: 1.0088 - val_accuracy: 0.5517\n","Epoch 76/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7582 - accuracy: 0.6920 - val_loss: 1.0051 - val_accuracy: 0.5527\n","Epoch 77/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7500 - accuracy: 0.7134 - val_loss: 0.9907 - val_accuracy: 0.5434\n","Epoch 78/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7500 - accuracy: 0.7119 - val_loss: 0.9805 - val_accuracy: 0.5517\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7457 - accuracy: 0.7101 - val_loss: 0.9880 - val_accuracy: 0.5372\n","Epoch 80/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7440 - accuracy: 0.7085 - val_loss: 0.9485 - val_accuracy: 0.5558\n","Epoch 81/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7491 - accuracy: 0.7065 - val_loss: 1.0247 - val_accuracy: 0.5362\n","Epoch 82/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7441 - accuracy: 0.7119 - val_loss: 0.9844 - val_accuracy: 0.5486\n","Epoch 83/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7370 - accuracy: 0.7075 - val_loss: 0.9865 - val_accuracy: 0.5486\n","Epoch 84/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7328 - accuracy: 0.7194 - val_loss: 1.0148 - val_accuracy: 0.5444\n","Epoch 85/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7343 - accuracy: 0.7181 - val_loss: 0.9824 - val_accuracy: 0.5465\n","Epoch 86/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7292 - accuracy: 0.7155 - val_loss: 0.9775 - val_accuracy: 0.5558\n","Epoch 87/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7268 - accuracy: 0.7256 - val_loss: 0.9579 - val_accuracy: 0.5517\n","Epoch 88/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7227 - accuracy: 0.7248 - val_loss: 0.9553 - val_accuracy: 0.5599\n","Epoch 89/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7633 - accuracy: 0.6915 - val_loss: 0.9667 - val_accuracy: 0.5227\n","Epoch 90/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7248 - accuracy: 0.7235 - val_loss: 0.9698 - val_accuracy: 0.5455\n","Epoch 91/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7201 - accuracy: 0.7258 - val_loss: 0.9664 - val_accuracy: 0.5455\n","Epoch 92/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7216 - accuracy: 0.7171 - val_loss: 1.0013 - val_accuracy: 0.5269\n","Epoch 93/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7163 - accuracy: 0.7328 - val_loss: 0.9888 - val_accuracy: 0.5424\n","Epoch 94/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7102 - accuracy: 0.7300 - val_loss: 0.9927 - val_accuracy: 0.5413\n","Epoch 95/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7112 - accuracy: 0.7251 - val_loss: 0.9672 - val_accuracy: 0.5496\n","Epoch 96/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7096 - accuracy: 0.7354 - val_loss: 1.0051 - val_accuracy: 0.5465\n","Epoch 97/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7082 - accuracy: 0.7385 - val_loss: 1.0122 - val_accuracy: 0.5351\n","Epoch 98/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7072 - accuracy: 0.7292 - val_loss: 1.0213 - val_accuracy: 0.5475\n","Epoch 99/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7177 - accuracy: 0.7282 - val_loss: 1.0127 - val_accuracy: 0.5424\n","Epoch 100/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7052 - accuracy: 0.7315 - val_loss: 1.0339 - val_accuracy: 0.5506\n","{'loss': [1.0001972913742065, 0.989936888217926, 0.9841628074645996, 0.978966236114502, 0.9732146263122559, 0.9682410955429077, 0.9645219445228577, 0.9586024284362793, 0.9547446966171265, 0.9488564133644104, 0.9444658160209656, 0.9399685263633728, 0.9351462721824646, 0.9322468042373657, 0.927995502948761, 0.9243937730789185, 0.9204086065292358, 0.9148107171058655, 0.9144250154495239, 0.910772442817688, 0.9044429659843445, 0.9003009796142578, 0.8964555859565735, 0.8938839435577393, 0.8898385167121887, 0.8857892155647278, 0.8830228447914124, 0.8779191374778748, 0.8776553273200989, 0.8734331727027893, 0.8713809847831726, 0.869489312171936, 0.8636383414268494, 0.8634164333343506, 0.8582774996757507, 0.8545199632644653, 0.8531281352043152, 0.8483843803405762, 0.8445510864257812, 0.8420038223266602, 0.8407616019248962, 0.8387131094932556, 0.8368582129478455, 0.831578254699707, 0.8310232162475586, 0.830467700958252, 0.829616367816925, 0.8213683366775513, 0.8233466744422913, 0.817093014717102, 0.8131479024887085, 0.8101308941841125, 0.8067864775657654, 0.8039931654930115, 0.8041350245475769, 0.8023803234100342, 0.7986526489257812, 0.7940409779548645, 0.79264235496521, 0.7889315485954285, 0.7872949838638306, 0.7892032861709595, 0.7812972664833069, 0.7803298234939575, 0.7770878672599792, 0.7741683721542358, 0.7707567811012268, 0.7671085596084595, 0.7684808373451233, 0.7636902332305908, 0.7664960622787476, 0.7610589861869812, 0.7578298449516296, 0.7544859051704407, 0.7542452216148376, 0.7581826448440552, 0.7500479817390442, 0.7500219345092773, 0.745702862739563, 0.7440220713615417, 0.7491039633750916, 0.7440572381019592, 0.7369731068611145, 0.7327515482902527, 0.7343352437019348, 0.7291905879974365, 0.7268216609954834, 0.7226799130439758, 0.7632789611816406, 0.7247682213783264, 0.7201374173164368, 0.721649706363678, 0.7162649035453796, 0.7102101445198059, 0.7112485766410828, 0.7095833420753479, 0.7082226872444153, 0.7072003483772278, 0.717677652835846, 0.7052080035209656], 'accuracy': [0.6139534711837769, 0.6139534711837769, 0.6217054128646851, 0.6217054128646851, 0.6242893934249878, 0.6222222447395325, 0.6271317601203918, 0.6304909586906433, 0.6268733739852905, 0.632041335105896, 0.6315245628356934, 0.6343669295310974, 0.6335917115211487, 0.6325581669807434, 0.6351421475410461, 0.6338501572608948, 0.6328165531158447, 0.6395348906517029, 0.632041335105896, 0.6374676823616028, 0.6436692476272583, 0.6405684947967529, 0.6454780101776123, 0.6426356434822083, 0.644444465637207, 0.6501291990280151, 0.6555555462837219, 0.6496124267578125, 0.6496124267578125, 0.6449612379074097, 0.6501291990280151, 0.654780387878418, 0.6596899032592773, 0.6581395268440247, 0.654780387878418, 0.6540051698684692, 0.6602067351341248, 0.6671834588050842, 0.667958676815033, 0.6630491018295288, 0.6586563587188721, 0.6658914685249329, 0.6687338352203369, 0.6718346476554871, 0.6684754490852356, 0.6638242602348328, 0.6645994782447815, 0.6793281435966492, 0.6643410921096802, 0.6767441630363464, 0.6798449754714966, 0.6837209463119507, 0.6832041144371033, 0.6875969171524048, 0.6896640658378601, 0.6852713227272034, 0.682945728302002, 0.697157621383667, 0.6875969171524048, 0.6984496116638184, 0.6894056797027588, 0.6865633130073547, 0.6881136894226074, 0.6930232644081116, 0.6937984228134155, 0.6987079977989197, 0.697157621383667, 0.6997416019439697, 0.6974160075187683, 0.7038759589195251, 0.6917312741279602, 0.7036175727844238, 0.7020671963691711, 0.7054263353347778, 0.7077519297599792, 0.6919896602630615, 0.7134366631507874, 0.7118862867355347, 0.7100775241851807, 0.708527147769928, 0.7064599394798279, 0.7118862867355347, 0.7074935436248779, 0.7193798422813416, 0.7180878520011902, 0.7155038714408875, 0.7255814075469971, 0.7248061895370483, 0.6914728879928589, 0.723514199256897, 0.7258397936820984, 0.7170542478561401, 0.7328165173530579, 0.7299741506576538, 0.7250645756721497, 0.7354004979133606, 0.7385013103485107, 0.7291989922523499, 0.7281653881072998, 0.7315245270729065], 'val_loss': [1.0472198724746704, 1.0419981479644775, 1.038023591041565, 1.034732460975647, 1.0293036699295044, 1.0255440473556519, 1.0220199823379517, 1.0172176361083984, 1.0129737854003906, 1.0085333585739136, 1.005021095275879, 0.9995211958885193, 0.9964752197265625, 0.992040753364563, 0.9881225228309631, 0.9856740832328796, 0.981634259223938, 0.9826204180717468, 0.9755370616912842, 0.9755202531814575, 0.9752615094184875, 0.9704610109329224, 0.9826399683952332, 0.9739673733711243, 0.9771424531936646, 0.9707938432693481, 0.9709508419036865, 0.9622966051101685, 0.9650852680206299, 0.9563232660293579, 0.9784978032112122, 0.9730095267295837, 0.978198230266571, 0.9675807952880859, 0.9581113457679749, 0.9621660113334656, 0.9636285901069641, 0.9694063067436218, 0.9635394811630249, 0.9612634778022766, 0.970054030418396, 0.9617801308631897, 0.9525047540664673, 0.9565409421920776, 0.9637507200241089, 0.9691793918609619, 0.9670515656471252, 0.9522511959075928, 0.9658596515655518, 0.960442304611206, 0.9534731507301331, 0.9844691157341003, 0.9567786455154419, 0.965062141418457, 0.9537705183029175, 0.9566337466239929, 0.958890438079834, 0.9705787301063538, 0.9500413537025452, 0.9638625383377075, 0.9553302526473999, 0.9518268704414368, 0.9480052590370178, 0.9681775569915771, 0.9500450491905212, 0.9587656855583191, 0.9649981260299683, 0.9433879256248474, 0.9762812256813049, 0.9694981575012207, 0.9610820412635803, 0.9735063910484314, 0.9701523780822754, 0.9915186762809753, 1.008791208267212, 1.0050562620162964, 0.9906727075576782, 0.9805275201797485, 0.988047182559967, 0.948506772518158, 1.0246860980987549, 0.984376847743988, 0.9865261316299438, 1.0147820711135864, 0.9824126958847046, 0.9774677157402039, 0.9578547477722168, 0.955336332321167, 0.9667233824729919, 0.9698207378387451, 0.9663523435592651, 1.0012797117233276, 0.988800048828125, 0.9927422404289246, 0.9672396183013916, 1.00507652759552, 1.0121697187423706, 1.0213165283203125, 1.012739658355713, 1.0338703393936157], 'val_accuracy': [0.5134297609329224, 0.5134297609329224, 0.5134297609329224, 0.5134297609329224, 0.5175619721412659, 0.5175619721412659, 0.5185950398445129, 0.5247933864593506, 0.5320248007774353, 0.5361570119857788, 0.5371900796890259, 0.5433884263038635, 0.5402892827987671, 0.5464876294136047, 0.5557851195335388, 0.5485537052154541, 0.5506198406219482, 0.5640496015548706, 0.5619834661483765, 0.5692148804664612, 0.5619834661483765, 0.56611567735672, 0.557851254940033, 0.5619834661483765, 0.55888432264328, 0.5640496015548706, 0.5681818127632141, 0.5619834661483765, 0.557851254940033, 0.5599173307418823, 0.5609503984451294, 0.557851254940033, 0.5557851195335388, 0.5599173307418823, 0.5733470916748047, 0.5640496015548706, 0.5516529083251953, 0.5568181872367859, 0.5526859760284424, 0.5557851195335388, 0.557851254940033, 0.5568181872367859, 0.5609503984451294, 0.5640496015548706, 0.55888432264328, 0.5506198406219482, 0.5568181872367859, 0.5640496015548706, 0.5557851195335388, 0.5609503984451294, 0.5630165338516235, 0.5526859760284424, 0.557851254940033, 0.557851254940033, 0.5609503984451294, 0.557851254940033, 0.5599173307418823, 0.5557851195335388, 0.5609503984451294, 0.5640496015548706, 0.5526859760284424, 0.5506198406219482, 0.5630165338516235, 0.5537189841270447, 0.5599173307418823, 0.5640496015548706, 0.547520637512207, 0.5609503984451294, 0.5485537052154541, 0.5568181872367859, 0.55888432264328, 0.5526859760284424, 0.5464876294136047, 0.5423553586006165, 0.5516529083251953, 0.5526859760284424, 0.5433884263038635, 0.5516529083251953, 0.5371900796890259, 0.5557851195335388, 0.5361570119857788, 0.5485537052154541, 0.5485537052154541, 0.5444214940071106, 0.5464876294136047, 0.5557851195335388, 0.5516529083251953, 0.5599173307418823, 0.5227272510528564, 0.5454545617103577, 0.5454545617103577, 0.5268595218658447, 0.5423553586006165, 0.5413222908973694, 0.5495867729187012, 0.5464876294136047, 0.5351239442825317, 0.547520637512207, 0.5423553586006165, 0.5506198406219482]}\n","32/32 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 5s 40ms/step - loss: 0.7454 - accuracy: 0.6977 - val_loss: 0.8886 - val_accuracy: 0.5032\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 1s 20ms/step - loss: 0.7312 - accuracy: 0.7058 - val_loss: 0.8860 - val_accuracy: 0.5065\n","Epoch 3/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7285 - accuracy: 0.7204 - val_loss: 0.8871 - val_accuracy: 0.5000\n","Epoch 4/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7238 - accuracy: 0.7136 - val_loss: 0.8815 - val_accuracy: 0.6002\n","Epoch 5/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7179 - accuracy: 0.7212 - val_loss: 0.8783 - val_accuracy: 0.5668\n","Epoch 6/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7227 - accuracy: 0.7088 - val_loss: 0.8831 - val_accuracy: 0.5916\n","Epoch 7/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7182 - accuracy: 0.7231 - val_loss: 0.8801 - val_accuracy: 0.5129\n","Epoch 8/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7173 - accuracy: 0.7185 - val_loss: 0.8741 - val_accuracy: 0.5280\n","Epoch 9/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7115 - accuracy: 0.7198 - val_loss: 0.8734 - val_accuracy: 0.5830\n","Epoch 10/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7072 - accuracy: 0.7301 - val_loss: 0.8750 - val_accuracy: 0.5312\n","Epoch 11/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7042 - accuracy: 0.7177 - val_loss: 0.8700 - val_accuracy: 0.6164\n","Epoch 12/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7042 - accuracy: 0.7258 - val_loss: 0.8566 - val_accuracy: 0.5970\n","Epoch 13/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7016 - accuracy: 0.7239 - val_loss: 0.8637 - val_accuracy: 0.5744\n","Epoch 14/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7001 - accuracy: 0.7355 - val_loss: 0.8587 - val_accuracy: 0.6002\n","Epoch 15/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.6975 - accuracy: 0.7209 - val_loss: 0.8469 - val_accuracy: 0.6185\n","Epoch 16/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6985 - accuracy: 0.7314 - val_loss: 0.8480 - val_accuracy: 0.6196\n","Epoch 17/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6930 - accuracy: 0.7346 - val_loss: 0.8543 - val_accuracy: 0.6250\n","Epoch 18/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6898 - accuracy: 0.7349 - val_loss: 0.8518 - val_accuracy: 0.6272\n","Epoch 19/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6907 - accuracy: 0.7271 - val_loss: 0.8482 - val_accuracy: 0.6228\n","Epoch 20/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.6865 - accuracy: 0.7322 - val_loss: 0.8397 - val_accuracy: 0.6422\n","Epoch 21/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6870 - accuracy: 0.7376 - val_loss: 0.8411 - val_accuracy: 0.6412\n","Epoch 22/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6824 - accuracy: 0.7379 - val_loss: 0.9054 - val_accuracy: 0.5970\n","Epoch 23/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6815 - accuracy: 0.7384 - val_loss: 0.8567 - val_accuracy: 0.6379\n","Epoch 24/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6779 - accuracy: 0.7425 - val_loss: 0.8789 - val_accuracy: 0.6358\n","Epoch 25/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6779 - accuracy: 0.7435 - val_loss: 0.8631 - val_accuracy: 0.6390\n","Epoch 26/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6764 - accuracy: 0.7449 - val_loss: 0.8725 - val_accuracy: 0.6293\n","Epoch 27/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6736 - accuracy: 0.7465 - val_loss: 0.8855 - val_accuracy: 0.6358\n","Epoch 28/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.6735 - accuracy: 0.7422 - val_loss: 0.8827 - val_accuracy: 0.6476\n","Epoch 29/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.6667 - accuracy: 0.7497 - val_loss: 0.8943 - val_accuracy: 0.6379\n","Epoch 30/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6693 - accuracy: 0.7495 - val_loss: 0.9151 - val_accuracy: 0.6466\n","Epoch 31/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6665 - accuracy: 0.7540 - val_loss: 0.9593 - val_accuracy: 0.6261\n","Epoch 32/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6690 - accuracy: 0.7468 - val_loss: 0.9302 - val_accuracy: 0.6304\n","Epoch 33/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6617 - accuracy: 0.7503 - val_loss: 0.9144 - val_accuracy: 0.6347\n","Epoch 34/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6579 - accuracy: 0.7495 - val_loss: 0.9216 - val_accuracy: 0.6272\n","Epoch 35/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6558 - accuracy: 0.7570 - val_loss: 0.9551 - val_accuracy: 0.6390\n","Epoch 36/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6545 - accuracy: 0.7548 - val_loss: 0.9026 - val_accuracy: 0.6412\n","Epoch 37/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6520 - accuracy: 0.7602 - val_loss: 0.9134 - val_accuracy: 0.6347\n","Epoch 38/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6497 - accuracy: 0.7589 - val_loss: 0.9008 - val_accuracy: 0.6347\n","Epoch 39/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6455 - accuracy: 0.7629 - val_loss: 0.9114 - val_accuracy: 0.6293\n","Epoch 40/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6469 - accuracy: 0.7546 - val_loss: 0.9147 - val_accuracy: 0.6347\n","Epoch 41/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6479 - accuracy: 0.7608 - val_loss: 0.9230 - val_accuracy: 0.6315\n","Epoch 42/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6449 - accuracy: 0.7597 - val_loss: 0.9665 - val_accuracy: 0.6228\n","Epoch 43/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6392 - accuracy: 0.7702 - val_loss: 0.9089 - val_accuracy: 0.6390\n","Epoch 44/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6396 - accuracy: 0.7691 - val_loss: 0.9253 - val_accuracy: 0.6347\n","Epoch 45/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.6425 - accuracy: 0.7527 - val_loss: 0.8994 - val_accuracy: 0.6509\n","Epoch 46/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6362 - accuracy: 0.7716 - val_loss: 0.9503 - val_accuracy: 0.6358\n","Epoch 47/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6334 - accuracy: 0.7697 - val_loss: 0.9160 - val_accuracy: 0.6336\n","Epoch 48/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6324 - accuracy: 0.7686 - val_loss: 0.9273 - val_accuracy: 0.6379\n","Epoch 49/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6310 - accuracy: 0.7707 - val_loss: 0.9526 - val_accuracy: 0.6390\n","Epoch 50/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6301 - accuracy: 0.7745 - val_loss: 0.9913 - val_accuracy: 0.6444\n","Epoch 51/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6337 - accuracy: 0.7632 - val_loss: 0.9151 - val_accuracy: 0.6379\n","Epoch 52/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6321 - accuracy: 0.7667 - val_loss: 0.9362 - val_accuracy: 0.6293\n","Epoch 53/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6227 - accuracy: 0.7756 - val_loss: 0.9876 - val_accuracy: 0.6282\n","Epoch 54/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6227 - accuracy: 0.7807 - val_loss: 0.9707 - val_accuracy: 0.6304\n","Epoch 55/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6176 - accuracy: 0.7767 - val_loss: 0.9472 - val_accuracy: 0.6487\n","Epoch 56/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6163 - accuracy: 0.7756 - val_loss: 0.9399 - val_accuracy: 0.6379\n","Epoch 57/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6186 - accuracy: 0.7740 - val_loss: 0.9402 - val_accuracy: 0.6422\n","Epoch 58/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6133 - accuracy: 0.7791 - val_loss: 0.9559 - val_accuracy: 0.6185\n","Epoch 59/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6141 - accuracy: 0.7756 - val_loss: 0.9368 - val_accuracy: 0.6433\n","Epoch 60/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6104 - accuracy: 0.7772 - val_loss: 0.9880 - val_accuracy: 0.6239\n","Epoch 61/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6071 - accuracy: 0.7848 - val_loss: 0.9606 - val_accuracy: 0.6315\n","Epoch 62/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6027 - accuracy: 0.7885 - val_loss: 0.9517 - val_accuracy: 0.6207\n","Epoch 63/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6092 - accuracy: 0.7839 - val_loss: 0.9263 - val_accuracy: 0.6315\n","Epoch 64/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6078 - accuracy: 0.7823 - val_loss: 0.9574 - val_accuracy: 0.6412\n","Epoch 65/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6100 - accuracy: 0.7821 - val_loss: 0.9380 - val_accuracy: 0.6444\n","Epoch 66/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5986 - accuracy: 0.7869 - val_loss: 1.0403 - val_accuracy: 0.6024\n","Epoch 67/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6094 - accuracy: 0.7732 - val_loss: 1.0253 - val_accuracy: 0.6034\n","Epoch 68/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6058 - accuracy: 0.7802 - val_loss: 1.0077 - val_accuracy: 0.6088\n","Epoch 69/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5902 - accuracy: 0.7955 - val_loss: 0.9759 - val_accuracy: 0.6412\n","Epoch 70/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5922 - accuracy: 0.7874 - val_loss: 0.9732 - val_accuracy: 0.6282\n","Epoch 71/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5889 - accuracy: 0.7955 - val_loss: 1.0231 - val_accuracy: 0.6056\n","Epoch 72/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5915 - accuracy: 0.7912 - val_loss: 0.9489 - val_accuracy: 0.6250\n","Epoch 73/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5844 - accuracy: 0.7920 - val_loss: 0.9782 - val_accuracy: 0.6304\n","Epoch 74/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5838 - accuracy: 0.7990 - val_loss: 0.9586 - val_accuracy: 0.6476\n","Epoch 75/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5905 - accuracy: 0.7955 - val_loss: 0.9640 - val_accuracy: 0.6239\n","Epoch 76/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5796 - accuracy: 0.8041 - val_loss: 1.0045 - val_accuracy: 0.6153\n","Epoch 77/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5772 - accuracy: 0.8025 - val_loss: 0.9951 - val_accuracy: 0.6455\n","Epoch 78/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5809 - accuracy: 0.7928 - val_loss: 0.9757 - val_accuracy: 0.6282\n","Epoch 79/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5766 - accuracy: 0.7920 - val_loss: 0.9996 - val_accuracy: 0.6315\n","Epoch 80/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5728 - accuracy: 0.8060 - val_loss: 0.9831 - val_accuracy: 0.6422\n","Epoch 81/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5734 - accuracy: 0.8050 - val_loss: 1.0068 - val_accuracy: 0.6369\n","Epoch 82/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5715 - accuracy: 0.8001 - val_loss: 0.9814 - val_accuracy: 0.6498\n","Epoch 83/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5692 - accuracy: 0.8098 - val_loss: 1.0339 - val_accuracy: 0.6078\n","Epoch 84/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5728 - accuracy: 0.7993 - val_loss: 1.0417 - val_accuracy: 0.6455\n","Epoch 85/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5698 - accuracy: 0.7998 - val_loss: 1.0309 - val_accuracy: 0.6422\n","Epoch 86/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5667 - accuracy: 0.8082 - val_loss: 1.0005 - val_accuracy: 0.6228\n","Epoch 87/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5583 - accuracy: 0.8138 - val_loss: 1.0006 - val_accuracy: 0.6250\n","Epoch 88/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5587 - accuracy: 0.8087 - val_loss: 1.0331 - val_accuracy: 0.6228\n","Epoch 89/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5580 - accuracy: 0.8079 - val_loss: 0.9718 - val_accuracy: 0.6412\n","Epoch 90/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5669 - accuracy: 0.8041 - val_loss: 1.1257 - val_accuracy: 0.6056\n","Epoch 91/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5637 - accuracy: 0.7953 - val_loss: 1.0769 - val_accuracy: 0.6131\n","Epoch 92/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5643 - accuracy: 0.7969 - val_loss: 1.0315 - val_accuracy: 0.6369\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5511 - accuracy: 0.8141 - val_loss: 1.0745 - val_accuracy: 0.6228\n","Epoch 94/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5527 - accuracy: 0.8155 - val_loss: 1.0991 - val_accuracy: 0.6293\n","Epoch 95/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5487 - accuracy: 0.8079 - val_loss: 0.9900 - val_accuracy: 0.6358\n","Epoch 96/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5486 - accuracy: 0.8147 - val_loss: 0.9875 - val_accuracy: 0.6293\n","Epoch 97/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5527 - accuracy: 0.8093 - val_loss: 1.0209 - val_accuracy: 0.6369\n","Epoch 98/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5462 - accuracy: 0.8165 - val_loss: 1.0478 - val_accuracy: 0.6088\n","Epoch 99/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5464 - accuracy: 0.8130 - val_loss: 0.9964 - val_accuracy: 0.6369\n","Epoch 100/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5473 - accuracy: 0.8230 - val_loss: 0.9957 - val_accuracy: 0.6315\n","{'loss': [0.7454404234886169, 0.7312006950378418, 0.7285065054893494, 0.7238014340400696, 0.7179071307182312, 0.7226685881614685, 0.7181582450866699, 0.7172908782958984, 0.7114635109901428, 0.7071697115898132, 0.7042020559310913, 0.7042114734649658, 0.70158851146698, 0.7000516653060913, 0.6974625587463379, 0.6984567046165466, 0.692969024181366, 0.6897664666175842, 0.6906608939170837, 0.6864611506462097, 0.6870172023773193, 0.6823705434799194, 0.6815134286880493, 0.6779046654701233, 0.677874743938446, 0.6764336228370667, 0.6736425161361694, 0.6735127568244934, 0.666718602180481, 0.6693488359451294, 0.6665322780609131, 0.668993353843689, 0.6617164611816406, 0.6579219102859497, 0.6557788252830505, 0.6545188426971436, 0.6520212888717651, 0.6496661901473999, 0.6454519629478455, 0.6469434499740601, 0.6479479670524597, 0.6449191570281982, 0.6391571760177612, 0.6396111249923706, 0.6425337791442871, 0.6361943483352661, 0.6333872675895691, 0.632384717464447, 0.6309829950332642, 0.6300799250602722, 0.6337395906448364, 0.6321076154708862, 0.6227085590362549, 0.6227340698242188, 0.6176097989082336, 0.6163343787193298, 0.6186096668243408, 0.6132837533950806, 0.6141385436058044, 0.6103723049163818, 0.6070690751075745, 0.6026540398597717, 0.6091570258140564, 0.6078158020973206, 0.6100381016731262, 0.5986348986625671, 0.6094200611114502, 0.6057700514793396, 0.590237557888031, 0.5922414660453796, 0.5888638496398926, 0.5915400981903076, 0.5843717455863953, 0.5838479399681091, 0.5905089378356934, 0.5796009302139282, 0.5771600008010864, 0.580930769443512, 0.5765838623046875, 0.5727670192718506, 0.5734483003616333, 0.5714769959449768, 0.5691651105880737, 0.572804868221283, 0.569836437702179, 0.566668689250946, 0.5583249926567078, 0.558655321598053, 0.5580223798751831, 0.5669289827346802, 0.5636882781982422, 0.5642527937889099, 0.5510575175285339, 0.5526664853096008, 0.5487483739852905, 0.5486003160476685, 0.5526858568191528, 0.5462226867675781, 0.5464140772819519, 0.5472627878189087], 'accuracy': [0.6977370977401733, 0.7058189511299133, 0.720366358757019, 0.7136314511299133, 0.7211745977401733, 0.7087823152542114, 0.7230603694915771, 0.7184805870056152, 0.7198275923728943, 0.7300646305084229, 0.7176724076271057, 0.7257543206214905, 0.7238685488700867, 0.7354525923728943, 0.7209051847457886, 0.7314116358757019, 0.7346444129943848, 0.7349137663841248, 0.7271012663841248, 0.7322198152542114, 0.7376077771186829, 0.7378771305084229, 0.7384159564971924, 0.7424569129943848, 0.743534505367279, 0.7448814511299133, 0.7464978694915771, 0.7421875, 0.7497305870056152, 0.7494612336158752, 0.7540409564971924, 0.7467672228813171, 0.7502694129943848, 0.7494612336158752, 0.7570043206214905, 0.7548491358757019, 0.7602370977401733, 0.7588900923728943, 0.7629310488700867, 0.7545797228813171, 0.7607758641242981, 0.7596982717514038, 0.7702047228813171, 0.7691271305084229, 0.7526939511299133, 0.7715517282485962, 0.7696659564971924, 0.7685883641242981, 0.7707435488700867, 0.7745150923728943, 0.7632004022598267, 0.7667025923728943, 0.7755926847457886, 0.7807112336158752, 0.7766702771186829, 0.7755926847457886, 0.7739762663841248, 0.7790948152542114, 0.7755926847457886, 0.7772090435028076, 0.7847521305084229, 0.7885237336158752, 0.7839439511299133, 0.7823275923728943, 0.7820581793785095, 0.7869073152542114, 0.7731680870056152, 0.7801724076271057, 0.795527994632721, 0.787446141242981, 0.795527994632721, 0.7912176847457886, 0.7920258641242981, 0.7990301847457886, 0.795527994632721, 0.8041487336158752, 0.8025323152542114, 0.7928340435028076, 0.7920258641242981, 0.806034505367279, 0.8049569129943848, 0.8001077771186829, 0.8098060488700867, 0.7992995977401733, 0.7998383641242981, 0.8081896305084229, 0.813847005367279, 0.8087284564971924, 0.8079202771186829, 0.8041487336158752, 0.795258641242981, 0.796875, 0.814116358757019, 0.8154633641242981, 0.8079202771186829, 0.8146551847457886, 0.8092672228813171, 0.8165409564971924, 0.8130387663841248, 0.8230064511299133], 'val_loss': [0.888603925704956, 0.8860079050064087, 0.8871350884437561, 0.8815300464630127, 0.8783455491065979, 0.883054256439209, 0.8801365494728088, 0.8740912079811096, 0.8733757138252258, 0.8749554753303528, 0.8700067400932312, 0.8566440939903259, 0.8636502027511597, 0.8586584329605103, 0.8469246029853821, 0.8479701280593872, 0.8543044924736023, 0.8517850637435913, 0.8481511473655701, 0.8396826386451721, 0.8411129117012024, 0.9054116010665894, 0.8567304015159607, 0.8788802027702332, 0.8631076216697693, 0.8725138902664185, 0.8854981660842896, 0.8826742768287659, 0.89427649974823, 0.91513991355896, 0.9593346118927002, 0.9302040338516235, 0.9144301414489746, 0.9215890169143677, 0.9550527334213257, 0.9025622010231018, 0.9133797883987427, 0.9007678627967834, 0.9114038944244385, 0.9146519303321838, 0.9230145215988159, 0.9664673209190369, 0.9088647365570068, 0.9253382682800293, 0.8993611335754395, 0.9503446221351624, 0.9160455465316772, 0.9273355007171631, 0.9525684714317322, 0.991338312625885, 0.9150695204734802, 0.936248242855072, 0.9875789284706116, 0.9706815481185913, 0.9471989870071411, 0.9399101734161377, 0.9402480125427246, 0.9558607339859009, 0.9367952942848206, 0.988025426864624, 0.960648238658905, 0.951657235622406, 0.9262659549713135, 0.9574035406112671, 0.9379505515098572, 1.0403484106063843, 1.0252695083618164, 1.007699728012085, 0.9759015440940857, 0.9731587171554565, 1.023073434829712, 0.9489245414733887, 0.9781754016876221, 0.9585780501365662, 0.9640443921089172, 1.0045298337936401, 0.9950617551803589, 0.9756588339805603, 0.9995667934417725, 0.9830939769744873, 1.0068199634552002, 0.9814366102218628, 1.033850908279419, 1.0417152643203735, 1.030874490737915, 1.0005156993865967, 1.0006234645843506, 1.0331151485443115, 0.9718450307846069, 1.1256943941116333, 1.0769017934799194, 1.0314748287200928, 1.0744872093200684, 1.0990898609161377, 0.9899959564208984, 0.9875228404998779, 1.020854115486145, 1.0477982759475708, 0.9963936805725098, 0.9956512451171875], 'val_accuracy': [0.5032327771186829, 0.506465494632721, 0.5, 0.600215494632721, 0.5668103694915771, 0.5915948152542114, 0.5129310488700867, 0.5280172228813171, 0.5829741358757019, 0.53125, 0.6163793206214905, 0.5969827771186829, 0.5743534564971924, 0.600215494632721, 0.618534505367279, 0.6196120977401733, 0.625, 0.6271551847457886, 0.6228448152542114, 0.642241358757019, 0.6411637663841248, 0.5969827771186829, 0.6379310488700867, 0.6357758641242981, 0.639008641242981, 0.6293103694915771, 0.6357758641242981, 0.6476293206214905, 0.6379310488700867, 0.6465517282485962, 0.6260775923728943, 0.6303879022598267, 0.6346982717514038, 0.6271551847457886, 0.639008641242981, 0.6411637663841248, 0.6346982717514038, 0.6346982717514038, 0.6293103694915771, 0.6346982717514038, 0.631465494632721, 0.6228448152542114, 0.639008641242981, 0.6346982717514038, 0.6508620977401733, 0.6357758641242981, 0.6336206793785095, 0.6379310488700867, 0.639008641242981, 0.6443965435028076, 0.6379310488700867, 0.6293103694915771, 0.6282327771186829, 0.6303879022598267, 0.6487069129943848, 0.6379310488700867, 0.642241358757019, 0.618534505367279, 0.6433189511299133, 0.6239224076271057, 0.631465494632721, 0.6206896305084229, 0.631465494632721, 0.6411637663841248, 0.6443965435028076, 0.6023706793785095, 0.6034482717514038, 0.6088362336158752, 0.6411637663841248, 0.6282327771186829, 0.6056034564971924, 0.625, 0.6303879022598267, 0.6476293206214905, 0.6239224076271057, 0.6153017282485962, 0.6454741358757019, 0.6282327771186829, 0.631465494632721, 0.642241358757019, 0.6368534564971924, 0.649784505367279, 0.607758641242981, 0.6454741358757019, 0.642241358757019, 0.6228448152542114, 0.625, 0.6228448152542114, 0.6411637663841248, 0.6056034564971924, 0.6131465435028076, 0.6368534564971924, 0.6228448152542114, 0.6293103694915771, 0.6357758641242981, 0.6293103694915771, 0.6368534564971924, 0.6088362336158752, 0.6368534564971924, 0.631465494632721]}\n","38/38 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 4s 35ms/step - loss: 0.7422 - accuracy: 0.7012 - val_loss: 0.8843 - val_accuracy: 0.5294\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["28/28 [==============================] - 1s 22ms/step - loss: 0.7286 - accuracy: 0.7111 - val_loss: 0.8814 - val_accuracy: 0.5679\n","Epoch 3/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7255 - accuracy: 0.7159 - val_loss: 0.8795 - val_accuracy: 0.5690\n","Epoch 4/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.7206 - accuracy: 0.7252 - val_loss: 0.8774 - val_accuracy: 0.6018\n","Epoch 5/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7206 - accuracy: 0.7210 - val_loss: 0.8735 - val_accuracy: 0.5769\n","Epoch 6/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.7217 - accuracy: 0.7235 - val_loss: 0.8713 - val_accuracy: 0.6075\n","Epoch 7/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7147 - accuracy: 0.7235 - val_loss: 0.8728 - val_accuracy: 0.5498\n","Epoch 8/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7117 - accuracy: 0.7286 - val_loss: 0.8664 - val_accuracy: 0.5803\n","Epoch 9/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7120 - accuracy: 0.7201 - val_loss: 0.8812 - val_accuracy: 0.5305\n","Epoch 10/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7237 - accuracy: 0.7108 - val_loss: 0.8592 - val_accuracy: 0.5905\n","Epoch 11/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.7080 - accuracy: 0.7207 - val_loss: 0.8563 - val_accuracy: 0.6199\n","Epoch 12/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7040 - accuracy: 0.7383 - val_loss: 0.8535 - val_accuracy: 0.5894\n","Epoch 13/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.7038 - accuracy: 0.7360 - val_loss: 0.8475 - val_accuracy: 0.6267\n","Epoch 14/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6973 - accuracy: 0.7340 - val_loss: 0.8479 - val_accuracy: 0.6052\n","Epoch 15/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6982 - accuracy: 0.7340 - val_loss: 0.8482 - val_accuracy: 0.5848\n","Epoch 16/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6988 - accuracy: 0.7408 - val_loss: 0.8393 - val_accuracy: 0.6346\n","Epoch 17/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6926 - accuracy: 0.7380 - val_loss: 0.8381 - val_accuracy: 0.6256\n","Epoch 18/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6883 - accuracy: 0.7417 - val_loss: 0.8331 - val_accuracy: 0.6324\n","Epoch 19/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6903 - accuracy: 0.7428 - val_loss: 0.8412 - val_accuracy: 0.6267\n","Epoch 20/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6882 - accuracy: 0.7445 - val_loss: 0.8433 - val_accuracy: 0.6165\n","Epoch 21/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6883 - accuracy: 0.7411 - val_loss: 0.8392 - val_accuracy: 0.6380\n","Epoch 22/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6897 - accuracy: 0.7400 - val_loss: 0.8413 - val_accuracy: 0.6278\n","Epoch 23/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6845 - accuracy: 0.7414 - val_loss: 0.8567 - val_accuracy: 0.6290\n","Epoch 24/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6801 - accuracy: 0.7459 - val_loss: 0.8467 - val_accuracy: 0.6369\n","Epoch 25/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6773 - accuracy: 0.7442 - val_loss: 0.8452 - val_accuracy: 0.6448\n","Epoch 26/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6744 - accuracy: 0.7496 - val_loss: 0.8572 - val_accuracy: 0.6380\n","Epoch 27/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6726 - accuracy: 0.7581 - val_loss: 0.8576 - val_accuracy: 0.6346\n","Epoch 28/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6705 - accuracy: 0.7547 - val_loss: 0.8567 - val_accuracy: 0.6346\n","Epoch 29/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6696 - accuracy: 0.7575 - val_loss: 0.8732 - val_accuracy: 0.6301\n","Epoch 30/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6705 - accuracy: 0.7561 - val_loss: 0.8757 - val_accuracy: 0.6278\n","Epoch 31/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6739 - accuracy: 0.7419 - val_loss: 0.8790 - val_accuracy: 0.6380\n","Epoch 32/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6654 - accuracy: 0.7586 - val_loss: 0.8819 - val_accuracy: 0.6403\n","Epoch 33/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6607 - accuracy: 0.7615 - val_loss: 0.8783 - val_accuracy: 0.6403\n","Epoch 34/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6581 - accuracy: 0.7615 - val_loss: 0.8914 - val_accuracy: 0.6357\n","Epoch 35/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6602 - accuracy: 0.7564 - val_loss: 0.9105 - val_accuracy: 0.6278\n","Epoch 36/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.6595 - accuracy: 0.7615 - val_loss: 0.8970 - val_accuracy: 0.6335\n","Epoch 37/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6618 - accuracy: 0.7615 - val_loss: 0.9075 - val_accuracy: 0.6267\n","Epoch 38/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6500 - accuracy: 0.7643 - val_loss: 0.8994 - val_accuracy: 0.6290\n","Epoch 39/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6496 - accuracy: 0.7680 - val_loss: 0.8961 - val_accuracy: 0.6335\n","Epoch 40/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.6485 - accuracy: 0.7688 - val_loss: 0.8897 - val_accuracy: 0.6335\n","Epoch 41/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6545 - accuracy: 0.7547 - val_loss: 0.9198 - val_accuracy: 0.6233\n","Epoch 42/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6442 - accuracy: 0.7666 - val_loss: 0.9015 - val_accuracy: 0.6244\n","Epoch 43/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.6480 - accuracy: 0.7657 - val_loss: 0.9294 - val_accuracy: 0.6256\n","Epoch 44/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.6426 - accuracy: 0.7694 - val_loss: 0.9290 - val_accuracy: 0.6324\n","Epoch 45/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6446 - accuracy: 0.7629 - val_loss: 0.9255 - val_accuracy: 0.6244\n","Epoch 46/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6439 - accuracy: 0.7663 - val_loss: 0.9287 - val_accuracy: 0.6075\n","Epoch 47/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6406 - accuracy: 0.7731 - val_loss: 0.9166 - val_accuracy: 0.6301\n","Epoch 48/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6368 - accuracy: 0.7731 - val_loss: 0.9146 - val_accuracy: 0.6244\n","Epoch 49/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6366 - accuracy: 0.7671 - val_loss: 0.9531 - val_accuracy: 0.6131\n","Epoch 50/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6325 - accuracy: 0.7762 - val_loss: 0.9289 - val_accuracy: 0.6210\n","Epoch 51/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6291 - accuracy: 0.7779 - val_loss: 0.9386 - val_accuracy: 0.6324\n","Epoch 52/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6293 - accuracy: 0.7790 - val_loss: 0.9381 - val_accuracy: 0.6210\n","Epoch 53/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6238 - accuracy: 0.7722 - val_loss: 0.9668 - val_accuracy: 0.6244\n","Epoch 54/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6290 - accuracy: 0.7742 - val_loss: 0.9301 - val_accuracy: 0.6290\n","Epoch 55/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6289 - accuracy: 0.7762 - val_loss: 0.9417 - val_accuracy: 0.6369\n","Epoch 56/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6298 - accuracy: 0.7733 - val_loss: 0.9503 - val_accuracy: 0.6301\n","Epoch 57/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6288 - accuracy: 0.7699 - val_loss: 0.9514 - val_accuracy: 0.6335\n","Epoch 58/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6249 - accuracy: 0.7748 - val_loss: 0.9582 - val_accuracy: 0.6199\n","Epoch 59/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6154 - accuracy: 0.7875 - val_loss: 0.9623 - val_accuracy: 0.6267\n","Epoch 60/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6228 - accuracy: 0.7776 - val_loss: 0.9960 - val_accuracy: 0.6301\n","Epoch 61/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6214 - accuracy: 0.7895 - val_loss: 0.9465 - val_accuracy: 0.6267\n","Epoch 62/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6130 - accuracy: 0.7799 - val_loss: 0.9856 - val_accuracy: 0.6256\n","Epoch 63/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6104 - accuracy: 0.7869 - val_loss: 0.9719 - val_accuracy: 0.6199\n","Epoch 64/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6093 - accuracy: 0.7858 - val_loss: 0.9887 - val_accuracy: 0.6222\n","Epoch 65/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6093 - accuracy: 0.7866 - val_loss: 0.9781 - val_accuracy: 0.6199\n","Epoch 66/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6040 - accuracy: 0.7920 - val_loss: 0.9677 - val_accuracy: 0.6256\n","Epoch 67/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6119 - accuracy: 0.7827 - val_loss: 0.9720 - val_accuracy: 0.6346\n","Epoch 68/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6152 - accuracy: 0.7807 - val_loss: 0.9786 - val_accuracy: 0.6267\n","Epoch 69/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6028 - accuracy: 0.7852 - val_loss: 1.0024 - val_accuracy: 0.6165\n","Epoch 70/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.6018 - accuracy: 0.7889 - val_loss: 0.9748 - val_accuracy: 0.6267\n","Epoch 71/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6031 - accuracy: 0.7835 - val_loss: 0.9818 - val_accuracy: 0.6301\n","Epoch 72/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5996 - accuracy: 0.7909 - val_loss: 1.0092 - val_accuracy: 0.6256\n","Epoch 73/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5985 - accuracy: 0.7906 - val_loss: 0.9912 - val_accuracy: 0.6267\n","Epoch 74/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5996 - accuracy: 0.7892 - val_loss: 0.9829 - val_accuracy: 0.6176\n","Epoch 75/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5914 - accuracy: 0.7971 - val_loss: 1.0105 - val_accuracy: 0.6199\n","Epoch 76/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5918 - accuracy: 0.7940 - val_loss: 1.0122 - val_accuracy: 0.6199\n","Epoch 77/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5859 - accuracy: 0.7985 - val_loss: 1.0157 - val_accuracy: 0.6357\n","Epoch 78/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5921 - accuracy: 0.7932 - val_loss: 1.0577 - val_accuracy: 0.6222\n","Epoch 79/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5959 - accuracy: 0.7977 - val_loss: 0.9948 - val_accuracy: 0.6165\n","Epoch 80/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5921 - accuracy: 0.7849 - val_loss: 1.0330 - val_accuracy: 0.6120\n","Epoch 81/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5880 - accuracy: 0.7963 - val_loss: 1.0161 - val_accuracy: 0.6176\n","Epoch 82/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5946 - accuracy: 0.7982 - val_loss: 1.0290 - val_accuracy: 0.6109\n","Epoch 83/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5841 - accuracy: 0.7957 - val_loss: 1.0319 - val_accuracy: 0.6278\n","Epoch 84/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5792 - accuracy: 0.8045 - val_loss: 1.0330 - val_accuracy: 0.6290\n","Epoch 85/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5755 - accuracy: 0.8065 - val_loss: 1.0348 - val_accuracy: 0.6357\n","Epoch 86/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5772 - accuracy: 0.8008 - val_loss: 1.0396 - val_accuracy: 0.6256\n","Epoch 87/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5739 - accuracy: 0.8031 - val_loss: 1.0305 - val_accuracy: 0.6165\n","Epoch 88/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5713 - accuracy: 0.8104 - val_loss: 1.0467 - val_accuracy: 0.6256\n","Epoch 89/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5753 - accuracy: 0.8002 - val_loss: 1.0715 - val_accuracy: 0.6233\n","Epoch 90/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5839 - accuracy: 0.8065 - val_loss: 1.0508 - val_accuracy: 0.6188\n","Epoch 91/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5678 - accuracy: 0.8101 - val_loss: 1.0584 - val_accuracy: 0.6131\n","Epoch 92/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5644 - accuracy: 0.8118 - val_loss: 1.0473 - val_accuracy: 0.6109\n","Epoch 93/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5624 - accuracy: 0.8181 - val_loss: 1.0523 - val_accuracy: 0.6165\n","Epoch 94/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5612 - accuracy: 0.8158 - val_loss: 1.0684 - val_accuracy: 0.6210\n","Epoch 95/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5696 - accuracy: 0.7977 - val_loss: 1.0651 - val_accuracy: 0.6199\n","Epoch 96/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5644 - accuracy: 0.8033 - val_loss: 1.1212 - val_accuracy: 0.6018\n","Epoch 97/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5601 - accuracy: 0.8110 - val_loss: 1.0704 - val_accuracy: 0.6199\n","Epoch 98/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5697 - accuracy: 0.8028 - val_loss: 1.0597 - val_accuracy: 0.6154\n","Epoch 99/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5705 - accuracy: 0.8076 - val_loss: 1.1538 - val_accuracy: 0.6143\n","Epoch 100/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5661 - accuracy: 0.8070 - val_loss: 1.0675 - val_accuracy: 0.6222\n","{'loss': [0.7422006726264954, 0.7285791635513306, 0.7254572510719299, 0.7206398248672485, 0.7206040024757385, 0.7216694355010986, 0.714702844619751, 0.7117369174957275, 0.7119660973548889, 0.7236734628677368, 0.7080305814743042, 0.7039835453033447, 0.7038472890853882, 0.6972836852073669, 0.6981762051582336, 0.6988164782524109, 0.6926483511924744, 0.688300609588623, 0.6903449892997742, 0.6881725788116455, 0.6883394718170166, 0.6896500587463379, 0.6845291256904602, 0.6800625324249268, 0.6772699356079102, 0.6743608713150024, 0.6725892424583435, 0.6704734563827515, 0.6696107387542725, 0.6705449223518372, 0.6739169955253601, 0.6654145121574402, 0.6606991291046143, 0.6580732464790344, 0.6602078080177307, 0.6595146656036377, 0.6618312001228333, 0.6499876379966736, 0.6496495008468628, 0.6484881639480591, 0.6545133590698242, 0.6441669464111328, 0.648014485836029, 0.6425897479057312, 0.6445938348770142, 0.6439261436462402, 0.640554666519165, 0.636776864528656, 0.6365867853164673, 0.6325299739837646, 0.6291035413742065, 0.6292984485626221, 0.6237989068031311, 0.6289815306663513, 0.6289063692092896, 0.629813015460968, 0.628765344619751, 0.6248520016670227, 0.6154164671897888, 0.6228083372116089, 0.621412456035614, 0.6129639744758606, 0.6103541254997253, 0.609313428401947, 0.6092868447303772, 0.6039575934410095, 0.6119446754455566, 0.6151785850524902, 0.6027973890304565, 0.6017553210258484, 0.603103518486023, 0.5995984673500061, 0.5985227227210999, 0.5996038913726807, 0.5913828611373901, 0.5918111801147461, 0.5858640074729919, 0.592125654220581, 0.5958735346794128, 0.5920735001564026, 0.5879724025726318, 0.5946487784385681, 0.5840641260147095, 0.5792025923728943, 0.5754615068435669, 0.5772014260292053, 0.5739321112632751, 0.5713317394256592, 0.5752744078636169, 0.5838851928710938, 0.5677621364593506, 0.5643505454063416, 0.5624240040779114, 0.5611578822135925, 0.5695992112159729, 0.5644124150276184, 0.5600610375404358, 0.5697101950645447, 0.5705239176750183, 0.5661130547523499], 'accuracy': [0.7011884450912476, 0.7110922336578369, 0.7159026861190796, 0.7252405285835266, 0.7209960222244263, 0.7235427498817444, 0.7235427498817444, 0.7286360859870911, 0.7201471328735352, 0.7108092904090881, 0.7207130789756775, 0.7382569313049316, 0.7359932065010071, 0.7340124249458313, 0.7340124249458313, 0.740803599357605, 0.7379739880561829, 0.7416524887084961, 0.7427843809127808, 0.744482159614563, 0.7410866022109985, 0.7399547100067139, 0.7413695454597473, 0.7458969950675964, 0.7441992163658142, 0.7495755553245544, 0.7580645084381104, 0.7546689510345459, 0.757498562335968, 0.7560837864875793, 0.7419354915618896, 0.7586304545402527, 0.7614601254463196, 0.7614601254463196, 0.7563667297363281, 0.7614601254463196, 0.7614601254463196, 0.7642897367477417, 0.7679682970046997, 0.7688171863555908, 0.7546689510345459, 0.7665534615516663, 0.7657045722007751, 0.7693831324577332, 0.7628749012947083, 0.7662705183029175, 0.7730616927146912, 0.7730616927146912, 0.7671194076538086, 0.7761743068695068, 0.7778720855712891, 0.7790039777755737, 0.7722128033638, 0.774193525314331, 0.7761743068695068, 0.7733446359634399, 0.7699490785598755, 0.7747594714164734, 0.7874929308891296, 0.7775891423225403, 0.7894737124443054, 0.7798528671264648, 0.7869269847869873, 0.7857951521873474, 0.7866440415382385, 0.7920203804969788, 0.7826825380325317, 0.780701756477356, 0.7852292060852051, 0.7889077663421631, 0.7835314273834229, 0.7908884882926941, 0.7906055450439453, 0.7891907095909119, 0.7971137762069702, 0.7940011024475098, 0.7985285520553589, 0.7931522130966187, 0.7976796627044678, 0.7849462628364563, 0.7962648272514343, 0.7982456088066101, 0.7956989407539368, 0.8044708371162415, 0.8064516186714172, 0.8007922768592834, 0.803056001663208, 0.810413122177124, 0.8002263903617859, 0.8064516186714172, 0.8101301789283752, 0.8118279576301575, 0.8180531859397888, 0.8157894611358643, 0.7976796627044678, 0.8033390045166016, 0.8109790682792664, 0.8027730584144592, 0.8075834512710571, 0.8070175647735596], 'val_loss': [0.8843158483505249, 0.8813768029212952, 0.8794973492622375, 0.8773907423019409, 0.8735466599464417, 0.8712878823280334, 0.8728379011154175, 0.8664335608482361, 0.8812389969825745, 0.8591863512992859, 0.8562734127044678, 0.8534783124923706, 0.8475192189216614, 0.8478900790214539, 0.8481529951095581, 0.839270830154419, 0.8380539417266846, 0.8330984711647034, 0.8412072658538818, 0.8433430790901184, 0.8391990661621094, 0.8412659764289856, 0.8566845655441284, 0.8466599583625793, 0.8451524376869202, 0.8572319746017456, 0.8576298952102661, 0.8566514253616333, 0.873180627822876, 0.8757261633872986, 0.8790282011032104, 0.8818609118461609, 0.8782945275306702, 0.8914234042167664, 0.9105173945426941, 0.8969725966453552, 0.9074850678443909, 0.8993887305259705, 0.8961373567581177, 0.8897233009338379, 0.919753909111023, 0.9014909267425537, 0.9293732643127441, 0.9289969205856323, 0.9254781603813171, 0.9287171363830566, 0.9165745973587036, 0.9145520329475403, 0.9531212449073792, 0.9289097189903259, 0.9386357069015503, 0.9381216764450073, 0.9668099880218506, 0.9300553202629089, 0.941737174987793, 0.950286328792572, 0.9514082670211792, 0.9582036733627319, 0.9622778296470642, 0.9960049390792847, 0.9465283155441284, 0.9856047034263611, 0.9719040989875793, 0.988679826259613, 0.9781105518341064, 0.9677135348320007, 0.9719903469085693, 0.9785743355751038, 1.0023527145385742, 0.9748407006263733, 0.9818337559700012, 1.0091630220413208, 0.9912353754043579, 0.9828914403915405, 1.0105338096618652, 1.0122215747833252, 1.0156745910644531, 1.0576677322387695, 0.9948258399963379, 1.0329762697219849, 1.0160750150680542, 1.0289946794509888, 1.0319260358810425, 1.033005952835083, 1.0347537994384766, 1.039631962776184, 1.0304728746414185, 1.0466886758804321, 1.0715464353561401, 1.0508463382720947, 1.0583858489990234, 1.0473036766052246, 1.0522677898406982, 1.0684415102005005, 1.065065860748291, 1.1212140321731567, 1.0703887939453125, 1.0597162246704102, 1.1537538766860962, 1.0675184726715088], 'val_accuracy': [0.529411792755127, 0.5678732991218567, 0.5690045356750488, 0.6018099784851074, 0.5769230723381042, 0.6074660420417786, 0.5497737526893616, 0.5803167223930359, 0.5305429697036743, 0.5904977321624756, 0.6199095249176025, 0.5893664956092834, 0.6266968250274658, 0.6052036285400391, 0.5848416090011597, 0.6346153616905212, 0.6255655884742737, 0.6323529481887817, 0.6266968250274658, 0.6165158152580261, 0.6380090713500977, 0.627828061580658, 0.6289592981338501, 0.6368778347969055, 0.6447963714599609, 0.6380090713500977, 0.6346153616905212, 0.6346153616905212, 0.6300904750823975, 0.627828061580658, 0.6380090713500977, 0.6402714848518372, 0.6402714848518372, 0.6357465982437134, 0.627828061580658, 0.6334841847419739, 0.6266968250274658, 0.6289592981338501, 0.6334841847419739, 0.6334841847419739, 0.6233031749725342, 0.6244344115257263, 0.6255655884742737, 0.6323529481887817, 0.6244344115257263, 0.6074660420417786, 0.6300904750823975, 0.6244344115257263, 0.6131221652030945, 0.6210407018661499, 0.6323529481887817, 0.6210407018661499, 0.6244344115257263, 0.6289592981338501, 0.6368778347969055, 0.6300904750823975, 0.6334841847419739, 0.6199095249176025, 0.6266968250274658, 0.6300904750823975, 0.6266968250274658, 0.6255655884742737, 0.6199095249176025, 0.622171938419342, 0.6199095249176025, 0.6255655884742737, 0.6346153616905212, 0.6266968250274658, 0.6165158152580261, 0.6266968250274658, 0.6300904750823975, 0.6255655884742737, 0.6266968250274658, 0.6176470518112183, 0.6199095249176025, 0.6199095249176025, 0.6357465982437134, 0.622171938419342, 0.6165158152580261, 0.6119909286499023, 0.6176470518112183, 0.610859751701355, 0.627828061580658, 0.6289592981338501, 0.6357465982437134, 0.6255655884742737, 0.6165158152580261, 0.6255655884742737, 0.6233031749725342, 0.6187782883644104, 0.6131221652030945, 0.610859751701355, 0.6165158152580261, 0.6210407018661499, 0.6199095249176025, 0.6018099784851074, 0.6199095249176025, 0.6153846383094788, 0.6142534017562866, 0.622171938419342]}\n","45/45 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 4s 32ms/step - loss: 0.7533 - accuracy: 0.6961 - val_loss: 0.8861 - val_accuracy: 0.5279\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 0s 15ms/step - loss: 0.7506 - accuracy: 0.6990 - val_loss: 0.8884 - val_accuracy: 0.5176\n","Epoch 3/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7384 - accuracy: 0.7080 - val_loss: 0.8822 - val_accuracy: 0.5723\n","Epoch 4/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7349 - accuracy: 0.7124 - val_loss: 0.8806 - val_accuracy: 0.5475\n","Epoch 5/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7310 - accuracy: 0.7109 - val_loss: 0.8788 - val_accuracy: 0.5413\n","Epoch 6/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7282 - accuracy: 0.7116 - val_loss: 0.8786 - val_accuracy: 0.5238\n","Epoch 7/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7284 - accuracy: 0.7145 - val_loss: 0.8728 - val_accuracy: 0.5671\n","Epoch 8/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7332 - accuracy: 0.7129 - val_loss: 0.8733 - val_accuracy: 0.5465\n","Epoch 9/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7261 - accuracy: 0.7186 - val_loss: 0.8662 - val_accuracy: 0.5837\n","Epoch 10/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7236 - accuracy: 0.7212 - val_loss: 0.8647 - val_accuracy: 0.5785\n","Epoch 11/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7333 - accuracy: 0.7142 - val_loss: 0.8601 - val_accuracy: 0.5816\n","Epoch 12/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7195 - accuracy: 0.7271 - val_loss: 0.8611 - val_accuracy: 0.5826\n","Epoch 13/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.7155 - accuracy: 0.7217 - val_loss: 0.8577 - val_accuracy: 0.5888\n","Epoch 14/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.7126 - accuracy: 0.7323 - val_loss: 0.8539 - val_accuracy: 0.5919\n","Epoch 15/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7139 - accuracy: 0.7214 - val_loss: 0.8554 - val_accuracy: 0.5868\n","Epoch 16/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7204 - accuracy: 0.7101 - val_loss: 0.8492 - val_accuracy: 0.5692\n","Epoch 17/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7058 - accuracy: 0.7344 - val_loss: 0.8554 - val_accuracy: 0.5909\n","Epoch 18/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7075 - accuracy: 0.7284 - val_loss: 0.8491 - val_accuracy: 0.5909\n","Epoch 19/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7008 - accuracy: 0.7323 - val_loss: 0.8511 - val_accuracy: 0.5795\n","Epoch 20/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6989 - accuracy: 0.7339 - val_loss: 0.8579 - val_accuracy: 0.5723\n","Epoch 21/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6958 - accuracy: 0.7401 - val_loss: 0.8516 - val_accuracy: 0.5961\n","Epoch 22/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6959 - accuracy: 0.7297 - val_loss: 0.8611 - val_accuracy: 0.5764\n","Epoch 23/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6944 - accuracy: 0.7318 - val_loss: 0.8697 - val_accuracy: 0.5950\n","Epoch 24/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6916 - accuracy: 0.7375 - val_loss: 0.8712 - val_accuracy: 0.5909\n","Epoch 25/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6906 - accuracy: 0.7364 - val_loss: 0.8856 - val_accuracy: 0.5744\n","Epoch 26/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6917 - accuracy: 0.7364 - val_loss: 0.8654 - val_accuracy: 0.5971\n","Epoch 27/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6859 - accuracy: 0.7395 - val_loss: 0.8992 - val_accuracy: 0.5682\n","Epoch 28/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6823 - accuracy: 0.7437 - val_loss: 0.8805 - val_accuracy: 0.5733\n","Epoch 29/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6847 - accuracy: 0.7437 - val_loss: 0.8804 - val_accuracy: 0.5950\n","Epoch 30/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6868 - accuracy: 0.7367 - val_loss: 0.8788 - val_accuracy: 0.5919\n","Epoch 31/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6861 - accuracy: 0.7393 - val_loss: 0.8780 - val_accuracy: 0.5640\n","Epoch 32/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6755 - accuracy: 0.7380 - val_loss: 0.8981 - val_accuracy: 0.5775\n","Epoch 33/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6763 - accuracy: 0.7424 - val_loss: 0.9089 - val_accuracy: 0.5785\n","Epoch 34/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6716 - accuracy: 0.7494 - val_loss: 0.8874 - val_accuracy: 0.5599\n","Epoch 35/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6746 - accuracy: 0.7475 - val_loss: 0.8834 - val_accuracy: 0.5847\n","Epoch 36/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6842 - accuracy: 0.7351 - val_loss: 0.9136 - val_accuracy: 0.5775\n","Epoch 37/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6693 - accuracy: 0.7509 - val_loss: 0.9464 - val_accuracy: 0.5692\n","Epoch 38/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6710 - accuracy: 0.7444 - val_loss: 0.9586 - val_accuracy: 0.5733\n","Epoch 39/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6675 - accuracy: 0.7540 - val_loss: 0.9311 - val_accuracy: 0.5754\n","Epoch 40/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6628 - accuracy: 0.7535 - val_loss: 0.9130 - val_accuracy: 0.5713\n","Epoch 41/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6630 - accuracy: 0.7579 - val_loss: 0.9679 - val_accuracy: 0.5640\n","Epoch 42/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6642 - accuracy: 0.7460 - val_loss: 0.9103 - val_accuracy: 0.5671\n","Epoch 43/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.6709 - accuracy: 0.7419 - val_loss: 0.8929 - val_accuracy: 0.5754\n","Epoch 44/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6566 - accuracy: 0.7473 - val_loss: 0.9106 - val_accuracy: 0.5733\n","Epoch 45/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6571 - accuracy: 0.7597 - val_loss: 0.8970 - val_accuracy: 0.5723\n","Epoch 46/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6563 - accuracy: 0.7530 - val_loss: 0.9154 - val_accuracy: 0.5816\n","Epoch 47/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6506 - accuracy: 0.7579 - val_loss: 0.9245 - val_accuracy: 0.5940\n","Epoch 48/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6550 - accuracy: 0.7574 - val_loss: 0.9256 - val_accuracy: 0.5671\n","Epoch 49/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6489 - accuracy: 0.7607 - val_loss: 0.9359 - val_accuracy: 0.5702\n","Epoch 50/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6442 - accuracy: 0.7618 - val_loss: 0.9590 - val_accuracy: 0.5754\n","Epoch 51/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6439 - accuracy: 0.7708 - val_loss: 0.9458 - val_accuracy: 0.5764\n","Epoch 52/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6435 - accuracy: 0.7623 - val_loss: 0.9500 - val_accuracy: 0.5630\n","Epoch 53/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6469 - accuracy: 0.7597 - val_loss: 0.9656 - val_accuracy: 0.5692\n","Epoch 54/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6424 - accuracy: 0.7633 - val_loss: 0.9305 - val_accuracy: 0.5816\n","Epoch 55/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6387 - accuracy: 0.7680 - val_loss: 0.9292 - val_accuracy: 0.5733\n","Epoch 56/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6470 - accuracy: 0.7545 - val_loss: 0.9870 - val_accuracy: 0.5775\n","Epoch 57/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6359 - accuracy: 0.7729 - val_loss: 0.9259 - val_accuracy: 0.5919\n","Epoch 58/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6405 - accuracy: 0.7587 - val_loss: 0.9469 - val_accuracy: 0.5713\n","Epoch 59/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6401 - accuracy: 0.7602 - val_loss: 0.9483 - val_accuracy: 0.5733\n","Epoch 60/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6362 - accuracy: 0.7643 - val_loss: 0.9302 - val_accuracy: 0.5837\n","Epoch 61/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6371 - accuracy: 0.7674 - val_loss: 0.9238 - val_accuracy: 0.5527\n","Epoch 62/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6268 - accuracy: 0.7674 - val_loss: 0.9656 - val_accuracy: 0.5486\n","Epoch 63/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6279 - accuracy: 0.7685 - val_loss: 0.9485 - val_accuracy: 0.5692\n","Epoch 64/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6272 - accuracy: 0.7736 - val_loss: 0.9212 - val_accuracy: 0.5486\n","Epoch 65/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6225 - accuracy: 0.7724 - val_loss: 0.9253 - val_accuracy: 0.5671\n","Epoch 66/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6284 - accuracy: 0.7718 - val_loss: 0.9303 - val_accuracy: 0.5754\n","Epoch 67/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6247 - accuracy: 0.7798 - val_loss: 0.9533 - val_accuracy: 0.5795\n","Epoch 68/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6137 - accuracy: 0.7840 - val_loss: 0.9889 - val_accuracy: 0.5640\n","Epoch 69/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6149 - accuracy: 0.7829 - val_loss: 0.9819 - val_accuracy: 0.5723\n","Epoch 70/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6088 - accuracy: 0.7822 - val_loss: 0.9437 - val_accuracy: 0.5599\n","Epoch 71/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6138 - accuracy: 0.7806 - val_loss: 0.9460 - val_accuracy: 0.5630\n","Epoch 72/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6168 - accuracy: 0.7786 - val_loss: 0.9398 - val_accuracy: 0.5651\n","Epoch 73/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6071 - accuracy: 0.7829 - val_loss: 0.9519 - val_accuracy: 0.5579\n","Epoch 74/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6058 - accuracy: 0.7819 - val_loss: 0.9767 - val_accuracy: 0.5651\n","Epoch 75/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6106 - accuracy: 0.7835 - val_loss: 0.9439 - val_accuracy: 0.5620\n","Epoch 76/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6026 - accuracy: 0.7837 - val_loss: 0.9605 - val_accuracy: 0.5671\n","Epoch 77/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6004 - accuracy: 0.7879 - val_loss: 0.9476 - val_accuracy: 0.5620\n","Epoch 78/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6058 - accuracy: 0.7889 - val_loss: 0.9773 - val_accuracy: 0.5548\n","Epoch 79/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6035 - accuracy: 0.7879 - val_loss: 0.9844 - val_accuracy: 0.5589\n","Epoch 80/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6030 - accuracy: 0.7829 - val_loss: 1.0021 - val_accuracy: 0.5620\n","Epoch 81/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6056 - accuracy: 0.7747 - val_loss: 1.0209 - val_accuracy: 0.5455\n","Epoch 82/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5983 - accuracy: 0.7873 - val_loss: 1.0176 - val_accuracy: 0.5837\n","Epoch 83/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5973 - accuracy: 0.7902 - val_loss: 0.9779 - val_accuracy: 0.5692\n","Epoch 84/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5954 - accuracy: 0.7920 - val_loss: 0.9985 - val_accuracy: 0.5837\n","Epoch 85/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5883 - accuracy: 0.7969 - val_loss: 0.9942 - val_accuracy: 0.5589\n","Epoch 86/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5858 - accuracy: 0.7943 - val_loss: 1.0071 - val_accuracy: 0.5599\n","Epoch 87/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5854 - accuracy: 0.8023 - val_loss: 1.0226 - val_accuracy: 0.5537\n","Epoch 88/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5975 - accuracy: 0.7860 - val_loss: 1.0794 - val_accuracy: 0.5610\n","Epoch 89/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5904 - accuracy: 0.7868 - val_loss: 1.0386 - val_accuracy: 0.5527\n","Epoch 90/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5961 - accuracy: 0.7814 - val_loss: 1.0021 - val_accuracy: 0.5548\n","Epoch 91/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5933 - accuracy: 0.7899 - val_loss: 1.0117 - val_accuracy: 0.5465\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6095 - accuracy: 0.7873 - val_loss: 1.0134 - val_accuracy: 0.5610\n","Epoch 93/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5951 - accuracy: 0.7904 - val_loss: 1.0117 - val_accuracy: 0.5775\n","Epoch 94/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5800 - accuracy: 0.7961 - val_loss: 1.0509 - val_accuracy: 0.5744\n","Epoch 95/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5766 - accuracy: 0.7982 - val_loss: 1.0137 - val_accuracy: 0.5651\n","Epoch 96/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5902 - accuracy: 0.7907 - val_loss: 1.0280 - val_accuracy: 0.5517\n","Epoch 97/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5729 - accuracy: 0.7974 - val_loss: 1.0088 - val_accuracy: 0.5640\n","Epoch 98/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5757 - accuracy: 0.8039 - val_loss: 1.0113 - val_accuracy: 0.5558\n","Epoch 99/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5701 - accuracy: 0.8031 - val_loss: 1.0241 - val_accuracy: 0.5455\n","Epoch 100/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5794 - accuracy: 0.7987 - val_loss: 1.0537 - val_accuracy: 0.5579\n","{'loss': [0.7532522678375244, 0.7505614161491394, 0.7383813858032227, 0.7348624467849731, 0.7309973835945129, 0.7282282114028931, 0.7284225225448608, 0.7331886887550354, 0.726123034954071, 0.7236023545265198, 0.7333428859710693, 0.7194772362709045, 0.715471088886261, 0.7126175165176392, 0.713860273361206, 0.7203713059425354, 0.7058430314064026, 0.7075228691101074, 0.7007520198822021, 0.6989179849624634, 0.6957803964614868, 0.6958991289138794, 0.6944085359573364, 0.6915721893310547, 0.6905522346496582, 0.6917312741279602, 0.6859322190284729, 0.682296097278595, 0.6847401857376099, 0.6867720484733582, 0.6861134171485901, 0.6754974722862244, 0.6763483881950378, 0.6716235876083374, 0.6745957732200623, 0.6841534376144409, 0.6692587733268738, 0.6710227131843567, 0.667481541633606, 0.6627534627914429, 0.6630338430404663, 0.6641653180122375, 0.6708694100379944, 0.6566314101219177, 0.6570643782615662, 0.6562731266021729, 0.6505941152572632, 0.6549991369247437, 0.6488574147224426, 0.6442015767097473, 0.6439340710639954, 0.6434874534606934, 0.6469124555587769, 0.6424255967140198, 0.6387040019035339, 0.6469660997390747, 0.6359183192253113, 0.6405308842658997, 0.6401325464248657, 0.6362237334251404, 0.6371171474456787, 0.6268303990364075, 0.6278802752494812, 0.6271551251411438, 0.6225112676620483, 0.6284435987472534, 0.6246739029884338, 0.6136555075645447, 0.6148930191993713, 0.6088403463363647, 0.6137530207633972, 0.6167960166931152, 0.6070955395698547, 0.6057841181755066, 0.6106454730033875, 0.6025616526603699, 0.6004167199134827, 0.6058147549629211, 0.6034870743751526, 0.6029673218727112, 0.605552613735199, 0.5982755422592163, 0.597271740436554, 0.5954084992408752, 0.5883193612098694, 0.5858067274093628, 0.585430383682251, 0.5974652171134949, 0.5904420614242554, 0.5961335301399231, 0.593258261680603, 0.6094895601272583, 0.5951104760169983, 0.5799806118011475, 0.5765884518623352, 0.5901762843132019, 0.5728570222854614, 0.5756635069847107, 0.5700610280036926, 0.579437255859375], 'accuracy': [0.6961240172386169, 0.698966383934021, 0.7080103158950806, 0.7124031186103821, 0.7108527421951294, 0.7116279006004333, 0.7144702672958374, 0.7129198908805847, 0.7186046242713928, 0.7211886048316956, 0.7142118811607361, 0.7271317839622498, 0.721705436706543, 0.7322997450828552, 0.7214470505714417, 0.7100775241851807, 0.7343669533729553, 0.7284237742424011, 0.7322997450828552, 0.7338501214981079, 0.7400516867637634, 0.7297157645225525, 0.7317829728126526, 0.7374677062034607, 0.7364341020584106, 0.7364341020584106, 0.739534854888916, 0.7436692714691162, 0.7436692714691162, 0.736692488193512, 0.7392764687538147, 0.7379844784736633, 0.7423772811889648, 0.7493540048599243, 0.7475452423095703, 0.7351421117782593, 0.750904381275177, 0.7444444298744202, 0.7540051937103271, 0.7534883618354797, 0.7578811645507812, 0.7459948062896729, 0.7418604493141174, 0.7472867965698242, 0.7596899271011353, 0.7529715895652771, 0.7578811645507812, 0.7573643326759338, 0.7607235312461853, 0.7617571353912354, 0.7708010077476501, 0.762273907661438, 0.7596899271011353, 0.763307511806488, 0.7679586410522461, 0.7545219659805298, 0.7728682160377502, 0.7586563229560852, 0.7602066993713379, 0.7643410563468933, 0.7674418687820435, 0.7674418687820435, 0.7684754729270935, 0.773643434047699, 0.7723514437675476, 0.7718346118927002, 0.7798449397087097, 0.7839793562889099, 0.7829457521438599, 0.7821705341339111, 0.7806201577186584, 0.7785529494285583, 0.7829457521438599, 0.7819121479988098, 0.7834625244140625, 0.7837209105491638, 0.7878552675247192, 0.7888888716697693, 0.7878552675247192, 0.7829457521438599, 0.7746769785881042, 0.7873384952545166, 0.7901808619499207, 0.7919896841049194, 0.7968991994857788, 0.7943152189254761, 0.8023256063461304, 0.7860465049743652, 0.786821722984314, 0.7813953757286072, 0.7899224758148193, 0.7873384952545166, 0.790439248085022, 0.7961240410804749, 0.7981911897659302, 0.7906976938247681, 0.7974160313606262, 0.8038759827613831, 0.8031007647514343, 0.7987080216407776], 'val_loss': [0.8861242532730103, 0.8884077072143555, 0.8822365403175354, 0.8806229829788208, 0.8788304328918457, 0.8786361217498779, 0.8727987408638, 0.8733420372009277, 0.8662447333335876, 0.8647211790084839, 0.860092282295227, 0.8610895276069641, 0.8576509952545166, 0.8539461493492126, 0.8554339408874512, 0.8492229580879211, 0.855418860912323, 0.8491265773773193, 0.8510596752166748, 0.8578853607177734, 0.8515911102294922, 0.861083984375, 0.8697340488433838, 0.8712350130081177, 0.8856072425842285, 0.8654221892356873, 0.8992277383804321, 0.8805450201034546, 0.8804253339767456, 0.8788277506828308, 0.87798011302948, 0.8981438279151917, 0.908875584602356, 0.8874125480651855, 0.8834088444709778, 0.9136013388633728, 0.9463769197463989, 0.9586231708526611, 0.9310667514801025, 0.912997305393219, 0.967947244644165, 0.9103254079818726, 0.8929276466369629, 0.9106253385543823, 0.8969923257827759, 0.915363609790802, 0.9244847297668457, 0.9255995750427246, 0.9358677864074707, 0.95904541015625, 0.9458209872245789, 0.9500264525413513, 0.9655656218528748, 0.9304632544517517, 0.9291859865188599, 0.9869611859321594, 0.9258855581283569, 0.9469254016876221, 0.9482735395431519, 0.9302037954330444, 0.9238286018371582, 0.965577244758606, 0.9484840631484985, 0.9212360382080078, 0.9253003597259521, 0.930253267288208, 0.953262209892273, 0.9888941049575806, 0.981911838054657, 0.9436799883842468, 0.945965588092804, 0.939761757850647, 0.9518827199935913, 0.976676881313324, 0.9439024329185486, 0.9604505300521851, 0.9475809931755066, 0.9773367047309875, 0.9844335913658142, 1.0020698308944702, 1.020911455154419, 1.0175708532333374, 0.9779411554336548, 0.9985448718070984, 0.9941579699516296, 1.0070809125900269, 1.0225858688354492, 1.079390048980713, 1.038559913635254, 1.0020560026168823, 1.0116935968399048, 1.013411521911621, 1.0117170810699463, 1.050862193107605, 1.013668179512024, 1.0279803276062012, 1.0088319778442383, 1.0113221406936646, 1.0241355895996094, 1.0537406206130981], 'val_accuracy': [0.5278925895690918, 0.5175619721412659, 0.5723140239715576, 0.547520637512207, 0.5413222908973694, 0.5237603187561035, 0.567148745059967, 0.5464876294136047, 0.5836777091026306, 0.5785123705863953, 0.5816115736961365, 0.5826446413993835, 0.5888429880142212, 0.5919421315193176, 0.586776852607727, 0.5692148804664612, 0.5909090638160706, 0.5909090638160706, 0.5795454382896423, 0.5723140239715576, 0.5960744023323059, 0.5764462947845459, 0.5950413346290588, 0.5909090638160706, 0.5743801593780518, 0.5971074104309082, 0.5681818127632141, 0.5733470916748047, 0.5950413346290588, 0.5919421315193176, 0.5640496015548706, 0.577479362487793, 0.5785123705863953, 0.5599173307418823, 0.5847107172012329, 0.577479362487793, 0.5692148804664612, 0.5733470916748047, 0.5754132270812988, 0.5712810158729553, 0.5640496015548706, 0.567148745059967, 0.5754132270812988, 0.5733470916748047, 0.5723140239715576, 0.5816115736961365, 0.5940082669258118, 0.567148745059967, 0.5702479481697083, 0.5754132270812988, 0.5764462947845459, 0.5630165338516235, 0.5692148804664612, 0.5816115736961365, 0.5733470916748047, 0.577479362487793, 0.5919421315193176, 0.5712810158729553, 0.5733470916748047, 0.5836777091026306, 0.5526859760284424, 0.5485537052154541, 0.5692148804664612, 0.5485537052154541, 0.567148745059967, 0.5754132270812988, 0.5795454382896423, 0.5640496015548706, 0.5723140239715576, 0.5599173307418823, 0.5630165338516235, 0.5650826692581177, 0.557851254940033, 0.5650826692581177, 0.5619834661483765, 0.567148745059967, 0.5619834661483765, 0.5547520518302917, 0.55888432264328, 0.5619834661483765, 0.5454545617103577, 0.5836777091026306, 0.5692148804664612, 0.5836777091026306, 0.55888432264328, 0.5599173307418823, 0.5537189841270447, 0.5609503984451294, 0.5526859760284424, 0.5547520518302917, 0.5464876294136047, 0.5609503984451294, 0.577479362487793, 0.5743801593780518, 0.5650826692581177, 0.5516529083251953, 0.5640496015548706, 0.5557851195335388, 0.5454545617103577, 0.557851254940033]}\n","32/32 [==============================] - 0s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 4s 32ms/step - loss: 0.6128 - accuracy: 0.7678 - val_loss: 0.8611 - val_accuracy: 0.4838\n","Epoch 2/100\n"," 1/29 [>.............................] - ETA: 0s - loss: 0.5924 - accuracy: 0.7734"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 1s 21ms/step - loss: 0.5995 - accuracy: 0.7740 - val_loss: 0.8536 - val_accuracy: 0.4925\n","Epoch 3/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.5961 - accuracy: 0.7907 - val_loss: 0.8516 - val_accuracy: 0.5011\n","Epoch 4/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5892 - accuracy: 0.7837 - val_loss: 0.8525 - val_accuracy: 0.5054\n","Epoch 5/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5840 - accuracy: 0.7856 - val_loss: 0.8521 - val_accuracy: 0.5065\n","Epoch 6/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5851 - accuracy: 0.7904 - val_loss: 0.8535 - val_accuracy: 0.5043\n","Epoch 7/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5856 - accuracy: 0.7880 - val_loss: 0.8473 - val_accuracy: 0.5086\n","Epoch 8/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5898 - accuracy: 0.7901 - val_loss: 0.8524 - val_accuracy: 0.5054\n","Epoch 9/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5854 - accuracy: 0.7885 - val_loss: 0.8322 - val_accuracy: 0.6304\n","Epoch 10/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5719 - accuracy: 0.7982 - val_loss: 0.8362 - val_accuracy: 0.5377\n","Epoch 11/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5693 - accuracy: 0.8055 - val_loss: 0.8268 - val_accuracy: 0.5722\n","Epoch 12/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.5674 - accuracy: 0.8031 - val_loss: 0.8230 - val_accuracy: 0.6358\n","Epoch 13/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.5676 - accuracy: 0.8095 - val_loss: 0.8115 - val_accuracy: 0.6455\n","Epoch 14/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5649 - accuracy: 0.8074 - val_loss: 0.8242 - val_accuracy: 0.6218\n","Epoch 15/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5645 - accuracy: 0.8077 - val_loss: 0.8539 - val_accuracy: 0.5603\n","Epoch 16/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5744 - accuracy: 0.7936 - val_loss: 0.8187 - val_accuracy: 0.6078\n","Epoch 17/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5581 - accuracy: 0.8068 - val_loss: 0.8052 - val_accuracy: 0.6649\n","Epoch 18/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5607 - accuracy: 0.8077 - val_loss: 0.8296 - val_accuracy: 0.6455\n","Epoch 19/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5635 - accuracy: 0.8031 - val_loss: 0.8081 - val_accuracy: 0.6573\n","Epoch 20/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.5607 - accuracy: 0.8015 - val_loss: 0.7939 - val_accuracy: 0.6746\n","Epoch 21/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5693 - accuracy: 0.8009 - val_loss: 0.8553 - val_accuracy: 0.6282\n","Epoch 22/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5549 - accuracy: 0.8082 - val_loss: 0.8418 - val_accuracy: 0.6573\n","Epoch 23/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5549 - accuracy: 0.8090 - val_loss: 0.8258 - val_accuracy: 0.6735\n","Epoch 24/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5484 - accuracy: 0.8152 - val_loss: 0.8448 - val_accuracy: 0.6864\n","Epoch 25/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5487 - accuracy: 0.8117 - val_loss: 0.8531 - val_accuracy: 0.6843\n","Epoch 26/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5471 - accuracy: 0.8079 - val_loss: 0.9525 - val_accuracy: 0.6713\n","Epoch 27/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5503 - accuracy: 0.8103 - val_loss: 0.9066 - val_accuracy: 0.6767\n","Epoch 28/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5429 - accuracy: 0.8217 - val_loss: 0.8837 - val_accuracy: 0.6853\n","Epoch 29/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5403 - accuracy: 0.8168 - val_loss: 0.8826 - val_accuracy: 0.6703\n","Epoch 30/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5371 - accuracy: 0.8219 - val_loss: 0.9225 - val_accuracy: 0.6649\n","Epoch 31/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5354 - accuracy: 0.8195 - val_loss: 0.8739 - val_accuracy: 0.6789\n","Epoch 32/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5380 - accuracy: 0.8163 - val_loss: 0.8858 - val_accuracy: 0.6756\n","Epoch 33/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5373 - accuracy: 0.8179 - val_loss: 0.8685 - val_accuracy: 0.6735\n","Epoch 34/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5349 - accuracy: 0.8252 - val_loss: 0.9033 - val_accuracy: 0.6756\n","Epoch 35/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5322 - accuracy: 0.8244 - val_loss: 1.0132 - val_accuracy: 0.6800\n","Epoch 36/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5337 - accuracy: 0.8192 - val_loss: 0.9177 - val_accuracy: 0.6864\n","Epoch 37/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5289 - accuracy: 0.8279 - val_loss: 0.9621 - val_accuracy: 0.6638\n","Epoch 38/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5294 - accuracy: 0.8281 - val_loss: 1.0156 - val_accuracy: 0.6789\n","Epoch 39/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5263 - accuracy: 0.8273 - val_loss: 0.8926 - val_accuracy: 0.6767\n","Epoch 40/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5368 - accuracy: 0.8106 - val_loss: 0.9068 - val_accuracy: 0.6746\n","Epoch 41/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5258 - accuracy: 0.8284 - val_loss: 1.0229 - val_accuracy: 0.6390\n","Epoch 42/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5259 - accuracy: 0.8341 - val_loss: 0.8890 - val_accuracy: 0.6810\n","Epoch 43/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5332 - accuracy: 0.8219 - val_loss: 0.9608 - val_accuracy: 0.6735\n","Epoch 44/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5197 - accuracy: 0.8362 - val_loss: 0.9705 - val_accuracy: 0.6713\n","Epoch 45/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5205 - accuracy: 0.8335 - val_loss: 0.8800 - val_accuracy: 0.6897\n","Epoch 46/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5175 - accuracy: 0.8338 - val_loss: 0.8800 - val_accuracy: 0.6843\n","Epoch 47/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5153 - accuracy: 0.8319 - val_loss: 0.9454 - val_accuracy: 0.6530\n","Epoch 48/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.5142 - accuracy: 0.8308 - val_loss: 0.9059 - val_accuracy: 0.6907\n","Epoch 49/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5112 - accuracy: 0.8357 - val_loss: 0.9900 - val_accuracy: 0.6444\n","Epoch 50/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5119 - accuracy: 0.8357 - val_loss: 0.9429 - val_accuracy: 0.6692\n","Epoch 51/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5099 - accuracy: 0.8300 - val_loss: 1.0472 - val_accuracy: 0.6692\n","Epoch 52/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5097 - accuracy: 0.8268 - val_loss: 1.0441 - val_accuracy: 0.6466\n","Epoch 53/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5042 - accuracy: 0.8405 - val_loss: 1.0102 - val_accuracy: 0.6659\n","Epoch 54/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.5094 - accuracy: 0.8384 - val_loss: 0.9219 - val_accuracy: 0.6800\n","Epoch 55/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5009 - accuracy: 0.8462 - val_loss: 0.9596 - val_accuracy: 0.6606\n","Epoch 56/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5020 - accuracy: 0.8435 - val_loss: 0.9839 - val_accuracy: 0.6541\n","Epoch 57/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5010 - accuracy: 0.8440 - val_loss: 1.0439 - val_accuracy: 0.6746\n","Epoch 58/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5003 - accuracy: 0.8381 - val_loss: 1.0729 - val_accuracy: 0.6778\n","Epoch 59/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4959 - accuracy: 0.8467 - val_loss: 1.0524 - val_accuracy: 0.6789\n","Epoch 60/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4968 - accuracy: 0.8502 - val_loss: 0.9955 - val_accuracy: 0.6843\n","Epoch 61/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4902 - accuracy: 0.8524 - val_loss: 0.9641 - val_accuracy: 0.6584\n","Epoch 62/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4928 - accuracy: 0.8456 - val_loss: 0.9734 - val_accuracy: 0.6756\n","Epoch 63/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4971 - accuracy: 0.8443 - val_loss: 0.9828 - val_accuracy: 0.6659\n","Epoch 64/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4901 - accuracy: 0.8499 - val_loss: 0.9621 - val_accuracy: 0.6810\n","Epoch 65/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4937 - accuracy: 0.8524 - val_loss: 0.9849 - val_accuracy: 0.6606\n","Epoch 66/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4855 - accuracy: 0.8518 - val_loss: 0.9445 - val_accuracy: 0.6681\n","Epoch 67/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5075 - accuracy: 0.8346 - val_loss: 1.0491 - val_accuracy: 0.6713\n","Epoch 68/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5197 - accuracy: 0.8230 - val_loss: 1.1205 - val_accuracy: 0.6466\n","Epoch 69/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5085 - accuracy: 0.8386 - val_loss: 1.0263 - val_accuracy: 0.6530\n","Epoch 70/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4962 - accuracy: 0.8524 - val_loss: 1.0627 - val_accuracy: 0.6703\n","Epoch 71/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4874 - accuracy: 0.8534 - val_loss: 1.0755 - val_accuracy: 0.6325\n","Epoch 72/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4808 - accuracy: 0.8613 - val_loss: 1.0050 - val_accuracy: 0.6670\n","Epoch 73/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4823 - accuracy: 0.8605 - val_loss: 1.0251 - val_accuracy: 0.6778\n","Epoch 74/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4792 - accuracy: 0.8594 - val_loss: 1.0928 - val_accuracy: 0.6649\n","Epoch 75/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4730 - accuracy: 0.8564 - val_loss: 0.9912 - val_accuracy: 0.6713\n","Epoch 76/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4705 - accuracy: 0.8588 - val_loss: 1.0159 - val_accuracy: 0.6541\n","Epoch 77/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4817 - accuracy: 0.8532 - val_loss: 1.1431 - val_accuracy: 0.6422\n","Epoch 78/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4740 - accuracy: 0.8559 - val_loss: 1.0003 - val_accuracy: 0.6606\n","Epoch 79/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4748 - accuracy: 0.8543 - val_loss: 1.0096 - val_accuracy: 0.6800\n","Epoch 80/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5119 - accuracy: 0.8219 - val_loss: 1.1192 - val_accuracy: 0.6142\n","Epoch 81/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.4764 - accuracy: 0.8578 - val_loss: 1.0777 - val_accuracy: 0.6649\n","Epoch 82/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4734 - accuracy: 0.8588 - val_loss: 1.0006 - val_accuracy: 0.6756\n","Epoch 83/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4688 - accuracy: 0.8623 - val_loss: 1.0037 - val_accuracy: 0.6778\n","Epoch 84/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4755 - accuracy: 0.8564 - val_loss: 1.1287 - val_accuracy: 0.6433\n","Epoch 85/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4701 - accuracy: 0.8591 - val_loss: 1.0516 - val_accuracy: 0.6476\n","Epoch 86/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4781 - accuracy: 0.8621 - val_loss: 0.9916 - val_accuracy: 0.6692\n","Epoch 87/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4839 - accuracy: 0.8510 - val_loss: 0.9850 - val_accuracy: 0.6821\n","Epoch 88/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4659 - accuracy: 0.8623 - val_loss: 1.0326 - val_accuracy: 0.6778\n","Epoch 89/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4629 - accuracy: 0.8629 - val_loss: 1.0378 - val_accuracy: 0.6444\n","Epoch 90/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4683 - accuracy: 0.8497 - val_loss: 1.1063 - val_accuracy: 0.6455\n","Epoch 91/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4575 - accuracy: 0.8688 - val_loss: 1.0116 - val_accuracy: 0.6703\n","Epoch 92/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4604 - accuracy: 0.8680 - val_loss: 1.0988 - val_accuracy: 0.6412\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4575 - accuracy: 0.8712 - val_loss: 1.0303 - val_accuracy: 0.6703\n","Epoch 94/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4580 - accuracy: 0.8702 - val_loss: 1.0583 - val_accuracy: 0.6756\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4514 - accuracy: 0.8675 - val_loss: 1.1001 - val_accuracy: 0.6584\n","Epoch 96/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4493 - accuracy: 0.8704 - val_loss: 1.0373 - val_accuracy: 0.6584\n","Epoch 97/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4495 - accuracy: 0.8712 - val_loss: 1.0522 - val_accuracy: 0.6444\n","Epoch 98/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4564 - accuracy: 0.8626 - val_loss: 1.0200 - val_accuracy: 0.6541\n","Epoch 99/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4610 - accuracy: 0.8637 - val_loss: 1.0649 - val_accuracy: 0.6713\n","Epoch 100/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4516 - accuracy: 0.8610 - val_loss: 1.0603 - val_accuracy: 0.6659\n","{'loss': [0.6128191351890564, 0.5994914770126343, 0.596084713935852, 0.5891899466514587, 0.5839539170265198, 0.585091769695282, 0.5856006145477295, 0.5897769927978516, 0.5854355096817017, 0.5719159841537476, 0.5692734122276306, 0.5674005150794983, 0.5676450133323669, 0.564932107925415, 0.5645132660865784, 0.5743541717529297, 0.5580905675888062, 0.5606517791748047, 0.5635120272636414, 0.5607316493988037, 0.5693073272705078, 0.5549066662788391, 0.5549153685569763, 0.5483918190002441, 0.548691987991333, 0.5470545887947083, 0.5503214001655579, 0.5429483652114868, 0.5403417348861694, 0.5371459722518921, 0.5353589653968811, 0.5379976034164429, 0.5373432040214539, 0.534913957118988, 0.5322469472885132, 0.5336892604827881, 0.5288764238357544, 0.5294480919837952, 0.5262866020202637, 0.5368004441261292, 0.5258271098136902, 0.5258601307868958, 0.5331694483757019, 0.5197451710700989, 0.5204825401306152, 0.5175029635429382, 0.5152541995048523, 0.5141957998275757, 0.5111625790596008, 0.511920690536499, 0.5098845958709717, 0.5096613764762878, 0.5042256116867065, 0.5094330906867981, 0.5008532404899597, 0.5019592642784119, 0.5009589195251465, 0.5003026723861694, 0.49587422609329224, 0.4967997372150421, 0.49016672372817993, 0.49282306432724, 0.49713656306266785, 0.4901089072227478, 0.4936681091785431, 0.48548293113708496, 0.5075049996376038, 0.5196686387062073, 0.5085291266441345, 0.49624234437942505, 0.4873858690261841, 0.4808163344860077, 0.4823487401008606, 0.4792184829711914, 0.4729561507701874, 0.4705081284046173, 0.48173168301582336, 0.4739798605442047, 0.47475677728652954, 0.5119045972824097, 0.47641870379447937, 0.47335147857666016, 0.4688199758529663, 0.4755227565765381, 0.4700876474380493, 0.478129118680954, 0.48386600613594055, 0.465915322303772, 0.4628916084766388, 0.46832963824272156, 0.45748022198677063, 0.4604136645793915, 0.45751065015792847, 0.4579518437385559, 0.45137205719947815, 0.44930022954940796, 0.4494706094264984, 0.45643454790115356, 0.4610399901866913, 0.4515976011753082], 'accuracy': [0.7677801847457886, 0.7739762663841248, 0.790678858757019, 0.7836745977401733, 0.7855603694915771, 0.790409505367279, 0.7879849076271057, 0.7901400923728943, 0.7885237336158752, 0.798222005367279, 0.8054956793785095, 0.803071141242981, 0.8095366358757019, 0.8073814511299133, 0.8076508641242981, 0.7936422228813171, 0.8068426847457886, 0.8076508641242981, 0.803071141242981, 0.8014547228813171, 0.8009159564971924, 0.8081896305084229, 0.8089978694915771, 0.8151939511299133, 0.8116918206214905, 0.8079202771186829, 0.8103448152542114, 0.821659505367279, 0.8168103694915771, 0.821928858757019, 0.8195043206214905, 0.8162715435028076, 0.8178879022598267, 0.8251616358757019, 0.8243534564971924, 0.8192349076271057, 0.8278555870056152, 0.828125, 0.8273168206214905, 0.8106142282485962, 0.8283944129943848, 0.8340517282485962, 0.821928858757019, 0.8362069129943848, 0.8335129022598267, 0.8337823152542114, 0.8318965435028076, 0.8308189511299133, 0.8356680870056152, 0.8356680870056152, 0.8300107717514038, 0.826777994632721, 0.8405172228813171, 0.8383620977401733, 0.8461745977401733, 0.8434805870056152, 0.8440194129943848, 0.8380926847457886, 0.8467133641242981, 0.850215494632721, 0.8523706793785095, 0.8456357717514038, 0.8442887663841248, 0.849946141242981, 0.8523706793785095, 0.8518319129943848, 0.834590494632721, 0.8230064511299133, 0.8386314511299133, 0.8523706793785095, 0.8534482717514038, 0.8612607717514038, 0.8604525923728943, 0.859375, 0.8564116358757019, 0.8588362336158752, 0.853178858757019, 0.8558728694915771, 0.8542564511299133, 0.821928858757019, 0.857758641242981, 0.8588362336158752, 0.8623383641242981, 0.8564116358757019, 0.8591055870056152, 0.8620689511299133, 0.8510237336158752, 0.8623383641242981, 0.8628771305084229, 0.8496767282485962, 0.868803858757019, 0.8679956793785095, 0.8712284564971924, 0.8701508641242981, 0.8674569129943848, 0.8704202771186829, 0.8712284564971924, 0.8626077771186829, 0.8636853694915771, 0.860991358757019], 'val_loss': [0.8611097931861877, 0.8535659313201904, 0.8516326546669006, 0.8525234460830688, 0.8521326780319214, 0.8534696698188782, 0.8472905158996582, 0.8523903489112854, 0.8321530818939209, 0.8362343907356262, 0.8268268704414368, 0.8230046629905701, 0.8114993572235107, 0.8242446780204773, 0.8539089560508728, 0.8187053799629211, 0.8052148222923279, 0.8295871615409851, 0.8081040978431702, 0.7939147353172302, 0.8553484082221985, 0.8417549133300781, 0.8257766366004944, 0.8448280096054077, 0.8530809879302979, 0.9524517059326172, 0.9065916538238525, 0.8836727738380432, 0.882597804069519, 0.9224609136581421, 0.87391197681427, 0.8857804536819458, 0.8684521913528442, 0.9033411145210266, 1.013203740119934, 0.9176703095436096, 0.9620513916015625, 1.0155870914459229, 0.8925989270210266, 0.9067611694335938, 1.022862434387207, 0.8890368342399597, 0.960843026638031, 0.9705352187156677, 0.8799655437469482, 0.8799718022346497, 0.9453896284103394, 0.9058945178985596, 0.9900124073028564, 0.9428629279136658, 1.0471855401992798, 1.0441300868988037, 1.010223627090454, 0.9218688607215881, 0.9595603346824646, 0.9839217662811279, 1.0439311265945435, 1.072859525680542, 1.052369475364685, 0.9954594969749451, 0.9640896320343018, 0.9733647704124451, 0.9828378558158875, 0.9621423482894897, 0.9849058389663696, 0.9444869756698608, 1.0491175651550293, 1.1204744577407837, 1.026312232017517, 1.0626707077026367, 1.0755038261413574, 1.0050320625305176, 1.0251423120498657, 1.0927653312683105, 0.9912301301956177, 1.0158919095993042, 1.1430647373199463, 1.0003437995910645, 1.0096113681793213, 1.1191933155059814, 1.077674388885498, 1.0006318092346191, 1.003718376159668, 1.1286524534225464, 1.051596760749817, 0.9916184544563293, 0.9850204586982727, 1.032647967338562, 1.03778874874115, 1.1062612533569336, 1.0116028785705566, 1.0988223552703857, 1.0302793979644775, 1.0583436489105225, 1.1001482009887695, 1.0372626781463623, 1.0522195100784302, 1.020005702972412, 1.0649293661117554, 1.0603137016296387], 'val_accuracy': [0.48383620381355286, 0.4924568831920624, 0.5010775923728943, 0.5053879022598267, 0.506465494632721, 0.5043103694915771, 0.5086206793785095, 0.5053879022598267, 0.6303879022598267, 0.537715494632721, 0.5721982717514038, 0.6357758641242981, 0.6454741358757019, 0.6217672228813171, 0.5603448152542114, 0.607758641242981, 0.6648706793785095, 0.6454741358757019, 0.6573275923728943, 0.6745689511299133, 0.6282327771186829, 0.6573275923728943, 0.673491358757019, 0.6864224076271057, 0.6842672228813171, 0.6713362336158752, 0.6767241358757019, 0.6853448152542114, 0.670258641242981, 0.6648706793785095, 0.6788793206214905, 0.6756465435028076, 0.673491358757019, 0.6756465435028076, 0.6799569129943848, 0.6864224076271057, 0.6637930870056152, 0.6788793206214905, 0.6767241358757019, 0.6745689511299133, 0.639008641242981, 0.681034505367279, 0.673491358757019, 0.6713362336158752, 0.6896551847457886, 0.6842672228813171, 0.6530172228813171, 0.6907327771186829, 0.6443965435028076, 0.6691810488700867, 0.6691810488700867, 0.6465517282485962, 0.6659482717514038, 0.6799569129943848, 0.6605603694915771, 0.6540948152542114, 0.6745689511299133, 0.6778017282485962, 0.6788793206214905, 0.6842672228813171, 0.6584051847457886, 0.6756465435028076, 0.6659482717514038, 0.681034505367279, 0.6605603694915771, 0.6681034564971924, 0.6713362336158752, 0.6465517282485962, 0.6530172228813171, 0.670258641242981, 0.6325430870056152, 0.6670258641242981, 0.6778017282485962, 0.6648706793785095, 0.6713362336158752, 0.6540948152542114, 0.642241358757019, 0.6605603694915771, 0.6799569129943848, 0.6142241358757019, 0.6648706793785095, 0.6756465435028076, 0.6778017282485962, 0.6433189511299133, 0.6476293206214905, 0.6691810488700867, 0.6821120977401733, 0.6778017282485962, 0.6443965435028076, 0.6454741358757019, 0.670258641242981, 0.6411637663841248, 0.670258641242981, 0.6756465435028076, 0.6584051847457886, 0.6584051847457886, 0.6443965435028076, 0.6540948152542114, 0.6713362336158752, 0.6659482717514038]}\n","38/38 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","26/28 [==========================>...] - ETA: 0s - loss: 0.6070 - accuracy: 0.7843"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 4s 35ms/step - loss: 0.6100 - accuracy: 0.7810 - val_loss: 0.8569 - val_accuracy: 0.5090\n","Epoch 2/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.6159 - accuracy: 0.7600 - val_loss: 0.8599 - val_accuracy: 0.5102\n","Epoch 3/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.5948 - accuracy: 0.7898 - val_loss: 0.8594 - val_accuracy: 0.5124\n","Epoch 4/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.5908 - accuracy: 0.7886 - val_loss: 0.8450 - val_accuracy: 0.5215\n","Epoch 5/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5873 - accuracy: 0.7906 - val_loss: 0.8541 - val_accuracy: 0.5215\n","Epoch 6/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.5821 - accuracy: 0.7974 - val_loss: 0.8347 - val_accuracy: 0.5339\n","Epoch 7/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.5815 - accuracy: 0.8056 - val_loss: 0.8424 - val_accuracy: 0.5351\n","Epoch 8/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5832 - accuracy: 0.7963 - val_loss: 0.8282 - val_accuracy: 0.5441\n","Epoch 9/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5818 - accuracy: 0.8033 - val_loss: 0.8286 - val_accuracy: 0.5520\n","Epoch 10/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5773 - accuracy: 0.7957 - val_loss: 0.8116 - val_accuracy: 0.6324\n","Epoch 11/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5745 - accuracy: 0.8016 - val_loss: 0.8095 - val_accuracy: 0.6041\n","Epoch 12/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5786 - accuracy: 0.8025 - val_loss: 0.8175 - val_accuracy: 0.5781\n","Epoch 13/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5714 - accuracy: 0.8016 - val_loss: 0.8009 - val_accuracy: 0.6052\n","Epoch 14/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5761 - accuracy: 0.8042 - val_loss: 0.8002 - val_accuracy: 0.6437\n","Epoch 15/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5650 - accuracy: 0.8019 - val_loss: 0.7918 - val_accuracy: 0.6708\n","Epoch 16/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5765 - accuracy: 0.7997 - val_loss: 0.7837 - val_accuracy: 0.6493\n","Epoch 17/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5734 - accuracy: 0.8045 - val_loss: 0.7787 - val_accuracy: 0.6606\n","Epoch 18/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5644 - accuracy: 0.8084 - val_loss: 0.7917 - val_accuracy: 0.6550\n","Epoch 19/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5599 - accuracy: 0.8118 - val_loss: 0.7832 - val_accuracy: 0.6505\n","Epoch 20/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5588 - accuracy: 0.8127 - val_loss: 0.7804 - val_accuracy: 0.6719\n","Epoch 21/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5612 - accuracy: 0.8087 - val_loss: 0.7977 - val_accuracy: 0.6867\n","Epoch 22/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5641 - accuracy: 0.8124 - val_loss: 0.8018 - val_accuracy: 0.6391\n","Epoch 23/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5551 - accuracy: 0.8121 - val_loss: 0.7858 - val_accuracy: 0.6776\n","Epoch 24/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5527 - accuracy: 0.8135 - val_loss: 0.8058 - val_accuracy: 0.6776\n","Epoch 25/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5538 - accuracy: 0.8127 - val_loss: 0.8238 - val_accuracy: 0.6572\n","Epoch 26/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5622 - accuracy: 0.8076 - val_loss: 0.7876 - val_accuracy: 0.6719\n","Epoch 27/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5691 - accuracy: 0.7963 - val_loss: 0.8449 - val_accuracy: 0.6776\n","Epoch 28/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5522 - accuracy: 0.8164 - val_loss: 0.8189 - val_accuracy: 0.6833\n","Epoch 29/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5538 - accuracy: 0.8062 - val_loss: 0.8444 - val_accuracy: 0.6753\n","Epoch 30/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5520 - accuracy: 0.8175 - val_loss: 0.8425 - val_accuracy: 0.6663\n","Epoch 31/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5651 - accuracy: 0.8036 - val_loss: 0.9381 - val_accuracy: 0.6391\n","Epoch 32/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5658 - accuracy: 0.8008 - val_loss: 0.8690 - val_accuracy: 0.6855\n","Epoch 33/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5584 - accuracy: 0.8189 - val_loss: 0.8865 - val_accuracy: 0.6663\n","Epoch 34/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5408 - accuracy: 0.8251 - val_loss: 0.9054 - val_accuracy: 0.6663\n","Epoch 35/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5482 - accuracy: 0.8093 - val_loss: 0.8653 - val_accuracy: 0.6753\n","Epoch 36/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5469 - accuracy: 0.8175 - val_loss: 0.8754 - val_accuracy: 0.6799\n","Epoch 37/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5422 - accuracy: 0.8229 - val_loss: 0.8635 - val_accuracy: 0.6776\n","Epoch 38/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5359 - accuracy: 0.8195 - val_loss: 0.9074 - val_accuracy: 0.6731\n","Epoch 39/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5347 - accuracy: 0.8316 - val_loss: 0.8785 - val_accuracy: 0.6787\n","Epoch 40/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5370 - accuracy: 0.8322 - val_loss: 1.0466 - val_accuracy: 0.6482\n","Epoch 41/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5632 - accuracy: 0.8059 - val_loss: 0.9043 - val_accuracy: 0.6753\n","Epoch 42/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5356 - accuracy: 0.8231 - val_loss: 0.9009 - val_accuracy: 0.6765\n","Epoch 43/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5246 - accuracy: 0.8322 - val_loss: 0.9006 - val_accuracy: 0.6787\n","Epoch 44/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5258 - accuracy: 0.8328 - val_loss: 0.8948 - val_accuracy: 0.6731\n","Epoch 45/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5292 - accuracy: 0.8316 - val_loss: 0.9404 - val_accuracy: 0.6719\n","Epoch 46/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5314 - accuracy: 0.8237 - val_loss: 0.9148 - val_accuracy: 0.6753\n","Epoch 47/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5195 - accuracy: 0.8447 - val_loss: 0.9138 - val_accuracy: 0.6731\n","Epoch 48/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5217 - accuracy: 0.8381 - val_loss: 0.9033 - val_accuracy: 0.6584\n","Epoch 49/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5197 - accuracy: 0.8347 - val_loss: 0.8995 - val_accuracy: 0.6719\n","Epoch 50/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5252 - accuracy: 0.8359 - val_loss: 0.9268 - val_accuracy: 0.6663\n","Epoch 51/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5316 - accuracy: 0.8209 - val_loss: 0.9662 - val_accuracy: 0.6527\n","Epoch 52/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5240 - accuracy: 0.8331 - val_loss: 0.9126 - val_accuracy: 0.6799\n","Epoch 53/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5162 - accuracy: 0.8376 - val_loss: 0.9441 - val_accuracy: 0.6776\n","Epoch 54/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5168 - accuracy: 0.8350 - val_loss: 0.9130 - val_accuracy: 0.6561\n","Epoch 55/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5238 - accuracy: 0.8285 - val_loss: 0.9378 - val_accuracy: 0.6697\n","Epoch 56/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5205 - accuracy: 0.8353 - val_loss: 0.9214 - val_accuracy: 0.6697\n","Epoch 57/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5169 - accuracy: 0.8331 - val_loss: 0.9391 - val_accuracy: 0.6674\n","Epoch 58/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5198 - accuracy: 0.8311 - val_loss: 1.0139 - val_accuracy: 0.6606\n","Epoch 59/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5190 - accuracy: 0.8359 - val_loss: 0.9660 - val_accuracy: 0.6595\n","Epoch 60/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5122 - accuracy: 0.8316 - val_loss: 0.9468 - val_accuracy: 0.6584\n","Epoch 61/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5030 - accuracy: 0.8463 - val_loss: 0.9458 - val_accuracy: 0.6640\n","Epoch 62/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5015 - accuracy: 0.8540 - val_loss: 0.9615 - val_accuracy: 0.6708\n","Epoch 63/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5115 - accuracy: 0.8415 - val_loss: 0.9431 - val_accuracy: 0.6606\n","Epoch 64/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5064 - accuracy: 0.8432 - val_loss: 0.9721 - val_accuracy: 0.6686\n","Epoch 65/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5061 - accuracy: 0.8390 - val_loss: 0.9518 - val_accuracy: 0.6640\n","Epoch 66/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5073 - accuracy: 0.8441 - val_loss: 0.9686 - val_accuracy: 0.6629\n","Epoch 67/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4962 - accuracy: 0.8455 - val_loss: 0.9508 - val_accuracy: 0.6595\n","Epoch 68/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5227 - accuracy: 0.8229 - val_loss: 0.9419 - val_accuracy: 0.6618\n","Epoch 69/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5079 - accuracy: 0.8483 - val_loss: 0.9764 - val_accuracy: 0.6618\n","Epoch 70/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5057 - accuracy: 0.8421 - val_loss: 0.9708 - val_accuracy: 0.6697\n","Epoch 71/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4950 - accuracy: 0.8492 - val_loss: 0.9764 - val_accuracy: 0.6584\n","Epoch 72/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5055 - accuracy: 0.8401 - val_loss: 1.0172 - val_accuracy: 0.6640\n","Epoch 73/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5125 - accuracy: 0.8455 - val_loss: 0.9850 - val_accuracy: 0.6629\n","Epoch 74/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4908 - accuracy: 0.8495 - val_loss: 1.0046 - val_accuracy: 0.6640\n","Epoch 75/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5033 - accuracy: 0.8410 - val_loss: 1.0272 - val_accuracy: 0.6595\n","Epoch 76/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5000 - accuracy: 0.8447 - val_loss: 1.0333 - val_accuracy: 0.6561\n","Epoch 77/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4942 - accuracy: 0.8480 - val_loss: 1.0074 - val_accuracy: 0.6606\n","Epoch 78/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4886 - accuracy: 0.8492 - val_loss: 0.9989 - val_accuracy: 0.6652\n","Epoch 79/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5027 - accuracy: 0.8396 - val_loss: 1.0167 - val_accuracy: 0.6561\n","Epoch 80/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4839 - accuracy: 0.8560 - val_loss: 1.0300 - val_accuracy: 0.6674\n","Epoch 81/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4860 - accuracy: 0.8461 - val_loss: 1.0048 - val_accuracy: 0.6629\n","Epoch 82/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4827 - accuracy: 0.8514 - val_loss: 1.0123 - val_accuracy: 0.6697\n","Epoch 83/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4755 - accuracy: 0.8568 - val_loss: 1.0082 - val_accuracy: 0.6674\n","Epoch 84/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4791 - accuracy: 0.8588 - val_loss: 1.0280 - val_accuracy: 0.6663\n","Epoch 85/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4761 - accuracy: 0.8639 - val_loss: 1.0158 - val_accuracy: 0.6595\n","Epoch 86/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4739 - accuracy: 0.8577 - val_loss: 1.0164 - val_accuracy: 0.6629\n","Epoch 87/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4827 - accuracy: 0.8540 - val_loss: 1.0816 - val_accuracy: 0.6584\n","Epoch 88/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4786 - accuracy: 0.8526 - val_loss: 1.0306 - val_accuracy: 0.6629\n","Epoch 89/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4993 - accuracy: 0.8435 - val_loss: 0.9927 - val_accuracy: 0.6652\n","Epoch 90/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4959 - accuracy: 0.8497 - val_loss: 1.0552 - val_accuracy: 0.6663\n","Epoch 91/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4890 - accuracy: 0.8480 - val_loss: 1.0420 - val_accuracy: 0.6584\n","Epoch 92/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4917 - accuracy: 0.8401 - val_loss: 1.0477 - val_accuracy: 0.6629\n","Epoch 93/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4885 - accuracy: 0.8500 - val_loss: 1.0213 - val_accuracy: 0.6595\n","Epoch 94/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4807 - accuracy: 0.8503 - val_loss: 1.1131 - val_accuracy: 0.6335\n","Epoch 95/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4761 - accuracy: 0.8551 - val_loss: 1.0404 - val_accuracy: 0.6572\n","Epoch 96/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4654 - accuracy: 0.8690 - val_loss: 1.0663 - val_accuracy: 0.6516\n","Epoch 97/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4744 - accuracy: 0.8582 - val_loss: 1.0412 - val_accuracy: 0.6629\n","Epoch 98/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4652 - accuracy: 0.8608 - val_loss: 1.0282 - val_accuracy: 0.6640\n","Epoch 99/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4600 - accuracy: 0.8664 - val_loss: 1.0404 - val_accuracy: 0.6640\n","Epoch 100/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4618 - accuracy: 0.8684 - val_loss: 1.0564 - val_accuracy: 0.6572\n","{'loss': [0.6100203990936279, 0.6159029006958008, 0.5948411822319031, 0.5908082723617554, 0.5872766375541687, 0.5821073651313782, 0.5815139412879944, 0.583234965801239, 0.5817933678627014, 0.5773243308067322, 0.5744688510894775, 0.5786088705062866, 0.5713520050048828, 0.576130747795105, 0.5650224685668945, 0.5765301585197449, 0.573436439037323, 0.5644031167030334, 0.5598745346069336, 0.5588217973709106, 0.5612344145774841, 0.5640876293182373, 0.5551419854164124, 0.5526874661445618, 0.5537825226783752, 0.5621593594551086, 0.5690541863441467, 0.5521514415740967, 0.5538415908813477, 0.5519638657569885, 0.565139651298523, 0.5658248066902161, 0.5584209561347961, 0.5407814383506775, 0.5481929779052734, 0.546859622001648, 0.5421814918518066, 0.5359094738960266, 0.534692108631134, 0.5369763970375061, 0.5631619691848755, 0.5356337428092957, 0.5246356725692749, 0.5258129239082336, 0.5292132496833801, 0.5314323306083679, 0.5195299983024597, 0.521672248840332, 0.5197324156761169, 0.5251609683036804, 0.5315775871276855, 0.5239902138710022, 0.5162429213523865, 0.516831636428833, 0.5238382816314697, 0.5205191969871521, 0.5169330835342407, 0.5198225378990173, 0.5189874768257141, 0.5122104287147522, 0.5029930472373962, 0.5014749765396118, 0.511480987071991, 0.5064025521278381, 0.5060967206954956, 0.5072858929634094, 0.4962013065814972, 0.5226878523826599, 0.5078514814376831, 0.505695104598999, 0.49498313665390015, 0.5055429339408875, 0.5125285983085632, 0.4907563626766205, 0.5032866597175598, 0.5000229477882385, 0.494236022233963, 0.48862212896347046, 0.5026841759681702, 0.48390012979507446, 0.4859621226787567, 0.4826758801937103, 0.4755403995513916, 0.4790802299976349, 0.47614163160324097, 0.4739314913749695, 0.4826970398426056, 0.4786234200000763, 0.49930936098098755, 0.49594756960868835, 0.4890226125717163, 0.49170204997062683, 0.4884509742259979, 0.480683296918869, 0.47606566548347473, 0.465410441160202, 0.47444045543670654, 0.4651552438735962, 0.4600176513195038, 0.46179914474487305], 'accuracy': [0.7809846997261047, 0.7600452899932861, 0.7897566556930542, 0.7886247634887695, 0.7906055450439453, 0.797396719455719, 0.8056027293205261, 0.7962648272514343, 0.8033390045166016, 0.7956989407539368, 0.8016412258148193, 0.8024901151657104, 0.8016412258148193, 0.8041878938674927, 0.8019241690635681, 0.7996604442596436, 0.8044708371162415, 0.808432400226593, 0.8118279576301575, 0.8126768469810486, 0.8087153434753418, 0.8123939037322998, 0.8121109008789062, 0.8135257363319397, 0.8126768469810486, 0.8075834512710571, 0.7962648272514343, 0.8163554072380066, 0.8061686754226685, 0.8174872398376465, 0.8036219477653503, 0.8007922768592834, 0.8189020752906799, 0.825127363204956, 0.8092812895774841, 0.8174872398376465, 0.8228636384010315, 0.8194680213928223, 0.8316355347633362, 0.8322014808654785, 0.8058856725692749, 0.8231465816497803, 0.8322014808654785, 0.8327674269676208, 0.8316355347633362, 0.8237125277519226, 0.8446519374847412, 0.8381437659263611, 0.8347481489181519, 0.8358800411224365, 0.8208828568458557, 0.8330503702163696, 0.8375778198242188, 0.8350311517715454, 0.8285229206085205, 0.8353140950202942, 0.8330503702163696, 0.8310695886611938, 0.8358800411224365, 0.8316355347633362, 0.8463497161865234, 0.853989839553833, 0.8415393233299255, 0.8432371020317078, 0.8389926552772522, 0.8440860509872437, 0.8455008268356323, 0.8228636384010315, 0.8483304977416992, 0.8421052694320679, 0.8491793870925903, 0.8401244878768921, 0.8455008268356323, 0.8494623899459839, 0.8409733772277832, 0.8446519374847412, 0.8480475544929504, 0.8491793870925903, 0.8395586013793945, 0.855970561504364, 0.8460667729377747, 0.8514431118965149, 0.8568194508552551, 0.8588002324104309, 0.8638936281204224, 0.8576683402061462, 0.853989839553833, 0.8525750041007996, 0.8435201048851013, 0.8497453331947327, 0.8480475544929504, 0.8401244878768921, 0.8500282764434814, 0.850311279296875, 0.8551216721534729, 0.868986964225769, 0.8582342863082886, 0.8607810139656067, 0.8664402961730957, 0.8684210777282715], 'val_loss': [0.85692298412323, 0.8598975539207458, 0.8594363927841187, 0.8449926376342773, 0.8540564775466919, 0.8347472548484802, 0.8424383401870728, 0.8281903862953186, 0.8286308646202087, 0.8115767240524292, 0.8095088601112366, 0.8175039887428284, 0.8008745908737183, 0.8001806139945984, 0.7918434143066406, 0.7837327718734741, 0.778738796710968, 0.7916942834854126, 0.7831578254699707, 0.7803709506988525, 0.7976537942886353, 0.8018239736557007, 0.7858027815818787, 0.8058030605316162, 0.8238425254821777, 0.787631630897522, 0.8449349403381348, 0.8188784718513489, 0.8444287180900574, 0.8425189852714539, 0.9380601048469543, 0.869001030921936, 0.8865306973457336, 0.9054460525512695, 0.8652642965316772, 0.8754459619522095, 0.86346834897995, 0.9073977470397949, 0.8785442113876343, 1.0465892553329468, 0.9042706489562988, 0.9008829593658447, 0.9005908370018005, 0.8947501182556152, 0.940390408039093, 0.9148336052894592, 0.9137961864471436, 0.9032971858978271, 0.8994849920272827, 0.9268275499343872, 0.9662088751792908, 0.9125558733940125, 0.944087028503418, 0.9130223393440247, 0.937836229801178, 0.9214038848876953, 0.9390511512756348, 1.013935923576355, 0.9660042524337769, 0.9468268752098083, 0.9458494782447815, 0.9615252614021301, 0.9430733919143677, 0.9721142649650574, 0.9517986178398132, 0.9685820937156677, 0.9508415460586548, 0.9418727159500122, 0.9764189124107361, 0.9707809090614319, 0.9764448404312134, 1.0172078609466553, 0.9850332140922546, 1.004623293876648, 1.0271639823913574, 1.0332688093185425, 1.0074166059494019, 0.9988629221916199, 1.0166690349578857, 1.0300487279891968, 1.0047518014907837, 1.0123084783554077, 1.0082203149795532, 1.028009295463562, 1.0157654285430908, 1.0164432525634766, 1.0816036462783813, 1.0305802822113037, 0.9927346110343933, 1.0552088022232056, 1.042040228843689, 1.047737956047058, 1.0212774276733398, 1.1130797863006592, 1.0403761863708496, 1.0663037300109863, 1.0412465333938599, 1.0282042026519775, 1.0403871536254883, 1.0564398765563965], 'val_accuracy': [0.5090497732162476, 0.5101810097694397, 0.5124434232711792, 0.5214931964874268, 0.5214931964874268, 0.5339366793632507, 0.5350678563117981, 0.5441176295280457, 0.5520362257957458, 0.6323529481887817, 0.6040723919868469, 0.5780543088912964, 0.6052036285400391, 0.6436651349067688, 0.6708144545555115, 0.6493212580680847, 0.6606335043907166, 0.6549773812294006, 0.6504524946212769, 0.6719456911087036, 0.6866515874862671, 0.639140248298645, 0.6776018142700195, 0.6776018142700195, 0.6572397947311401, 0.6719456911087036, 0.6776018142700195, 0.6832579374313354, 0.6753393411636353, 0.6662895679473877, 0.639140248298645, 0.685520350933075, 0.6662895679473877, 0.6662895679473877, 0.6753393411636353, 0.679864227771759, 0.6776018142700195, 0.6730769276618958, 0.6787330508232117, 0.6481900215148926, 0.6753393411636353, 0.6764705777168274, 0.6787330508232117, 0.6730769276618958, 0.6719456911087036, 0.6753393411636353, 0.6730769276618958, 0.6583710312843323, 0.6719456911087036, 0.6662895679473877, 0.6527149081230164, 0.679864227771759, 0.6776018142700195, 0.6561086177825928, 0.6696832776069641, 0.6696832776069641, 0.6674208045005798, 0.6606335043907166, 0.6595022678375244, 0.6583710312843323, 0.6640271544456482, 0.6708144545555115, 0.6606335043907166, 0.668552041053772, 0.6640271544456482, 0.662895917892456, 0.6595022678375244, 0.6617646813392639, 0.6617646813392639, 0.6696832776069641, 0.6583710312843323, 0.6640271544456482, 0.662895917892456, 0.6640271544456482, 0.6595022678375244, 0.6561086177825928, 0.6606335043907166, 0.6651583909988403, 0.6561086177825928, 0.6674208045005798, 0.662895917892456, 0.6696832776069641, 0.6674208045005798, 0.6662895679473877, 0.6595022678375244, 0.662895917892456, 0.6583710312843323, 0.662895917892456, 0.6651583909988403, 0.6662895679473877, 0.6583710312843323, 0.662895917892456, 0.6595022678375244, 0.6334841847419739, 0.6572397947311401, 0.651583731174469, 0.662895917892456, 0.6640271544456482, 0.6640271544456482, 0.6572397947311401]}\n","45/45 [==============================] - 0s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 5s 32ms/step - loss: 0.6444 - accuracy: 0.7587 - val_loss: 0.8470 - val_accuracy: 0.5919\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6216 - accuracy: 0.7687 - val_loss: 0.8506 - val_accuracy: 0.5062\n","Epoch 3/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6161 - accuracy: 0.7713 - val_loss: 0.8468 - val_accuracy: 0.5176\n","Epoch 4/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6097 - accuracy: 0.7798 - val_loss: 0.8470 - val_accuracy: 0.5145\n","Epoch 5/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6084 - accuracy: 0.7729 - val_loss: 0.8455 - val_accuracy: 0.5196\n","Epoch 6/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6065 - accuracy: 0.7796 - val_loss: 0.8406 - val_accuracy: 0.5207\n","Epoch 7/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6139 - accuracy: 0.7762 - val_loss: 0.8404 - val_accuracy: 0.5258\n","Epoch 8/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6094 - accuracy: 0.7641 - val_loss: 0.8378 - val_accuracy: 0.5310\n","Epoch 9/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5929 - accuracy: 0.7884 - val_loss: 0.8213 - val_accuracy: 0.5837\n","Epoch 10/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5960 - accuracy: 0.7829 - val_loss: 0.8186 - val_accuracy: 0.5816\n","Epoch 11/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6055 - accuracy: 0.7804 - val_loss: 0.8231 - val_accuracy: 0.5610\n","Epoch 12/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5993 - accuracy: 0.7873 - val_loss: 0.8281 - val_accuracy: 0.5517\n","Epoch 13/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5888 - accuracy: 0.7871 - val_loss: 0.8142 - val_accuracy: 0.5764\n","Epoch 14/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5923 - accuracy: 0.7863 - val_loss: 0.8224 - val_accuracy: 0.5806\n","Epoch 15/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5903 - accuracy: 0.7842 - val_loss: 0.8148 - val_accuracy: 0.5702\n","Epoch 16/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5861 - accuracy: 0.7930 - val_loss: 0.8057 - val_accuracy: 0.6033\n","Epoch 17/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5809 - accuracy: 0.7894 - val_loss: 0.8160 - val_accuracy: 0.5992\n","Epoch 18/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5882 - accuracy: 0.7941 - val_loss: 0.8219 - val_accuracy: 0.6240\n","Epoch 19/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.5792 - accuracy: 0.7961 - val_loss: 0.8317 - val_accuracy: 0.6250\n","Epoch 20/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.5870 - accuracy: 0.7886 - val_loss: 0.8108 - val_accuracy: 0.6343\n","Epoch 21/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5917 - accuracy: 0.7889 - val_loss: 0.8064 - val_accuracy: 0.6240\n","Epoch 22/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5754 - accuracy: 0.7987 - val_loss: 0.8099 - val_accuracy: 0.6188\n","Epoch 23/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5759 - accuracy: 0.7974 - val_loss: 0.8501 - val_accuracy: 0.6064\n","Epoch 24/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.5749 - accuracy: 0.7995 - val_loss: 0.8282 - val_accuracy: 0.6374\n","Epoch 25/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5943 - accuracy: 0.7801 - val_loss: 0.8551 - val_accuracy: 0.6002\n","Epoch 26/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5775 - accuracy: 0.7938 - val_loss: 0.8381 - val_accuracy: 0.6209\n","Epoch 27/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5734 - accuracy: 0.7863 - val_loss: 0.8468 - val_accuracy: 0.6281\n","Epoch 28/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5730 - accuracy: 0.7907 - val_loss: 0.8729 - val_accuracy: 0.6415\n","Epoch 29/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5759 - accuracy: 0.7884 - val_loss: 0.8691 - val_accuracy: 0.6209\n","Epoch 30/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5671 - accuracy: 0.7995 - val_loss: 0.8843 - val_accuracy: 0.6209\n","Epoch 31/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5643 - accuracy: 0.8072 - val_loss: 0.8666 - val_accuracy: 0.6250\n","Epoch 32/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5660 - accuracy: 0.8010 - val_loss: 0.9154 - val_accuracy: 0.6260\n","Epoch 33/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5596 - accuracy: 0.8080 - val_loss: 0.8614 - val_accuracy: 0.6126\n","Epoch 34/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5671 - accuracy: 0.7990 - val_loss: 0.9187 - val_accuracy: 0.6033\n","Epoch 35/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5532 - accuracy: 0.8165 - val_loss: 0.8841 - val_accuracy: 0.6322\n","Epoch 36/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5587 - accuracy: 0.8049 - val_loss: 0.9170 - val_accuracy: 0.6353\n","Epoch 37/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5568 - accuracy: 0.8016 - val_loss: 0.9365 - val_accuracy: 0.6271\n","Epoch 38/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5560 - accuracy: 0.8096 - val_loss: 0.8841 - val_accuracy: 0.6147\n","Epoch 39/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5533 - accuracy: 0.8083 - val_loss: 0.8925 - val_accuracy: 0.6023\n","Epoch 40/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5568 - accuracy: 0.8111 - val_loss: 0.9156 - val_accuracy: 0.6260\n","Epoch 41/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5562 - accuracy: 0.8114 - val_loss: 0.9020 - val_accuracy: 0.6333\n","Epoch 42/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5768 - accuracy: 0.8047 - val_loss: 0.9311 - val_accuracy: 0.6188\n","Epoch 43/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5582 - accuracy: 0.8137 - val_loss: 0.9331 - val_accuracy: 0.6209\n","Epoch 44/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5566 - accuracy: 0.8090 - val_loss: 0.9297 - val_accuracy: 0.6333\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5528 - accuracy: 0.8103 - val_loss: 0.9183 - val_accuracy: 0.6167\n","Epoch 46/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5420 - accuracy: 0.8204 - val_loss: 0.8956 - val_accuracy: 0.6157\n","Epoch 47/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5569 - accuracy: 0.8062 - val_loss: 0.9517 - val_accuracy: 0.5919\n","Epoch 48/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5446 - accuracy: 0.8116 - val_loss: 0.9258 - val_accuracy: 0.5940\n","Epoch 49/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5367 - accuracy: 0.8173 - val_loss: 0.9046 - val_accuracy: 0.6209\n","Epoch 50/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5390 - accuracy: 0.8178 - val_loss: 0.9729 - val_accuracy: 0.6291\n","Epoch 51/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5368 - accuracy: 0.8202 - val_loss: 0.9298 - val_accuracy: 0.5981\n","Epoch 52/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5483 - accuracy: 0.8062 - val_loss: 0.9290 - val_accuracy: 0.6198\n","Epoch 53/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5347 - accuracy: 0.8245 - val_loss: 0.9069 - val_accuracy: 0.6157\n","Epoch 54/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5408 - accuracy: 0.8178 - val_loss: 0.9123 - val_accuracy: 0.6147\n","Epoch 55/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5431 - accuracy: 0.8147 - val_loss: 0.9128 - val_accuracy: 0.5961\n","Epoch 56/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5438 - accuracy: 0.8165 - val_loss: 1.0155 - val_accuracy: 0.6002\n","Epoch 57/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5544 - accuracy: 0.8075 - val_loss: 1.0157 - val_accuracy: 0.5992\n","Epoch 58/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5381 - accuracy: 0.8171 - val_loss: 0.9208 - val_accuracy: 0.6188\n","Epoch 59/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5436 - accuracy: 0.8178 - val_loss: 0.9447 - val_accuracy: 0.6064\n","Epoch 60/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5372 - accuracy: 0.8243 - val_loss: 0.9349 - val_accuracy: 0.6116\n","Epoch 61/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5332 - accuracy: 0.8225 - val_loss: 0.9343 - val_accuracy: 0.6012\n","Epoch 62/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5252 - accuracy: 0.8302 - val_loss: 0.9750 - val_accuracy: 0.5888\n","Epoch 63/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5234 - accuracy: 0.8276 - val_loss: 0.9427 - val_accuracy: 0.6167\n","Epoch 64/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5396 - accuracy: 0.8279 - val_loss: 0.9655 - val_accuracy: 0.6229\n","Epoch 65/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5331 - accuracy: 0.8225 - val_loss: 0.9878 - val_accuracy: 0.6229\n","Epoch 66/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5294 - accuracy: 0.8240 - val_loss: 1.0242 - val_accuracy: 0.5857\n","Epoch 67/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5318 - accuracy: 0.8274 - val_loss: 0.9418 - val_accuracy: 0.6054\n","Epoch 68/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5238 - accuracy: 0.8204 - val_loss: 0.9347 - val_accuracy: 0.6105\n","Epoch 69/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5238 - accuracy: 0.8289 - val_loss: 0.9538 - val_accuracy: 0.6054\n","Epoch 70/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5288 - accuracy: 0.8217 - val_loss: 0.9979 - val_accuracy: 0.6250\n","Epoch 71/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5170 - accuracy: 0.8333 - val_loss: 0.9324 - val_accuracy: 0.6043\n","Epoch 72/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5135 - accuracy: 0.8388 - val_loss: 0.9448 - val_accuracy: 0.6188\n","Epoch 73/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5128 - accuracy: 0.8305 - val_loss: 0.9851 - val_accuracy: 0.5992\n","Epoch 74/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5465 - accuracy: 0.8147 - val_loss: 0.9887 - val_accuracy: 0.5878\n","Epoch 75/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5147 - accuracy: 0.8302 - val_loss: 1.0109 - val_accuracy: 0.6178\n","Epoch 76/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5146 - accuracy: 0.8344 - val_loss: 0.9328 - val_accuracy: 0.6126\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5096 - accuracy: 0.8346 - val_loss: 0.9716 - val_accuracy: 0.6116\n","Epoch 78/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5099 - accuracy: 0.8289 - val_loss: 0.9487 - val_accuracy: 0.6240\n","Epoch 79/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5174 - accuracy: 0.8171 - val_loss: 0.9481 - val_accuracy: 0.5826\n","Epoch 80/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5161 - accuracy: 0.8401 - val_loss: 0.9891 - val_accuracy: 0.5868\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5074 - accuracy: 0.8390 - val_loss: 0.9863 - val_accuracy: 0.6209\n","Epoch 82/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5039 - accuracy: 0.8401 - val_loss: 1.0200 - val_accuracy: 0.6105\n","Epoch 83/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5303 - accuracy: 0.8235 - val_loss: 1.0664 - val_accuracy: 0.6033\n","Epoch 84/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5138 - accuracy: 0.8439 - val_loss: 0.9928 - val_accuracy: 0.6074\n","Epoch 85/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5098 - accuracy: 0.8269 - val_loss: 0.9934 - val_accuracy: 0.6178\n","Epoch 86/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4995 - accuracy: 0.8390 - val_loss: 1.0830 - val_accuracy: 0.5899\n","Epoch 87/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5069 - accuracy: 0.8411 - val_loss: 0.9913 - val_accuracy: 0.6105\n","Epoch 88/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4990 - accuracy: 0.8419 - val_loss: 1.0220 - val_accuracy: 0.6136\n","Epoch 89/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5013 - accuracy: 0.8475 - val_loss: 1.0533 - val_accuracy: 0.6209\n","Epoch 90/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5215 - accuracy: 0.8377 - val_loss: 1.0248 - val_accuracy: 0.6074\n","Epoch 91/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5114 - accuracy: 0.8455 - val_loss: 1.0779 - val_accuracy: 0.6198\n","Epoch 92/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5079 - accuracy: 0.8313 - val_loss: 1.0040 - val_accuracy: 0.5930\n","Epoch 93/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4913 - accuracy: 0.8545 - val_loss: 0.9977 - val_accuracy: 0.5961\n","Epoch 94/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4930 - accuracy: 0.8463 - val_loss: 1.0701 - val_accuracy: 0.6198\n","Epoch 95/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4997 - accuracy: 0.8323 - val_loss: 1.0733 - val_accuracy: 0.6229\n","Epoch 96/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4982 - accuracy: 0.8401 - val_loss: 1.0279 - val_accuracy: 0.6012\n","Epoch 97/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4940 - accuracy: 0.8488 - val_loss: 1.0264 - val_accuracy: 0.6033\n","Epoch 98/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4896 - accuracy: 0.8475 - val_loss: 1.0138 - val_accuracy: 0.5992\n","Epoch 99/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4896 - accuracy: 0.8530 - val_loss: 0.9934 - val_accuracy: 0.6043\n","Epoch 100/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4926 - accuracy: 0.8519 - val_loss: 1.0206 - val_accuracy: 0.5950\n","{'loss': [0.6444353461265564, 0.6216453909873962, 0.6161147356033325, 0.6096929907798767, 0.6084014177322388, 0.606453537940979, 0.6139424443244934, 0.6094470024108887, 0.5929422378540039, 0.5959663987159729, 0.6054665446281433, 0.5992867946624756, 0.5888330936431885, 0.5922594666481018, 0.5902938842773438, 0.5861458778381348, 0.580882728099823, 0.5882311463356018, 0.5792016386985779, 0.5870180130004883, 0.5917401909828186, 0.5754126906394958, 0.5758797526359558, 0.5749041438102722, 0.5942566394805908, 0.5775305032730103, 0.5733701586723328, 0.5729855895042419, 0.5759478807449341, 0.5671214461326599, 0.5642715096473694, 0.56598961353302, 0.559589147567749, 0.5671347975730896, 0.5531876087188721, 0.5587183833122253, 0.5568372011184692, 0.5560157895088196, 0.5532978773117065, 0.5567865371704102, 0.5561888813972473, 0.576778769493103, 0.5581668615341187, 0.5565600991249084, 0.5528370141983032, 0.5420417189598083, 0.5568990707397461, 0.5445634126663208, 0.5366958379745483, 0.5389625430107117, 0.5367881655693054, 0.5482682585716248, 0.534722089767456, 0.5408374667167664, 0.5430522561073303, 0.5437500476837158, 0.5543712973594666, 0.5380897521972656, 0.5435596704483032, 0.5371577143669128, 0.5332228541374207, 0.5252475738525391, 0.523445188999176, 0.5395781397819519, 0.5330743193626404, 0.5294271111488342, 0.531764805316925, 0.5237872004508972, 0.5238070487976074, 0.528753399848938, 0.5170045495033264, 0.513511598110199, 0.5127739310264587, 0.5464905500411987, 0.5147215127944946, 0.5146069526672363, 0.5095713138580322, 0.5098915696144104, 0.5173909068107605, 0.5161151885986328, 0.5073602199554443, 0.5038923621177673, 0.5302953124046326, 0.5137850046157837, 0.5097811222076416, 0.49946561455726624, 0.5068702697753906, 0.4989663362503052, 0.5013464093208313, 0.5215120315551758, 0.5114387273788452, 0.5079429745674133, 0.49128228425979614, 0.49297091364860535, 0.4997144043445587, 0.49824610352516174, 0.49399974942207336, 0.4895758628845215, 0.48958173394203186, 0.492565393447876], 'accuracy': [0.7586563229560852, 0.7687338590621948, 0.7713178396224976, 0.7798449397087097, 0.7728682160377502, 0.7795865535736084, 0.7762274146080017, 0.764082670211792, 0.7883720993995667, 0.7829457521438599, 0.7803617715835571, 0.7873384952545166, 0.7870801091194153, 0.7863048911094666, 0.7842377424240112, 0.7930232286453247, 0.7894057035446167, 0.7940568327903748, 0.7961240410804749, 0.788630485534668, 0.7888888716697693, 0.7987080216407776, 0.7974160313606262, 0.7994831800460815, 0.7801033854484558, 0.7937984466552734, 0.7863048911094666, 0.7906976938247681, 0.7883720993995667, 0.7994831800460815, 0.8072351217269897, 0.801033616065979, 0.8080103397369385, 0.7989664077758789, 0.8165374398231506, 0.8049095869064331, 0.8015503883361816, 0.8095607161521912, 0.8082687258720398, 0.8111110925674438, 0.8113695383071899, 0.804651141166687, 0.8136950731277466, 0.8090439438819885, 0.8103359341621399, 0.8204134106636047, 0.8062015771865845, 0.8116279244422913, 0.8173126578330994, 0.817829430103302, 0.8201550245285034, 0.8062015771865845, 0.8245478272438049, 0.817829430103302, 0.8147286772727966, 0.8165374398231506, 0.8074935674667358, 0.817054271697998, 0.817829430103302, 0.8242893815040588, 0.8224806189537048, 0.830232560634613, 0.8276485800743103, 0.8279069662094116, 0.8224806189537048, 0.8240309953689575, 0.827390193939209, 0.8204134106636047, 0.8289405703544617, 0.8217054009437561, 0.8333333134651184, 0.8387596607208252, 0.8304909467697144, 0.8147286772727966, 0.830232560634613, 0.8343669176101685, 0.8346253037452698, 0.8289405703544617, 0.817054271697998, 0.8400516510009766, 0.8390181064605713, 0.8400516510009766, 0.8235142230987549, 0.8439276218414307, 0.8268733620643616, 0.8390181064605713, 0.8410852551460266, 0.8418604731559753, 0.8475452065467834, 0.8377261161804199, 0.8454780578613281, 0.8312661647796631, 0.8545219898223877, 0.8462532162666321, 0.8322997689247131, 0.8400516510009766, 0.8488371968269348, 0.8475452065467834, 0.8529715538024902, 0.851938009262085], 'val_loss': [0.8469704985618591, 0.8506134152412415, 0.8467697501182556, 0.847019612789154, 0.8454751372337341, 0.8405627608299255, 0.8404343128204346, 0.8377518653869629, 0.8212770819664001, 0.8185518980026245, 0.8231328129768372, 0.8281263113021851, 0.8142005801200867, 0.8223863244056702, 0.8148273229598999, 0.8057281374931335, 0.8159898519515991, 0.821936309337616, 0.8316771388053894, 0.8107767701148987, 0.8063792586326599, 0.8098888397216797, 0.8501451015472412, 0.828161358833313, 0.855108380317688, 0.8381088376045227, 0.8468057513237, 0.872893214225769, 0.8691431879997253, 0.8842512965202332, 0.8666357398033142, 0.9154479503631592, 0.8614203333854675, 0.9187294244766235, 0.8840677738189697, 0.917009174823761, 0.9365335702896118, 0.8840740323066711, 0.8924981355667114, 0.9155653715133667, 0.90200275182724, 0.9311404228210449, 0.9330562949180603, 0.9296533465385437, 0.9182872176170349, 0.8955806493759155, 0.9517196416854858, 0.9257645606994629, 0.9046497941017151, 0.9729409217834473, 0.9298076033592224, 0.9289979934692383, 0.9069189429283142, 0.9122608304023743, 0.9128204584121704, 1.0155198574066162, 1.0157020092010498, 0.9208018183708191, 0.9447043538093567, 0.9348632097244263, 0.9343147873878479, 0.9749863147735596, 0.9427425861358643, 0.9655290246009827, 0.9877876043319702, 1.024223804473877, 0.9418267607688904, 0.9347244501113892, 0.9537779688835144, 0.9978933930397034, 0.9324238300323486, 0.9448428750038147, 0.9851140975952148, 0.9887055158615112, 1.010852575302124, 0.9328001141548157, 0.9715735912322998, 0.9486970901489258, 0.9481257200241089, 0.9891425967216492, 0.9863374829292297, 1.020018458366394, 1.0663840770721436, 0.9928033351898193, 0.9934203028678894, 1.0829999446868896, 0.9912673234939575, 1.0220016241073608, 1.0532777309417725, 1.0247776508331299, 1.077884554862976, 1.0039786100387573, 0.9977155923843384, 1.070069670677185, 1.0733009576797485, 1.0279464721679688, 1.0263910293579102, 1.01382315158844, 0.9933777451515198, 1.0205615758895874], 'val_accuracy': [0.5919421315193176, 0.5061983466148376, 0.5175619721412659, 0.5144628286361694, 0.51962810754776, 0.5206611752510071, 0.5258264541625977, 0.5309917330741882, 0.5836777091026306, 0.5816115736961365, 0.5609503984451294, 0.5516529083251953, 0.5764462947845459, 0.5805785059928894, 0.5702479481697083, 0.6033057570457458, 0.5991735458374023, 0.6239669322967529, 0.625, 0.6342975497245789, 0.6239669322967529, 0.6188016533851624, 0.6064049601554871, 0.6373966932296753, 0.6002066135406494, 0.6208677887916565, 0.6280992031097412, 0.6415289044380188, 0.6208677887916565, 0.6208677887916565, 0.625, 0.6260330677032471, 0.6126033067703247, 0.6033057570457458, 0.6322314143180847, 0.6353305578231812, 0.6270661354064941, 0.6146694421768188, 0.6022727489471436, 0.6260330677032471, 0.6332644820213318, 0.6188016533851624, 0.6208677887916565, 0.6332644820213318, 0.6167355179786682, 0.6157024502754211, 0.5919421315193176, 0.5940082669258118, 0.6208677887916565, 0.6291322112083435, 0.5981404781341553, 0.6198347210884094, 0.6157024502754211, 0.6146694421768188, 0.5960744023323059, 0.6002066135406494, 0.5991735458374023, 0.6188016533851624, 0.6064049601554871, 0.6115702390670776, 0.6012396812438965, 0.5888429880142212, 0.6167355179786682, 0.6229338645935059, 0.6229338645935059, 0.58574378490448, 0.60537189245224, 0.6105371713638306, 0.60537189245224, 0.625, 0.6043388247489929, 0.6188016533851624, 0.5991735458374023, 0.5878099203109741, 0.6177685856819153, 0.6126033067703247, 0.6115702390670776, 0.6239669322967529, 0.5826446413993835, 0.586776852607727, 0.6208677887916565, 0.6105371713638306, 0.6033057570457458, 0.6074380278587341, 0.6177685856819153, 0.5898760557174683, 0.6105371713638306, 0.6136363744735718, 0.6208677887916565, 0.6074380278587341, 0.6198347210884094, 0.5929751992225647, 0.5960744023323059, 0.6198347210884094, 0.6229338645935059, 0.6012396812438965, 0.6033057570457458, 0.5991735458374023, 0.6043388247489929, 0.5950413346290588]}\n","32/32 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 4s 33ms/step - loss: 0.5307 - accuracy: 0.8227 - val_loss: 0.8584 - val_accuracy: 0.4838\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 1s 19ms/step - loss: 0.5048 - accuracy: 0.8327 - val_loss: 0.8624 - val_accuracy: 0.4914\n","Epoch 3/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.5063 - accuracy: 0.8246 - val_loss: 0.8485 - val_accuracy: 0.5054\n","Epoch 4/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.5098 - accuracy: 0.8367 - val_loss: 0.8453 - val_accuracy: 0.6056\n","Epoch 5/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4940 - accuracy: 0.8481 - val_loss: 0.8423 - val_accuracy: 0.5291\n","Epoch 6/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4986 - accuracy: 0.8303 - val_loss: 0.8619 - val_accuracy: 0.5022\n","Epoch 7/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4903 - accuracy: 0.8446 - val_loss: 0.8562 - val_accuracy: 0.5075\n","Epoch 8/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4852 - accuracy: 0.8462 - val_loss: 0.8225 - val_accuracy: 0.5927\n","Epoch 9/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4886 - accuracy: 0.8448 - val_loss: 0.8628 - val_accuracy: 0.5237\n","Epoch 10/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4910 - accuracy: 0.8338 - val_loss: 0.8293 - val_accuracy: 0.5420\n","Epoch 11/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4889 - accuracy: 0.8470 - val_loss: 0.8142 - val_accuracy: 0.5884\n","Epoch 12/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.4825 - accuracy: 0.8508 - val_loss: 0.7980 - val_accuracy: 0.6422\n","Epoch 13/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4821 - accuracy: 0.8483 - val_loss: 0.8005 - val_accuracy: 0.6552\n","Epoch 14/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.4770 - accuracy: 0.8502 - val_loss: 0.8068 - val_accuracy: 0.6616\n","Epoch 15/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4712 - accuracy: 0.8578 - val_loss: 0.8250 - val_accuracy: 0.6466\n","Epoch 16/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4758 - accuracy: 0.8494 - val_loss: 0.8326 - val_accuracy: 0.5884\n","Epoch 17/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4798 - accuracy: 0.8556 - val_loss: 0.8293 - val_accuracy: 0.6767\n","Epoch 18/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4740 - accuracy: 0.8540 - val_loss: 0.8115 - val_accuracy: 0.6487\n","Epoch 19/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4714 - accuracy: 0.8564 - val_loss: 0.8145 - val_accuracy: 0.6994\n","Epoch 20/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4829 - accuracy: 0.8440 - val_loss: 0.8342 - val_accuracy: 0.6810\n","Epoch 21/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.4745 - accuracy: 0.8510 - val_loss: 0.8291 - val_accuracy: 0.7047\n","Epoch 22/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4662 - accuracy: 0.8621 - val_loss: 0.8892 - val_accuracy: 0.6886\n","Epoch 23/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.4700 - accuracy: 0.8532 - val_loss: 0.8096 - val_accuracy: 0.7155\n","Epoch 24/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4738 - accuracy: 0.8553 - val_loss: 0.9106 - val_accuracy: 0.6272\n","Epoch 25/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4670 - accuracy: 0.8615 - val_loss: 0.8382 - val_accuracy: 0.6864\n","Epoch 26/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4662 - accuracy: 0.8580 - val_loss: 0.8376 - val_accuracy: 0.7188\n","Epoch 27/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4620 - accuracy: 0.8572 - val_loss: 0.8549 - val_accuracy: 0.7091\n","Epoch 28/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4584 - accuracy: 0.8640 - val_loss: 0.8591 - val_accuracy: 0.6918\n","Epoch 29/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4661 - accuracy: 0.8634 - val_loss: 0.8783 - val_accuracy: 0.7080\n","Epoch 30/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4519 - accuracy: 0.8688 - val_loss: 0.8784 - val_accuracy: 0.7123\n","Epoch 31/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4608 - accuracy: 0.8580 - val_loss: 0.8837 - val_accuracy: 0.6972\n","Epoch 32/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4808 - accuracy: 0.8572 - val_loss: 0.9054 - val_accuracy: 0.6778\n","Epoch 33/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4654 - accuracy: 0.8591 - val_loss: 0.8844 - val_accuracy: 0.7101\n","Epoch 34/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4562 - accuracy: 0.8672 - val_loss: 0.9008 - val_accuracy: 0.7080\n","Epoch 35/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4495 - accuracy: 0.8696 - val_loss: 0.8701 - val_accuracy: 0.7101\n","Epoch 36/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4467 - accuracy: 0.8755 - val_loss: 0.9265 - val_accuracy: 0.6800\n","Epoch 37/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4469 - accuracy: 0.8723 - val_loss: 0.9082 - val_accuracy: 0.7026\n","Epoch 38/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4451 - accuracy: 0.8693 - val_loss: 0.9868 - val_accuracy: 0.7144\n","Epoch 39/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4476 - accuracy: 0.8699 - val_loss: 0.8843 - val_accuracy: 0.6897\n","Epoch 40/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4425 - accuracy: 0.8728 - val_loss: 0.9838 - val_accuracy: 0.7037\n","Epoch 41/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4482 - accuracy: 0.8613 - val_loss: 0.9008 - val_accuracy: 0.7069\n","Epoch 42/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4392 - accuracy: 0.8782 - val_loss: 0.9196 - val_accuracy: 0.7091\n","Epoch 43/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4360 - accuracy: 0.8807 - val_loss: 0.9311 - val_accuracy: 0.7037\n","Epoch 44/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4383 - accuracy: 0.8737 - val_loss: 0.9065 - val_accuracy: 0.7144\n","Epoch 45/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4617 - accuracy: 0.8610 - val_loss: 1.1763 - val_accuracy: 0.6972\n","Epoch 46/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4432 - accuracy: 0.8696 - val_loss: 0.9763 - val_accuracy: 0.6810\n","Epoch 47/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4351 - accuracy: 0.8817 - val_loss: 1.1528 - val_accuracy: 0.6918\n","Epoch 48/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4430 - accuracy: 0.8685 - val_loss: 0.9848 - val_accuracy: 0.7026\n","Epoch 49/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4411 - accuracy: 0.8712 - val_loss: 0.9888 - val_accuracy: 0.7047\n","Epoch 50/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4351 - accuracy: 0.8761 - val_loss: 1.0259 - val_accuracy: 0.6843\n","Epoch 51/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4322 - accuracy: 0.8747 - val_loss: 0.9795 - val_accuracy: 0.7015\n","Epoch 52/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4310 - accuracy: 0.8858 - val_loss: 0.9901 - val_accuracy: 0.6864\n","Epoch 53/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4477 - accuracy: 0.8718 - val_loss: 1.0110 - val_accuracy: 0.6800\n","Epoch 54/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4510 - accuracy: 0.8766 - val_loss: 1.2089 - val_accuracy: 0.6552\n","Epoch 55/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4584 - accuracy: 0.8591 - val_loss: 1.1207 - val_accuracy: 0.6713\n","Epoch 56/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4371 - accuracy: 0.8750 - val_loss: 0.9735 - val_accuracy: 0.7004\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4309 - accuracy: 0.8777 - val_loss: 0.9959 - val_accuracy: 0.6918\n","Epoch 58/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4332 - accuracy: 0.8793 - val_loss: 0.9587 - val_accuracy: 0.7091\n","Epoch 59/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4387 - accuracy: 0.8747 - val_loss: 1.1008 - val_accuracy: 0.7026\n","Epoch 60/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4404 - accuracy: 0.8734 - val_loss: 1.0468 - val_accuracy: 0.6821\n","Epoch 61/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4276 - accuracy: 0.8836 - val_loss: 1.0583 - val_accuracy: 0.6584\n","Epoch 62/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4247 - accuracy: 0.8809 - val_loss: 1.0259 - val_accuracy: 0.7091\n","Epoch 63/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4154 - accuracy: 0.8914 - val_loss: 0.9643 - val_accuracy: 0.7112\n","Epoch 64/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4174 - accuracy: 0.8920 - val_loss: 0.9865 - val_accuracy: 0.7080\n","Epoch 65/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4186 - accuracy: 0.8828 - val_loss: 1.0033 - val_accuracy: 0.6853\n","Epoch 66/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4251 - accuracy: 0.8788 - val_loss: 1.0420 - val_accuracy: 0.6983\n","Epoch 67/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4239 - accuracy: 0.8834 - val_loss: 1.0020 - val_accuracy: 0.7037\n","Epoch 68/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4103 - accuracy: 0.8933 - val_loss: 1.0753 - val_accuracy: 0.6875\n","Epoch 69/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4111 - accuracy: 0.8930 - val_loss: 1.0504 - val_accuracy: 0.6897\n","Epoch 70/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4100 - accuracy: 0.8906 - val_loss: 1.0267 - val_accuracy: 0.7004\n","Epoch 71/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4101 - accuracy: 0.8939 - val_loss: 0.9901 - val_accuracy: 0.7047\n","Epoch 72/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4148 - accuracy: 0.8828 - val_loss: 1.0201 - val_accuracy: 0.6940\n","Epoch 73/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4078 - accuracy: 0.8869 - val_loss: 1.0046 - val_accuracy: 0.7058\n","Epoch 74/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4079 - accuracy: 0.8863 - val_loss: 1.0376 - val_accuracy: 0.6832\n","Epoch 75/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4144 - accuracy: 0.8947 - val_loss: 1.0318 - val_accuracy: 0.6864\n","Epoch 76/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4146 - accuracy: 0.8906 - val_loss: 1.0446 - val_accuracy: 0.6907\n","Epoch 77/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4273 - accuracy: 0.8790 - val_loss: 1.0312 - val_accuracy: 0.6735\n","Epoch 78/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4203 - accuracy: 0.8887 - val_loss: 1.0415 - val_accuracy: 0.6950\n","Epoch 79/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4062 - accuracy: 0.8906 - val_loss: 1.0875 - val_accuracy: 0.6907\n","Epoch 80/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4021 - accuracy: 0.8990 - val_loss: 1.1492 - val_accuracy: 0.6541\n","Epoch 81/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4049 - accuracy: 0.8930 - val_loss: 1.0446 - val_accuracy: 0.7015\n","Epoch 82/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4019 - accuracy: 0.8914 - val_loss: 1.1137 - val_accuracy: 0.6810\n","Epoch 83/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4120 - accuracy: 0.8960 - val_loss: 1.0756 - val_accuracy: 0.6843\n","Epoch 84/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4075 - accuracy: 0.8879 - val_loss: 1.0352 - val_accuracy: 0.6778\n","Epoch 85/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4077 - accuracy: 0.8882 - val_loss: 1.0779 - val_accuracy: 0.6929\n","Epoch 86/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3980 - accuracy: 0.9044 - val_loss: 1.1480 - val_accuracy: 0.6875\n","Epoch 87/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4100 - accuracy: 0.8944 - val_loss: 1.0816 - val_accuracy: 0.6972\n","Epoch 88/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4144 - accuracy: 0.8828 - val_loss: 1.1668 - val_accuracy: 0.6735\n","Epoch 89/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4076 - accuracy: 0.8893 - val_loss: 1.1839 - val_accuracy: 0.6810\n","Epoch 90/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4003 - accuracy: 0.8928 - val_loss: 1.1565 - val_accuracy: 0.6907\n","Epoch 91/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3975 - accuracy: 0.8947 - val_loss: 1.1501 - val_accuracy: 0.6562\n","Epoch 92/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3929 - accuracy: 0.8971 - val_loss: 1.1208 - val_accuracy: 0.6994\n","Epoch 93/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3890 - accuracy: 0.9006 - val_loss: 1.2075 - val_accuracy: 0.6767\n","Epoch 94/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4073 - accuracy: 0.8788 - val_loss: 1.1689 - val_accuracy: 0.6983\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4003 - accuracy: 0.8979 - val_loss: 1.2245 - val_accuracy: 0.6595\n","Epoch 96/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3940 - accuracy: 0.8982 - val_loss: 1.1254 - val_accuracy: 0.6875\n","Epoch 97/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3887 - accuracy: 0.9003 - val_loss: 1.1421 - val_accuracy: 0.6907\n","Epoch 98/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3941 - accuracy: 0.9003 - val_loss: 1.3075 - val_accuracy: 0.6498\n","Epoch 99/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4094 - accuracy: 0.8863 - val_loss: 1.4245 - val_accuracy: 0.6455\n","Epoch 100/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4057 - accuracy: 0.8917 - val_loss: 1.1118 - val_accuracy: 0.6800\n","{'loss': [0.5306922197341919, 0.5048315525054932, 0.5063026547431946, 0.5097740292549133, 0.4940345287322998, 0.4985637962818146, 0.49034395813941956, 0.4852277934551239, 0.4885988235473633, 0.4909595549106598, 0.4889200031757355, 0.4824748635292053, 0.48214656114578247, 0.476960688829422, 0.4711800813674927, 0.4757848381996155, 0.47982174158096313, 0.4739953279495239, 0.4713864028453827, 0.48290762305259705, 0.4745498299598694, 0.46617552638053894, 0.47001707553863525, 0.47383517026901245, 0.4669695198535919, 0.46615755558013916, 0.46195411682128906, 0.45840680599212646, 0.4660518169403076, 0.45185938477516174, 0.460750550031662, 0.4808342754840851, 0.46540889143943787, 0.4561920762062073, 0.44951438903808594, 0.4467253088951111, 0.4469466805458069, 0.4450703561306, 0.4475807249546051, 0.4425371289253235, 0.4482172131538391, 0.43916982412338257, 0.43601149320602417, 0.4382631778717041, 0.4617372453212738, 0.44320449233055115, 0.43506407737731934, 0.4430431127548218, 0.4410991072654724, 0.4351148009300232, 0.4321519732475281, 0.43101468682289124, 0.4476547837257385, 0.45104745030403137, 0.45838525891304016, 0.4371185302734375, 0.4309179186820984, 0.4331677258014679, 0.4387350082397461, 0.44035789370536804, 0.42755022644996643, 0.42468851804733276, 0.4153887629508972, 0.4174047112464905, 0.41861027479171753, 0.4251057803630829, 0.4238571226596832, 0.4102569818496704, 0.4111020863056183, 0.4100451171398163, 0.41009122133255005, 0.4147692322731018, 0.40779680013656616, 0.40790632367134094, 0.41442394256591797, 0.41459470987319946, 0.427322119474411, 0.42027929425239563, 0.40622469782829285, 0.402078241109848, 0.40493544936180115, 0.4018677771091461, 0.41197627782821655, 0.40750738978385925, 0.40767887234687805, 0.3980029225349426, 0.41003739833831787, 0.4143821597099304, 0.4075947403907776, 0.40025103092193604, 0.3974885940551758, 0.3928792178630829, 0.3889523446559906, 0.40725308656692505, 0.40025559067726135, 0.39398714900016785, 0.38865283131599426, 0.39405399560928345, 0.4093559980392456, 0.4056563079357147], 'accuracy': [0.8227370977401733, 0.8327047228813171, 0.8246228694915771, 0.8367456793785095, 0.8480603694915771, 0.8302801847457886, 0.8445581793785095, 0.8461745977401733, 0.8448275923728943, 0.8337823152542114, 0.8469827771186829, 0.8507543206214905, 0.8483297228813171, 0.850215494632721, 0.857758641242981, 0.8494073152542114, 0.8556034564971924, 0.8539870977401733, 0.8564116358757019, 0.8440194129943848, 0.8510237336158752, 0.8620689511299133, 0.853178858757019, 0.8553340435028076, 0.8615301847457886, 0.858027994632721, 0.8572198152542114, 0.8639547228813171, 0.8634159564971924, 0.868803858757019, 0.858027994632721, 0.8572198152542114, 0.8591055870056152, 0.8671875, 0.8696120977401733, 0.8755387663841248, 0.8723060488700867, 0.8693426847457886, 0.8698814511299133, 0.8728448152542114, 0.8612607717514038, 0.8782327771186829, 0.8806573152542114, 0.873652994632721, 0.860991358757019, 0.8696120977401733, 0.8817349076271057, 0.868534505367279, 0.8712284564971924, 0.8760775923728943, 0.8747305870056152, 0.8857758641242981, 0.8717672228813171, 0.876616358757019, 0.8591055870056152, 0.875, 0.8776939511299133, 0.8793103694915771, 0.8747305870056152, 0.873383641242981, 0.8836206793785095, 0.8809267282485962, 0.8914331793785095, 0.891972005367279, 0.8828125, 0.8787715435028076, 0.8833512663841248, 0.8933189511299133, 0.8930495977401733, 0.890625, 0.8938577771186829, 0.8828125, 0.8868534564971924, 0.8863146305084229, 0.8946659564971924, 0.890625, 0.8790409564971924, 0.8887392282485962, 0.890625, 0.8989762663841248, 0.8930495977401733, 0.8914331793785095, 0.8960129022598267, 0.8879310488700867, 0.8882004022598267, 0.9043642282485962, 0.8943965435028076, 0.8828125, 0.889277994632721, 0.8927801847457886, 0.8946659564971924, 0.897090494632721, 0.9005926847457886, 0.8787715435028076, 0.8978987336158752, 0.8981680870056152, 0.9003232717514038, 0.9003232717514038, 0.8863146305084229, 0.8917025923728943], 'val_loss': [0.8583654165267944, 0.86238032579422, 0.8485038876533508, 0.8452766537666321, 0.8423284888267517, 0.8618918657302856, 0.8561910390853882, 0.8224653601646423, 0.8628186583518982, 0.8293172121047974, 0.8141843676567078, 0.7980042099952698, 0.8004732728004456, 0.8067811727523804, 0.824962854385376, 0.8326157331466675, 0.8293195366859436, 0.8115133047103882, 0.8145270347595215, 0.8341804146766663, 0.829122006893158, 0.8891520500183105, 0.8095828294754028, 0.9106024503707886, 0.8381931781768799, 0.8375652432441711, 0.854936957359314, 0.8591121435165405, 0.8783067464828491, 0.8784260153770447, 0.8837252855300903, 0.9053908586502075, 0.88441002368927, 0.9008291959762573, 0.8700920343399048, 0.9265124201774597, 0.908187747001648, 0.9868450164794922, 0.8843339085578918, 0.983803927898407, 0.90081787109375, 0.9196003079414368, 0.9311246275901794, 0.9064776301383972, 1.1762945652008057, 0.976340115070343, 1.1527502536773682, 0.9848102927207947, 0.9887798428535461, 1.025925636291504, 0.9794760942459106, 0.9900969862937927, 1.0110270977020264, 1.208930492401123, 1.1207001209259033, 0.9734964966773987, 0.9959405660629272, 0.9586963057518005, 1.1008164882659912, 1.0468212366104126, 1.058290958404541, 1.0259137153625488, 0.9643363356590271, 0.9865060448646545, 1.003272294998169, 1.041968822479248, 1.0020248889923096, 1.0753474235534668, 1.0504318475723267, 1.0267163515090942, 0.9901224970817566, 1.0201438665390015, 1.0045864582061768, 1.0375902652740479, 1.0317927598953247, 1.0445852279663086, 1.0312018394470215, 1.041535496711731, 1.0874912738800049, 1.149219036102295, 1.0446394681930542, 1.113679051399231, 1.075636863708496, 1.035236120223999, 1.077884554862976, 1.1479711532592773, 1.0815997123718262, 1.166794776916504, 1.1839348077774048, 1.1565498113632202, 1.1500624418258667, 1.1207728385925293, 1.207545280456543, 1.1688653230667114, 1.2245445251464844, 1.1254360675811768, 1.1420756578445435, 1.307507038116455, 1.4245020151138306, 1.1118446588516235], 'val_accuracy': [0.48383620381355286, 0.4913793206214905, 0.5053879022598267, 0.6056034564971924, 0.5290948152542114, 0.5021551847457886, 0.5075430870056152, 0.5926724076271057, 0.5237069129943848, 0.5420258641242981, 0.5883620977401733, 0.642241358757019, 0.6551724076271057, 0.6616379022598267, 0.6465517282485962, 0.5883620977401733, 0.6767241358757019, 0.6487069129943848, 0.6993534564971924, 0.681034505367279, 0.704741358757019, 0.6885775923728943, 0.7155172228813171, 0.6271551847457886, 0.6864224076271057, 0.71875, 0.7090517282485962, 0.6918103694915771, 0.7079741358757019, 0.712284505367279, 0.6971982717514038, 0.6778017282485962, 0.7101293206214905, 0.7079741358757019, 0.7101293206214905, 0.6799569129943848, 0.7025862336158752, 0.7144396305084229, 0.6896551847457886, 0.7036637663841248, 0.7068965435028076, 0.7090517282485962, 0.7036637663841248, 0.7144396305084229, 0.6971982717514038, 0.681034505367279, 0.6918103694915771, 0.7025862336158752, 0.704741358757019, 0.6842672228813171, 0.701508641242981, 0.6864224076271057, 0.6799569129943848, 0.6551724076271057, 0.6713362336158752, 0.7004310488700867, 0.6918103694915771, 0.7090517282485962, 0.7025862336158752, 0.6821120977401733, 0.6584051847457886, 0.7090517282485962, 0.7112069129943848, 0.7079741358757019, 0.6853448152542114, 0.6982758641242981, 0.7036637663841248, 0.6875, 0.6896551847457886, 0.7004310488700867, 0.704741358757019, 0.693965494632721, 0.7058189511299133, 0.6831896305084229, 0.6864224076271057, 0.6907327771186829, 0.673491358757019, 0.6950430870056152, 0.6907327771186829, 0.6540948152542114, 0.701508641242981, 0.681034505367279, 0.6842672228813171, 0.6778017282485962, 0.6928879022598267, 0.6875, 0.6971982717514038, 0.673491358757019, 0.681034505367279, 0.6907327771186829, 0.65625, 0.6993534564971924, 0.6767241358757019, 0.6982758641242981, 0.6594827771186829, 0.6875, 0.6907327771186829, 0.649784505367279, 0.6454741358757019, 0.6799569129943848]}\n","38/38 [==============================] - 0s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 5s 35ms/step - loss: 0.5478 - accuracy: 0.8147 - val_loss: 0.8526 - val_accuracy: 0.5102\n","Epoch 2/100\n"," 1/28 [>.............................] - ETA: 0s - loss: 0.4233 - accuracy: 0.9141"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["28/28 [==============================] - 0s 13ms/step - loss: 0.5206 - accuracy: 0.8345 - val_loss: 0.8691 - val_accuracy: 0.5079\n","Epoch 3/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5081 - accuracy: 0.8325 - val_loss: 0.8522 - val_accuracy: 0.5181\n","Epoch 4/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5047 - accuracy: 0.8362 - val_loss: 0.8214 - val_accuracy: 0.5588\n","Epoch 5/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5066 - accuracy: 0.8415 - val_loss: 0.8305 - val_accuracy: 0.5328\n","Epoch 6/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4927 - accuracy: 0.8540 - val_loss: 0.8473 - val_accuracy: 0.5204\n","Epoch 7/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4868 - accuracy: 0.8520 - val_loss: 0.8310 - val_accuracy: 0.5373\n","Epoch 8/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4863 - accuracy: 0.8492 - val_loss: 0.8437 - val_accuracy: 0.5328\n","Epoch 9/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4874 - accuracy: 0.8495 - val_loss: 0.8358 - val_accuracy: 0.5486\n","Epoch 10/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4882 - accuracy: 0.8526 - val_loss: 0.8154 - val_accuracy: 0.5577\n","Epoch 11/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4964 - accuracy: 0.8438 - val_loss: 0.8150 - val_accuracy: 0.5611\n","Epoch 12/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4965 - accuracy: 0.8458 - val_loss: 0.7818 - val_accuracy: 0.6086\n","Epoch 13/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5043 - accuracy: 0.8384 - val_loss: 0.8019 - val_accuracy: 0.5837\n","Epoch 14/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.4816 - accuracy: 0.8588 - val_loss: 0.7583 - val_accuracy: 0.6538\n","Epoch 15/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4886 - accuracy: 0.8580 - val_loss: 0.7737 - val_accuracy: 0.6380\n","Epoch 16/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4743 - accuracy: 0.8577 - val_loss: 0.7571 - val_accuracy: 0.6731\n","Epoch 17/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4743 - accuracy: 0.8611 - val_loss: 0.7996 - val_accuracy: 0.6222\n","Epoch 18/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4908 - accuracy: 0.8463 - val_loss: 0.7574 - val_accuracy: 0.6425\n","Epoch 19/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.4841 - accuracy: 0.8574 - val_loss: 0.8201 - val_accuracy: 0.6199\n","Epoch 20/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4776 - accuracy: 0.8557 - val_loss: 0.7890 - val_accuracy: 0.6414\n","Epoch 21/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.4701 - accuracy: 0.8684 - val_loss: 0.7395 - val_accuracy: 0.7059\n","Epoch 22/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.4668 - accuracy: 0.8568 - val_loss: 0.7584 - val_accuracy: 0.7217\n","Epoch 23/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4679 - accuracy: 0.8639 - val_loss: 0.7327 - val_accuracy: 0.7081\n","Epoch 24/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.4727 - accuracy: 0.8619 - val_loss: 0.7617 - val_accuracy: 0.7251\n","Epoch 25/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4654 - accuracy: 0.8625 - val_loss: 0.7744 - val_accuracy: 0.7206\n","Epoch 26/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4648 - accuracy: 0.8667 - val_loss: 0.7923 - val_accuracy: 0.7115\n","Epoch 27/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.4628 - accuracy: 0.8642 - val_loss: 0.7830 - val_accuracy: 0.7330\n","Epoch 28/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4621 - accuracy: 0.8656 - val_loss: 0.8226 - val_accuracy: 0.7036\n","Epoch 29/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4745 - accuracy: 0.8577 - val_loss: 0.7950 - val_accuracy: 0.7149\n","Epoch 30/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4685 - accuracy: 0.8585 - val_loss: 0.8364 - val_accuracy: 0.7138\n","Epoch 31/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4715 - accuracy: 0.8565 - val_loss: 0.8889 - val_accuracy: 0.7093\n","Epoch 32/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4864 - accuracy: 0.8373 - val_loss: 0.8401 - val_accuracy: 0.7138\n","Epoch 33/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4662 - accuracy: 0.8599 - val_loss: 0.8778 - val_accuracy: 0.7138\n","Epoch 34/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4698 - accuracy: 0.8630 - val_loss: 0.8795 - val_accuracy: 0.7048\n","Epoch 35/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4645 - accuracy: 0.8596 - val_loss: 0.8583 - val_accuracy: 0.7127\n","Epoch 36/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4537 - accuracy: 0.8679 - val_loss: 0.8598 - val_accuracy: 0.7081\n","Epoch 37/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4628 - accuracy: 0.8701 - val_loss: 0.9749 - val_accuracy: 0.6833\n","Epoch 38/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4667 - accuracy: 0.8602 - val_loss: 0.8469 - val_accuracy: 0.7138\n","Epoch 39/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4509 - accuracy: 0.8670 - val_loss: 0.8451 - val_accuracy: 0.7104\n","Epoch 40/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4447 - accuracy: 0.8752 - val_loss: 0.8482 - val_accuracy: 0.7149\n","Epoch 41/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4455 - accuracy: 0.8713 - val_loss: 0.8725 - val_accuracy: 0.7036\n","Epoch 42/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4482 - accuracy: 0.8698 - val_loss: 0.8928 - val_accuracy: 0.7183\n","Epoch 43/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4444 - accuracy: 0.8834 - val_loss: 0.8869 - val_accuracy: 0.7093\n","Epoch 44/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4492 - accuracy: 0.8803 - val_loss: 0.8558 - val_accuracy: 0.7104\n","Epoch 45/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4513 - accuracy: 0.8769 - val_loss: 0.8881 - val_accuracy: 0.7036\n","Epoch 46/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4577 - accuracy: 0.8650 - val_loss: 0.9138 - val_accuracy: 0.7161\n","Epoch 47/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4414 - accuracy: 0.8758 - val_loss: 0.8639 - val_accuracy: 0.7025\n","Epoch 48/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4379 - accuracy: 0.8814 - val_loss: 0.9009 - val_accuracy: 0.7025\n","Epoch 49/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4354 - accuracy: 0.8826 - val_loss: 0.8814 - val_accuracy: 0.7081\n","Epoch 50/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4328 - accuracy: 0.8826 - val_loss: 0.8842 - val_accuracy: 0.7104\n","Epoch 51/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4368 - accuracy: 0.8823 - val_loss: 0.8850 - val_accuracy: 0.7127\n","Epoch 52/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4283 - accuracy: 0.8851 - val_loss: 0.8956 - val_accuracy: 0.7172\n","Epoch 53/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4491 - accuracy: 0.8744 - val_loss: 0.9556 - val_accuracy: 0.6629\n","Epoch 54/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.4491 - accuracy: 0.8755 - val_loss: 0.9274 - val_accuracy: 0.7127\n","Epoch 55/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4523 - accuracy: 0.8704 - val_loss: 0.8711 - val_accuracy: 0.6968\n","Epoch 56/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4587 - accuracy: 0.8701 - val_loss: 0.9068 - val_accuracy: 0.7014\n","Epoch 57/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4322 - accuracy: 0.8817 - val_loss: 0.8862 - val_accuracy: 0.7127\n","Epoch 58/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4389 - accuracy: 0.8766 - val_loss: 0.8915 - val_accuracy: 0.6946\n","Epoch 59/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4390 - accuracy: 0.8772 - val_loss: 0.9251 - val_accuracy: 0.7036\n","Epoch 60/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4247 - accuracy: 0.8879 - val_loss: 0.9048 - val_accuracy: 0.7081\n","Epoch 61/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4307 - accuracy: 0.8741 - val_loss: 0.9152 - val_accuracy: 0.7025\n","Epoch 62/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4272 - accuracy: 0.8803 - val_loss: 0.8903 - val_accuracy: 0.7048\n","Epoch 63/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4240 - accuracy: 0.8894 - val_loss: 0.9202 - val_accuracy: 0.6889\n","Epoch 64/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4259 - accuracy: 0.8806 - val_loss: 0.9444 - val_accuracy: 0.7025\n","Epoch 65/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4219 - accuracy: 0.8879 - val_loss: 0.9435 - val_accuracy: 0.7161\n","Epoch 66/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4216 - accuracy: 0.8862 - val_loss: 0.9248 - val_accuracy: 0.7048\n","Epoch 67/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4335 - accuracy: 0.8857 - val_loss: 0.9826 - val_accuracy: 0.7138\n","Epoch 68/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4205 - accuracy: 0.8860 - val_loss: 0.9203 - val_accuracy: 0.7161\n","Epoch 69/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4189 - accuracy: 0.8843 - val_loss: 0.9388 - val_accuracy: 0.6957\n","Epoch 70/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4231 - accuracy: 0.8792 - val_loss: 0.9673 - val_accuracy: 0.6968\n","Epoch 71/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4273 - accuracy: 0.8778 - val_loss: 0.9635 - val_accuracy: 0.6968\n","Epoch 72/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4347 - accuracy: 0.8749 - val_loss: 1.0362 - val_accuracy: 0.6878\n","Epoch 73/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4364 - accuracy: 0.8727 - val_loss: 1.0020 - val_accuracy: 0.6912\n","Epoch 74/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4557 - accuracy: 0.8724 - val_loss: 0.9711 - val_accuracy: 0.6833\n","Epoch 75/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4598 - accuracy: 0.8704 - val_loss: 0.9503 - val_accuracy: 0.6968\n","Epoch 76/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4216 - accuracy: 0.8928 - val_loss: 0.9823 - val_accuracy: 0.6957\n","Epoch 77/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4263 - accuracy: 0.8814 - val_loss: 0.9661 - val_accuracy: 0.6946\n","Epoch 78/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4673 - accuracy: 0.8645 - val_loss: 0.9696 - val_accuracy: 0.6697\n","Epoch 79/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4501 - accuracy: 0.8829 - val_loss: 0.9637 - val_accuracy: 0.7014\n","Epoch 80/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4218 - accuracy: 0.8908 - val_loss: 1.0176 - val_accuracy: 0.6753\n","Epoch 81/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4057 - accuracy: 0.8911 - val_loss: 0.9278 - val_accuracy: 0.7036\n","Epoch 82/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4033 - accuracy: 0.8998 - val_loss: 0.9691 - val_accuracy: 0.6991\n","Epoch 83/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4179 - accuracy: 0.8905 - val_loss: 0.9732 - val_accuracy: 0.6934\n","Epoch 84/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4016 - accuracy: 0.8959 - val_loss: 0.9558 - val_accuracy: 0.6934\n","Epoch 85/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4063 - accuracy: 0.9004 - val_loss: 0.9596 - val_accuracy: 0.7002\n","Epoch 86/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4015 - accuracy: 0.8942 - val_loss: 0.9749 - val_accuracy: 0.6968\n","Epoch 87/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3974 - accuracy: 0.9010 - val_loss: 0.9856 - val_accuracy: 0.6980\n","Epoch 88/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3967 - accuracy: 0.9004 - val_loss: 1.0023 - val_accuracy: 0.7002\n","Epoch 89/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3969 - accuracy: 0.8956 - val_loss: 0.9662 - val_accuracy: 0.6889\n","Epoch 90/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.3938 - accuracy: 0.9024 - val_loss: 0.9719 - val_accuracy: 0.6923\n","Epoch 91/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3953 - accuracy: 0.8998 - val_loss: 0.9945 - val_accuracy: 0.7127\n","Epoch 92/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.3942 - accuracy: 0.9046 - val_loss: 0.9831 - val_accuracy: 0.7127\n","Epoch 93/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.3987 - accuracy: 0.8976 - val_loss: 1.0368 - val_accuracy: 0.7036\n","Epoch 94/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.3957 - accuracy: 0.9004 - val_loss: 1.0415 - val_accuracy: 0.6946\n","Epoch 95/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.3908 - accuracy: 0.9029 - val_loss: 0.9919 - val_accuracy: 0.7059\n","Epoch 96/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.3914 - accuracy: 0.9024 - val_loss: 0.9746 - val_accuracy: 0.6934\n","Epoch 97/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3953 - accuracy: 0.8908 - val_loss: 1.0663 - val_accuracy: 0.6867\n","Epoch 98/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4134 - accuracy: 0.8964 - val_loss: 1.0255 - val_accuracy: 0.6934\n","Epoch 99/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4033 - accuracy: 0.8868 - val_loss: 0.9801 - val_accuracy: 0.6946\n","Epoch 100/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3902 - accuracy: 0.9029 - val_loss: 1.0059 - val_accuracy: 0.6844\n","{'loss': [0.547842264175415, 0.5205934047698975, 0.508073627948761, 0.5047321915626526, 0.5065831542015076, 0.49268215894699097, 0.48677030205726624, 0.4863193929195404, 0.48740291595458984, 0.48816850781440735, 0.49644309282302856, 0.49653691053390503, 0.5043418407440186, 0.48163488507270813, 0.488609254360199, 0.47433072328567505, 0.47433874011039734, 0.49080634117126465, 0.4840638041496277, 0.47756558656692505, 0.4700964093208313, 0.4667656421661377, 0.4678640365600586, 0.4727241098880768, 0.46544119715690613, 0.46481117606163025, 0.4628413915634155, 0.4621172249317169, 0.47446033358573914, 0.4684734046459198, 0.4715011417865753, 0.4864136278629303, 0.4661606252193451, 0.46982327103614807, 0.4644850790500641, 0.4536733031272888, 0.46280917525291443, 0.46666863560676575, 0.4508521258831024, 0.44470611214637756, 0.44545307755470276, 0.4482380151748657, 0.44438984990119934, 0.4492340385913849, 0.45134544372558594, 0.45771095156669617, 0.4413914978504181, 0.43785491585731506, 0.43540677428245544, 0.4327942430973053, 0.4368317127227783, 0.4283352494239807, 0.44906526803970337, 0.4490680694580078, 0.45230886340141296, 0.4587421715259552, 0.4322432279586792, 0.43894729018211365, 0.4390242099761963, 0.42467162013053894, 0.43068230152130127, 0.4271969199180603, 0.4239593744277954, 0.42593756318092346, 0.4218820035457611, 0.42163676023483276, 0.43346279859542847, 0.42046090960502625, 0.4188975393772125, 0.4230533838272095, 0.42734912037849426, 0.4346526861190796, 0.4364362061023712, 0.45572084188461304, 0.4598025381565094, 0.4216206669807434, 0.42629992961883545, 0.46729931235313416, 0.4501294195652008, 0.4218418598175049, 0.4057196080684662, 0.4032832086086273, 0.41790807247161865, 0.4016292095184326, 0.40626585483551025, 0.40147364139556885, 0.3974438011646271, 0.3966815769672394, 0.39690274000167847, 0.39378800988197327, 0.3952886760234833, 0.3941665291786194, 0.39870399236679077, 0.39574745297431946, 0.39075443148612976, 0.39137300848960876, 0.39534252882003784, 0.4133667051792145, 0.403313547372818, 0.39019495248794556], 'accuracy': [0.8146576285362244, 0.8344652056694031, 0.8324844241142273, 0.8361629843711853, 0.8415393233299255, 0.853989839553833, 0.8520090579986572, 0.8491793870925903, 0.8494623899459839, 0.8525750041007996, 0.8438030481338501, 0.8457838296890259, 0.8384267091751099, 0.8588002324104309, 0.8579513430595398, 0.8576683402061462, 0.8610639572143555, 0.8463497161865234, 0.8573853969573975, 0.8556876182556152, 0.8684210777282715, 0.8568194508552551, 0.8638936281204224, 0.8619128465652466, 0.8624787926673889, 0.8667232394218445, 0.8641765713691711, 0.8655914068222046, 0.8576683402061462, 0.8585172891616821, 0.8565365076065063, 0.83729487657547, 0.8599320650100708, 0.8630446791648865, 0.859649121761322, 0.8678551316261292, 0.8701188564300537, 0.8602150678634644, 0.867006242275238, 0.8752122521400452, 0.8712506890296936, 0.8698358535766602, 0.8834182024002075, 0.8803055882453918, 0.8769100308418274, 0.8650254607200623, 0.8757781386375427, 0.8814374804496765, 0.8825693130493164, 0.8825693130493164, 0.8822863698005676, 0.8851160407066345, 0.8743633031845093, 0.875495195388794, 0.8704017996788025, 0.8701188564300537, 0.8817204236984253, 0.8766270279884338, 0.8771929740905762, 0.8879456520080566, 0.8740803599357605, 0.8803055882453918, 0.8893604874610901, 0.8805885910987854, 0.8879456520080566, 0.8862478733062744, 0.8856819272041321, 0.8859649300575256, 0.8842670917510986, 0.879173755645752, 0.8777589201927185, 0.8749292492866516, 0.872665524482727, 0.8723825812339783, 0.8704017996788025, 0.8927561044692993, 0.8814374804496765, 0.8644595146179199, 0.88285231590271, 0.8907753229141235, 0.8910582661628723, 0.8998302221298218, 0.8904923796653748, 0.895868718624115, 0.9003961682319641, 0.8941709399223328, 0.9009620547294617, 0.9003961682319641, 0.8955857157707214, 0.9023768901824951, 0.8998302221298218, 0.9046406149864197, 0.8975664973258972, 0.9003961682319641, 0.9029428362846375, 0.9023768901824951, 0.8907753229141235, 0.8964346647262573, 0.8868138194084167, 0.9029428362846375], 'val_loss': [0.8525671362876892, 0.8691079616546631, 0.852175772190094, 0.8214459419250488, 0.8304869532585144, 0.8473321795463562, 0.8309767246246338, 0.8437479138374329, 0.8357535004615784, 0.8154076337814331, 0.8150238990783691, 0.7818494439125061, 0.8019183874130249, 0.7583457231521606, 0.7737258076667786, 0.757103681564331, 0.7996459007263184, 0.7573861479759216, 0.8201218247413635, 0.7889564037322998, 0.7395483255386353, 0.7583674788475037, 0.7327092289924622, 0.7617480158805847, 0.7744319438934326, 0.7923169136047363, 0.7830236554145813, 0.8226081132888794, 0.7950314283370972, 0.8364403247833252, 0.8888804316520691, 0.8401191830635071, 0.8777713775634766, 0.8795232176780701, 0.8582889437675476, 0.8597761392593384, 0.974911630153656, 0.8469325304031372, 0.8450592160224915, 0.8482378125190735, 0.8725265860557556, 0.8928126096725464, 0.8869490623474121, 0.8558255434036255, 0.8881092071533203, 0.9137840867042542, 0.8638725876808167, 0.9008784294128418, 0.8813521265983582, 0.8842272162437439, 0.8849633932113647, 0.8955756425857544, 0.9556127786636353, 0.9273608326911926, 0.8711038827896118, 0.906755805015564, 0.8861668109893799, 0.8915390372276306, 0.9250739812850952, 0.9047698378562927, 0.915236234664917, 0.8902955055236816, 0.9201704859733582, 0.9443559050559998, 0.9434776902198792, 0.924763560295105, 0.9826303720474243, 0.9202684164047241, 0.9387974739074707, 0.9673454165458679, 0.9635294675827026, 1.03615403175354, 1.002030849456787, 0.971143901348114, 0.9503467082977295, 0.9823081493377686, 0.966070830821991, 0.9696264266967773, 0.963664174079895, 1.0176019668579102, 0.9277818202972412, 0.9691301584243774, 0.9731703996658325, 0.9557749032974243, 0.9595752358436584, 0.9748782515525818, 0.9855939149856567, 1.002349615097046, 0.9661763310432434, 0.9718659520149231, 0.9944891333580017, 0.9830792546272278, 1.0368318557739258, 1.041454792022705, 0.9919272065162659, 0.9745980501174927, 1.0663256645202637, 1.0254799127578735, 0.9800798892974854, 1.0058884620666504], 'val_accuracy': [0.5101810097694397, 0.5079185366630554, 0.5180995464324951, 0.5588235259056091, 0.5328054428100586, 0.5203620195388794, 0.5373303294181824, 0.5328054428100586, 0.5486425161361694, 0.557692289352417, 0.5610859990119934, 0.6085972785949707, 0.5837104320526123, 0.6538461446762085, 0.6380090713500977, 0.6730769276618958, 0.622171938419342, 0.6425339579582214, 0.6199095249176025, 0.6414027214050293, 0.7058823704719543, 0.7217194437980652, 0.7081447839736938, 0.7251130938529968, 0.720588207244873, 0.7115384340286255, 0.733031690120697, 0.7036198973655701, 0.7149321436882019, 0.7138009071350098, 0.709276020526886, 0.7138009071350098, 0.7138009071350098, 0.7047511339187622, 0.7126696705818176, 0.7081447839736938, 0.6832579374313354, 0.7138009071350098, 0.7104072570800781, 0.7149321436882019, 0.7036198973655701, 0.7183257937431335, 0.709276020526886, 0.7104072570800781, 0.7036198973655701, 0.7160633206367493, 0.7024886608123779, 0.7024886608123779, 0.7081447839736938, 0.7104072570800781, 0.7126696705818176, 0.7171945571899414, 0.662895917892456, 0.7126696705818176, 0.6968325972557068, 0.7013574838638306, 0.7126696705818176, 0.6945701241493225, 0.7036198973655701, 0.7081447839736938, 0.7024886608123779, 0.7047511339187622, 0.6889140009880066, 0.7024886608123779, 0.7160633206367493, 0.7047511339187622, 0.7138009071350098, 0.7160633206367493, 0.6957013607025146, 0.6968325972557068, 0.6968325972557068, 0.6877828240394592, 0.6911764740943909, 0.6832579374313354, 0.6968325972557068, 0.6957013607025146, 0.6945701241493225, 0.6696832776069641, 0.7013574838638306, 0.6753393411636353, 0.7036198973655701, 0.6990950107574463, 0.6934388875961304, 0.6934388875961304, 0.7002262473106384, 0.6968325972557068, 0.6979637742042542, 0.7002262473106384, 0.6889140009880066, 0.692307710647583, 0.7126696705818176, 0.7126696705818176, 0.7036198973655701, 0.6945701241493225, 0.7058823704719543, 0.6934388875961304, 0.6866515874862671, 0.6934388875961304, 0.6945701241493225, 0.6843891143798828]}\n","45/45 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 3072)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1573376   \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3900993 (14.88 MB)\n","Trainable params: 3900481 (14.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 4s 33ms/step - loss: 0.5530 - accuracy: 0.8052 - val_loss: 0.8779 - val_accuracy: 0.4969\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 1s 21ms/step - loss: 0.5268 - accuracy: 0.8183 - val_loss: 0.8403 - val_accuracy: 0.5021\n","Epoch 3/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5227 - accuracy: 0.8276 - val_loss: 0.8631 - val_accuracy: 0.4979\n","Epoch 4/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5355 - accuracy: 0.8142 - val_loss: 0.8539 - val_accuracy: 0.5062\n","Epoch 5/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.5318 - accuracy: 0.8165 - val_loss: 0.8392 - val_accuracy: 0.5227\n","Epoch 6/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5193 - accuracy: 0.8251 - val_loss: 0.8514 - val_accuracy: 0.5186\n","Epoch 7/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5205 - accuracy: 0.8274 - val_loss: 0.8454 - val_accuracy: 0.5207\n","Epoch 8/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.5189 - accuracy: 0.8344 - val_loss: 0.8138 - val_accuracy: 0.5610\n","Epoch 9/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5035 - accuracy: 0.8346 - val_loss: 0.8263 - val_accuracy: 0.5444\n","Epoch 10/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.5165 - accuracy: 0.8209 - val_loss: 0.7939 - val_accuracy: 0.6374\n","Epoch 11/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5093 - accuracy: 0.8385 - val_loss: 0.8245 - val_accuracy: 0.5537\n","Epoch 12/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5193 - accuracy: 0.8287 - val_loss: 0.7850 - val_accuracy: 0.6519\n","Epoch 13/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5231 - accuracy: 0.8258 - val_loss: 0.8028 - val_accuracy: 0.5806\n","Epoch 14/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5281 - accuracy: 0.8204 - val_loss: 0.8186 - val_accuracy: 0.5764\n","Epoch 15/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5074 - accuracy: 0.8331 - val_loss: 0.7991 - val_accuracy: 0.6105\n","Epoch 16/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5165 - accuracy: 0.8269 - val_loss: 0.8027 - val_accuracy: 0.6229\n","Epoch 17/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5046 - accuracy: 0.8377 - val_loss: 0.7849 - val_accuracy: 0.6488\n","Epoch 18/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4968 - accuracy: 0.8447 - val_loss: 0.8027 - val_accuracy: 0.6281\n","Epoch 19/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.4930 - accuracy: 0.8419 - val_loss: 0.7703 - val_accuracy: 0.6612\n","Epoch 20/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5134 - accuracy: 0.8315 - val_loss: 0.8552 - val_accuracy: 0.6219\n","Epoch 21/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5052 - accuracy: 0.8315 - val_loss: 0.8220 - val_accuracy: 0.6498\n","Epoch 22/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4969 - accuracy: 0.8444 - val_loss: 0.8195 - val_accuracy: 0.6519\n","Epoch 23/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5026 - accuracy: 0.8380 - val_loss: 0.8298 - val_accuracy: 0.6467\n","Epoch 24/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5248 - accuracy: 0.8411 - val_loss: 0.9483 - val_accuracy: 0.6229\n","Epoch 25/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5028 - accuracy: 0.8336 - val_loss: 0.8486 - val_accuracy: 0.6777\n","Epoch 26/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4943 - accuracy: 0.8408 - val_loss: 0.8702 - val_accuracy: 0.6653\n","Epoch 27/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4920 - accuracy: 0.8470 - val_loss: 0.9084 - val_accuracy: 0.6622\n","Epoch 28/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4870 - accuracy: 0.8444 - val_loss: 0.8385 - val_accuracy: 0.6890\n","Epoch 29/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4795 - accuracy: 0.8545 - val_loss: 0.8399 - val_accuracy: 0.6674\n","Epoch 30/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4782 - accuracy: 0.8483 - val_loss: 0.8703 - val_accuracy: 0.6601\n","Epoch 31/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4791 - accuracy: 0.8548 - val_loss: 0.8718 - val_accuracy: 0.6643\n","Epoch 32/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4897 - accuracy: 0.8444 - val_loss: 0.8775 - val_accuracy: 0.6570\n","Epoch 33/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4884 - accuracy: 0.8494 - val_loss: 0.9563 - val_accuracy: 0.6622\n","Epoch 34/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5076 - accuracy: 0.8354 - val_loss: 0.8656 - val_accuracy: 0.6653\n","Epoch 35/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4975 - accuracy: 0.8450 - val_loss: 0.9531 - val_accuracy: 0.6612\n","Epoch 36/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4890 - accuracy: 0.8437 - val_loss: 0.9302 - val_accuracy: 0.6705\n","Epoch 37/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4854 - accuracy: 0.8468 - val_loss: 0.9328 - val_accuracy: 0.6457\n","Epoch 38/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5012 - accuracy: 0.8457 - val_loss: 0.8605 - val_accuracy: 0.6601\n","Epoch 39/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4842 - accuracy: 0.8486 - val_loss: 0.9184 - val_accuracy: 0.6395\n","Epoch 40/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4692 - accuracy: 0.8581 - val_loss: 0.8778 - val_accuracy: 0.6736\n","Epoch 41/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4658 - accuracy: 0.8594 - val_loss: 0.8675 - val_accuracy: 0.6560\n","Epoch 42/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4704 - accuracy: 0.8543 - val_loss: 0.8623 - val_accuracy: 0.6550\n","Epoch 43/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4751 - accuracy: 0.8581 - val_loss: 0.8860 - val_accuracy: 0.6405\n","Epoch 44/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4777 - accuracy: 0.8548 - val_loss: 0.9872 - val_accuracy: 0.6374\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4939 - accuracy: 0.8434 - val_loss: 0.9387 - val_accuracy: 0.6477\n","Epoch 46/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4855 - accuracy: 0.8408 - val_loss: 0.9772 - val_accuracy: 0.6601\n","Epoch 47/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4765 - accuracy: 0.8566 - val_loss: 1.0121 - val_accuracy: 0.6477\n","Epoch 48/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4756 - accuracy: 0.8623 - val_loss: 0.9020 - val_accuracy: 0.6508\n","Epoch 49/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4656 - accuracy: 0.8674 - val_loss: 0.9307 - val_accuracy: 0.6746\n","Epoch 50/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4728 - accuracy: 0.8543 - val_loss: 0.9219 - val_accuracy: 0.6684\n","Epoch 51/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4735 - accuracy: 0.8574 - val_loss: 0.9151 - val_accuracy: 0.6694\n","Epoch 52/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4664 - accuracy: 0.8633 - val_loss: 0.8876 - val_accuracy: 0.6622\n","Epoch 53/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4688 - accuracy: 0.8599 - val_loss: 1.0102 - val_accuracy: 0.6550\n","Epoch 54/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4758 - accuracy: 0.8623 - val_loss: 0.9436 - val_accuracy: 0.6539\n","Epoch 55/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4671 - accuracy: 0.8638 - val_loss: 0.9338 - val_accuracy: 0.6446\n","Epoch 56/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4679 - accuracy: 0.8548 - val_loss: 0.9648 - val_accuracy: 0.6767\n","Epoch 57/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4706 - accuracy: 0.8566 - val_loss: 0.9151 - val_accuracy: 0.6384\n","Epoch 58/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4764 - accuracy: 0.8540 - val_loss: 0.9122 - val_accuracy: 0.6715\n","Epoch 59/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4706 - accuracy: 0.8499 - val_loss: 0.9937 - val_accuracy: 0.6694\n","Epoch 60/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4672 - accuracy: 0.8496 - val_loss: 0.9698 - val_accuracy: 0.6333\n","Epoch 61/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4522 - accuracy: 0.8695 - val_loss: 0.9298 - val_accuracy: 0.6498\n","Epoch 62/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4527 - accuracy: 0.8695 - val_loss: 0.9970 - val_accuracy: 0.6415\n","Epoch 63/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4697 - accuracy: 0.8630 - val_loss: 0.9673 - val_accuracy: 0.6694\n","Epoch 64/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4866 - accuracy: 0.8478 - val_loss: 0.9538 - val_accuracy: 0.6612\n","Epoch 65/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4615 - accuracy: 0.8674 - val_loss: 0.9200 - val_accuracy: 0.6705\n","Epoch 66/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4632 - accuracy: 0.8651 - val_loss: 0.9499 - val_accuracy: 0.6457\n","Epoch 67/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4739 - accuracy: 0.8589 - val_loss: 1.0333 - val_accuracy: 0.6539\n","Epoch 68/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4642 - accuracy: 0.8633 - val_loss: 1.0132 - val_accuracy: 0.6684\n","Epoch 69/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4615 - accuracy: 0.8656 - val_loss: 0.9314 - val_accuracy: 0.6508\n","Epoch 70/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4509 - accuracy: 0.8775 - val_loss: 0.9581 - val_accuracy: 0.6601\n","Epoch 71/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4450 - accuracy: 0.8690 - val_loss: 0.9573 - val_accuracy: 0.6384\n","Epoch 72/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4516 - accuracy: 0.8674 - val_loss: 0.9336 - val_accuracy: 0.6529\n","Epoch 73/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4440 - accuracy: 0.8726 - val_loss: 0.9183 - val_accuracy: 0.6508\n","Epoch 74/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4526 - accuracy: 0.8734 - val_loss: 0.9614 - val_accuracy: 0.6674\n","Epoch 75/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4430 - accuracy: 0.8708 - val_loss: 0.9484 - val_accuracy: 0.6529\n","Epoch 76/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4529 - accuracy: 0.8654 - val_loss: 0.9854 - val_accuracy: 0.6632\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4435 - accuracy: 0.8809 - val_loss: 0.9248 - val_accuracy: 0.6560\n","Epoch 78/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4426 - accuracy: 0.8736 - val_loss: 1.0243 - val_accuracy: 0.6157\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4369 - accuracy: 0.8773 - val_loss: 1.0731 - val_accuracy: 0.6612\n","Epoch 80/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4364 - accuracy: 0.8716 - val_loss: 1.0254 - val_accuracy: 0.6178\n","Epoch 81/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4348 - accuracy: 0.8744 - val_loss: 0.9683 - val_accuracy: 0.6674\n","Epoch 82/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4438 - accuracy: 0.8798 - val_loss: 1.0070 - val_accuracy: 0.6570\n","Epoch 83/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4345 - accuracy: 0.8817 - val_loss: 0.9467 - val_accuracy: 0.6684\n","Epoch 84/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4394 - accuracy: 0.8731 - val_loss: 0.9977 - val_accuracy: 0.6457\n","Epoch 85/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4305 - accuracy: 0.8775 - val_loss: 1.0487 - val_accuracy: 0.6198\n","Epoch 86/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4392 - accuracy: 0.8842 - val_loss: 1.0311 - val_accuracy: 0.6426\n","Epoch 87/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4314 - accuracy: 0.8786 - val_loss: 1.0032 - val_accuracy: 0.6694\n","Epoch 88/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4385 - accuracy: 0.8811 - val_loss: 0.9635 - val_accuracy: 0.6436\n","Epoch 89/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4360 - accuracy: 0.8806 - val_loss: 0.9557 - val_accuracy: 0.6446\n","Epoch 90/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4443 - accuracy: 0.8752 - val_loss: 1.1101 - val_accuracy: 0.6560\n","Epoch 91/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4685 - accuracy: 0.8661 - val_loss: 1.0134 - val_accuracy: 0.6188\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4655 - accuracy: 0.8581 - val_loss: 1.0429 - val_accuracy: 0.6395\n","Epoch 93/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4215 - accuracy: 0.8822 - val_loss: 0.9875 - val_accuracy: 0.6725\n","Epoch 94/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4265 - accuracy: 0.8860 - val_loss: 1.0358 - val_accuracy: 0.6374\n","Epoch 95/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4206 - accuracy: 0.8891 - val_loss: 1.0636 - val_accuracy: 0.6240\n","Epoch 96/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4174 - accuracy: 0.8817 - val_loss: 1.0078 - val_accuracy: 0.6498\n","Epoch 97/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4177 - accuracy: 0.8873 - val_loss: 0.9976 - val_accuracy: 0.6426\n","Epoch 98/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4310 - accuracy: 0.8860 - val_loss: 0.9953 - val_accuracy: 0.6519\n","Epoch 99/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4455 - accuracy: 0.8724 - val_loss: 1.0872 - val_accuracy: 0.6570\n","Epoch 100/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4280 - accuracy: 0.8773 - val_loss: 1.1259 - val_accuracy: 0.6178\n","{'loss': [0.5530309677124023, 0.5268052816390991, 0.5226793885231018, 0.5354886651039124, 0.5318397283554077, 0.5193445086479187, 0.5205187201499939, 0.5188732147216797, 0.5034945011138916, 0.5165173411369324, 0.5092900395393372, 0.5193071365356445, 0.5231162905693054, 0.5280566215515137, 0.5073946118354797, 0.516450047492981, 0.5046451091766357, 0.49683767557144165, 0.49296343326568604, 0.5134461522102356, 0.5051900148391724, 0.4968794882297516, 0.5026149153709412, 0.5247582793235779, 0.5028275847434998, 0.49426841735839844, 0.4919697344303131, 0.4870300590991974, 0.47950100898742676, 0.4782170355319977, 0.4791198670864105, 0.4897131621837616, 0.48840370774269104, 0.5076004862785339, 0.4974849820137024, 0.4890185296535492, 0.4853613078594208, 0.5011560320854187, 0.4842248857021332, 0.4691839814186096, 0.46577075123786926, 0.4703860580921173, 0.4751460552215576, 0.47771382331848145, 0.4938991665840149, 0.48553377389907837, 0.47653883695602417, 0.4755832254886627, 0.4655707776546478, 0.47279536724090576, 0.47347939014434814, 0.4663802683353424, 0.4688456952571869, 0.47582459449768066, 0.46706098318099976, 0.4678773283958435, 0.47063693404197693, 0.47635725140571594, 0.4706073999404907, 0.4671664237976074, 0.45222198963165283, 0.4527166187763214, 0.46972230076789856, 0.48663267493247986, 0.461458683013916, 0.4632245600223541, 0.4738582372665405, 0.46418339014053345, 0.46153658628463745, 0.45086154341697693, 0.4450482428073883, 0.45163241028785706, 0.444038987159729, 0.4525865614414215, 0.4429929256439209, 0.4529027044773102, 0.4435412585735321, 0.44255632162094116, 0.43688568472862244, 0.43642279505729675, 0.4347907602787018, 0.4437762498855591, 0.43449291586875916, 0.43943092226982117, 0.43051403760910034, 0.4391569495201111, 0.43141984939575195, 0.43851128220558167, 0.4360039234161377, 0.44429871439933777, 0.46850377321243286, 0.46553799510002136, 0.421542763710022, 0.42653676867485046, 0.42063871026039124, 0.41739335656166077, 0.41768795251846313, 0.43096280097961426, 0.44549885392189026, 0.42797574400901794], 'accuracy': [0.8051679730415344, 0.8183462619781494, 0.8276485800743103, 0.814211905002594, 0.8165374398231506, 0.8250645995140076, 0.827390193939209, 0.8343669176101685, 0.8346253037452698, 0.8209302425384521, 0.8385012745857239, 0.8286821842193604, 0.8258398175239563, 0.8204134106636047, 0.8330749273300171, 0.8268733620643616, 0.8377261161804199, 0.8447028398513794, 0.8418604731559753, 0.8315245509147644, 0.8315245509147644, 0.8444444537162781, 0.8379845023155212, 0.8410852551460266, 0.8335917592048645, 0.8408268690109253, 0.8470284342765808, 0.8444444537162781, 0.8545219898223877, 0.8483204245567322, 0.854780375957489, 0.8444444537162781, 0.8493540287017822, 0.8354005217552185, 0.8449612259864807, 0.8436692357063293, 0.8467700481414795, 0.8457364439964294, 0.8485788106918335, 0.8581395149230957, 0.8594315052032471, 0.8542635440826416, 0.8581395149230957, 0.854780375957489, 0.843410849571228, 0.8408268690109253, 0.856589138507843, 0.8622739315032959, 0.8674418330192566, 0.8542635440826416, 0.8573643565177917, 0.8633074760437012, 0.8599483370780945, 0.8622739315032959, 0.8638243079185486, 0.854780375957489, 0.856589138507843, 0.8540051579475403, 0.8498708009719849, 0.8496124148368835, 0.8695090413093567, 0.8695090413093567, 0.8630490899085999, 0.8478035926818848, 0.8674418330192566, 0.8651162981987, 0.8589147329330444, 0.8633074760437012, 0.8656330704689026, 0.8775193691253662, 0.868992269039154, 0.8674418330192566, 0.8726097941398621, 0.8733850121498108, 0.8708010315895081, 0.8653746843338013, 0.8808785676956177, 0.8736433982849121, 0.8772609829902649, 0.8715762495994568, 0.8744186162948608, 0.8798449635505676, 0.8816537261009216, 0.8731266260147095, 0.8775193691253662, 0.8842377066612244, 0.8785529732704163, 0.881136953830719, 0.8806201815605164, 0.8751937747001648, 0.8661498427391052, 0.8581395149230957, 0.882170557975769, 0.8860465288162231, 0.8891472816467285, 0.8816537261009216, 0.8873385190963745, 0.8860465288162231, 0.8723514080047607, 0.8772609829902649], 'val_loss': [0.8778513073921204, 0.8403406143188477, 0.863084614276886, 0.85394686460495, 0.8391857147216797, 0.8514044880867004, 0.845405101776123, 0.8137540221214294, 0.8263306021690369, 0.7938531637191772, 0.8245370984077454, 0.7850202918052673, 0.8028462529182434, 0.8186377882957458, 0.7990836501121521, 0.8026965260505676, 0.7849063277244568, 0.8026812076568604, 0.7703354954719543, 0.8552073836326599, 0.8220358490943909, 0.8195255994796753, 0.8297602534294128, 0.9483489990234375, 0.8485724925994873, 0.8701812624931335, 0.9084244966506958, 0.8384724855422974, 0.8399127125740051, 0.8702660202980042, 0.8717904686927795, 0.877510130405426, 0.9563344120979309, 0.8655776381492615, 0.9531482458114624, 0.930162787437439, 0.9327627420425415, 0.8605353236198425, 0.9184396266937256, 0.877803385257721, 0.867452085018158, 0.8622890114784241, 0.8859679102897644, 0.9872497320175171, 0.9386950135231018, 0.9771872162818909, 1.0121192932128906, 0.9019639492034912, 0.9306500554084778, 0.921905517578125, 0.9151023626327515, 0.8876384496688843, 1.0101518630981445, 0.943601667881012, 0.9337714314460754, 0.964762806892395, 0.9151232242584229, 0.9121960997581482, 0.9936870336532593, 0.9697929620742798, 0.9298498630523682, 0.9970409870147705, 0.9673173427581787, 0.9537643790245056, 0.9199764132499695, 0.9499049186706543, 1.0333142280578613, 1.0131871700286865, 0.931395411491394, 0.9580919146537781, 0.9573265910148621, 0.9335875511169434, 0.9183251857757568, 0.9614270329475403, 0.9483655095100403, 0.9854366183280945, 0.9248380064964294, 1.0242629051208496, 1.0731008052825928, 1.0253973007202148, 0.968289852142334, 1.006966233253479, 0.946657121181488, 0.9976656436920166, 1.048712968826294, 1.0310642719268799, 1.0032143592834473, 0.9634522795677185, 0.9557345509529114, 1.1100668907165527, 1.0133850574493408, 1.0428835153579712, 0.9875141382217407, 1.0358128547668457, 1.0636159181594849, 1.0078175067901611, 0.9975882172584534, 0.9952937364578247, 1.0872154235839844, 1.1258869171142578], 'val_accuracy': [0.4969008266925812, 0.5020661354064941, 0.49793389439582825, 0.5061983466148376, 0.5227272510528564, 0.5185950398445129, 0.5206611752510071, 0.5609503984451294, 0.5444214940071106, 0.6373966932296753, 0.5537189841270447, 0.6518595218658447, 0.5805785059928894, 0.5764462947845459, 0.6105371713638306, 0.6229338645935059, 0.6487603187561035, 0.6280992031097412, 0.6611570119857788, 0.6219007968902588, 0.6497933864593506, 0.6518595218658447, 0.6466942429542542, 0.6229338645935059, 0.6776859760284424, 0.6652892827987671, 0.6621900796890259, 0.6890496015548706, 0.6673553586006165, 0.6601239442825317, 0.66425621509552, 0.6570248007774353, 0.6621900796890259, 0.6652892827987671, 0.6611570119857788, 0.6704545617103577, 0.6456611752510071, 0.6601239442825317, 0.6394628286361694, 0.6735537052154541, 0.6559917330741882, 0.6549586653709412, 0.6404958963394165, 0.6373966932296753, 0.6477272510528564, 0.6601239442825317, 0.6477272510528564, 0.6508264541625977, 0.6745867729187012, 0.6683884263038635, 0.6694214940071106, 0.6621900796890259, 0.6549586653709412, 0.6539255976676941, 0.64462810754776, 0.6766529083251953, 0.6384297609329224, 0.6714876294136047, 0.6694214940071106, 0.6332644820213318, 0.6497933864593506, 0.6415289044380188, 0.6694214940071106, 0.6611570119857788, 0.6704545617103577, 0.6456611752510071, 0.6539255976676941, 0.6683884263038635, 0.6508264541625977, 0.6601239442825317, 0.6384297609329224, 0.6528925895690918, 0.6508264541625977, 0.6673553586006165, 0.6528925895690918, 0.663223147392273, 0.6559917330741882, 0.6157024502754211, 0.6611570119857788, 0.6177685856819153, 0.6673553586006165, 0.6570248007774353, 0.6683884263038635, 0.6456611752510071, 0.6198347210884094, 0.6425619721412659, 0.6694214940071106, 0.6435950398445129, 0.64462810754776, 0.6559917330741882, 0.6188016533851624, 0.6394628286361694, 0.672520637512207, 0.6373966932296753, 0.6239669322967529, 0.6497933864593506, 0.6425619721412659, 0.6518595218658447, 0.6570248007774353, 0.6177685856819153]}\n","32/32 [==============================] - 0s 3ms/step\n"]}]},{"cell_type":"code","source":["metrics_df_CNN.round(3)"],"metadata":{"id":"y3RXIk-qZ7ts","colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"status":"ok","timestamp":1717505645444,"user_tz":-360,"elapsed":24,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}},"outputId":"ea701586-3051-479c-df7d-4c8b9cbadd2b","collapsed":true},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Client  Accuracy  Precision  Recall     F1  Sensitivity  Specificity  \\\n","0        0     0.539      0.556   0.380  0.452        0.380        0.697   \n","1        1     0.554      0.586   0.367  0.451        0.367        0.740   \n","2        2     0.552      0.576   0.396  0.469        0.396        0.709   \n","3        0     0.564      0.571   0.518  0.543        0.518        0.611   \n","4        1     0.572      0.590   0.472  0.524        0.472        0.672   \n","5        2     0.589      0.600   0.534  0.565        0.534        0.645   \n","6        0     0.604      0.601   0.616  0.609        0.616        0.591   \n","7        1     0.600      0.606   0.571  0.588        0.571        0.629   \n","8        2     0.649      0.720   0.486  0.580        0.486        0.811   \n","9        0     0.622      0.610   0.678  0.642        0.678        0.566   \n","10       1     0.600      0.577   0.751  0.653        0.751        0.449   \n","11       2     0.671      0.767   0.490  0.598        0.490        0.851   \n","12       0     0.670      0.650   0.735  0.690        0.735        0.605   \n","13       1     0.678      0.700   0.623  0.659        0.623        0.733   \n","14       2     0.712      0.780   0.590  0.672        0.590        0.833   \n","\n","    Kappa  \n","0   0.077  \n","1   0.107  \n","2   0.104  \n","3   0.129  \n","4   0.144  \n","5   0.179  \n","6   0.208  \n","7   0.199  \n","8   0.297  \n","9   0.245  \n","10  0.201  \n","11  0.341  \n","12  0.340  \n","13  0.356  \n","14  0.424  "],"text/html":["\n","  <div id=\"df-ee9350cc-6b14-4b5b-82ce-35806886a573\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Client</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Sensitivity</th>\n","      <th>Specificity</th>\n","      <th>Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.539</td>\n","      <td>0.556</td>\n","      <td>0.380</td>\n","      <td>0.452</td>\n","      <td>0.380</td>\n","      <td>0.697</td>\n","      <td>0.077</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.554</td>\n","      <td>0.586</td>\n","      <td>0.367</td>\n","      <td>0.451</td>\n","      <td>0.367</td>\n","      <td>0.740</td>\n","      <td>0.107</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.552</td>\n","      <td>0.576</td>\n","      <td>0.396</td>\n","      <td>0.469</td>\n","      <td>0.396</td>\n","      <td>0.709</td>\n","      <td>0.104</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.564</td>\n","      <td>0.571</td>\n","      <td>0.518</td>\n","      <td>0.543</td>\n","      <td>0.518</td>\n","      <td>0.611</td>\n","      <td>0.129</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.572</td>\n","      <td>0.590</td>\n","      <td>0.472</td>\n","      <td>0.524</td>\n","      <td>0.472</td>\n","      <td>0.672</td>\n","      <td>0.144</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2</td>\n","      <td>0.589</td>\n","      <td>0.600</td>\n","      <td>0.534</td>\n","      <td>0.565</td>\n","      <td>0.534</td>\n","      <td>0.645</td>\n","      <td>0.179</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0.604</td>\n","      <td>0.601</td>\n","      <td>0.616</td>\n","      <td>0.609</td>\n","      <td>0.616</td>\n","      <td>0.591</td>\n","      <td>0.208</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0.600</td>\n","      <td>0.606</td>\n","      <td>0.571</td>\n","      <td>0.588</td>\n","      <td>0.571</td>\n","      <td>0.629</td>\n","      <td>0.199</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>0.649</td>\n","      <td>0.720</td>\n","      <td>0.486</td>\n","      <td>0.580</td>\n","      <td>0.486</td>\n","      <td>0.811</td>\n","      <td>0.297</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>0.622</td>\n","      <td>0.610</td>\n","      <td>0.678</td>\n","      <td>0.642</td>\n","      <td>0.678</td>\n","      <td>0.566</td>\n","      <td>0.245</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0.600</td>\n","      <td>0.577</td>\n","      <td>0.751</td>\n","      <td>0.653</td>\n","      <td>0.751</td>\n","      <td>0.449</td>\n","      <td>0.201</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2</td>\n","      <td>0.671</td>\n","      <td>0.767</td>\n","      <td>0.490</td>\n","      <td>0.598</td>\n","      <td>0.490</td>\n","      <td>0.851</td>\n","      <td>0.341</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>0.670</td>\n","      <td>0.650</td>\n","      <td>0.735</td>\n","      <td>0.690</td>\n","      <td>0.735</td>\n","      <td>0.605</td>\n","      <td>0.340</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0.678</td>\n","      <td>0.700</td>\n","      <td>0.623</td>\n","      <td>0.659</td>\n","      <td>0.623</td>\n","      <td>0.733</td>\n","      <td>0.356</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2</td>\n","      <td>0.712</td>\n","      <td>0.780</td>\n","      <td>0.590</td>\n","      <td>0.672</td>\n","      <td>0.590</td>\n","      <td>0.833</td>\n","      <td>0.424</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee9350cc-6b14-4b5b-82ce-35806886a573')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ee9350cc-6b14-4b5b-82ce-35806886a573 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ee9350cc-6b14-4b5b-82ce-35806886a573');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c9e3ffbb-0eeb-4302-ab3a-0b91af531eb6\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c9e3ffbb-0eeb-4302-ab3a-0b91af531eb6')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c9e3ffbb-0eeb-4302-ab3a-0b91af531eb6 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"metrics_df_CNN\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Client\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05328289281436085,\n        \"min\": 0.539,\n        \"max\": 0.712,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.622,\n          0.67,\n          0.539\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07343477831835996,\n        \"min\": 0.556,\n        \"max\": 0.78,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.61,\n          0.767,\n          0.556\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12064639398238299,\n        \"min\": 0.367,\n        \"max\": 0.751,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.678,\n          0.49,\n          0.38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07895809618006852,\n        \"min\": 0.451,\n        \"max\": 0.69,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.642,\n          0.598,\n          0.452\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sensitivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12064639398238299,\n        \"min\": 0.367,\n        \"max\": 0.751,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.678,\n          0.49,\n          0.38\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Specificity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10878541568917528,\n        \"min\": 0.449,\n        \"max\": 0.851,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.566,\n          0.851,\n          0.697\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kappa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10653356278656975,\n        \"min\": 0.077,\n        \"max\": 0.424,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.245,\n          0.341,\n          0.077\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["metrics_df_CNN.round(4).to_csv('/content/drive/MyDrive/EEG Signal /results/FLresults/TF Domain/CNN/Delta_tf_CNN.csv', index = False)"],"metadata":{"id":"_iOLsKpkfzdG","executionInfo":{"status":"ok","timestamp":1717505646360,"user_tz":-360,"elapsed":936,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# GRU"],"metadata":{"id":"GPlWZUcV48bB"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, LSTM, Dropout, GRU\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","import tensorflow_addons as tfa\n","\n","class FederatedData:\n","    def __init__(self, data_path, num_clients, scaler_type='MinMax', test_size=0.2):\n","        self.data_path = data_path\n","        self.num_clients = num_clients\n","        self.scaler_type = scaler_type\n","        self.test_size = test_size\n","        self.partitions = []  # Initialize partitions attribute\n","        self.load_data()\n","        self.scale_data()\n","        self.create_partitions()\n","\n","    def load_data(self):\n","        try:\n","            data = np.load(self.data_path, allow_pickle=True)\n","            self.X = np.moveaxis(data['X'], 1, 2)  # Move axis here\n","            self.Y = data['Y']\n","            self.groups = data['Group']  # Assuming 'Group' contains group identifiers\n","        except KeyError as e:\n","            raise ValueError(f\"Missing expected data field: {e}\")\n","        except FileNotFoundError as e:\n","            raise ValueError(f\"Data file not found: {e}\")\n","\n","    def scale_data(self):\n","        # Reshape data to 2D array for scaling\n","        X_reshaped = self.X.reshape(-1, self.X.shape[-1])\n","\n","        # Select scaler based on input\n","        if self.scaler_type == 'Standard':\n","            scaler = StandardScaler()\n","        elif self.scaler_type == 'MinMax':\n","            scaler = MinMaxScaler(feature_range=(0, 1))\n","        else:\n","            raise ValueError(\"Unsupported scaler type. Choose either 'Standard' or 'MinMax'.\")\n","\n","        # Fit and transform the data\n","        scaled_data_reshaped = scaler.fit_transform(X_reshaped)\n","\n","        # Reshape back to original shape\n","        self.X = scaled_data_reshaped.reshape(self.X.shape)\n","\n","    def create_partitions(self):\n","        gss = GroupShuffleSplit(n_splits=self.num_clients, test_size=self.test_size, random_state=42)\n","        for train_index, test_index in gss.split(self.X, self.Y, self.groups):\n","            X_train, X_test = self.X[train_index], self.X[test_index]\n","            Y_train, Y_test = self.Y[train_index], self.Y[test_index]\n","            self.partitions.append((X_train, Y_train, X_test, Y_test))\n","            print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n","\n","    def get_training_and_validation_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        partition_X_train, partition_Y_train, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        X_train, X_val, Y_train, Y_val = train_test_split(partition_X_train, partition_Y_train, test_size=0.2, random_state=42)\n","        return X_train, X_val, Y_train, Y_val, partition_X_test, partition_Y_test\n","\n","    def get_testing_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        _, _, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        return partition_X_test, partition_Y_test\n","\n","def build_sequential_model_gru(input_shape):\n","    clear_session()\n","    model = Sequential()\n","\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\", input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(512, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(filters=1024, kernel_size=3, strides=1, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(GRU(256, return_sequences=True))\n","    model.add(GRU(256, return_sequences=True))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(256, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n","\n","    # opt = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","    opt = tfa.optimizers.RectifiedAdam(learning_rate=0.00009)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = recall\n","\n","    return accuracy, precision, recall, f1, sensitivity, specificity, kappa\n","\n","def federated_learning_gru(data_path):\n","    federated_data = FederatedData(data_path, num_clients=3, scaler_type='MinMax')\n","    federated_data.create_partitions()\n","\n","    # Get the input shape from the data\n","    input_shape = federated_data.X.shape[1:]\n","    global_model = build_sequential_model_gru(input_shape)\n","\n","    num_clients = 3\n","    local_epochs = 5\n","    global_optimizer = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","\n","    # Initialize m and v for Adam optimizer\n","    m = [np.zeros_like(w) for w in global_model.get_weights()]\n","    v = [np.zeros_like(w) for w in global_model.get_weights()]\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 1e-7\n","    t = 0\n","\n","    client_data = []\n","    for client_idx in range(num_clients):\n","        x_train, x_val, y_train, y_val, x_test, y_test = federated_data.get_training_and_validation_data(client_idx)\n","        client_data.append((x_train, x_val, y_train, y_val, x_test, y_test))\n","\n","    metrics_list = []\n","\n","    for epoch in range(local_epochs):\n","        client_models = []\n","\n","        for client in range(num_clients):\n","            x_train, x_val, y_train, y_val, x_test, y_test = client_data[client]\n","            client_model = build_sequential_model_gru(input_shape)\n","            client_model.set_weights(global_model.get_weights())  # Initialize with global weights\n","            opt = Adam(learning_rate=0.000009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","            client_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","            # Set up callbacks\n","            run_name = f\"epoch_{epoch}_client_{client}\"\n","            checkpoint_filepath = f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time+Frequency/Delta/CNN_GRU/best_model_{run_name}.h5'\n","            checkpoint_callback = ModelCheckpoint(\n","                checkpoint_filepath,\n","                monitor='val_accuracy',\n","                save_best_only=True,\n","                mode='max'\n","            )\n","            csv_logger = CSVLogger(f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time+Frequency/Delta/CNN_GRU/training_log_{run_name}.csv', append=True, separator=';')\n","\n","            history = client_model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_val, y_val), callbacks=[checkpoint_callback, csv_logger])\n","            client_models.append(client_model)\n","\n","            print(history.history)\n","\n","            # Load the best model before evaluation\n","            best_model = load_model(checkpoint_filepath)\n","\n","            # Evaluate client model\n","            y_pred = (best_model.predict(x_test) > 0.5).astype(\"int32\")\n","            accuracy, precision, recall, f1, sensitivity, specificity, kappa = compute_metrics(y_test, y_pred)\n","            metrics_list.append({\n","                'Client': client,\n","                'Accuracy': accuracy,\n","                'Precision': precision,\n","                'Recall': recall,\n","                'F1': f1,\n","                'Sensitivity': sensitivity,\n","                'Specificity': specificity,\n","                'Kappa': kappa\n","            })\n","\n","        global_weights = global_model.get_weights()\n","        layer_start_idx = 0\n","        for layer in global_model.layers:\n","            layer_weights = []\n","            layer_biases = []\n","            num_params = len(layer.get_weights())\n","            for i in range(num_clients):\n","                layer_params = client_models[i].get_weights()[layer_start_idx:layer_start_idx + num_params]\n","                if num_params > 0:\n","                    layer_weights.append(layer_params[0])\n","                if num_params > 1:\n","                    layer_biases.append(layer_params[1])\n","\n","            if len(layer_weights) > 0:\n","                averaged_layer_weights = np.mean(layer_weights, axis=0)\n","                if averaged_layer_weights.shape == global_weights[layer_start_idx].shape:\n","                    global_weights[layer_start_idx] = averaged_layer_weights\n","                else:\n","                    print(f\"Warning: Shape mismatch for weights at layer {layer.name}, expected {global_weights[layer_start_idx].shape} but got {averaged_layer_weights.shape}\")\n","            if len(layer_biases) > 0:\n","                averaged_layer_biases = np.mean(layer_biases, axis=0)\n","                if averaged_layer_biases.shape == global_weights[layer_start_idx + 1].shape:\n","                    global_weights[layer_start_idx + 1] = averaged_layer_biases\n","                else:\n","                    print(f\"Warning: Shape mismatch for biases at layer {layer.name}, expected {global_weights[layer_start_idx + 1].shape} but got {averaged_layer_biases.shape}\")\n","\n","            layer_start_idx += num_params\n","\n","        # Apply FedOpt (Adam) update to global weights\n","        t += 1\n","        for i in range(len(global_weights)):\n","            g_t = global_weights[i] - global_model.get_weights()[i]  # Gradient\n","            m[i] = beta1 * m[i] + (1 - beta1) * g_t\n","            v[i] = beta2 * v[i] + (1 - beta2) * (g_t ** 2)\n","            m_hat = m[i] / (1 - beta1 ** t)\n","            v_hat = v[i] / (1 - beta2 ** t)\n","            global_weights[i] -= global_optimizer.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n","\n","        global_model.set_weights(global_weights)\n","\n","    metrics_df = pd.DataFrame(metrics_list)\n","    return global_model, metrics_df\n","\n","# Example usage\n","# data_path = 'path/to/your/data.npz'\n","# global_model, metrics_df = federated_learning(data_path)\n","# print(metrics_df)\n","\n"],"metadata":{"id":"ieXSN-9PI4Dx","executionInfo":{"status":"ok","timestamp":1717505646361,"user_tz":-360,"elapsed":4,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["global_model_gru, metrics_df_gru = federated_learning_gru(Theta_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"lD5S-Pvy5B-r","executionInfo":{"status":"ok","timestamp":1717506995236,"user_tz":-360,"elapsed":1348879,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}},"outputId":"893054b4-e6ec-4bf1-a774-b3382200db75"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (4640, 37, 29), Test shape: (1194, 37, 29)\n","Train shape: (4418, 37, 29), Test shape: (1416, 37, 29)\n","Train shape: (4838, 37, 29), Test shape: (996, 37, 29)\n","Train shape: (4640, 37, 29), Test shape: (1194, 37, 29)\n","Train shape: (4418, 37, 29), Test shape: (1416, 37, 29)\n","Train shape: (4838, 37, 29), Test shape: (996, 37, 29)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - ETA: 0s - loss: 1.7025 - accuracy: 0.5027"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 14s 108ms/step - loss: 1.7025 - accuracy: 0.5027 - val_loss: 1.6974 - val_accuracy: 0.4849\n","Epoch 2/100\n","29/29 [==============================] - 1s 30ms/step - loss: 1.6922 - accuracy: 0.5032 - val_loss: 1.6873 - val_accuracy: 0.4849\n","Epoch 3/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.6820 - accuracy: 0.5038 - val_loss: 1.6773 - val_accuracy: 0.4849\n","Epoch 4/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.6720 - accuracy: 0.5038 - val_loss: 1.6674 - val_accuracy: 0.4849\n","Epoch 5/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.6620 - accuracy: 0.5038 - val_loss: 1.6576 - val_accuracy: 0.4849\n","Epoch 6/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.6521 - accuracy: 0.5038 - val_loss: 1.6479 - val_accuracy: 0.4849\n","Epoch 7/100\n","29/29 [==============================] - 1s 29ms/step - loss: 1.6424 - accuracy: 0.5038 - val_loss: 1.6382 - val_accuracy: 0.4849\n","Epoch 8/100\n","29/29 [==============================] - 1s 31ms/step - loss: 1.6326 - accuracy: 0.5038 - val_loss: 1.6287 - val_accuracy: 0.4849\n","Epoch 9/100\n","29/29 [==============================] - 1s 31ms/step - loss: 1.6232 - accuracy: 0.5005 - val_loss: 1.6192 - val_accuracy: 0.4849\n","Epoch 10/100\n","29/29 [==============================] - 1s 30ms/step - loss: 1.6137 - accuracy: 0.5040 - val_loss: 1.6099 - val_accuracy: 0.4849\n","Epoch 11/100\n","29/29 [==============================] - 2s 60ms/step - loss: 1.6043 - accuracy: 0.5008 - val_loss: 1.6006 - val_accuracy: 0.5086\n","Epoch 12/100\n","29/29 [==============================] - 2s 61ms/step - loss: 1.5951 - accuracy: 0.5065 - val_loss: 1.5914 - val_accuracy: 0.5550\n","Epoch 13/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.5858 - accuracy: 0.5067 - val_loss: 1.5823 - val_accuracy: 0.5539\n","Epoch 14/100\n","29/29 [==============================] - 1s 42ms/step - loss: 1.5768 - accuracy: 0.5092 - val_loss: 1.5732 - val_accuracy: 0.5560\n","Epoch 15/100\n","29/29 [==============================] - 1s 25ms/step - loss: 1.5679 - accuracy: 0.5119 - val_loss: 1.5642 - val_accuracy: 0.5560\n","Epoch 16/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.5591 - accuracy: 0.5108 - val_loss: 1.5554 - val_accuracy: 0.5539\n","Epoch 17/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.5503 - accuracy: 0.5124 - val_loss: 1.5465 - val_accuracy: 0.5560\n","Epoch 18/100\n","29/29 [==============================] - 1s 29ms/step - loss: 1.5413 - accuracy: 0.5189 - val_loss: 1.5378 - val_accuracy: 0.5539\n","Epoch 19/100\n","29/29 [==============================] - 1s 27ms/step - loss: 1.5325 - accuracy: 0.5162 - val_loss: 1.5290 - val_accuracy: 0.5442\n","Epoch 20/100\n","29/29 [==============================] - 1s 31ms/step - loss: 1.5239 - accuracy: 0.5210 - val_loss: 1.5204 - val_accuracy: 0.5474\n","Epoch 21/100\n","29/29 [==============================] - 1s 27ms/step - loss: 1.5153 - accuracy: 0.5216 - val_loss: 1.5118 - val_accuracy: 0.5431\n","Epoch 22/100\n","29/29 [==============================] - 1s 25ms/step - loss: 1.5068 - accuracy: 0.5240 - val_loss: 1.5033 - val_accuracy: 0.5323\n","Epoch 23/100\n","29/29 [==============================] - 1s 28ms/step - loss: 1.4983 - accuracy: 0.5291 - val_loss: 1.4949 - val_accuracy: 0.5302\n","Epoch 24/100\n","29/29 [==============================] - 1s 38ms/step - loss: 1.4899 - accuracy: 0.5269 - val_loss: 1.4866 - val_accuracy: 0.5269\n","Epoch 25/100\n","29/29 [==============================] - 1s 52ms/step - loss: 1.4818 - accuracy: 0.5304 - val_loss: 1.4784 - val_accuracy: 0.5119\n","Epoch 26/100\n","29/29 [==============================] - 1s 42ms/step - loss: 1.4734 - accuracy: 0.5337 - val_loss: 1.4703 - val_accuracy: 0.5172\n","Epoch 27/100\n","29/29 [==============================] - 1s 34ms/step - loss: 1.4649 - accuracy: 0.5334 - val_loss: 1.4624 - val_accuracy: 0.5097\n","Epoch 28/100\n","29/29 [==============================] - 1s 37ms/step - loss: 1.4568 - accuracy: 0.5415 - val_loss: 1.4545 - val_accuracy: 0.5129\n","Epoch 29/100\n","29/29 [==============================] - 1s 39ms/step - loss: 1.4486 - accuracy: 0.5380 - val_loss: 1.4466 - val_accuracy: 0.5086\n","Epoch 30/100\n","29/29 [==============================] - 1s 28ms/step - loss: 1.4405 - accuracy: 0.5396 - val_loss: 1.4388 - val_accuracy: 0.5086\n","Epoch 31/100\n","29/29 [==============================] - 1s 25ms/step - loss: 1.4328 - accuracy: 0.5469 - val_loss: 1.4312 - val_accuracy: 0.5097\n","Epoch 32/100\n","29/29 [==============================] - 1s 29ms/step - loss: 1.4246 - accuracy: 0.5396 - val_loss: 1.4236 - val_accuracy: 0.5140\n","Epoch 33/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.4166 - accuracy: 0.5477 - val_loss: 1.4160 - val_accuracy: 0.5129\n","Epoch 34/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.4091 - accuracy: 0.5409 - val_loss: 1.4085 - val_accuracy: 0.5194\n","Epoch 35/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.4008 - accuracy: 0.5533 - val_loss: 1.4015 - val_accuracy: 0.5140\n","Epoch 36/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3929 - accuracy: 0.5498 - val_loss: 1.3938 - val_accuracy: 0.5259\n","Epoch 37/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3857 - accuracy: 0.5506 - val_loss: 1.3874 - val_accuracy: 0.5183\n","Epoch 38/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3775 - accuracy: 0.5525 - val_loss: 1.3801 - val_accuracy: 0.5259\n","Epoch 39/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3700 - accuracy: 0.5498 - val_loss: 1.3726 - val_accuracy: 0.5237\n","Epoch 40/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3624 - accuracy: 0.5568 - val_loss: 1.3658 - val_accuracy: 0.5205\n","Epoch 41/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3540 - accuracy: 0.5587 - val_loss: 1.3590 - val_accuracy: 0.5291\n","Epoch 42/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3462 - accuracy: 0.5644 - val_loss: 1.3549 - val_accuracy: 0.5054\n","Epoch 43/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3402 - accuracy: 0.5493 - val_loss: 1.3466 - val_accuracy: 0.5194\n","Epoch 44/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3321 - accuracy: 0.5692 - val_loss: 1.3410 - val_accuracy: 0.5054\n","Epoch 45/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3248 - accuracy: 0.5695 - val_loss: 1.3333 - val_accuracy: 0.5291\n","Epoch 46/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3162 - accuracy: 0.5700 - val_loss: 1.3281 - val_accuracy: 0.5312\n","Epoch 47/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3094 - accuracy: 0.5744 - val_loss: 1.3215 - val_accuracy: 0.5269\n","Epoch 48/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3021 - accuracy: 0.5738 - val_loss: 1.3162 - val_accuracy: 0.5453\n","Epoch 49/100\n","29/29 [==============================] - 2s 58ms/step - loss: 1.2951 - accuracy: 0.5781 - val_loss: 1.3096 - val_accuracy: 0.5593\n","Epoch 50/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2878 - accuracy: 0.5811 - val_loss: 1.3052 - val_accuracy: 0.5259\n","Epoch 51/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2804 - accuracy: 0.5768 - val_loss: 1.2988 - val_accuracy: 0.5356\n","Epoch 52/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2724 - accuracy: 0.5808 - val_loss: 1.2933 - val_accuracy: 0.5345\n","Epoch 53/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2667 - accuracy: 0.5752 - val_loss: 1.2872 - val_accuracy: 0.5453\n","Epoch 54/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.2590 - accuracy: 0.5846 - val_loss: 1.2826 - val_accuracy: 0.5399\n","Epoch 55/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2525 - accuracy: 0.5819 - val_loss: 1.2776 - val_accuracy: 0.5485\n","Epoch 56/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2455 - accuracy: 0.5857 - val_loss: 1.2726 - val_accuracy: 0.5420\n","Epoch 57/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2405 - accuracy: 0.5824 - val_loss: 1.2678 - val_accuracy: 0.5506\n","Epoch 58/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2320 - accuracy: 0.5873 - val_loss: 1.2647 - val_accuracy: 0.5356\n","Epoch 59/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2261 - accuracy: 0.5832 - val_loss: 1.2602 - val_accuracy: 0.5496\n","Epoch 60/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2193 - accuracy: 0.5921 - val_loss: 1.2542 - val_accuracy: 0.5474\n","Epoch 61/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.2125 - accuracy: 0.5897 - val_loss: 1.2500 - val_accuracy: 0.5453\n","Epoch 62/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2063 - accuracy: 0.5981 - val_loss: 1.2463 - val_accuracy: 0.5453\n","Epoch 63/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1994 - accuracy: 0.5894 - val_loss: 1.2405 - val_accuracy: 0.5453\n","Epoch 64/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.1944 - accuracy: 0.5921 - val_loss: 1.2391 - val_accuracy: 0.5334\n","Epoch 65/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1905 - accuracy: 0.5927 - val_loss: 1.2340 - val_accuracy: 0.5463\n","Epoch 66/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1814 - accuracy: 0.6016 - val_loss: 1.2296 - val_accuracy: 0.5431\n","Epoch 67/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1731 - accuracy: 0.6029 - val_loss: 1.2271 - val_accuracy: 0.5485\n","Epoch 68/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1699 - accuracy: 0.5994 - val_loss: 1.2235 - val_accuracy: 0.5560\n","Epoch 69/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1619 - accuracy: 0.6072 - val_loss: 1.2179 - val_accuracy: 0.5453\n","Epoch 70/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1589 - accuracy: 0.6024 - val_loss: 1.2144 - val_accuracy: 0.5388\n","Epoch 71/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1521 - accuracy: 0.6096 - val_loss: 1.2134 - val_accuracy: 0.5345\n","Epoch 72/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1442 - accuracy: 0.6118 - val_loss: 1.2102 - val_accuracy: 0.5496\n","Epoch 73/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1430 - accuracy: 0.6002 - val_loss: 1.2045 - val_accuracy: 0.5496\n","Epoch 74/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1330 - accuracy: 0.6064 - val_loss: 1.1990 - val_accuracy: 0.5474\n","Epoch 75/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1283 - accuracy: 0.6153 - val_loss: 1.1983 - val_accuracy: 0.5442\n","Epoch 76/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1244 - accuracy: 0.6091 - val_loss: 1.1969 - val_accuracy: 0.5453\n","Epoch 77/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1188 - accuracy: 0.6129 - val_loss: 1.1929 - val_accuracy: 0.5560\n","Epoch 78/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1114 - accuracy: 0.6161 - val_loss: 1.1897 - val_accuracy: 0.5388\n","Epoch 79/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.1060 - accuracy: 0.6148 - val_loss: 1.1852 - val_accuracy: 0.5496\n","Epoch 80/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.0992 - accuracy: 0.6169 - val_loss: 1.1817 - val_accuracy: 0.5582\n","Epoch 81/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.0933 - accuracy: 0.6317 - val_loss: 1.1823 - val_accuracy: 0.5356\n","Epoch 82/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.0927 - accuracy: 0.6142 - val_loss: 1.1777 - val_accuracy: 0.5571\n","Epoch 83/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.0851 - accuracy: 0.6148 - val_loss: 1.1735 - val_accuracy: 0.5377\n","Epoch 84/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.0800 - accuracy: 0.6140 - val_loss: 1.1760 - val_accuracy: 0.5356\n","Epoch 85/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.0730 - accuracy: 0.6226 - val_loss: 1.1681 - val_accuracy: 0.5431\n","Epoch 86/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.0675 - accuracy: 0.6247 - val_loss: 1.1707 - val_accuracy: 0.5399\n","Epoch 87/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.0625 - accuracy: 0.6301 - val_loss: 1.1640 - val_accuracy: 0.5560\n","Epoch 88/100\n","29/29 [==============================] - 2s 64ms/step - loss: 1.0565 - accuracy: 0.6312 - val_loss: 1.1648 - val_accuracy: 0.5657\n","Epoch 89/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0547 - accuracy: 0.6269 - val_loss: 1.1609 - val_accuracy: 0.5420\n","Epoch 90/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0493 - accuracy: 0.6301 - val_loss: 1.1578 - val_accuracy: 0.5474\n","Epoch 91/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0427 - accuracy: 0.6334 - val_loss: 1.1542 - val_accuracy: 0.5571\n","Epoch 92/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0382 - accuracy: 0.6334 - val_loss: 1.1528 - val_accuracy: 0.5420\n","Epoch 93/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.0337 - accuracy: 0.6312 - val_loss: 1.1552 - val_accuracy: 0.5700\n","Epoch 94/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0278 - accuracy: 0.6350 - val_loss: 1.1538 - val_accuracy: 0.5474\n","Epoch 95/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0264 - accuracy: 0.6401 - val_loss: 1.1613 - val_accuracy: 0.5668\n","Epoch 96/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0232 - accuracy: 0.6414 - val_loss: 1.1448 - val_accuracy: 0.5614\n","Epoch 97/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0137 - accuracy: 0.6430 - val_loss: 1.1470 - val_accuracy: 0.5517\n","Epoch 98/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0073 - accuracy: 0.6441 - val_loss: 1.1440 - val_accuracy: 0.5474\n","Epoch 99/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0076 - accuracy: 0.6282 - val_loss: 1.1695 - val_accuracy: 0.5140\n","Epoch 100/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9965 - accuracy: 0.6468 - val_loss: 1.1432 - val_accuracy: 0.5571\n","{'loss': [1.7024608850479126, 1.6922099590301514, 1.6820015907287598, 1.6719528436660767, 1.6619672775268555, 1.652100920677185, 1.642398715019226, 1.632630467414856, 1.6231545209884644, 1.6136740446090698, 1.604263424873352, 1.5951346158981323, 1.5858384370803833, 1.5768139362335205, 1.567934513092041, 1.5590941905975342, 1.5503275394439697, 1.5413421392440796, 1.53253972530365, 1.523940086364746, 1.5152755975723267, 1.506803274154663, 1.4983223676681519, 1.4899483919143677, 1.4817622900009155, 1.4734199047088623, 1.4649202823638916, 1.4567841291427612, 1.448552131652832, 1.4405189752578735, 1.4327828884124756, 1.4246333837509155, 1.416581392288208, 1.409095287322998, 1.400801181793213, 1.3928594589233398, 1.3857189416885376, 1.377533197402954, 1.3700361251831055, 1.3623692989349365, 1.3540352582931519, 1.346174716949463, 1.340232491493225, 1.3320614099502563, 1.3247989416122437, 1.316224217414856, 1.3094216585159302, 1.302065134048462, 1.2951250076293945, 1.2877947092056274, 1.2804059982299805, 1.2724239826202393, 1.2667330503463745, 1.2589884996414185, 1.2525465488433838, 1.245538353919983, 1.2404992580413818, 1.2320284843444824, 1.2261009216308594, 1.2193019390106201, 1.2124905586242676, 1.2062596082687378, 1.199407696723938, 1.194413661956787, 1.1905463933944702, 1.1814308166503906, 1.1730834245681763, 1.1698663234710693, 1.1619285345077515, 1.158911943435669, 1.1520521640777588, 1.1441569328308105, 1.1430057287216187, 1.1330257654190063, 1.1283104419708252, 1.1244325637817383, 1.1188465356826782, 1.1114293336868286, 1.1060305833816528, 1.099207878112793, 1.0933066606521606, 1.0927008390426636, 1.0850919485092163, 1.0799572467803955, 1.072998285293579, 1.0675128698349, 1.0625263452529907, 1.0564672946929932, 1.054713487625122, 1.0493072271347046, 1.0427398681640625, 1.0381860733032227, 1.0337426662445068, 1.0277798175811768, 1.0264019966125488, 1.023187279701233, 1.0136866569519043, 1.0072962045669556, 1.0076156854629517, 0.9965260624885559], 'accuracy': [0.5026939511299133, 0.5032327771186829, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5037715435028076, 0.5005387663841248, 0.5040409564971924, 0.5008081793785095, 0.506465494632721, 0.5067349076271057, 0.509159505367279, 0.5118534564971924, 0.5107758641242981, 0.5123922228813171, 0.5188577771186829, 0.5161637663841248, 0.5210129022598267, 0.5215517282485962, 0.5239762663841248, 0.5290948152542114, 0.5269396305084229, 0.5304418206214905, 0.5336745977401733, 0.5334051847457886, 0.5414870977401733, 0.5379849076271057, 0.5396012663841248, 0.546875, 0.5396012663841248, 0.5476831793785095, 0.5409482717514038, 0.553340494632721, 0.5498383641242981, 0.5506465435028076, 0.5525323152542114, 0.5498383641242981, 0.5568426847457886, 0.5587284564971924, 0.5643857717514038, 0.5492995977401733, 0.5692349076271057, 0.5695043206214905, 0.5700430870056152, 0.5743534564971924, 0.5738146305084229, 0.578125, 0.5810883641242981, 0.576777994632721, 0.5808189511299133, 0.5751616358757019, 0.584590494632721, 0.5818965435028076, 0.5856680870056152, 0.5824353694915771, 0.587284505367279, 0.5832435488700867, 0.592133641242981, 0.5897090435028076, 0.5980603694915771, 0.5894396305084229, 0.592133641242981, 0.5926724076271057, 0.6015625, 0.602909505367279, 0.5994073152542114, 0.6072198152542114, 0.6023706793785095, 0.6096444129943848, 0.6117995977401733, 0.600215494632721, 0.6064116358757019, 0.6153017282485962, 0.6091055870056152, 0.6128771305084229, 0.6161099076271057, 0.6147629022598267, 0.6169180870056152, 0.6317349076271057, 0.6142241358757019, 0.6147629022598267, 0.6139547228813171, 0.6225754022598267, 0.6247305870056152, 0.6301185488700867, 0.631196141242981, 0.6268857717514038, 0.6301185488700867, 0.6333512663841248, 0.6333512663841248, 0.631196141242981, 0.6349676847457886, 0.6400862336158752, 0.6414331793785095, 0.6430495977401733, 0.6441271305084229, 0.6282327771186829, 0.646821141242981], 'val_loss': [1.6973520517349243, 1.687262773513794, 1.677262306213379, 1.6673662662506104, 1.6575610637664795, 1.6478508710861206, 1.6382200717926025, 1.6286852359771729, 1.6192444562911987, 1.609865427017212, 1.6005829572677612, 1.5913782119750977, 1.5822999477386475, 1.573226809501648, 1.5642374753952026, 1.5553734302520752, 1.5465278625488281, 1.5378074645996094, 1.5289555788040161, 1.5204123258590698, 1.5118119716644287, 1.5033209323883057, 1.4949071407318115, 1.4865994453430176, 1.4784091711044312, 1.470295786857605, 1.4623644351959229, 1.4544790983200073, 1.4465968608856201, 1.4387516975402832, 1.431178331375122, 1.4235949516296387, 1.416040062904358, 1.4085180759429932, 1.4014679193496704, 1.3937937021255493, 1.3873635530471802, 1.3800643682479858, 1.3726086616516113, 1.3658356666564941, 1.358986258506775, 1.3549174070358276, 1.346588373184204, 1.3409678936004639, 1.333280086517334, 1.3280680179595947, 1.3214787244796753, 1.3162498474121094, 1.309597373008728, 1.3052211999893188, 1.298833966255188, 1.2932509183883667, 1.287240743637085, 1.282609462738037, 1.2776401042938232, 1.2725967168807983, 1.2678465843200684, 1.264747142791748, 1.2602139711380005, 1.2541660070419312, 1.250012993812561, 1.246315360069275, 1.2404757738113403, 1.2391475439071655, 1.233963131904602, 1.2295624017715454, 1.227115511894226, 1.2235289812088013, 1.2179434299468994, 1.2144296169281006, 1.213443636894226, 1.2102179527282715, 1.2045414447784424, 1.1990395784378052, 1.198291540145874, 1.1968687772750854, 1.1929372549057007, 1.1896876096725464, 1.1852471828460693, 1.1816729307174683, 1.1823314428329468, 1.1776705980300903, 1.1734720468521118, 1.1760200262069702, 1.1681352853775024, 1.1706759929656982, 1.1639792919158936, 1.1648353338241577, 1.1609116792678833, 1.1578104496002197, 1.1542080640792847, 1.1528396606445312, 1.1552001237869263, 1.1538465023040771, 1.1613013744354248, 1.1447871923446655, 1.1470493078231812, 1.143964171409607, 1.1695138216018677, 1.1431756019592285], 'val_accuracy': [0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.5086206793785095, 0.5549569129943848, 0.5538793206214905, 0.556034505367279, 0.556034505367279, 0.5538793206214905, 0.556034505367279, 0.5538793206214905, 0.5441810488700867, 0.5474137663841248, 0.5431034564971924, 0.5323275923728943, 0.5301724076271057, 0.5269396305084229, 0.5118534564971924, 0.517241358757019, 0.5096982717514038, 0.5129310488700867, 0.5086206793785095, 0.5086206793785095, 0.5096982717514038, 0.514008641242981, 0.5129310488700867, 0.5193965435028076, 0.514008641242981, 0.5258620977401733, 0.5183189511299133, 0.5258620977401733, 0.5237069129943848, 0.5204741358757019, 0.5290948152542114, 0.5053879022598267, 0.5193965435028076, 0.5053879022598267, 0.5290948152542114, 0.53125, 0.5269396305084229, 0.545258641242981, 0.5592672228813171, 0.5258620977401733, 0.5355603694915771, 0.5344827771186829, 0.545258641242981, 0.5398706793785095, 0.548491358757019, 0.5420258641242981, 0.5506465435028076, 0.5355603694915771, 0.5495689511299133, 0.5474137663841248, 0.545258641242981, 0.545258641242981, 0.545258641242981, 0.5334051847457886, 0.5463362336158752, 0.5431034564971924, 0.548491358757019, 0.556034505367279, 0.545258641242981, 0.5387930870056152, 0.5344827771186829, 0.5495689511299133, 0.5495689511299133, 0.5474137663841248, 0.5441810488700867, 0.545258641242981, 0.556034505367279, 0.5387930870056152, 0.5495689511299133, 0.5581896305084229, 0.5355603694915771, 0.5571120977401733, 0.537715494632721, 0.5355603694915771, 0.5431034564971924, 0.5398706793785095, 0.556034505367279, 0.5657327771186829, 0.5420258641242981, 0.5474137663841248, 0.5571120977401733, 0.5420258641242981, 0.5700430870056152, 0.5474137663841248, 0.5668103694915771, 0.5614224076271057, 0.5517241358757019, 0.5474137663841248, 0.514008641242981, 0.5571120977401733]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 10s 57ms/step - loss: 1.7026 - accuracy: 0.5017 - val_loss: 1.6977 - val_accuracy: 0.4955\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.6926 - accuracy: 0.5011 - val_loss: 1.6879 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.6827 - accuracy: 0.5017 - val_loss: 1.6782 - val_accuracy: 0.5305\n","Epoch 4/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.6729 - accuracy: 0.5017 - val_loss: 1.6687 - val_accuracy: 0.5373\n","Epoch 5/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.6632 - accuracy: 0.5011 - val_loss: 1.6592 - val_accuracy: 0.5181\n","Epoch 6/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.6535 - accuracy: 0.5037 - val_loss: 1.6498 - val_accuracy: 0.5249\n","Epoch 7/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.6440 - accuracy: 0.5110 - val_loss: 1.6405 - val_accuracy: 0.5204\n","Epoch 8/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.6346 - accuracy: 0.5119 - val_loss: 1.6313 - val_accuracy: 0.5204\n","Epoch 9/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.6252 - accuracy: 0.5156 - val_loss: 1.6222 - val_accuracy: 0.5226\n","Epoch 10/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.6160 - accuracy: 0.5201 - val_loss: 1.6132 - val_accuracy: 0.5215\n","Epoch 11/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.6068 - accuracy: 0.5207 - val_loss: 1.6042 - val_accuracy: 0.5226\n","Epoch 12/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.5978 - accuracy: 0.5286 - val_loss: 1.5954 - val_accuracy: 0.5226\n","Epoch 13/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.5889 - accuracy: 0.5345 - val_loss: 1.5866 - val_accuracy: 0.5226\n","Epoch 14/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.5802 - accuracy: 0.5331 - val_loss: 1.5779 - val_accuracy: 0.5294\n","Epoch 15/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.5714 - accuracy: 0.5342 - val_loss: 1.5693 - val_accuracy: 0.5204\n","Epoch 16/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.5627 - accuracy: 0.5357 - val_loss: 1.5607 - val_accuracy: 0.5204\n","Epoch 17/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.5541 - accuracy: 0.5331 - val_loss: 1.5523 - val_accuracy: 0.5215\n","Epoch 18/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.5454 - accuracy: 0.5399 - val_loss: 1.5439 - val_accuracy: 0.5339\n","Epoch 19/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.5370 - accuracy: 0.5379 - val_loss: 1.5357 - val_accuracy: 0.5328\n","Epoch 20/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.5286 - accuracy: 0.5422 - val_loss: 1.5275 - val_accuracy: 0.5271\n","Epoch 21/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.5202 - accuracy: 0.5365 - val_loss: 1.5194 - val_accuracy: 0.5407\n","Epoch 22/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.5118 - accuracy: 0.5402 - val_loss: 1.5114 - val_accuracy: 0.5362\n","Epoch 23/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.5035 - accuracy: 0.5413 - val_loss: 1.5035 - val_accuracy: 0.5464\n","Epoch 24/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.4951 - accuracy: 0.5475 - val_loss: 1.4957 - val_accuracy: 0.5464\n","Epoch 25/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.4870 - accuracy: 0.5390 - val_loss: 1.4885 - val_accuracy: 0.5351\n","Epoch 26/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.4785 - accuracy: 0.5492 - val_loss: 1.4804 - val_accuracy: 0.5441\n","Epoch 27/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.4707 - accuracy: 0.5504 - val_loss: 1.4729 - val_accuracy: 0.5452\n","Epoch 28/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.4623 - accuracy: 0.5577 - val_loss: 1.4658 - val_accuracy: 0.5430\n","Epoch 29/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.4542 - accuracy: 0.5512 - val_loss: 1.4578 - val_accuracy: 0.5441\n","Epoch 30/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.4461 - accuracy: 0.5535 - val_loss: 1.4511 - val_accuracy: 0.5452\n","Epoch 31/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.4382 - accuracy: 0.5543 - val_loss: 1.4438 - val_accuracy: 0.5554\n","Epoch 32/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.4302 - accuracy: 0.5586 - val_loss: 1.4368 - val_accuracy: 0.5566\n","Epoch 33/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.4223 - accuracy: 0.5591 - val_loss: 1.4289 - val_accuracy: 0.5486\n","Epoch 34/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.4149 - accuracy: 0.5614 - val_loss: 1.4217 - val_accuracy: 0.5475\n","Epoch 35/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.4065 - accuracy: 0.5679 - val_loss: 1.4172 - val_accuracy: 0.5566\n","Epoch 36/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.3995 - accuracy: 0.5523 - val_loss: 1.4070 - val_accuracy: 0.5520\n","Epoch 37/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3906 - accuracy: 0.5665 - val_loss: 1.4030 - val_accuracy: 0.5464\n","Epoch 38/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3841 - accuracy: 0.5535 - val_loss: 1.3930 - val_accuracy: 0.5498\n","Epoch 39/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.3760 - accuracy: 0.5654 - val_loss: 1.3893 - val_accuracy: 0.5509\n","Epoch 40/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.3687 - accuracy: 0.5668 - val_loss: 1.3812 - val_accuracy: 0.5566\n","Epoch 41/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.3609 - accuracy: 0.5727 - val_loss: 1.3752 - val_accuracy: 0.5566\n","Epoch 42/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.3541 - accuracy: 0.5747 - val_loss: 1.3712 - val_accuracy: 0.5566\n","Epoch 43/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.3473 - accuracy: 0.5702 - val_loss: 1.3643 - val_accuracy: 0.5554\n","Epoch 44/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.3394 - accuracy: 0.5659 - val_loss: 1.3555 - val_accuracy: 0.5532\n","Epoch 45/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.3317 - accuracy: 0.5758 - val_loss: 1.3510 - val_accuracy: 0.5554\n","Epoch 46/100\n","28/28 [==============================] - 1s 32ms/step - loss: 1.3253 - accuracy: 0.5789 - val_loss: 1.3421 - val_accuracy: 0.5600\n","Epoch 47/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.3173 - accuracy: 0.5781 - val_loss: 1.3362 - val_accuracy: 0.5588\n","Epoch 48/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3109 - accuracy: 0.5804 - val_loss: 1.3317 - val_accuracy: 0.5520\n","Epoch 49/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3031 - accuracy: 0.5815 - val_loss: 1.3249 - val_accuracy: 0.5577\n","Epoch 50/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2993 - accuracy: 0.5739 - val_loss: 1.3195 - val_accuracy: 0.5554\n","Epoch 51/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2904 - accuracy: 0.5804 - val_loss: 1.3147 - val_accuracy: 0.5577\n","Epoch 52/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2833 - accuracy: 0.5804 - val_loss: 1.3142 - val_accuracy: 0.5566\n","Epoch 53/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2780 - accuracy: 0.5835 - val_loss: 1.3064 - val_accuracy: 0.5566\n","Epoch 54/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2704 - accuracy: 0.5880 - val_loss: 1.3004 - val_accuracy: 0.5486\n","Epoch 55/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2634 - accuracy: 0.5886 - val_loss: 1.3004 - val_accuracy: 0.5588\n","Epoch 56/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2570 - accuracy: 0.5914 - val_loss: 1.2917 - val_accuracy: 0.5532\n","Epoch 57/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2520 - accuracy: 0.5908 - val_loss: 1.2852 - val_accuracy: 0.5577\n","Epoch 58/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2447 - accuracy: 0.5914 - val_loss: 1.2813 - val_accuracy: 0.5554\n","Epoch 59/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2376 - accuracy: 0.5905 - val_loss: 1.2759 - val_accuracy: 0.5554\n","Epoch 60/100\n","28/28 [==============================] - 1s 25ms/step - loss: 1.2333 - accuracy: 0.5903 - val_loss: 1.2764 - val_accuracy: 0.5713\n","Epoch 61/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2277 - accuracy: 0.5908 - val_loss: 1.2673 - val_accuracy: 0.5520\n","Epoch 62/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2209 - accuracy: 0.5925 - val_loss: 1.2641 - val_accuracy: 0.5509\n","Epoch 63/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2133 - accuracy: 0.6044 - val_loss: 1.2633 - val_accuracy: 0.5690\n","Epoch 64/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2085 - accuracy: 0.6019 - val_loss: 1.2559 - val_accuracy: 0.5588\n","Epoch 65/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.2029 - accuracy: 0.6016 - val_loss: 1.2540 - val_accuracy: 0.5701\n","Epoch 66/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1973 - accuracy: 0.6019 - val_loss: 1.2476 - val_accuracy: 0.5645\n","Epoch 67/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1905 - accuracy: 0.6019 - val_loss: 1.2448 - val_accuracy: 0.5577\n","Epoch 68/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.1853 - accuracy: 0.5999 - val_loss: 1.2433 - val_accuracy: 0.5622\n","Epoch 69/100\n","28/28 [==============================] - 1s 31ms/step - loss: 1.1790 - accuracy: 0.6067 - val_loss: 1.2405 - val_accuracy: 0.5747\n","Epoch 70/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.1745 - accuracy: 0.5996 - val_loss: 1.2323 - val_accuracy: 0.5577\n","Epoch 71/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.1682 - accuracy: 0.6007 - val_loss: 1.2327 - val_accuracy: 0.5543\n","Epoch 72/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.1637 - accuracy: 0.6075 - val_loss: 1.2258 - val_accuracy: 0.5543\n","Epoch 73/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.1571 - accuracy: 0.6109 - val_loss: 1.2233 - val_accuracy: 0.5566\n","Epoch 74/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.1525 - accuracy: 0.6061 - val_loss: 1.2195 - val_accuracy: 0.5554\n","Epoch 75/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.1491 - accuracy: 0.6087 - val_loss: 1.2218 - val_accuracy: 0.5588\n","Epoch 76/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1441 - accuracy: 0.6115 - val_loss: 1.2120 - val_accuracy: 0.5566\n","Epoch 77/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1371 - accuracy: 0.6098 - val_loss: 1.2115 - val_accuracy: 0.5611\n","Epoch 78/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1299 - accuracy: 0.6177 - val_loss: 1.2020 - val_accuracy: 0.5600\n","Epoch 79/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.1233 - accuracy: 0.6177 - val_loss: 1.2069 - val_accuracy: 0.5769\n","Epoch 80/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1206 - accuracy: 0.6171 - val_loss: 1.2055 - val_accuracy: 0.5690\n","Epoch 81/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1135 - accuracy: 0.6152 - val_loss: 1.1954 - val_accuracy: 0.5645\n","Epoch 82/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1082 - accuracy: 0.6166 - val_loss: 1.1908 - val_accuracy: 0.5667\n","Epoch 83/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1043 - accuracy: 0.6217 - val_loss: 1.1924 - val_accuracy: 0.5622\n","Epoch 84/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0989 - accuracy: 0.6157 - val_loss: 1.1846 - val_accuracy: 0.5656\n","Epoch 85/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.0945 - accuracy: 0.6228 - val_loss: 1.1898 - val_accuracy: 0.5679\n","Epoch 86/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0882 - accuracy: 0.6203 - val_loss: 1.1788 - val_accuracy: 0.5633\n","Epoch 87/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0854 - accuracy: 0.6174 - val_loss: 1.1794 - val_accuracy: 0.5566\n","Epoch 88/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0814 - accuracy: 0.6310 - val_loss: 1.1796 - val_accuracy: 0.5701\n","Epoch 89/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0772 - accuracy: 0.6265 - val_loss: 1.1761 - val_accuracy: 0.5588\n","Epoch 90/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0697 - accuracy: 0.6296 - val_loss: 1.1775 - val_accuracy: 0.5679\n","Epoch 91/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0640 - accuracy: 0.6296 - val_loss: 1.1819 - val_accuracy: 0.5690\n","Epoch 92/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.0600 - accuracy: 0.6313 - val_loss: 1.1655 - val_accuracy: 0.5667\n","Epoch 93/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.0568 - accuracy: 0.6355 - val_loss: 1.1794 - val_accuracy: 0.5645\n","Epoch 94/100\n","28/28 [==============================] - 1s 36ms/step - loss: 1.0522 - accuracy: 0.6313 - val_loss: 1.1621 - val_accuracy: 0.5633\n","Epoch 95/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.0502 - accuracy: 0.6225 - val_loss: 1.1665 - val_accuracy: 0.5645\n","Epoch 96/100\n","28/28 [==============================] - 1s 23ms/step - loss: 1.0396 - accuracy: 0.6358 - val_loss: 1.1642 - val_accuracy: 0.5645\n","Epoch 97/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.0358 - accuracy: 0.6398 - val_loss: 1.1603 - val_accuracy: 0.5656\n","Epoch 98/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.0319 - accuracy: 0.6406 - val_loss: 1.1556 - val_accuracy: 0.5577\n","Epoch 99/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.0270 - accuracy: 0.6412 - val_loss: 1.1643 - val_accuracy: 0.5667\n","Epoch 100/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.0214 - accuracy: 0.6463 - val_loss: 1.1559 - val_accuracy: 0.5611\n","{'loss': [1.7026492357254028, 1.6926243305206299, 1.6827301979064941, 1.6728830337524414, 1.6631512641906738, 1.6534842252731323, 1.6439894437789917, 1.6345937252044678, 1.6252427101135254, 1.6159929037094116, 1.6067867279052734, 1.5978490114212036, 1.5889198780059814, 1.580204725265503, 1.5713956356048584, 1.5627326965332031, 1.554095983505249, 1.545397400856018, 1.536962866783142, 1.5286446809768677, 1.5201748609542847, 1.5118097066879272, 1.503487467765808, 1.4950639009475708, 1.4869588613510132, 1.4785406589508057, 1.4707199335098267, 1.4623079299926758, 1.454244613647461, 1.4460910558700562, 1.4381706714630127, 1.430154800415039, 1.4223326444625854, 1.414919137954712, 1.40647554397583, 1.399527907371521, 1.3905627727508545, 1.3841495513916016, 1.3759922981262207, 1.3686667680740356, 1.3608604669570923, 1.3540589809417725, 1.3473339080810547, 1.3394240140914917, 1.3316742181777954, 1.3253151178359985, 1.3173288106918335, 1.3109372854232788, 1.3030873537063599, 1.2993319034576416, 1.2903778553009033, 1.283274531364441, 1.278026819229126, 1.27037513256073, 1.263379454612732, 1.257045865058899, 1.2520140409469604, 1.2447435855865479, 1.2375664710998535, 1.233340859413147, 1.2276657819747925, 1.2209056615829468, 1.2132917642593384, 1.2085093259811401, 1.202919363975525, 1.1973305940628052, 1.1905328035354614, 1.1853443384170532, 1.1790164709091187, 1.1744894981384277, 1.1682435274124146, 1.16368567943573, 1.1570557355880737, 1.152510643005371, 1.1490898132324219, 1.1440602540969849, 1.1371266841888428, 1.1299327611923218, 1.1233367919921875, 1.120559573173523, 1.1134910583496094, 1.108195185661316, 1.1043208837509155, 1.0989383459091187, 1.0945096015930176, 1.088232398033142, 1.085430383682251, 1.0813795328140259, 1.0772223472595215, 1.0696532726287842, 1.0639816522598267, 1.0600284337997437, 1.0567524433135986, 1.0521639585494995, 1.050171971321106, 1.039648175239563, 1.0358446836471558, 1.031894326210022, 1.0270370244979858, 1.0214287042617798], 'accuracy': [0.5016977787017822, 0.5011318325996399, 0.5016977787017822, 0.5016977787017822, 0.5011318325996399, 0.503678560256958, 0.511035680770874, 0.5118845701217651, 0.5155631303787231, 0.5200905203819275, 0.5206564664840698, 0.5285795331001282, 0.534521758556366, 0.5331069827079773, 0.5342388153076172, 0.5356536507606506, 0.5331069827079773, 0.539898157119751, 0.5379173755645752, 0.5421618819236755, 0.5365025401115417, 0.5401811003684998, 0.5413129329681396, 0.5475382208824158, 0.5390492081642151, 0.549235999584198, 0.5503678321838379, 0.5577249526977539, 0.5512167811393738, 0.5534804463386536, 0.5543293952941895, 0.558573842048645, 0.5591397881507874, 0.5614035129547119, 0.5679117441177368, 0.5523486137390137, 0.5664969086647034, 0.5534804463386536, 0.5653650164604187, 0.5667798519134521, 0.5727221369743347, 0.5747028589248657, 0.5701754093170166, 0.565930962562561, 0.5758347511291504, 0.5789473652839661, 0.578098475933075, 0.5803622007369995, 0.5814940333366394, 0.5738539695739746, 0.5803622007369995, 0.5803622007369995, 0.5834748148918152, 0.5880022644996643, 0.5885682106018066, 0.5913978219032288, 0.5908319354057312, 0.5913978219032288, 0.5905489325523376, 0.5902659893035889, 0.5908319354057312, 0.5925297141075134, 0.6044142842292786, 0.6018675565719604, 0.6015846133232117, 0.6018675565719604, 0.6018675565719604, 0.5998868346214294, 0.6066780090332031, 0.5996038317680359, 0.6007357239723206, 0.6075268983840942, 0.6109224557876587, 0.6061120629310608, 0.6086587309837341, 0.611488401889801, 0.6097906231880188, 0.6177136301994324, 0.6177136301994324, 0.61714768409729, 0.615166962146759, 0.6165817975997925, 0.6216751337051392, 0.6157329082489014, 0.6228070259094238, 0.6202603578567505, 0.6174306869506836, 0.631013035774231, 0.6264855861663818, 0.6295982003211975, 0.6295982003211975, 0.6312959790229797, 0.6355404853820801, 0.6312959790229797, 0.6225240230560303, 0.6358234286308289, 0.6397849321365356, 0.6406338214874268, 0.6411997675895691, 0.6462931632995605], 'val_loss': [1.697672963142395, 1.687909483909607, 1.6782383918762207, 1.668668270111084, 1.6591918468475342, 1.649823546409607, 1.6405491828918457, 1.6313424110412598, 1.622234582901001, 1.613203763961792, 1.6042474508285522, 1.5954039096832275, 1.5866234302520752, 1.577904224395752, 1.569269061088562, 1.5607471466064453, 1.5523231029510498, 1.5439257621765137, 1.5356663465499878, 1.5274595022201538, 1.519360065460205, 1.5114240646362305, 1.5034717321395874, 1.495703935623169, 1.488505244255066, 1.4803987741470337, 1.4728556871414185, 1.4658054113388062, 1.4578044414520264, 1.4510514736175537, 1.4437625408172607, 1.4367696046829224, 1.4288533926010132, 1.4216899871826172, 1.4172405004501343, 1.4070361852645874, 1.4030184745788574, 1.3930027484893799, 1.3892627954483032, 1.3812131881713867, 1.3751980066299438, 1.3712044954299927, 1.364298701286316, 1.3555418252944946, 1.3509507179260254, 1.3421131372451782, 1.336248517036438, 1.3317170143127441, 1.3249375820159912, 1.3194831609725952, 1.3147287368774414, 1.3141603469848633, 1.3064489364624023, 1.3004354238510132, 1.3003534078598022, 1.2917041778564453, 1.285192847251892, 1.2812830209732056, 1.275863766670227, 1.276437520980835, 1.2672642469406128, 1.2641355991363525, 1.2632805109024048, 1.2559168338775635, 1.25400710105896, 1.247633934020996, 1.2448164224624634, 1.2433146238327026, 1.2405478954315186, 1.2322909832000732, 1.2326573133468628, 1.225773572921753, 1.223296880722046, 1.2195086479187012, 1.2217576503753662, 1.211997151374817, 1.2114771604537964, 1.2020373344421387, 1.2068967819213867, 1.2054929733276367, 1.1953508853912354, 1.190768837928772, 1.1923880577087402, 1.1846462488174438, 1.1898095607757568, 1.1788216829299927, 1.1794418096542358, 1.1795897483825684, 1.1760568618774414, 1.1774650812149048, 1.1818647384643555, 1.1654950380325317, 1.1794357299804688, 1.1620829105377197, 1.166482925415039, 1.1642178297042847, 1.1602987051010132, 1.1556105613708496, 1.1643041372299194, 1.1559456586837769], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.5305429697036743, 0.5373303294181824, 0.5180995464324951, 0.5248869061470032, 0.5203620195388794, 0.5203620195388794, 0.5226244330406189, 0.5214931964874268, 0.5226244330406189, 0.5226244330406189, 0.5226244330406189, 0.529411792755127, 0.5203620195388794, 0.5203620195388794, 0.5214931964874268, 0.5339366793632507, 0.5328054428100586, 0.5271493196487427, 0.540723979473114, 0.5361990928649902, 0.5463801026344299, 0.5463801026344299, 0.5350678563117981, 0.5441176295280457, 0.5452488660812378, 0.5429864525794983, 0.5441176295280457, 0.5452488660812378, 0.5554298758506775, 0.5565611124038696, 0.5486425161361694, 0.5475113391876221, 0.5565611124038696, 0.5520362257957458, 0.5463801026344299, 0.5497737526893616, 0.5509049892425537, 0.5565611124038696, 0.5565611124038696, 0.5565611124038696, 0.5554298758506775, 0.5531674027442932, 0.5554298758506775, 0.5599547624588013, 0.5588235259056091, 0.5520362257957458, 0.557692289352417, 0.5554298758506775, 0.557692289352417, 0.5565611124038696, 0.5565611124038696, 0.5486425161361694, 0.5588235259056091, 0.5531674027442932, 0.557692289352417, 0.5554298758506775, 0.5554298758506775, 0.5712669491767883, 0.5520362257957458, 0.5509049892425537, 0.5690045356750488, 0.5588235259056091, 0.570135772228241, 0.564479649066925, 0.557692289352417, 0.5622171759605408, 0.5746606588363647, 0.557692289352417, 0.5542986392974854, 0.5542986392974854, 0.5565611124038696, 0.5554298758506775, 0.5588235259056091, 0.5565611124038696, 0.5610859990119934, 0.5599547624588013, 0.5769230723381042, 0.5690045356750488, 0.564479649066925, 0.5667420625686646, 0.5622171759605408, 0.5656108856201172, 0.5678732991218567, 0.5633484125137329, 0.5565611124038696, 0.570135772228241, 0.5588235259056091, 0.5678732991218567, 0.5690045356750488, 0.5667420625686646, 0.564479649066925, 0.5633484125137329, 0.564479649066925, 0.564479649066925, 0.5656108856201172, 0.557692289352417, 0.5667420625686646, 0.5610859990119934]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - ETA: 0s - loss: 1.7020 - accuracy: 0.5028"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 10s 62ms/step - loss: 1.7020 - accuracy: 0.5028 - val_loss: 1.6966 - val_accuracy: 0.4855\n","Epoch 2/100\n","31/31 [==============================] - 1s 24ms/step - loss: 1.6908 - accuracy: 0.5028 - val_loss: 1.6858 - val_accuracy: 0.5258\n","Epoch 3/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.6797 - accuracy: 0.5160 - val_loss: 1.6752 - val_accuracy: 0.5186\n","Epoch 4/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.6686 - accuracy: 0.5183 - val_loss: 1.6647 - val_accuracy: 0.5248\n","Epoch 5/100\n","31/31 [==============================] - 1s 25ms/step - loss: 1.6578 - accuracy: 0.5220 - val_loss: 1.6543 - val_accuracy: 0.5279\n","Epoch 6/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.6470 - accuracy: 0.5253 - val_loss: 1.6440 - val_accuracy: 0.5258\n","Epoch 7/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.6366 - accuracy: 0.5253 - val_loss: 1.6339 - val_accuracy: 0.5279\n","Epoch 8/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.6261 - accuracy: 0.5274 - val_loss: 1.6238 - val_accuracy: 0.5227\n","Epoch 9/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.6161 - accuracy: 0.5238 - val_loss: 1.6139 - val_accuracy: 0.5217\n","Epoch 10/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.6059 - accuracy: 0.5302 - val_loss: 1.6041 - val_accuracy: 0.5238\n","Epoch 11/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.5961 - accuracy: 0.5300 - val_loss: 1.5944 - val_accuracy: 0.5238\n","Epoch 12/100\n","31/31 [==============================] - 1s 24ms/step - loss: 1.5863 - accuracy: 0.5336 - val_loss: 1.5848 - val_accuracy: 0.5289\n","Epoch 13/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.5766 - accuracy: 0.5375 - val_loss: 1.5753 - val_accuracy: 0.5238\n","Epoch 14/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.5670 - accuracy: 0.5408 - val_loss: 1.5660 - val_accuracy: 0.5279\n","Epoch 15/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.5576 - accuracy: 0.5411 - val_loss: 1.5568 - val_accuracy: 0.5279\n","Epoch 16/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.5481 - accuracy: 0.5424 - val_loss: 1.5477 - val_accuracy: 0.5279\n","Epoch 17/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.5389 - accuracy: 0.5426 - val_loss: 1.5388 - val_accuracy: 0.5217\n","Epoch 18/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.5295 - accuracy: 0.5426 - val_loss: 1.5299 - val_accuracy: 0.5196\n","Epoch 19/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.5205 - accuracy: 0.5460 - val_loss: 1.5212 - val_accuracy: 0.5248\n","Epoch 20/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.5116 - accuracy: 0.5465 - val_loss: 1.5127 - val_accuracy: 0.5238\n","Epoch 21/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.5027 - accuracy: 0.5468 - val_loss: 1.5042 - val_accuracy: 0.5217\n","Epoch 22/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.4934 - accuracy: 0.5491 - val_loss: 1.4961 - val_accuracy: 0.5269\n","Epoch 23/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.4848 - accuracy: 0.5465 - val_loss: 1.4873 - val_accuracy: 0.5269\n","Epoch 24/100\n","31/31 [==============================] - 1s 30ms/step - loss: 1.4757 - accuracy: 0.5473 - val_loss: 1.4795 - val_accuracy: 0.5300\n","Epoch 25/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4671 - accuracy: 0.5455 - val_loss: 1.4719 - val_accuracy: 0.5238\n","Epoch 26/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4584 - accuracy: 0.5481 - val_loss: 1.4634 - val_accuracy: 0.5279\n","Epoch 27/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.4500 - accuracy: 0.5532 - val_loss: 1.4555 - val_accuracy: 0.5258\n","Epoch 28/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4414 - accuracy: 0.5496 - val_loss: 1.4477 - val_accuracy: 0.5289\n","Epoch 29/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4329 - accuracy: 0.5589 - val_loss: 1.4394 - val_accuracy: 0.5300\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4242 - accuracy: 0.5530 - val_loss: 1.4311 - val_accuracy: 0.5258\n","Epoch 31/100\n","31/31 [==============================] - 1s 25ms/step - loss: 1.4162 - accuracy: 0.5594 - val_loss: 1.4244 - val_accuracy: 0.5310\n","Epoch 32/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.4077 - accuracy: 0.5563 - val_loss: 1.4165 - val_accuracy: 0.5258\n","Epoch 33/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.3997 - accuracy: 0.5568 - val_loss: 1.4068 - val_accuracy: 0.5207\n","Epoch 34/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.3913 - accuracy: 0.5589 - val_loss: 1.4009 - val_accuracy: 0.5258\n","Epoch 35/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.3832 - accuracy: 0.5615 - val_loss: 1.3924 - val_accuracy: 0.5279\n","Epoch 36/100\n","31/31 [==============================] - 1s 24ms/step - loss: 1.3756 - accuracy: 0.5654 - val_loss: 1.3845 - val_accuracy: 0.5372\n","Epoch 37/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.3674 - accuracy: 0.5672 - val_loss: 1.3777 - val_accuracy: 0.5341\n","Epoch 38/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.3593 - accuracy: 0.5690 - val_loss: 1.3737 - val_accuracy: 0.5238\n","Epoch 39/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.3518 - accuracy: 0.5643 - val_loss: 1.3648 - val_accuracy: 0.5258\n","Epoch 40/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.3442 - accuracy: 0.5667 - val_loss: 1.3577 - val_accuracy: 0.5331\n","Epoch 41/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.3363 - accuracy: 0.5680 - val_loss: 1.3524 - val_accuracy: 0.5217\n","Epoch 42/100\n","31/31 [==============================] - 1s 32ms/step - loss: 1.3288 - accuracy: 0.5744 - val_loss: 1.3429 - val_accuracy: 0.5403\n","Epoch 43/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.3221 - accuracy: 0.5661 - val_loss: 1.3376 - val_accuracy: 0.5279\n","Epoch 44/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.3135 - accuracy: 0.5693 - val_loss: 1.3344 - val_accuracy: 0.5248\n","Epoch 45/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.3065 - accuracy: 0.5708 - val_loss: 1.3252 - val_accuracy: 0.5248\n","Epoch 46/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.2992 - accuracy: 0.5711 - val_loss: 1.3165 - val_accuracy: 0.5341\n","Epoch 47/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.2913 - accuracy: 0.5762 - val_loss: 1.3149 - val_accuracy: 0.5289\n","Epoch 48/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.2841 - accuracy: 0.5767 - val_loss: 1.3066 - val_accuracy: 0.5393\n","Epoch 49/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.2765 - accuracy: 0.5814 - val_loss: 1.3022 - val_accuracy: 0.5248\n","Epoch 50/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.2697 - accuracy: 0.5775 - val_loss: 1.2969 - val_accuracy: 0.5300\n","Epoch 51/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.2628 - accuracy: 0.5806 - val_loss: 1.2900 - val_accuracy: 0.5310\n","Epoch 52/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2562 - accuracy: 0.5848 - val_loss: 1.2989 - val_accuracy: 0.5217\n","Epoch 53/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2508 - accuracy: 0.5747 - val_loss: 1.2801 - val_accuracy: 0.5300\n","Epoch 54/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2431 - accuracy: 0.5796 - val_loss: 1.2733 - val_accuracy: 0.5258\n","Epoch 55/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2344 - accuracy: 0.5866 - val_loss: 1.2671 - val_accuracy: 0.5269\n","Epoch 56/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2284 - accuracy: 0.5897 - val_loss: 1.2627 - val_accuracy: 0.5269\n","Epoch 57/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2221 - accuracy: 0.5884 - val_loss: 1.2528 - val_accuracy: 0.5258\n","Epoch 58/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2148 - accuracy: 0.5873 - val_loss: 1.2462 - val_accuracy: 0.5217\n","Epoch 59/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.2086 - accuracy: 0.5889 - val_loss: 1.2449 - val_accuracy: 0.5300\n","Epoch 60/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2019 - accuracy: 0.5933 - val_loss: 1.2376 - val_accuracy: 0.5289\n","Epoch 61/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.1973 - accuracy: 0.5912 - val_loss: 1.2293 - val_accuracy: 0.5258\n","Epoch 62/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.1906 - accuracy: 0.5938 - val_loss: 1.2311 - val_accuracy: 0.5320\n","Epoch 63/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.1838 - accuracy: 0.5995 - val_loss: 1.2218 - val_accuracy: 0.5258\n","Epoch 64/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.1768 - accuracy: 0.6000 - val_loss: 1.2211 - val_accuracy: 0.5248\n","Epoch 65/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.1707 - accuracy: 0.6028 - val_loss: 1.2233 - val_accuracy: 0.5331\n","Epoch 66/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.1650 - accuracy: 0.5992 - val_loss: 1.2092 - val_accuracy: 0.5217\n","Epoch 67/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.1593 - accuracy: 0.6036 - val_loss: 1.2058 - val_accuracy: 0.5258\n","Epoch 68/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.1521 - accuracy: 0.5982 - val_loss: 1.2074 - val_accuracy: 0.5279\n","Epoch 69/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.1463 - accuracy: 0.6052 - val_loss: 1.1977 - val_accuracy: 0.5227\n","Epoch 70/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.1413 - accuracy: 0.6047 - val_loss: 1.1958 - val_accuracy: 0.5310\n","Epoch 71/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1351 - accuracy: 0.6000 - val_loss: 1.1961 - val_accuracy: 0.5341\n","Epoch 72/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.1287 - accuracy: 0.6106 - val_loss: 1.1817 - val_accuracy: 0.5279\n","Epoch 73/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.1226 - accuracy: 0.6090 - val_loss: 1.2058 - val_accuracy: 0.5351\n","Epoch 74/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.1206 - accuracy: 0.6021 - val_loss: 1.1825 - val_accuracy: 0.5279\n","Epoch 75/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1127 - accuracy: 0.6109 - val_loss: 1.1838 - val_accuracy: 0.5279\n","Epoch 76/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1058 - accuracy: 0.6142 - val_loss: 1.1656 - val_accuracy: 0.5238\n","Epoch 77/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1017 - accuracy: 0.6106 - val_loss: 1.1764 - val_accuracy: 0.5279\n","Epoch 78/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0955 - accuracy: 0.6168 - val_loss: 1.1657 - val_accuracy: 0.5269\n","Epoch 79/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0870 - accuracy: 0.6163 - val_loss: 1.1681 - val_accuracy: 0.5362\n","Epoch 80/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0849 - accuracy: 0.6186 - val_loss: 1.1585 - val_accuracy: 0.5320\n","Epoch 81/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0795 - accuracy: 0.6168 - val_loss: 1.1532 - val_accuracy: 0.5207\n","Epoch 82/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0733 - accuracy: 0.6189 - val_loss: 1.1714 - val_accuracy: 0.5269\n","Epoch 83/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0672 - accuracy: 0.6145 - val_loss: 1.1414 - val_accuracy: 0.5331\n","Epoch 84/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0646 - accuracy: 0.6207 - val_loss: 1.1605 - val_accuracy: 0.5176\n","Epoch 85/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0554 - accuracy: 0.6253 - val_loss: 1.1510 - val_accuracy: 0.5300\n","Epoch 86/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0503 - accuracy: 0.6287 - val_loss: 1.1543 - val_accuracy: 0.5207\n","Epoch 87/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0470 - accuracy: 0.6217 - val_loss: 1.1304 - val_accuracy: 0.5289\n","Epoch 88/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0414 - accuracy: 0.6199 - val_loss: 1.1497 - val_accuracy: 0.5248\n","Epoch 89/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0374 - accuracy: 0.6256 - val_loss: 1.1261 - val_accuracy: 0.5310\n","Epoch 90/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0343 - accuracy: 0.6243 - val_loss: 1.1230 - val_accuracy: 0.5372\n","Epoch 91/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0243 - accuracy: 0.6398 - val_loss: 1.1267 - val_accuracy: 0.5331\n","Epoch 92/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0194 - accuracy: 0.6289 - val_loss: 1.1231 - val_accuracy: 0.5300\n","Epoch 93/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.0180 - accuracy: 0.6300 - val_loss: 1.1134 - val_accuracy: 0.5351\n","Epoch 94/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0103 - accuracy: 0.6375 - val_loss: 1.1419 - val_accuracy: 0.5227\n","Epoch 95/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0070 - accuracy: 0.6331 - val_loss: 1.1320 - val_accuracy: 0.5300\n","Epoch 96/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0014 - accuracy: 0.6318 - val_loss: 1.1171 - val_accuracy: 0.5258\n","Epoch 97/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9972 - accuracy: 0.6297 - val_loss: 1.1367 - val_accuracy: 0.5227\n","Epoch 98/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9942 - accuracy: 0.6326 - val_loss: 1.1245 - val_accuracy: 0.5300\n","Epoch 99/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.9843 - accuracy: 0.6408 - val_loss: 1.1241 - val_accuracy: 0.5258\n","Epoch 100/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9868 - accuracy: 0.6318 - val_loss: 1.1310 - val_accuracy: 0.5248\n","{'loss': [1.7020405530929565, 1.690790057182312, 1.6797089576721191, 1.6685998439788818, 1.6577794551849365, 1.6470263004302979, 1.6365537643432617, 1.626069188117981, 1.6160565614700317, 1.6059317588806152, 1.5961098670959473, 1.5862598419189453, 1.576554298400879, 1.566975712776184, 1.5576084852218628, 1.5481268167495728, 1.5388753414154053, 1.529524564743042, 1.5205103158950806, 1.51164972782135, 1.502668857574463, 1.4933935403823853, 1.4847712516784668, 1.4757487773895264, 1.4670835733413696, 1.4584201574325562, 1.4499784708023071, 1.4414364099502563, 1.432936668395996, 1.4241570234298706, 1.4162203073501587, 1.4076547622680664, 1.3996950387954712, 1.3912791013717651, 1.3832473754882812, 1.375629186630249, 1.3674207925796509, 1.359336256980896, 1.3518131971359253, 1.344206690788269, 1.3362585306167603, 1.3287681341171265, 1.322086215019226, 1.3135401010513306, 1.3064682483673096, 1.299237847328186, 1.2913063764572144, 1.28408944606781, 1.276536226272583, 1.2697348594665527, 1.2628028392791748, 1.2561709880828857, 1.2507675886154175, 1.243064284324646, 1.2343840599060059, 1.2283872365951538, 1.2221077680587769, 1.2147655487060547, 1.2086154222488403, 1.2019169330596924, 1.1972538232803345, 1.190617561340332, 1.1838098764419556, 1.1768192052841187, 1.1707420349121094, 1.164983868598938, 1.159283995628357, 1.1521304845809937, 1.146314263343811, 1.141310691833496, 1.1351391077041626, 1.1286916732788086, 1.122567057609558, 1.1205575466156006, 1.1127424240112305, 1.1058365106582642, 1.1017483472824097, 1.0955066680908203, 1.0870141983032227, 1.0849193334579468, 1.0794756412506104, 1.073301911354065, 1.0671800374984741, 1.064643144607544, 1.0554463863372803, 1.0503044128417969, 1.0469954013824463, 1.0413950681686401, 1.0373550653457642, 1.0342624187469482, 1.0243221521377563, 1.019361972808838, 1.0179743766784668, 1.0103248357772827, 1.0070096254348755, 1.001378059387207, 0.9972230792045593, 0.9941527843475342, 0.9843435287475586, 0.9868219494819641], 'accuracy': [0.502842366695404, 0.502842366695404, 0.516020655632019, 0.5183462500572205, 0.5219638347625732, 0.5253229737281799, 0.5253229737281799, 0.52739018201828, 0.5237725973129272, 0.5302325487136841, 0.5299741625785828, 0.5335917472839355, 0.5374677181243896, 0.5408268570899963, 0.5410852432250977, 0.542377233505249, 0.5426356792449951, 0.5426356792449951, 0.5459948182106018, 0.5465116500854492, 0.5467700362205505, 0.549095630645752, 0.5465116500854492, 0.5472868084907532, 0.5454780459403992, 0.5480620265007019, 0.5532299876213074, 0.5496124029159546, 0.5589147210121155, 0.552971601486206, 0.5594315528869629, 0.5563307404518127, 0.5568475723266602, 0.5589147210121155, 0.5614987015724182, 0.5653746724128723, 0.5671834349632263, 0.5689922571182251, 0.5643410682678223, 0.5666666626930237, 0.567958652973175, 0.5744186043739319, 0.566149890422821, 0.5692506432533264, 0.5708010196685791, 0.5710594058036804, 0.5762273669242859, 0.5767441987991333, 0.5813953280448914, 0.5775193572044373, 0.5806201696395874, 0.5847545266151428, 0.5746769905090332, 0.5795865654945374, 0.5865632891654968, 0.589664101600647, 0.5883721113204956, 0.5873385071754456, 0.5888888835906982, 0.593281626701355, 0.5912144780158997, 0.5937984585762024, 0.5994831919670105, 0.6000000238418579, 0.602842390537262, 0.5992248058319092, 0.6036175489425659, 0.5981912016868591, 0.6051679849624634, 0.604651153087616, 0.6000000238418579, 0.6105943322181702, 0.6090439558029175, 0.6020671725273132, 0.6108527183532715, 0.6142118573188782, 0.6105943322181702, 0.6167958378791809, 0.6162790656089783, 0.6186046600341797, 0.6167958378791809, 0.618863046169281, 0.6144703030586243, 0.620671808719635, 0.6253229975700378, 0.6286821961402893, 0.6217054128646851, 0.619896650314331, 0.6255813837051392, 0.6242893934249878, 0.6397932767868042, 0.6289405822753906, 0.6299741864204407, 0.6374676823616028, 0.633074939250946, 0.6317829489707947, 0.6297157406806946, 0.6325581669807434, 0.6408268809318542, 0.6317829489707947], 'val_loss': [1.6966116428375244, 1.685835599899292, 1.6751936674118042, 1.664681315422058, 1.6542813777923584, 1.6440119743347168, 1.6338520050048828, 1.6238086223602295, 1.613881230354309, 1.60405695438385, 1.5943573713302612, 1.5848140716552734, 1.575323224067688, 1.5660096406936646, 1.5567876100540161, 1.5477242469787598, 1.538794755935669, 1.5299153327941895, 1.5211983919143677, 1.5127321481704712, 1.5042222738265991, 1.4961317777633667, 1.4872918128967285, 1.4795262813568115, 1.4718557596206665, 1.463363528251648, 1.4555009603500366, 1.4476609230041504, 1.4393818378448486, 1.431147575378418, 1.424398422241211, 1.416450023651123, 1.406790852546692, 1.4009455442428589, 1.3923956155776978, 1.3845479488372803, 1.377732276916504, 1.3737268447875977, 1.3648042678833008, 1.3577440977096558, 1.3523869514465332, 1.34291410446167, 1.3375624418258667, 1.334389567375183, 1.3252105712890625, 1.3164658546447754, 1.3149099349975586, 1.3065518140792847, 1.3021551370620728, 1.2969269752502441, 1.29002046585083, 1.2989020347595215, 1.2801450490951538, 1.2733172178268433, 1.2671319246292114, 1.262711763381958, 1.2528005838394165, 1.2461512088775635, 1.2449262142181396, 1.2375942468643188, 1.2292873859405518, 1.2310653924942017, 1.22183096408844, 1.221073031425476, 1.2233132123947144, 1.2092407941818237, 1.205772042274475, 1.207392930984497, 1.1976933479309082, 1.1958341598510742, 1.196097731590271, 1.1817013025283813, 1.2058404684066772, 1.18251371383667, 1.183803915977478, 1.165593147277832, 1.1764092445373535, 1.1656917333602905, 1.168065071105957, 1.1585228443145752, 1.1532416343688965, 1.1714354753494263, 1.1414031982421875, 1.160506248474121, 1.150976538658142, 1.1543395519256592, 1.1303565502166748, 1.1497406959533691, 1.126075267791748, 1.1229609251022339, 1.1266742944717407, 1.123051643371582, 1.1134392023086548, 1.141922116279602, 1.131983995437622, 1.1170932054519653, 1.1367261409759521, 1.1245388984680176, 1.1240592002868652, 1.1309735774993896], 'val_accuracy': [0.48553720116615295, 0.5258264541625977, 0.5185950398445129, 0.5247933864593506, 0.5278925895690918, 0.5258264541625977, 0.5278925895690918, 0.5227272510528564, 0.5216942429542542, 0.5237603187561035, 0.5237603187561035, 0.5289255976676941, 0.5237603187561035, 0.5278925895690918, 0.5278925895690918, 0.5278925895690918, 0.5216942429542542, 0.51962810754776, 0.5247933864593506, 0.5237603187561035, 0.5216942429542542, 0.5268595218658447, 0.5268595218658447, 0.5299586653709412, 0.5237603187561035, 0.5278925895690918, 0.5258264541625977, 0.5289255976676941, 0.5299586653709412, 0.5258264541625977, 0.5309917330741882, 0.5258264541625977, 0.5206611752510071, 0.5258264541625977, 0.5278925895690918, 0.5371900796890259, 0.5340909361839294, 0.5237603187561035, 0.5258264541625977, 0.5330578684806824, 0.5216942429542542, 0.5402892827987671, 0.5278925895690918, 0.5247933864593506, 0.5247933864593506, 0.5340909361839294, 0.5289255976676941, 0.53925621509552, 0.5247933864593506, 0.5299586653709412, 0.5309917330741882, 0.5216942429542542, 0.5299586653709412, 0.5258264541625977, 0.5268595218658447, 0.5268595218658447, 0.5258264541625977, 0.5216942429542542, 0.5299586653709412, 0.5289255976676941, 0.5258264541625977, 0.5320248007774353, 0.5258264541625977, 0.5247933864593506, 0.5330578684806824, 0.5216942429542542, 0.5258264541625977, 0.5278925895690918, 0.5227272510528564, 0.5309917330741882, 0.5340909361839294, 0.5278925895690918, 0.5351239442825317, 0.5278925895690918, 0.5278925895690918, 0.5237603187561035, 0.5278925895690918, 0.5268595218658447, 0.5361570119857788, 0.5320248007774353, 0.5206611752510071, 0.5268595218658447, 0.5330578684806824, 0.5175619721412659, 0.5299586653709412, 0.5206611752510071, 0.5289255976676941, 0.5247933864593506, 0.5309917330741882, 0.5371900796890259, 0.5330578684806824, 0.5299586653709412, 0.5351239442825317, 0.5227272510528564, 0.5299586653709412, 0.5258264541625977, 0.5227272510528564, 0.5299586653709412, 0.5258264541625977, 0.5247933864593506]}\n","32/32 [==============================] - 1s 6ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/29 [===========================>..] - ETA: 0s - loss: 1.0437 - accuracy: 0.6057"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 7s 57ms/step - loss: 1.0428 - accuracy: 0.6043 - val_loss: 1.0816 - val_accuracy: 0.5162\n","Epoch 2/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.0337 - accuracy: 0.6061 - val_loss: 1.0753 - val_accuracy: 0.5172\n","Epoch 3/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0267 - accuracy: 0.6075 - val_loss: 1.0736 - val_accuracy: 0.5172\n","Epoch 4/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.0220 - accuracy: 0.6045 - val_loss: 1.0690 - val_accuracy: 0.5172\n","Epoch 5/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.0189 - accuracy: 0.6091 - val_loss: 1.0659 - val_accuracy: 0.5172\n","Epoch 6/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.0139 - accuracy: 0.6078 - val_loss: 1.0605 - val_accuracy: 0.5183\n","Epoch 7/100\n","29/29 [==============================] - 1s 26ms/step - loss: 1.0076 - accuracy: 0.6142 - val_loss: 1.0582 - val_accuracy: 0.5194\n","Epoch 8/100\n","29/29 [==============================] - 1s 29ms/step - loss: 0.9994 - accuracy: 0.6245 - val_loss: 1.0527 - val_accuracy: 0.5248\n","Epoch 9/100\n","29/29 [==============================] - 1s 32ms/step - loss: 0.9963 - accuracy: 0.6218 - val_loss: 1.0469 - val_accuracy: 0.5356\n","Epoch 10/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.9902 - accuracy: 0.6123 - val_loss: 1.0431 - val_accuracy: 0.5356\n","Epoch 11/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.9868 - accuracy: 0.6210 - val_loss: 1.0377 - val_accuracy: 0.5550\n","Epoch 12/100\n","29/29 [==============================] - 1s 32ms/step - loss: 0.9818 - accuracy: 0.6309 - val_loss: 1.0311 - val_accuracy: 0.5722\n","Epoch 13/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.9762 - accuracy: 0.6234 - val_loss: 1.0311 - val_accuracy: 0.5636\n","Epoch 14/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.9745 - accuracy: 0.6212 - val_loss: 1.0262 - val_accuracy: 0.5625\n","Epoch 15/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9702 - accuracy: 0.6247 - val_loss: 1.0259 - val_accuracy: 0.5625\n","Epoch 16/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9661 - accuracy: 0.6334 - val_loss: 1.0247 - val_accuracy: 0.5614\n","Epoch 17/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.9641 - accuracy: 0.6193 - val_loss: 1.0226 - val_accuracy: 0.5657\n","Epoch 18/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.9575 - accuracy: 0.6298 - val_loss: 1.0076 - val_accuracy: 0.5862\n","Epoch 19/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9535 - accuracy: 0.6255 - val_loss: 1.0033 - val_accuracy: 0.5841\n","Epoch 20/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.9467 - accuracy: 0.6436 - val_loss: 1.0079 - val_accuracy: 0.5862\n","Epoch 21/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9458 - accuracy: 0.6309 - val_loss: 1.0023 - val_accuracy: 0.5841\n","Epoch 22/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.9404 - accuracy: 0.6304 - val_loss: 1.0026 - val_accuracy: 0.5905\n","Epoch 23/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.9340 - accuracy: 0.6358 - val_loss: 1.0014 - val_accuracy: 0.5819\n","Epoch 24/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.9359 - accuracy: 0.6390 - val_loss: 0.9957 - val_accuracy: 0.5884\n","Epoch 25/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.9315 - accuracy: 0.6277 - val_loss: 0.9933 - val_accuracy: 0.5927\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9248 - accuracy: 0.6374 - val_loss: 0.9961 - val_accuracy: 0.5819\n","Epoch 27/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.9221 - accuracy: 0.6317 - val_loss: 0.9988 - val_accuracy: 0.5808\n","Epoch 28/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.9174 - accuracy: 0.6420 - val_loss: 0.9915 - val_accuracy: 0.5970\n","Epoch 29/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9149 - accuracy: 0.6350 - val_loss: 0.9946 - val_accuracy: 0.5744\n","Epoch 30/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9094 - accuracy: 0.6369 - val_loss: 0.9916 - val_accuracy: 0.5862\n","Epoch 31/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9084 - accuracy: 0.6395 - val_loss: 0.9893 - val_accuracy: 0.5884\n","Epoch 32/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9010 - accuracy: 0.6401 - val_loss: 0.9902 - val_accuracy: 0.5970\n","Epoch 33/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8978 - accuracy: 0.6363 - val_loss: 0.9864 - val_accuracy: 0.5970\n","Epoch 34/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8955 - accuracy: 0.6492 - val_loss: 0.9912 - val_accuracy: 0.5851\n","Epoch 35/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8865 - accuracy: 0.6519 - val_loss: 0.9895 - val_accuracy: 0.5819\n","Epoch 36/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8926 - accuracy: 0.6401 - val_loss: 0.9870 - val_accuracy: 0.5797\n","Epoch 37/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8852 - accuracy: 0.6325 - val_loss: 0.9835 - val_accuracy: 0.5884\n","Epoch 38/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8767 - accuracy: 0.6595 - val_loss: 0.9883 - val_accuracy: 0.5830\n","Epoch 39/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.8800 - accuracy: 0.6447 - val_loss: 0.9902 - val_accuracy: 0.5765\n","Epoch 40/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8733 - accuracy: 0.6441 - val_loss: 0.9840 - val_accuracy: 0.5873\n","Epoch 41/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8698 - accuracy: 0.6498 - val_loss: 0.9819 - val_accuracy: 0.5959\n","Epoch 42/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8639 - accuracy: 0.6490 - val_loss: 0.9978 - val_accuracy: 0.5528\n","Epoch 43/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8656 - accuracy: 0.6525 - val_loss: 0.9805 - val_accuracy: 0.5905\n","Epoch 44/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.8593 - accuracy: 0.6533 - val_loss: 0.9782 - val_accuracy: 0.5959\n","Epoch 45/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8593 - accuracy: 0.6536 - val_loss: 0.9768 - val_accuracy: 0.5873\n","Epoch 46/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8577 - accuracy: 0.6414 - val_loss: 0.9796 - val_accuracy: 0.5970\n","Epoch 47/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.8492 - accuracy: 0.6649 - val_loss: 0.9787 - val_accuracy: 0.5927\n","Epoch 48/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.8465 - accuracy: 0.6592 - val_loss: 0.9757 - val_accuracy: 0.5787\n","Epoch 49/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.8432 - accuracy: 0.6600 - val_loss: 0.9737 - val_accuracy: 0.5981\n","Epoch 50/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8381 - accuracy: 0.6643 - val_loss: 0.9793 - val_accuracy: 0.5905\n","Epoch 51/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8410 - accuracy: 0.6498 - val_loss: 0.9786 - val_accuracy: 0.5916\n","Epoch 52/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8352 - accuracy: 0.6600 - val_loss: 0.9731 - val_accuracy: 0.5916\n","Epoch 53/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8291 - accuracy: 0.6627 - val_loss: 0.9777 - val_accuracy: 0.5894\n","Epoch 54/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8278 - accuracy: 0.6587 - val_loss: 0.9780 - val_accuracy: 0.5884\n","Epoch 55/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8260 - accuracy: 0.6684 - val_loss: 0.9738 - val_accuracy: 0.5927\n","Epoch 56/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8245 - accuracy: 0.6608 - val_loss: 0.9775 - val_accuracy: 0.5862\n","Epoch 57/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8215 - accuracy: 0.6600 - val_loss: 0.9649 - val_accuracy: 0.5959\n","Epoch 58/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8197 - accuracy: 0.6614 - val_loss: 1.0008 - val_accuracy: 0.5442\n","Epoch 59/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8165 - accuracy: 0.6689 - val_loss: 0.9806 - val_accuracy: 0.5851\n","Epoch 60/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8122 - accuracy: 0.6721 - val_loss: 0.9762 - val_accuracy: 0.5841\n","Epoch 61/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8078 - accuracy: 0.6754 - val_loss: 0.9746 - val_accuracy: 0.5776\n","Epoch 62/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8013 - accuracy: 0.6786 - val_loss: 0.9757 - val_accuracy: 0.5819\n","Epoch 63/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8052 - accuracy: 0.6627 - val_loss: 0.9770 - val_accuracy: 0.5765\n","Epoch 64/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8008 - accuracy: 0.6797 - val_loss: 0.9703 - val_accuracy: 0.5959\n","Epoch 65/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7953 - accuracy: 0.6789 - val_loss: 0.9792 - val_accuracy: 0.5916\n","Epoch 66/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.7951 - accuracy: 0.6767 - val_loss: 0.9738 - val_accuracy: 0.5884\n","Epoch 67/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7930 - accuracy: 0.6840 - val_loss: 0.9824 - val_accuracy: 0.5851\n","Epoch 68/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7926 - accuracy: 0.6727 - val_loss: 0.9735 - val_accuracy: 0.5700\n","Epoch 69/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.7947 - accuracy: 0.6651 - val_loss: 0.9940 - val_accuracy: 0.5884\n","Epoch 70/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.7850 - accuracy: 0.6867 - val_loss: 0.9755 - val_accuracy: 0.5830\n","Epoch 71/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.7818 - accuracy: 0.6770 - val_loss: 0.9780 - val_accuracy: 0.5841\n","Epoch 72/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7758 - accuracy: 0.6765 - val_loss: 0.9824 - val_accuracy: 0.5819\n","Epoch 73/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.7725 - accuracy: 0.6851 - val_loss: 0.9682 - val_accuracy: 0.5884\n","Epoch 74/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7702 - accuracy: 0.6894 - val_loss: 0.9823 - val_accuracy: 0.5894\n","Epoch 75/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7678 - accuracy: 0.6856 - val_loss: 0.9721 - val_accuracy: 0.5894\n","Epoch 76/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7653 - accuracy: 0.6923 - val_loss: 0.9905 - val_accuracy: 0.5808\n","Epoch 77/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7629 - accuracy: 0.6870 - val_loss: 1.0014 - val_accuracy: 0.5711\n","Epoch 78/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7666 - accuracy: 0.6856 - val_loss: 0.9796 - val_accuracy: 0.5873\n","Epoch 79/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7570 - accuracy: 0.6907 - val_loss: 0.9830 - val_accuracy: 0.5981\n","Epoch 80/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7605 - accuracy: 0.6837 - val_loss: 0.9748 - val_accuracy: 0.5873\n","Epoch 81/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7490 - accuracy: 0.6991 - val_loss: 0.9769 - val_accuracy: 0.5765\n","Epoch 82/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7496 - accuracy: 0.6959 - val_loss: 0.9813 - val_accuracy: 0.5744\n","Epoch 83/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7518 - accuracy: 0.6959 - val_loss: 0.9765 - val_accuracy: 0.5916\n","Epoch 84/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7496 - accuracy: 0.6905 - val_loss: 1.0008 - val_accuracy: 0.5593\n","Epoch 85/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7433 - accuracy: 0.6975 - val_loss: 0.9944 - val_accuracy: 0.5841\n","Epoch 86/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7563 - accuracy: 0.6862 - val_loss: 0.9824 - val_accuracy: 0.5754\n","Epoch 87/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7445 - accuracy: 0.6961 - val_loss: 1.0036 - val_accuracy: 0.5754\n","Epoch 88/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7378 - accuracy: 0.6983 - val_loss: 0.9902 - val_accuracy: 0.5916\n","Epoch 89/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7372 - accuracy: 0.6999 - val_loss: 0.9754 - val_accuracy: 0.5862\n","Epoch 90/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7291 - accuracy: 0.6950 - val_loss: 0.9802 - val_accuracy: 0.5873\n","Epoch 91/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7287 - accuracy: 0.7031 - val_loss: 0.9914 - val_accuracy: 0.5916\n","Epoch 92/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7257 - accuracy: 0.7142 - val_loss: 0.9861 - val_accuracy: 0.5776\n","Epoch 93/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7275 - accuracy: 0.7018 - val_loss: 1.0072 - val_accuracy: 0.5744\n","Epoch 94/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7280 - accuracy: 0.7082 - val_loss: 1.0091 - val_accuracy: 0.5765\n","Epoch 95/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7135 - accuracy: 0.7139 - val_loss: 0.9962 - val_accuracy: 0.5959\n","Epoch 96/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7149 - accuracy: 0.7074 - val_loss: 0.9968 - val_accuracy: 0.5765\n","Epoch 97/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7186 - accuracy: 0.7196 - val_loss: 1.0173 - val_accuracy: 0.5841\n","Epoch 98/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7130 - accuracy: 0.7055 - val_loss: 0.9965 - val_accuracy: 0.5873\n","Epoch 99/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.7140 - accuracy: 0.7128 - val_loss: 1.0076 - val_accuracy: 0.5948\n","Epoch 100/100\n","29/29 [==============================] - 2s 71ms/step - loss: 0.7064 - accuracy: 0.7158 - val_loss: 0.9993 - val_accuracy: 0.6013\n","{'loss': [1.0428287982940674, 1.0336850881576538, 1.0267481803894043, 1.0219652652740479, 1.0188974142074585, 1.0139133930206299, 1.0075916051864624, 0.9993913769721985, 0.9962601661682129, 0.9902487993240356, 0.9867799878120422, 0.9818187355995178, 0.9761784076690674, 0.9744547009468079, 0.9701508283615112, 0.9660833477973938, 0.9641465544700623, 0.9575214385986328, 0.9535293579101562, 0.9466578960418701, 0.9457608461380005, 0.9403837323188782, 0.9340275526046753, 0.9358728528022766, 0.931495189666748, 0.924842119216919, 0.9221282601356506, 0.9173890352249146, 0.9148968458175659, 0.9094029068946838, 0.9084352850914001, 0.9010401368141174, 0.8977758288383484, 0.8954511880874634, 0.8865001797676086, 0.8926451206207275, 0.885180652141571, 0.8766754865646362, 0.8800411820411682, 0.8732931613922119, 0.8697579503059387, 0.8638522028923035, 0.8655892610549927, 0.8592761754989624, 0.8593124747276306, 0.8576526641845703, 0.84919673204422, 0.8465359807014465, 0.84316486120224, 0.8380730152130127, 0.8409779071807861, 0.8352090716362, 0.8291077017784119, 0.8278128504753113, 0.8260344862937927, 0.8244768381118774, 0.821514368057251, 0.8197113275527954, 0.8165279030799866, 0.8122482895851135, 0.807762086391449, 0.8012645840644836, 0.8051873445510864, 0.800835371017456, 0.7952765226364136, 0.7950891256332397, 0.7930110692977905, 0.7926110625267029, 0.7946552038192749, 0.7849595546722412, 0.7817919254302979, 0.7758111357688904, 0.7725123167037964, 0.7701852321624756, 0.7678084373474121, 0.7652934789657593, 0.762906551361084, 0.7666281461715698, 0.7570292353630066, 0.7605106830596924, 0.7490171194076538, 0.7496048212051392, 0.7518323063850403, 0.7496413588523865, 0.7433476448059082, 0.7562846541404724, 0.7445279359817505, 0.7378482818603516, 0.7372429370880127, 0.7291245460510254, 0.7287124395370483, 0.7256808280944824, 0.7274580597877502, 0.7279877066612244, 0.7135223150253296, 0.7148948311805725, 0.718558669090271, 0.7130462527275085, 0.7140355110168457, 0.7064048051834106], 'accuracy': [0.6042564511299133, 0.6061422228813171, 0.6074892282485962, 0.6045258641242981, 0.6091055870056152, 0.607758641242981, 0.6142241358757019, 0.6244612336158752, 0.6217672228813171, 0.6123383641242981, 0.6209590435028076, 0.6309267282485962, 0.623383641242981, 0.6212284564971924, 0.6247305870056152, 0.6333512663841248, 0.6193426847457886, 0.6298491358757019, 0.6255387663841248, 0.6435883641242981, 0.6309267282485962, 0.6303879022598267, 0.6357758641242981, 0.639008641242981, 0.6276939511299133, 0.6373922228813171, 0.6317349076271057, 0.641972005367279, 0.6349676847457886, 0.6368534564971924, 0.6395474076271057, 0.6400862336158752, 0.6363146305084229, 0.6492456793785095, 0.6519396305084229, 0.6400862336158752, 0.6325430870056152, 0.6594827771186829, 0.6446659564971924, 0.6441271305084229, 0.649784505367279, 0.6489762663841248, 0.6524784564971924, 0.6532866358757019, 0.6535560488700867, 0.6414331793785095, 0.6648706793785095, 0.6592133641242981, 0.6600215435028076, 0.6643319129943848, 0.649784505367279, 0.6600215435028076, 0.662715494632721, 0.6586745977401733, 0.6683728694915771, 0.6608297228813171, 0.6600215435028076, 0.6613685488700867, 0.6689116358757019, 0.6721444129943848, 0.6753771305084229, 0.6786099076271057, 0.662715494632721, 0.6796875, 0.6788793206214905, 0.6767241358757019, 0.6839978694915771, 0.6726831793785095, 0.6651400923728943, 0.6866918206214905, 0.6769935488700867, 0.6764547228813171, 0.6850754022598267, 0.6893857717514038, 0.6856142282485962, 0.6923491358757019, 0.6869612336158752, 0.6856142282485962, 0.6907327771186829, 0.6837284564971924, 0.6990840435028076, 0.6958512663841248, 0.6958512663841248, 0.6904633641242981, 0.6974676847457886, 0.686152994632721, 0.6961206793785095, 0.6982758641242981, 0.6998922228813171, 0.6950430870056152, 0.703125, 0.7141702771186829, 0.701777994632721, 0.7082435488700867, 0.7139008641242981, 0.7074353694915771, 0.7195581793785095, 0.7055495977401733, 0.7128232717514038, 0.7157866358757019], 'val_loss': [1.0815849304199219, 1.0753334760665894, 1.0736172199249268, 1.0690020322799683, 1.065936803817749, 1.06045663356781, 1.058160662651062, 1.0527070760726929, 1.0469225645065308, 1.0430757999420166, 1.0376676321029663, 1.0310741662979126, 1.0310662984848022, 1.0262330770492554, 1.0259095430374146, 1.024704098701477, 1.0226041078567505, 1.0076130628585815, 1.0033131837844849, 1.0078929662704468, 1.002289056777954, 1.002560019493103, 1.0014071464538574, 0.9957295656204224, 0.9932537078857422, 0.9961313009262085, 0.9988026022911072, 0.9915143847465515, 0.9945916533470154, 0.9916113018989563, 0.9893485307693481, 0.9901976585388184, 0.9863524436950684, 0.9911602735519409, 0.9895414113998413, 0.9870472550392151, 0.9835125207901001, 0.9883121848106384, 0.9901676774024963, 0.984004557132721, 0.9818570613861084, 0.9977653622627258, 0.9805476665496826, 0.978163480758667, 0.9768324494361877, 0.9795623421669006, 0.9786681532859802, 0.9757160544395447, 0.9736850261688232, 0.9792519807815552, 0.9785882234573364, 0.9730502367019653, 0.9777137041091919, 0.9779745936393738, 0.9737584590911865, 0.9775093197822571, 0.9649128317832947, 1.0008461475372314, 0.980592668056488, 0.9762401580810547, 0.9746418595314026, 0.9756733179092407, 0.976995050907135, 0.9702650308609009, 0.9792239665985107, 0.973811686038971, 0.9823786020278931, 0.9734838008880615, 0.9940359592437744, 0.9754958748817444, 0.9780178666114807, 0.9823735356330872, 0.9681613445281982, 0.9822536110877991, 0.9720826148986816, 0.9905075430870056, 1.0014041662216187, 0.9795821905136108, 0.9830474257469177, 0.9748418927192688, 0.9768519401550293, 0.9812632203102112, 0.9765489101409912, 1.0008201599121094, 0.9944381713867188, 0.9824186563491821, 1.0035583972930908, 0.9902120232582092, 0.9754118919372559, 0.9802253842353821, 0.9913831949234009, 0.9861443638801575, 1.007180094718933, 1.0091238021850586, 0.9962112903594971, 0.9968367218971252, 1.0172690153121948, 0.9964632391929626, 1.0076111555099487, 0.9992771148681641], 'val_accuracy': [0.5161637663841248, 0.517241358757019, 0.517241358757019, 0.517241358757019, 0.517241358757019, 0.5183189511299133, 0.5193965435028076, 0.524784505367279, 0.5355603694915771, 0.5355603694915771, 0.5549569129943848, 0.5721982717514038, 0.5635775923728943, 0.5625, 0.5625, 0.5614224076271057, 0.5657327771186829, 0.5862069129943848, 0.5840517282485962, 0.5862069129943848, 0.5840517282485962, 0.5905172228813171, 0.5818965435028076, 0.5883620977401733, 0.5926724076271057, 0.5818965435028076, 0.5808189511299133, 0.5969827771186829, 0.5743534564971924, 0.5862069129943848, 0.5883620977401733, 0.5969827771186829, 0.5969827771186829, 0.5851293206214905, 0.5818965435028076, 0.579741358757019, 0.5883620977401733, 0.5829741358757019, 0.576508641242981, 0.587284505367279, 0.5959051847457886, 0.5528017282485962, 0.5905172228813171, 0.5959051847457886, 0.587284505367279, 0.5969827771186829, 0.5926724076271057, 0.5786637663841248, 0.5980603694915771, 0.5905172228813171, 0.5915948152542114, 0.5915948152542114, 0.5894396305084229, 0.5883620977401733, 0.5926724076271057, 0.5862069129943848, 0.5959051847457886, 0.5441810488700867, 0.5851293206214905, 0.5840517282485962, 0.5775862336158752, 0.5818965435028076, 0.576508641242981, 0.5959051847457886, 0.5915948152542114, 0.5883620977401733, 0.5851293206214905, 0.5700430870056152, 0.5883620977401733, 0.5829741358757019, 0.5840517282485962, 0.5818965435028076, 0.5883620977401733, 0.5894396305084229, 0.5894396305084229, 0.5808189511299133, 0.5711206793785095, 0.587284505367279, 0.5980603694915771, 0.587284505367279, 0.576508641242981, 0.5743534564971924, 0.5915948152542114, 0.5592672228813171, 0.5840517282485962, 0.5754310488700867, 0.5754310488700867, 0.5915948152542114, 0.5862069129943848, 0.587284505367279, 0.5915948152542114, 0.5775862336158752, 0.5743534564971924, 0.576508641242981, 0.5959051847457886, 0.576508641242981, 0.5840517282485962, 0.587284505367279, 0.5948275923728943, 0.6012930870056152]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","25/28 [=========================>....] - ETA: 0s - loss: 1.0401 - accuracy: 0.5941"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 9s 59ms/step - loss: 1.0397 - accuracy: 0.5928 - val_loss: 1.0825 - val_accuracy: 0.5045\n","Epoch 2/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0263 - accuracy: 0.6112 - val_loss: 1.0806 - val_accuracy: 0.5045\n","Epoch 3/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.0230 - accuracy: 0.6058 - val_loss: 1.0781 - val_accuracy: 0.5045\n","Epoch 4/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0189 - accuracy: 0.6118 - val_loss: 1.0743 - val_accuracy: 0.5045\n","Epoch 5/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0137 - accuracy: 0.6112 - val_loss: 1.0690 - val_accuracy: 0.5045\n","Epoch 6/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0078 - accuracy: 0.6160 - val_loss: 1.0652 - val_accuracy: 0.5045\n","Epoch 7/100\n","28/28 [==============================] - 1s 23ms/step - loss: 1.0032 - accuracy: 0.6149 - val_loss: 1.0593 - val_accuracy: 0.5090\n","Epoch 8/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.0001 - accuracy: 0.6234 - val_loss: 1.0534 - val_accuracy: 0.5181\n","Epoch 9/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9938 - accuracy: 0.6279 - val_loss: 1.0560 - val_accuracy: 0.5079\n","Epoch 10/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9903 - accuracy: 0.6146 - val_loss: 1.0547 - val_accuracy: 0.5045\n","Epoch 11/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.9867 - accuracy: 0.6222 - val_loss: 1.0455 - val_accuracy: 0.5192\n","Epoch 12/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.9813 - accuracy: 0.6290 - val_loss: 1.0429 - val_accuracy: 0.5226\n","Epoch 13/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.9784 - accuracy: 0.6333 - val_loss: 1.0327 - val_accuracy: 0.5396\n","Epoch 14/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9742 - accuracy: 0.6205 - val_loss: 1.0314 - val_accuracy: 0.5317\n","Epoch 15/100\n","28/28 [==============================] - 1s 32ms/step - loss: 0.9678 - accuracy: 0.6234 - val_loss: 1.0199 - val_accuracy: 0.5543\n","Epoch 16/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.9683 - accuracy: 0.6180 - val_loss: 1.0178 - val_accuracy: 0.5543\n","Epoch 17/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.9576 - accuracy: 0.6324 - val_loss: 1.0146 - val_accuracy: 0.5532\n","Epoch 18/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.9575 - accuracy: 0.6265 - val_loss: 1.0163 - val_accuracy: 0.5407\n","Epoch 19/100\n","28/28 [==============================] - 1s 31ms/step - loss: 0.9539 - accuracy: 0.6254 - val_loss: 1.0007 - val_accuracy: 0.5905\n","Epoch 20/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.9472 - accuracy: 0.6313 - val_loss: 1.0008 - val_accuracy: 0.5724\n","Epoch 21/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.9400 - accuracy: 0.6341 - val_loss: 0.9972 - val_accuracy: 0.5735\n","Epoch 22/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9389 - accuracy: 0.6285 - val_loss: 0.9967 - val_accuracy: 0.5713\n","Epoch 23/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9347 - accuracy: 0.6392 - val_loss: 0.9934 - val_accuracy: 0.5837\n","Epoch 24/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9340 - accuracy: 0.6375 - val_loss: 0.9914 - val_accuracy: 0.5871\n","Epoch 25/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.9285 - accuracy: 0.6344 - val_loss: 0.9873 - val_accuracy: 0.5928\n","Epoch 26/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.9257 - accuracy: 0.6355 - val_loss: 0.9860 - val_accuracy: 0.5939\n","Epoch 27/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9201 - accuracy: 0.6426 - val_loss: 0.9923 - val_accuracy: 0.5882\n","Epoch 28/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9254 - accuracy: 0.6321 - val_loss: 0.9822 - val_accuracy: 0.5837\n","Epoch 29/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9142 - accuracy: 0.6452 - val_loss: 0.9833 - val_accuracy: 0.5882\n","Epoch 30/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9101 - accuracy: 0.6389 - val_loss: 0.9873 - val_accuracy: 0.5724\n","Epoch 31/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9057 - accuracy: 0.6370 - val_loss: 0.9843 - val_accuracy: 0.5837\n","Epoch 32/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9037 - accuracy: 0.6503 - val_loss: 0.9815 - val_accuracy: 0.5837\n","Epoch 33/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8960 - accuracy: 0.6452 - val_loss: 0.9842 - val_accuracy: 0.5871\n","Epoch 34/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8947 - accuracy: 0.6505 - val_loss: 0.9835 - val_accuracy: 0.5860\n","Epoch 35/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.8921 - accuracy: 0.6491 - val_loss: 0.9780 - val_accuracy: 0.5973\n","Epoch 36/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8843 - accuracy: 0.6559 - val_loss: 0.9798 - val_accuracy: 0.5769\n","Epoch 37/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8848 - accuracy: 0.6559 - val_loss: 0.9829 - val_accuracy: 0.5758\n","Epoch 38/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8828 - accuracy: 0.6520 - val_loss: 0.9812 - val_accuracy: 0.5826\n","Epoch 39/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8748 - accuracy: 0.6559 - val_loss: 0.9774 - val_accuracy: 0.5701\n","Epoch 40/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8731 - accuracy: 0.6539 - val_loss: 0.9901 - val_accuracy: 0.5928\n","Epoch 41/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8737 - accuracy: 0.6488 - val_loss: 0.9868 - val_accuracy: 0.5826\n","Epoch 42/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8712 - accuracy: 0.6494 - val_loss: 0.9823 - val_accuracy: 0.5826\n","Epoch 43/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.8603 - accuracy: 0.6703 - val_loss: 0.9868 - val_accuracy: 0.5758\n","Epoch 44/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.8591 - accuracy: 0.6559 - val_loss: 0.9907 - val_accuracy: 0.5622\n","Epoch 45/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.8588 - accuracy: 0.6613 - val_loss: 0.9924 - val_accuracy: 0.5950\n","Epoch 46/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.8557 - accuracy: 0.6653 - val_loss: 0.9803 - val_accuracy: 0.5928\n","Epoch 47/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.8543 - accuracy: 0.6604 - val_loss: 0.9909 - val_accuracy: 0.5554\n","Epoch 48/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.8512 - accuracy: 0.6633 - val_loss: 0.9855 - val_accuracy: 0.5747\n","Epoch 49/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.8462 - accuracy: 0.6582 - val_loss: 0.9848 - val_accuracy: 0.5747\n","Epoch 50/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.8418 - accuracy: 0.6621 - val_loss: 0.9922 - val_accuracy: 0.5803\n","Epoch 51/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8411 - accuracy: 0.6610 - val_loss: 0.9873 - val_accuracy: 0.5792\n","Epoch 52/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8342 - accuracy: 0.6720 - val_loss: 0.9811 - val_accuracy: 0.5781\n","Epoch 53/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8343 - accuracy: 0.6562 - val_loss: 0.9862 - val_accuracy: 0.5826\n","Epoch 54/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8292 - accuracy: 0.6650 - val_loss: 0.9848 - val_accuracy: 0.5747\n","Epoch 55/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8249 - accuracy: 0.6689 - val_loss: 0.9844 - val_accuracy: 0.5747\n","Epoch 56/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8193 - accuracy: 0.6800 - val_loss: 0.9846 - val_accuracy: 0.5713\n","Epoch 57/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8216 - accuracy: 0.6703 - val_loss: 0.9854 - val_accuracy: 0.5781\n","Epoch 58/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8206 - accuracy: 0.6650 - val_loss: 0.9904 - val_accuracy: 0.5837\n","Epoch 59/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8151 - accuracy: 0.6596 - val_loss: 0.9931 - val_accuracy: 0.5769\n","Epoch 60/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8068 - accuracy: 0.6737 - val_loss: 0.9901 - val_accuracy: 0.5826\n","Epoch 61/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8109 - accuracy: 0.6720 - val_loss: 0.9877 - val_accuracy: 0.5792\n","Epoch 62/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8018 - accuracy: 0.6834 - val_loss: 1.0073 - val_accuracy: 0.5826\n","Epoch 63/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8000 - accuracy: 0.6777 - val_loss: 1.0016 - val_accuracy: 0.5814\n","Epoch 64/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7976 - accuracy: 0.6814 - val_loss: 1.0051 - val_accuracy: 0.5633\n","Epoch 65/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8062 - accuracy: 0.6723 - val_loss: 0.9965 - val_accuracy: 0.5747\n","Epoch 66/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7985 - accuracy: 0.6740 - val_loss: 0.9869 - val_accuracy: 0.5769\n","Epoch 67/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7934 - accuracy: 0.6797 - val_loss: 1.0062 - val_accuracy: 0.5860\n","Epoch 68/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7886 - accuracy: 0.6870 - val_loss: 0.9985 - val_accuracy: 0.5701\n","Epoch 69/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7904 - accuracy: 0.6811 - val_loss: 1.0092 - val_accuracy: 0.5747\n","Epoch 70/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7893 - accuracy: 0.6800 - val_loss: 1.0143 - val_accuracy: 0.5769\n","Epoch 71/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7832 - accuracy: 0.6851 - val_loss: 1.0052 - val_accuracy: 0.5724\n","Epoch 72/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7765 - accuracy: 0.6899 - val_loss: 0.9940 - val_accuracy: 0.5701\n","Epoch 73/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7797 - accuracy: 0.6876 - val_loss: 1.0175 - val_accuracy: 0.5826\n","Epoch 74/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7750 - accuracy: 0.6924 - val_loss: 1.0025 - val_accuracy: 0.5781\n","Epoch 75/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7716 - accuracy: 0.6845 - val_loss: 1.0041 - val_accuracy: 0.5735\n","Epoch 76/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7673 - accuracy: 0.6910 - val_loss: 0.9965 - val_accuracy: 0.5792\n","Epoch 77/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7691 - accuracy: 0.6848 - val_loss: 1.0343 - val_accuracy: 0.5532\n","Epoch 78/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7638 - accuracy: 0.6975 - val_loss: 0.9999 - val_accuracy: 0.5837\n","Epoch 79/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7594 - accuracy: 0.6986 - val_loss: 0.9985 - val_accuracy: 0.5814\n","Epoch 80/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7565 - accuracy: 0.7054 - val_loss: 1.0079 - val_accuracy: 0.5826\n","Epoch 81/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7548 - accuracy: 0.6930 - val_loss: 1.0444 - val_accuracy: 0.5509\n","Epoch 82/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7568 - accuracy: 0.7060 - val_loss: 1.0240 - val_accuracy: 0.5588\n","Epoch 83/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7496 - accuracy: 0.6986 - val_loss: 1.0223 - val_accuracy: 0.5611\n","Epoch 84/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7405 - accuracy: 0.7023 - val_loss: 1.0067 - val_accuracy: 0.5747\n","Epoch 85/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7439 - accuracy: 0.7018 - val_loss: 1.0077 - val_accuracy: 0.5792\n","Epoch 86/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7444 - accuracy: 0.7100 - val_loss: 1.0059 - val_accuracy: 0.5769\n","Epoch 87/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7539 - accuracy: 0.6930 - val_loss: 1.0305 - val_accuracy: 0.5860\n","Epoch 88/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7453 - accuracy: 0.6924 - val_loss: 1.0079 - val_accuracy: 0.5826\n","Epoch 89/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7341 - accuracy: 0.7035 - val_loss: 1.0235 - val_accuracy: 0.5611\n","Epoch 90/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7322 - accuracy: 0.7097 - val_loss: 1.0257 - val_accuracy: 0.5826\n","Epoch 91/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7341 - accuracy: 0.7057 - val_loss: 1.0290 - val_accuracy: 0.5758\n","Epoch 92/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7324 - accuracy: 0.7085 - val_loss: 1.0453 - val_accuracy: 0.5622\n","Epoch 93/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7327 - accuracy: 0.7085 - val_loss: 1.0264 - val_accuracy: 0.5713\n","Epoch 94/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7276 - accuracy: 0.7083 - val_loss: 1.0185 - val_accuracy: 0.5814\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7242 - accuracy: 0.7119 - val_loss: 1.0342 - val_accuracy: 0.5758\n","Epoch 96/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7140 - accuracy: 0.7216 - val_loss: 1.0279 - val_accuracy: 0.5747\n","Epoch 97/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7156 - accuracy: 0.7187 - val_loss: 1.0302 - val_accuracy: 0.5769\n","Epoch 98/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7217 - accuracy: 0.7074 - val_loss: 1.0426 - val_accuracy: 0.5814\n","Epoch 99/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7227 - accuracy: 0.7122 - val_loss: 1.0401 - val_accuracy: 0.5803\n","Epoch 100/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7112 - accuracy: 0.7244 - val_loss: 1.0312 - val_accuracy: 0.5735\n","{'loss': [1.0396672487258911, 1.0263335704803467, 1.0229820013046265, 1.0188851356506348, 1.0136841535568237, 1.007784128189087, 1.0032358169555664, 1.0000770092010498, 0.9937666058540344, 0.9903407692909241, 0.9867300987243652, 0.9812708497047424, 0.9784364700317383, 0.9741776585578918, 0.967792809009552, 0.968340277671814, 0.9575810432434082, 0.9574903845787048, 0.9538614749908447, 0.9471947550773621, 0.939978301525116, 0.938948929309845, 0.9347015619277954, 0.9339620471000671, 0.9285421371459961, 0.9256778359413147, 0.920078694820404, 0.9253920912742615, 0.9142071008682251, 0.9100887775421143, 0.9056835174560547, 0.9036612510681152, 0.896045982837677, 0.8946725726127625, 0.8921040892601013, 0.8843119144439697, 0.8848463296890259, 0.8827716708183289, 0.8747930526733398, 0.87308669090271, 0.873719334602356, 0.8712092638015747, 0.8603166937828064, 0.8591142892837524, 0.8587895035743713, 0.8557459115982056, 0.8542690277099609, 0.8511823415756226, 0.8462077975273132, 0.8417608141899109, 0.8410999178886414, 0.8341747522354126, 0.834338903427124, 0.8291952610015869, 0.824942946434021, 0.8192658424377441, 0.8215827345848083, 0.8206192255020142, 0.8150790929794312, 0.806796133518219, 0.8108766674995422, 0.8018099069595337, 0.799970269203186, 0.7976171374320984, 0.806165337562561, 0.7985110282897949, 0.7933919429779053, 0.7886147499084473, 0.790366530418396, 0.7893446087837219, 0.7832286357879639, 0.7765445113182068, 0.7797243595123291, 0.7750499844551086, 0.7716118097305298, 0.7673287391662598, 0.7691099047660828, 0.7637701630592346, 0.7594249844551086, 0.7565093040466309, 0.7547556757926941, 0.7568466663360596, 0.7495925426483154, 0.740478515625, 0.7439128160476685, 0.7444296479225159, 0.7539063692092896, 0.7452935576438904, 0.734067440032959, 0.732184112071991, 0.7341259717941284, 0.7324339747428894, 0.7326620221138, 0.7276227474212646, 0.7242045402526855, 0.7140118479728699, 0.7156414985656738, 0.7217336893081665, 0.7227221131324768, 0.7111630439758301], 'accuracy': [0.5928126573562622, 0.6112054586410522, 0.6058290600776672, 0.6117713451385498, 0.6112054586410522, 0.6160158514976501, 0.6148839592933655, 0.6233729720115662, 0.6279004216194153, 0.6146010160446167, 0.6222410798072815, 0.6290322542190552, 0.6332767605781555, 0.6205433011054993, 0.6233729720115662, 0.6179966330528259, 0.6324278712272644, 0.6264855861663818, 0.6253536939620972, 0.6312959790229797, 0.6341256499290466, 0.6284663081169128, 0.6392189860343933, 0.6375212073326111, 0.6344085931777954, 0.6355404853820801, 0.6426146030426025, 0.6321448683738708, 0.6451612710952759, 0.6389360427856445, 0.6369553208351135, 0.6502546668052673, 0.6451612710952759, 0.6505376100540161, 0.6491228342056274, 0.6559139490127563, 0.6559139490127563, 0.6519524455070496, 0.6559139490127563, 0.6539332270622253, 0.6488398313522339, 0.6494057774543762, 0.6703452467918396, 0.6559139490127563, 0.6612903475761414, 0.6652518510818481, 0.6604413986206055, 0.6632710695266724, 0.6581776738166809, 0.6621392369270325, 0.6610073447227478, 0.6720430254936218, 0.6561969518661499, 0.6649688482284546, 0.6689304113388062, 0.6799660325050354, 0.6703452467918396, 0.6649688482284546, 0.6595925092697144, 0.673740804195404, 0.6720430254936218, 0.6833616495132446, 0.6777023077011108, 0.6813808679580688, 0.6723259687423706, 0.6740237474441528, 0.6796830892562866, 0.6870402097702026, 0.6810979247093201, 0.6799660325050354, 0.6850594282150269, 0.6898698210716248, 0.6876060962677002, 0.6924165487289429, 0.6844934821128845, 0.6910017132759094, 0.6847764849662781, 0.6975098848342896, 0.6986417770385742, 0.7054329514503479, 0.6929824352264404, 0.7059988975524902, 0.6986417770385742, 0.7023203372955322, 0.7017543911933899, 0.709960401058197, 0.6929824352264404, 0.6924165487289429, 0.7034521698951721, 0.7096773982048035, 0.7057158946990967, 0.7085455656051636, 0.7085455656051636, 0.70826256275177, 0.711941123008728, 0.7215619683265686, 0.7187322974205017, 0.7074136734008789, 0.7122241258621216, 0.7243916392326355], 'val_loss': [1.0825122594833374, 1.0805954933166504, 1.0780915021896362, 1.0743305683135986, 1.0689905881881714, 1.0652047395706177, 1.059256672859192, 1.053425908088684, 1.0560078620910645, 1.0547122955322266, 1.0455403327941895, 1.0429432392120361, 1.0326800346374512, 1.0313889980316162, 1.0198676586151123, 1.01783287525177, 1.014567494392395, 1.0162687301635742, 1.000745415687561, 1.000835657119751, 0.9971924424171448, 0.9967445135116577, 0.9933506846427917, 0.9913666248321533, 0.9872763156890869, 0.9859668016433716, 0.9922779202461243, 0.9822224974632263, 0.9833316802978516, 0.987277090549469, 0.98434978723526, 0.9814623594284058, 0.9842442274093628, 0.9835431575775146, 0.9780225157737732, 0.9797949194908142, 0.9828929901123047, 0.9811842441558838, 0.9773702621459961, 0.9901483654975891, 0.9868229627609253, 0.9822807908058167, 0.9867855906486511, 0.9907320737838745, 0.9923829436302185, 0.980294406414032, 0.9909014701843262, 0.9855245351791382, 0.9847897291183472, 0.9921614527702332, 0.9872775077819824, 0.981088399887085, 0.9862258434295654, 0.9848139882087708, 0.9844005703926086, 0.9846038222312927, 0.9853970408439636, 0.9903609752655029, 0.9930862784385681, 0.9901378750801086, 0.9877148866653442, 1.0072927474975586, 1.0016316175460815, 1.0051194429397583, 0.9964932799339294, 0.9868640303611755, 1.006173849105835, 0.9984546303749084, 1.0092138051986694, 1.014343500137329, 1.0051836967468262, 0.9939691424369812, 1.0174907445907593, 1.0024687051773071, 1.0040721893310547, 0.9965411424636841, 1.0342540740966797, 0.9999011158943176, 0.9984838366508484, 1.0079420804977417, 1.044416904449463, 1.0240217447280884, 1.022250771522522, 1.0066970586776733, 1.0076731443405151, 1.005911946296692, 1.0305215120315552, 1.0078729391098022, 1.023526668548584, 1.0257028341293335, 1.0289767980575562, 1.0453182458877563, 1.0264055728912354, 1.0184528827667236, 1.034191370010376, 1.0279181003570557, 1.0301883220672607, 1.0426080226898193, 1.0400632619857788, 1.0311990976333618], 'val_accuracy': [0.5045248866081238, 0.5045248866081238, 0.5045248866081238, 0.5045248866081238, 0.5045248866081238, 0.5045248866081238, 0.5090497732162476, 0.5180995464324951, 0.5079185366630554, 0.5045248866081238, 0.5192307829856873, 0.5226244330406189, 0.5395927429199219, 0.5316742062568665, 0.5542986392974854, 0.5542986392974854, 0.5531674027442932, 0.540723979473114, 0.5904977321624756, 0.5723981857299805, 0.5735294222831726, 0.5712669491767883, 0.5837104320526123, 0.587104082107544, 0.5927602052688599, 0.5938913822174072, 0.5882353186607361, 0.5837104320526123, 0.5882353186607361, 0.5723981857299805, 0.5837104320526123, 0.5837104320526123, 0.587104082107544, 0.5859728455543518, 0.5972850918769836, 0.5769230723381042, 0.5757918357849121, 0.5825791954994202, 0.570135772228241, 0.5927602052688599, 0.5825791954994202, 0.5825791954994202, 0.5757918357849121, 0.5622171759605408, 0.5950226187705994, 0.5927602052688599, 0.5554298758506775, 0.5746606588363647, 0.5746606588363647, 0.5803167223930359, 0.5791855454444885, 0.5780543088912964, 0.5825791954994202, 0.5746606588363647, 0.5746606588363647, 0.5712669491767883, 0.5780543088912964, 0.5837104320526123, 0.5769230723381042, 0.5825791954994202, 0.5791855454444885, 0.5825791954994202, 0.581447958946228, 0.5633484125137329, 0.5746606588363647, 0.5769230723381042, 0.5859728455543518, 0.570135772228241, 0.5746606588363647, 0.5769230723381042, 0.5723981857299805, 0.570135772228241, 0.5825791954994202, 0.5780543088912964, 0.5735294222831726, 0.5791855454444885, 0.5531674027442932, 0.5837104320526123, 0.581447958946228, 0.5825791954994202, 0.5509049892425537, 0.5588235259056091, 0.5610859990119934, 0.5746606588363647, 0.5791855454444885, 0.5769230723381042, 0.5859728455543518, 0.5825791954994202, 0.5610859990119934, 0.5825791954994202, 0.5757918357849121, 0.5622171759605408, 0.5712669491767883, 0.581447958946228, 0.5757918357849121, 0.5746606588363647, 0.5769230723381042, 0.581447958946228, 0.5803167223930359, 0.5735294222831726]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/31 [==========================>...] - ETA: 0s - loss: 1.0423 - accuracy: 0.5924"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 8s 55ms/step - loss: 1.0411 - accuracy: 0.5969 - val_loss: 1.0819 - val_accuracy: 0.5145\n","Epoch 2/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.0303 - accuracy: 0.6165 - val_loss: 1.0775 - val_accuracy: 0.5145\n","Epoch 3/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.0245 - accuracy: 0.6083 - val_loss: 1.0742 - val_accuracy: 0.5145\n","Epoch 4/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.0190 - accuracy: 0.6196 - val_loss: 1.0677 - val_accuracy: 0.5165\n","Epoch 5/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.0140 - accuracy: 0.6116 - val_loss: 1.0656 - val_accuracy: 0.5155\n","Epoch 6/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.0079 - accuracy: 0.6178 - val_loss: 1.0595 - val_accuracy: 0.5196\n","Epoch 7/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.0058 - accuracy: 0.6119 - val_loss: 1.0570 - val_accuracy: 0.5176\n","Epoch 8/100\n","31/31 [==============================] - 1s 25ms/step - loss: 1.0007 - accuracy: 0.6233 - val_loss: 1.0523 - val_accuracy: 0.5227\n","Epoch 9/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.9969 - accuracy: 0.6119 - val_loss: 1.0439 - val_accuracy: 0.5372\n","Epoch 10/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9893 - accuracy: 0.6235 - val_loss: 1.0405 - val_accuracy: 0.5372\n","Epoch 11/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9855 - accuracy: 0.6189 - val_loss: 1.0375 - val_accuracy: 0.5331\n","Epoch 12/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.9810 - accuracy: 0.6233 - val_loss: 1.0341 - val_accuracy: 0.5382\n","Epoch 13/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.9766 - accuracy: 0.6269 - val_loss: 1.0271 - val_accuracy: 0.5444\n","Epoch 14/100\n","31/31 [==============================] - 1s 31ms/step - loss: 0.9721 - accuracy: 0.6266 - val_loss: 1.0234 - val_accuracy: 0.5496\n","Epoch 15/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.9693 - accuracy: 0.6212 - val_loss: 1.0194 - val_accuracy: 0.5486\n","Epoch 16/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.9614 - accuracy: 0.6359 - val_loss: 1.0156 - val_accuracy: 0.5496\n","Epoch 17/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.9601 - accuracy: 0.6225 - val_loss: 1.0155 - val_accuracy: 0.5465\n","Epoch 18/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.9566 - accuracy: 0.6287 - val_loss: 1.0122 - val_accuracy: 0.5630\n","Epoch 19/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9523 - accuracy: 0.6276 - val_loss: 1.0108 - val_accuracy: 0.5579\n","Epoch 20/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.9472 - accuracy: 0.6295 - val_loss: 1.0049 - val_accuracy: 0.5651\n","Epoch 21/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9431 - accuracy: 0.6292 - val_loss: 1.0186 - val_accuracy: 0.5548\n","Epoch 22/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9386 - accuracy: 0.6279 - val_loss: 1.0037 - val_accuracy: 0.5506\n","Epoch 23/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.9370 - accuracy: 0.6328 - val_loss: 0.9996 - val_accuracy: 0.5568\n","Epoch 24/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9299 - accuracy: 0.6398 - val_loss: 1.0107 - val_accuracy: 0.5599\n","Epoch 25/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9257 - accuracy: 0.6393 - val_loss: 1.0152 - val_accuracy: 0.5475\n","Epoch 26/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.9244 - accuracy: 0.6313 - val_loss: 1.0096 - val_accuracy: 0.5640\n","Epoch 27/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9215 - accuracy: 0.6349 - val_loss: 0.9970 - val_accuracy: 0.5610\n","Epoch 28/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9150 - accuracy: 0.6380 - val_loss: 1.0017 - val_accuracy: 0.5568\n","Epoch 29/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9112 - accuracy: 0.6398 - val_loss: 0.9963 - val_accuracy: 0.5610\n","Epoch 30/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.9080 - accuracy: 0.6486 - val_loss: 1.0147 - val_accuracy: 0.5475\n","Epoch 31/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.9030 - accuracy: 0.6444 - val_loss: 0.9918 - val_accuracy: 0.5599\n","Epoch 32/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8966 - accuracy: 0.6442 - val_loss: 1.0117 - val_accuracy: 0.5599\n","Epoch 33/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8984 - accuracy: 0.6333 - val_loss: 0.9993 - val_accuracy: 0.5548\n","Epoch 34/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8925 - accuracy: 0.6457 - val_loss: 1.0281 - val_accuracy: 0.5517\n","Epoch 35/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8911 - accuracy: 0.6382 - val_loss: 0.9925 - val_accuracy: 0.5527\n","Epoch 36/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8856 - accuracy: 0.6401 - val_loss: 0.9980 - val_accuracy: 0.5599\n","Epoch 37/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.8815 - accuracy: 0.6537 - val_loss: 0.9939 - val_accuracy: 0.5579\n","Epoch 38/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.8802 - accuracy: 0.6388 - val_loss: 0.9811 - val_accuracy: 0.5527\n","Epoch 39/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.8760 - accuracy: 0.6506 - val_loss: 1.0195 - val_accuracy: 0.5506\n","Epoch 40/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.8736 - accuracy: 0.6380 - val_loss: 0.9904 - val_accuracy: 0.5558\n","Epoch 41/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.8704 - accuracy: 0.6494 - val_loss: 1.0008 - val_accuracy: 0.5537\n","Epoch 42/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.8654 - accuracy: 0.6584 - val_loss: 0.9768 - val_accuracy: 0.5537\n","Epoch 43/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.8629 - accuracy: 0.6525 - val_loss: 0.9896 - val_accuracy: 0.5640\n","Epoch 44/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.8587 - accuracy: 0.6499 - val_loss: 1.0001 - val_accuracy: 0.5434\n","Epoch 45/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.8533 - accuracy: 0.6532 - val_loss: 0.9696 - val_accuracy: 0.5475\n","Epoch 46/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8515 - accuracy: 0.6576 - val_loss: 1.0030 - val_accuracy: 0.5548\n","Epoch 47/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8501 - accuracy: 0.6636 - val_loss: 1.0074 - val_accuracy: 0.5506\n","Epoch 48/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8467 - accuracy: 0.6584 - val_loss: 0.9993 - val_accuracy: 0.5589\n","Epoch 49/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8450 - accuracy: 0.6576 - val_loss: 0.9786 - val_accuracy: 0.5444\n","Epoch 50/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8389 - accuracy: 0.6589 - val_loss: 0.9942 - val_accuracy: 0.5548\n","Epoch 51/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8376 - accuracy: 0.6566 - val_loss: 0.9756 - val_accuracy: 0.5527\n","Epoch 52/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8330 - accuracy: 0.6574 - val_loss: 0.9728 - val_accuracy: 0.5465\n","Epoch 53/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8296 - accuracy: 0.6667 - val_loss: 0.9792 - val_accuracy: 0.5517\n","Epoch 54/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8233 - accuracy: 0.6736 - val_loss: 0.9872 - val_accuracy: 0.5496\n","Epoch 55/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8239 - accuracy: 0.6597 - val_loss: 0.9884 - val_accuracy: 0.5558\n","Epoch 56/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8217 - accuracy: 0.6669 - val_loss: 0.9781 - val_accuracy: 0.5527\n","Epoch 57/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8188 - accuracy: 0.6651 - val_loss: 0.9804 - val_accuracy: 0.5465\n","Epoch 58/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8152 - accuracy: 0.6669 - val_loss: 0.9788 - val_accuracy: 0.5486\n","Epoch 59/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8127 - accuracy: 0.6659 - val_loss: 0.9887 - val_accuracy: 0.5589\n","Epoch 60/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8059 - accuracy: 0.6716 - val_loss: 0.9907 - val_accuracy: 0.5568\n","Epoch 61/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8029 - accuracy: 0.6739 - val_loss: 0.9756 - val_accuracy: 0.5548\n","Epoch 62/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8078 - accuracy: 0.6778 - val_loss: 0.9507 - val_accuracy: 0.5558\n","Epoch 63/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8030 - accuracy: 0.6724 - val_loss: 0.9830 - val_accuracy: 0.5506\n","Epoch 64/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8045 - accuracy: 0.6685 - val_loss: 0.9696 - val_accuracy: 0.5455\n","Epoch 65/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7908 - accuracy: 0.6791 - val_loss: 0.9787 - val_accuracy: 0.5465\n","Epoch 66/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7916 - accuracy: 0.6744 - val_loss: 0.9746 - val_accuracy: 0.5496\n","Epoch 67/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7965 - accuracy: 0.6713 - val_loss: 1.0019 - val_accuracy: 0.5599\n","Epoch 68/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7867 - accuracy: 0.6814 - val_loss: 0.9789 - val_accuracy: 0.5496\n","Epoch 69/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7797 - accuracy: 0.6780 - val_loss: 0.9578 - val_accuracy: 0.5413\n","Epoch 70/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7844 - accuracy: 0.6690 - val_loss: 0.9548 - val_accuracy: 0.5548\n","Epoch 71/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7864 - accuracy: 0.6685 - val_loss: 0.9760 - val_accuracy: 0.5496\n","Epoch 72/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7807 - accuracy: 0.6747 - val_loss: 0.9731 - val_accuracy: 0.5486\n","Epoch 73/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7739 - accuracy: 0.6842 - val_loss: 0.9594 - val_accuracy: 0.5393\n","Epoch 74/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7735 - accuracy: 0.6788 - val_loss: 1.0118 - val_accuracy: 0.5548\n","Epoch 75/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7692 - accuracy: 0.6788 - val_loss: 0.9862 - val_accuracy: 0.5589\n","Epoch 76/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7762 - accuracy: 0.6736 - val_loss: 0.9667 - val_accuracy: 0.5486\n","Epoch 77/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7663 - accuracy: 0.6868 - val_loss: 0.9647 - val_accuracy: 0.5475\n","Epoch 78/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7598 - accuracy: 0.6824 - val_loss: 0.9479 - val_accuracy: 0.5496\n","Epoch 79/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7602 - accuracy: 0.6840 - val_loss: 0.9718 - val_accuracy: 0.5568\n","Epoch 80/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7549 - accuracy: 0.6891 - val_loss: 0.9502 - val_accuracy: 0.5413\n","Epoch 81/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7542 - accuracy: 0.6817 - val_loss: 0.9517 - val_accuracy: 0.5475\n","Epoch 82/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7482 - accuracy: 0.6899 - val_loss: 1.0260 - val_accuracy: 0.5362\n","Epoch 83/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7519 - accuracy: 0.6811 - val_loss: 0.9884 - val_accuracy: 0.5496\n","Epoch 84/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7433 - accuracy: 0.6990 - val_loss: 0.9764 - val_accuracy: 0.5496\n","Epoch 85/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7469 - accuracy: 0.6801 - val_loss: 1.0077 - val_accuracy: 0.5568\n","Epoch 86/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7551 - accuracy: 0.6868 - val_loss: 0.9818 - val_accuracy: 0.5362\n","Epoch 87/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7345 - accuracy: 0.6966 - val_loss: 1.0026 - val_accuracy: 0.5506\n","Epoch 88/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7446 - accuracy: 0.6827 - val_loss: 1.0438 - val_accuracy: 0.5599\n","Epoch 89/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7392 - accuracy: 0.6809 - val_loss: 0.9757 - val_accuracy: 0.5444\n","Epoch 90/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7325 - accuracy: 0.6948 - val_loss: 0.9674 - val_accuracy: 0.5413\n","Epoch 91/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7258 - accuracy: 0.7034 - val_loss: 0.9672 - val_accuracy: 0.5506\n","Epoch 92/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7335 - accuracy: 0.6982 - val_loss: 1.0111 - val_accuracy: 0.5537\n","Epoch 93/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7315 - accuracy: 0.6920 - val_loss: 1.0935 - val_accuracy: 0.5320\n","Epoch 94/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7267 - accuracy: 0.6977 - val_loss: 0.9691 - val_accuracy: 0.5434\n","Epoch 95/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7240 - accuracy: 0.6959 - val_loss: 1.0021 - val_accuracy: 0.5496\n","Epoch 96/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7210 - accuracy: 0.7057 - val_loss: 1.0059 - val_accuracy: 0.5506\n","Epoch 97/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7140 - accuracy: 0.7150 - val_loss: 1.0258 - val_accuracy: 0.5579\n","Epoch 98/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7205 - accuracy: 0.6907 - val_loss: 1.0883 - val_accuracy: 0.5403\n","Epoch 99/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7220 - accuracy: 0.6868 - val_loss: 1.0032 - val_accuracy: 0.5403\n","Epoch 100/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7154 - accuracy: 0.6997 - val_loss: 0.9609 - val_accuracy: 0.5506\n","{'loss': [1.041082739830017, 1.030348777770996, 1.0245262384414673, 1.019046664237976, 1.0140135288238525, 1.0078635215759277, 1.0057651996612549, 1.0007156133651733, 0.9969058036804199, 0.9893079400062561, 0.9855303168296814, 0.9810269474983215, 0.976612389087677, 0.9721193313598633, 0.9692510366439819, 0.9613717794418335, 0.9600948095321655, 0.9565914869308472, 0.9523037672042847, 0.9472452402114868, 0.9431389570236206, 0.9385948181152344, 0.93702232837677, 0.9298847913742065, 0.9256757497787476, 0.9243764877319336, 0.9215485453605652, 0.9149739146232605, 0.9111716747283936, 0.9080246090888977, 0.9029761552810669, 0.8966459631919861, 0.8983674049377441, 0.8924576044082642, 0.8911206126213074, 0.8855596780776978, 0.8815336227416992, 0.8801699280738831, 0.8760399222373962, 0.8736437559127808, 0.8704205751419067, 0.8654457926750183, 0.86288982629776, 0.858653724193573, 0.8533332347869873, 0.8514929413795471, 0.8500930666923523, 0.8467435240745544, 0.8449660539627075, 0.8388586640357971, 0.8376153111457825, 0.8329538106918335, 0.8296015858650208, 0.8232787251472473, 0.8238580226898193, 0.821744978427887, 0.8188439607620239, 0.8152174949645996, 0.8127203583717346, 0.8059157133102417, 0.8028961420059204, 0.8077542781829834, 0.8029929399490356, 0.8044562935829163, 0.7907999157905579, 0.7916363477706909, 0.7965000867843628, 0.7866539359092712, 0.779749870300293, 0.7843710780143738, 0.7864254117012024, 0.7807376980781555, 0.7739410996437073, 0.7734552621841431, 0.7691641449928284, 0.776215136051178, 0.7663372159004211, 0.7597846388816833, 0.7601683139801025, 0.754903256893158, 0.754183292388916, 0.7482249736785889, 0.7518612742424011, 0.7433051466941833, 0.7468529343605042, 0.7551094889640808, 0.7345195412635803, 0.7445501685142517, 0.7391958832740784, 0.7325002551078796, 0.725758969783783, 0.7335472106933594, 0.7315481305122375, 0.7267151474952698, 0.7239719033241272, 0.7210361957550049, 0.7139765620231628, 0.720496416091919, 0.7219765186309814, 0.7153851985931396], 'accuracy': [0.5968992114067078, 0.6165374517440796, 0.6082687377929688, 0.6196382641792297, 0.6116279363632202, 0.617829442024231, 0.6118863224983215, 0.6232557892799377, 0.6118863224983215, 0.6235142350196838, 0.618863046169281, 0.6232557892799377, 0.6268733739852905, 0.6266149878501892, 0.6211886405944824, 0.6359173059463501, 0.6224806308746338, 0.6286821961402893, 0.6276485919952393, 0.6294573545455933, 0.6291989684104919, 0.6279069781303406, 0.6328165531158447, 0.6397932767868042, 0.6392765045166016, 0.631266176700592, 0.6348837018013, 0.6379845142364502, 0.6397932767868042, 0.6485788226127625, 0.644444465637207, 0.6441860198974609, 0.6333333253860474, 0.6457364559173584, 0.6382429003715515, 0.6400516629219055, 0.6537467837333679, 0.6387596726417542, 0.6506459712982178, 0.6379845142364502, 0.6493539810180664, 0.658397912979126, 0.6524547934532166, 0.6498708128929138, 0.6532299518585205, 0.657622754573822, 0.6635658740997314, 0.658397912979126, 0.657622754573822, 0.6589147448539734, 0.656589150428772, 0.6573643684387207, 0.6666666865348816, 0.6736434102058411, 0.6596899032592773, 0.6669250726699829, 0.6651162505149841, 0.6669250726699829, 0.6658914685249329, 0.671576201915741, 0.6739017963409424, 0.6777777671813965, 0.6723514199256897, 0.6684754490852356, 0.6790697574615479, 0.6744186282157898, 0.6713178157806396, 0.6813953518867493, 0.6780361533164978, 0.6689922213554382, 0.6684754490852356, 0.6746770143508911, 0.6842377185821533, 0.6788113713264465, 0.6788113713264465, 0.6736434102058411, 0.686821699142456, 0.6824289560317993, 0.683979332447052, 0.6891472935676575, 0.6816537380218506, 0.6899224519729614, 0.681136965751648, 0.698966383934021, 0.6801033616065979, 0.686821699142456, 0.6966408491134644, 0.6826873421669006, 0.6808785796165466, 0.6948320269584656, 0.7033591866493225, 0.698191225528717, 0.6919896602630615, 0.6976743936538696, 0.6958656311035156, 0.7056847810745239, 0.7149870991706848, 0.6906976699829102, 0.686821699142456, 0.6997416019439697], 'val_loss': [1.081893801689148, 1.077543020248413, 1.0741921663284302, 1.0676720142364502, 1.0655734539031982, 1.059451937675476, 1.0570224523544312, 1.052324891090393, 1.043864130973816, 1.0404837131500244, 1.0375486612319946, 1.0341103076934814, 1.027081847190857, 1.0233843326568604, 1.019388198852539, 1.0155558586120605, 1.015466570854187, 1.0122497081756592, 1.0108182430267334, 1.0049183368682861, 1.0186245441436768, 1.0037436485290527, 0.9996415376663208, 1.01071035861969, 1.0151772499084473, 1.009602665901184, 0.9970433115959167, 1.0017186403274536, 0.996293306350708, 1.0146890878677368, 0.9918050169944763, 1.0117191076278687, 0.9993321299552917, 1.0280847549438477, 0.9924576878547668, 0.9980205297470093, 0.9938506484031677, 0.981113612651825, 1.019478678703308, 0.9903613924980164, 1.00081205368042, 0.9768078327178955, 0.9895775318145752, 1.0001457929611206, 0.969552755355835, 1.0029507875442505, 1.0073562860488892, 0.9993221759796143, 0.9786463379859924, 0.9942048192024231, 0.9756041169166565, 0.9728057980537415, 0.9791896939277649, 0.9872222542762756, 0.9883724451065063, 0.9780839681625366, 0.9804208278656006, 0.9788312315940857, 0.9887242317199707, 0.9906840324401855, 0.97557133436203, 0.950680136680603, 0.9829739928245544, 0.9696210026741028, 0.9786705374717712, 0.9746445417404175, 1.0018638372421265, 0.9788988828659058, 0.95782470703125, 0.9547796249389648, 0.9759870171546936, 0.9730621576309204, 0.9594227075576782, 1.0117818117141724, 0.9861801266670227, 0.966677188873291, 0.9646714925765991, 0.9478834271430969, 0.9718354940414429, 0.9502326250076294, 0.9516807794570923, 1.0259652137756348, 0.9883919358253479, 0.9763613343238831, 1.0077173709869385, 0.9818459153175354, 1.0026311874389648, 1.043763518333435, 0.9757383465766907, 0.9673816561698914, 0.9671610593795776, 1.0110976696014404, 1.0934544801712036, 0.9691367149353027, 1.0020869970321655, 1.005927562713623, 1.0257902145385742, 1.0883328914642334, 1.0031726360321045, 0.9608603119850159], 'val_accuracy': [0.5144628286361694, 0.5144628286361694, 0.5144628286361694, 0.5165289044380188, 0.5154958963394165, 0.51962810754776, 0.5175619721412659, 0.5227272510528564, 0.5371900796890259, 0.5371900796890259, 0.5330578684806824, 0.538223147392273, 0.5444214940071106, 0.5495867729187012, 0.5485537052154541, 0.5495867729187012, 0.5464876294136047, 0.5630165338516235, 0.557851254940033, 0.5650826692581177, 0.5547520518302917, 0.5506198406219482, 0.5568181872367859, 0.5599173307418823, 0.547520637512207, 0.5640496015548706, 0.5609503984451294, 0.5568181872367859, 0.5609503984451294, 0.547520637512207, 0.5599173307418823, 0.5599173307418823, 0.5547520518302917, 0.5516529083251953, 0.5526859760284424, 0.5599173307418823, 0.557851254940033, 0.5526859760284424, 0.5506198406219482, 0.5557851195335388, 0.5537189841270447, 0.5537189841270447, 0.5640496015548706, 0.5433884263038635, 0.547520637512207, 0.5547520518302917, 0.5506198406219482, 0.55888432264328, 0.5444214940071106, 0.5547520518302917, 0.5526859760284424, 0.5464876294136047, 0.5516529083251953, 0.5495867729187012, 0.5557851195335388, 0.5526859760284424, 0.5464876294136047, 0.5485537052154541, 0.55888432264328, 0.5568181872367859, 0.5547520518302917, 0.5557851195335388, 0.5506198406219482, 0.5454545617103577, 0.5464876294136047, 0.5495867729187012, 0.5599173307418823, 0.5495867729187012, 0.5413222908973694, 0.5547520518302917, 0.5495867729187012, 0.5485537052154541, 0.53925621509552, 0.5547520518302917, 0.55888432264328, 0.5485537052154541, 0.547520637512207, 0.5495867729187012, 0.5568181872367859, 0.5413222908973694, 0.547520637512207, 0.5361570119857788, 0.5495867729187012, 0.5495867729187012, 0.5568181872367859, 0.5361570119857788, 0.5506198406219482, 0.5599173307418823, 0.5444214940071106, 0.5413222908973694, 0.5506198406219482, 0.5537189841270447, 0.5320248007774353, 0.5433884263038635, 0.5495867729187012, 0.5506198406219482, 0.557851254940033, 0.5402892827987671, 0.5402892827987671, 0.5506198406219482]}\n","32/32 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - ETA: 0s - loss: 0.7649 - accuracy: 0.6711"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 8s 57ms/step - loss: 0.7649 - accuracy: 0.6711 - val_loss: 0.8820 - val_accuracy: 0.5172\n","Epoch 2/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7509 - accuracy: 0.6740 - val_loss: 0.8831 - val_accuracy: 0.5172\n","Epoch 3/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7419 - accuracy: 0.6905 - val_loss: 0.8846 - val_accuracy: 0.5172\n","Epoch 4/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.7442 - accuracy: 0.6773 - val_loss: 0.8813 - val_accuracy: 0.5183\n","Epoch 5/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.7322 - accuracy: 0.6926 - val_loss: 0.8692 - val_accuracy: 0.5366\n","Epoch 6/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.7355 - accuracy: 0.6905 - val_loss: 0.8678 - val_accuracy: 0.5399\n","Epoch 7/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7315 - accuracy: 0.6853 - val_loss: 0.8818 - val_accuracy: 0.5280\n","Epoch 8/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7330 - accuracy: 0.6940 - val_loss: 0.8902 - val_accuracy: 0.5269\n","Epoch 9/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7248 - accuracy: 0.6932 - val_loss: 0.8663 - val_accuracy: 0.5377\n","Epoch 10/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7266 - accuracy: 0.6956 - val_loss: 0.8732 - val_accuracy: 0.5399\n","Epoch 11/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.7288 - accuracy: 0.6940 - val_loss: 0.8717 - val_accuracy: 0.5431\n","Epoch 12/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.7265 - accuracy: 0.6918 - val_loss: 0.8484 - val_accuracy: 0.6067\n","Epoch 13/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7178 - accuracy: 0.7034 - val_loss: 0.8463 - val_accuracy: 0.6024\n","Epoch 14/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7161 - accuracy: 0.6902 - val_loss: 0.8520 - val_accuracy: 0.5841\n","Epoch 15/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.7244 - accuracy: 0.6867 - val_loss: 0.8388 - val_accuracy: 0.6002\n","Epoch 16/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.7251 - accuracy: 0.6932 - val_loss: 0.8374 - val_accuracy: 0.6164\n","Epoch 17/100\n","29/29 [==============================] - 1s 29ms/step - loss: 0.7075 - accuracy: 0.6977 - val_loss: 0.8500 - val_accuracy: 0.6304\n","Epoch 18/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7106 - accuracy: 0.7045 - val_loss: 0.8409 - val_accuracy: 0.6175\n","Epoch 19/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.7082 - accuracy: 0.7002 - val_loss: 0.8496 - val_accuracy: 0.6261\n","Epoch 20/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.6990 - accuracy: 0.7061 - val_loss: 0.8459 - val_accuracy: 0.6390\n","Epoch 21/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6960 - accuracy: 0.7123 - val_loss: 0.8400 - val_accuracy: 0.6185\n","Epoch 22/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6990 - accuracy: 0.7096 - val_loss: 0.8549 - val_accuracy: 0.5938\n","Epoch 23/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6969 - accuracy: 0.7066 - val_loss: 0.8567 - val_accuracy: 0.6293\n","Epoch 24/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7051 - accuracy: 0.6999 - val_loss: 0.8646 - val_accuracy: 0.6142\n","Epoch 25/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6914 - accuracy: 0.7117 - val_loss: 0.8614 - val_accuracy: 0.6379\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6917 - accuracy: 0.7091 - val_loss: 0.8614 - val_accuracy: 0.6390\n","Epoch 27/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6789 - accuracy: 0.7155 - val_loss: 0.8781 - val_accuracy: 0.6078\n","Epoch 28/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6938 - accuracy: 0.7080 - val_loss: 0.8560 - val_accuracy: 0.6369\n","Epoch 29/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6789 - accuracy: 0.7096 - val_loss: 0.8638 - val_accuracy: 0.6336\n","Epoch 30/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6797 - accuracy: 0.7239 - val_loss: 0.8781 - val_accuracy: 0.6336\n","Epoch 31/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.6749 - accuracy: 0.7201 - val_loss: 0.8756 - val_accuracy: 0.6444\n","Epoch 32/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6820 - accuracy: 0.7231 - val_loss: 0.9053 - val_accuracy: 0.5991\n","Epoch 33/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6772 - accuracy: 0.7196 - val_loss: 0.8933 - val_accuracy: 0.6088\n","Epoch 34/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.6735 - accuracy: 0.7166 - val_loss: 0.8754 - val_accuracy: 0.6476\n","Epoch 35/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6665 - accuracy: 0.7293 - val_loss: 0.8932 - val_accuracy: 0.6250\n","Epoch 36/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6685 - accuracy: 0.7214 - val_loss: 0.9118 - val_accuracy: 0.5991\n","Epoch 37/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6740 - accuracy: 0.7214 - val_loss: 0.8985 - val_accuracy: 0.6067\n","Epoch 38/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6585 - accuracy: 0.7301 - val_loss: 0.8908 - val_accuracy: 0.6422\n","Epoch 39/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6687 - accuracy: 0.7266 - val_loss: 0.8980 - val_accuracy: 0.6455\n","Epoch 40/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6669 - accuracy: 0.7249 - val_loss: 0.8908 - val_accuracy: 0.6325\n","Epoch 41/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6561 - accuracy: 0.7268 - val_loss: 0.9691 - val_accuracy: 0.5690\n","Epoch 42/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6593 - accuracy: 0.7282 - val_loss: 0.9158 - val_accuracy: 0.6056\n","Epoch 43/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.6581 - accuracy: 0.7268 - val_loss: 0.9173 - val_accuracy: 0.6153\n","Epoch 44/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6565 - accuracy: 0.7325 - val_loss: 0.9084 - val_accuracy: 0.6099\n","Epoch 45/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6573 - accuracy: 0.7271 - val_loss: 0.9040 - val_accuracy: 0.6315\n","Epoch 46/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6466 - accuracy: 0.7306 - val_loss: 0.8975 - val_accuracy: 0.6347\n","Epoch 47/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6478 - accuracy: 0.7295 - val_loss: 0.9123 - val_accuracy: 0.6304\n","Epoch 48/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.6444 - accuracy: 0.7352 - val_loss: 0.9101 - val_accuracy: 0.6304\n","Epoch 49/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6451 - accuracy: 0.7392 - val_loss: 0.9040 - val_accuracy: 0.6390\n","Epoch 50/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.6421 - accuracy: 0.7314 - val_loss: 0.9283 - val_accuracy: 0.6325\n","Epoch 51/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6403 - accuracy: 0.7349 - val_loss: 0.9342 - val_accuracy: 0.6034\n","Epoch 52/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.6407 - accuracy: 0.7419 - val_loss: 0.9085 - val_accuracy: 0.6390\n","Epoch 53/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6349 - accuracy: 0.7411 - val_loss: 1.0213 - val_accuracy: 0.5614\n","Epoch 54/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.6418 - accuracy: 0.7303 - val_loss: 1.0352 - val_accuracy: 0.5614\n","Epoch 55/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.6313 - accuracy: 0.7338 - val_loss: 0.9607 - val_accuracy: 0.6056\n","Epoch 56/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.6328 - accuracy: 0.7390 - val_loss: 0.9257 - val_accuracy: 0.6325\n","Epoch 57/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.6384 - accuracy: 0.7406 - val_loss: 0.9397 - val_accuracy: 0.6261\n","Epoch 58/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.6354 - accuracy: 0.7317 - val_loss: 0.9525 - val_accuracy: 0.6261\n","Epoch 59/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.6348 - accuracy: 0.7311 - val_loss: 0.9696 - val_accuracy: 0.5959\n","Epoch 60/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.6227 - accuracy: 0.7492 - val_loss: 0.9275 - val_accuracy: 0.6390\n","Epoch 61/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.6304 - accuracy: 0.7368 - val_loss: 0.9458 - val_accuracy: 0.6121\n","Epoch 62/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.6223 - accuracy: 0.7441 - val_loss: 0.9382 - val_accuracy: 0.6401\n","Epoch 63/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6223 - accuracy: 0.7443 - val_loss: 0.9610 - val_accuracy: 0.5970\n","Epoch 64/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6172 - accuracy: 0.7395 - val_loss: 0.9493 - val_accuracy: 0.6164\n","Epoch 65/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.6179 - accuracy: 0.7532 - val_loss: 0.9261 - val_accuracy: 0.6401\n","Epoch 66/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.6066 - accuracy: 0.7600 - val_loss: 0.9356 - val_accuracy: 0.6422\n","Epoch 67/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.6109 - accuracy: 0.7589 - val_loss: 0.9325 - val_accuracy: 0.6336\n","Epoch 68/100\n","29/29 [==============================] - 1s 33ms/step - loss: 0.6091 - accuracy: 0.7575 - val_loss: 0.9809 - val_accuracy: 0.6379\n","Epoch 69/100\n","29/29 [==============================] - 1s 32ms/step - loss: 0.6216 - accuracy: 0.7414 - val_loss: 0.9436 - val_accuracy: 0.6272\n","Epoch 70/100\n","29/29 [==============================] - 1s 29ms/step - loss: 0.6082 - accuracy: 0.7516 - val_loss: 0.9595 - val_accuracy: 0.6207\n","Epoch 71/100\n","29/29 [==============================] - 1s 30ms/step - loss: 0.5989 - accuracy: 0.7635 - val_loss: 0.9455 - val_accuracy: 0.6282\n","Epoch 72/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.6095 - accuracy: 0.7578 - val_loss: 0.9417 - val_accuracy: 0.6379\n","Epoch 73/100\n","29/29 [==============================] - 1s 29ms/step - loss: 0.5994 - accuracy: 0.7484 - val_loss: 0.9981 - val_accuracy: 0.6228\n","Epoch 74/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.6162 - accuracy: 0.7468 - val_loss: 0.9981 - val_accuracy: 0.6272\n","Epoch 75/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.5914 - accuracy: 0.7621 - val_loss: 0.9665 - val_accuracy: 0.6369\n","Epoch 76/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.6018 - accuracy: 0.7584 - val_loss: 0.9792 - val_accuracy: 0.6304\n","Epoch 77/100\n","29/29 [==============================] - 1s 32ms/step - loss: 0.5963 - accuracy: 0.7573 - val_loss: 0.9734 - val_accuracy: 0.6282\n","Epoch 78/100\n","29/29 [==============================] - 2s 82ms/step - loss: 0.5945 - accuracy: 0.7535 - val_loss: 0.9511 - val_accuracy: 0.6509\n","Epoch 79/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5972 - accuracy: 0.7548 - val_loss: 0.9764 - val_accuracy: 0.6325\n","Epoch 80/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5916 - accuracy: 0.7589 - val_loss: 1.0055 - val_accuracy: 0.6261\n","Epoch 81/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.5855 - accuracy: 0.7605 - val_loss: 0.9737 - val_accuracy: 0.6379\n","Epoch 82/100\n","29/29 [==============================] - 1s 33ms/step - loss: 0.5887 - accuracy: 0.7570 - val_loss: 1.0089 - val_accuracy: 0.6045\n","Epoch 83/100\n","29/29 [==============================] - 1s 34ms/step - loss: 0.5870 - accuracy: 0.7586 - val_loss: 0.9823 - val_accuracy: 0.6379\n","Epoch 84/100\n","29/29 [==============================] - 1s 36ms/step - loss: 0.5804 - accuracy: 0.7678 - val_loss: 0.9933 - val_accuracy: 0.6196\n","Epoch 85/100\n","29/29 [==============================] - 1s 34ms/step - loss: 0.5818 - accuracy: 0.7702 - val_loss: 0.9873 - val_accuracy: 0.6315\n","Epoch 86/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5820 - accuracy: 0.7651 - val_loss: 1.0123 - val_accuracy: 0.6390\n","Epoch 87/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5752 - accuracy: 0.7753 - val_loss: 1.0059 - val_accuracy: 0.6293\n","Epoch 88/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5736 - accuracy: 0.7718 - val_loss: 1.1005 - val_accuracy: 0.5690\n","Epoch 89/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5787 - accuracy: 0.7683 - val_loss: 1.0184 - val_accuracy: 0.6121\n","Epoch 90/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5735 - accuracy: 0.7667 - val_loss: 1.0018 - val_accuracy: 0.6293\n","Epoch 91/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5667 - accuracy: 0.7716 - val_loss: 1.0350 - val_accuracy: 0.6002\n","Epoch 92/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5672 - accuracy: 0.7742 - val_loss: 1.1491 - val_accuracy: 0.5571\n","Epoch 93/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5662 - accuracy: 0.7764 - val_loss: 1.0566 - val_accuracy: 0.5948\n","Epoch 94/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5586 - accuracy: 0.7818 - val_loss: 1.0225 - val_accuracy: 0.6099\n","Epoch 95/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5732 - accuracy: 0.7745 - val_loss: 1.0271 - val_accuracy: 0.6142\n","Epoch 96/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5604 - accuracy: 0.7804 - val_loss: 1.0006 - val_accuracy: 0.6379\n","Epoch 97/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5589 - accuracy: 0.7848 - val_loss: 1.0130 - val_accuracy: 0.6390\n","Epoch 98/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5592 - accuracy: 0.7848 - val_loss: 1.0159 - val_accuracy: 0.6325\n","Epoch 99/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5582 - accuracy: 0.7764 - val_loss: 1.0387 - val_accuracy: 0.6336\n","Epoch 100/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5652 - accuracy: 0.7737 - val_loss: 1.0502 - val_accuracy: 0.6196\n","{'loss': [0.7648711204528809, 0.7508875727653503, 0.7419064044952393, 0.7442231178283691, 0.7322450280189514, 0.7355086803436279, 0.7315167188644409, 0.7330412864685059, 0.724848210811615, 0.7265978455543518, 0.7287784814834595, 0.7264693975448608, 0.7177822589874268, 0.716067373752594, 0.7244151830673218, 0.725130021572113, 0.7074553370475769, 0.7105813026428223, 0.7081959247589111, 0.699047863483429, 0.695967972278595, 0.699047863483429, 0.6968666315078735, 0.7051411867141724, 0.691374659538269, 0.6917052268981934, 0.6788881421089172, 0.6937834620475769, 0.6789421439170837, 0.6796697378158569, 0.6748848557472229, 0.6819573640823364, 0.6771738529205322, 0.6735482811927795, 0.6665334105491638, 0.6685463190078735, 0.6740451455116272, 0.6584941148757935, 0.6687309145927429, 0.6668811440467834, 0.6560593247413635, 0.6593390107154846, 0.6581113338470459, 0.6565292477607727, 0.6572780013084412, 0.6466174721717834, 0.6477874517440796, 0.6444067358970642, 0.6450731158256531, 0.6421366930007935, 0.6403136849403381, 0.6406522989273071, 0.634905993938446, 0.6417914032936096, 0.6312581300735474, 0.6328234672546387, 0.6383521556854248, 0.6354143619537354, 0.6348220705986023, 0.6226969957351685, 0.630376935005188, 0.6222534775733948, 0.6223241090774536, 0.6172484159469604, 0.6178632378578186, 0.6066442131996155, 0.6109455823898315, 0.6091272234916687, 0.6215795278549194, 0.6082140207290649, 0.5988615155220032, 0.6094681024551392, 0.5993974208831787, 0.6161971688270569, 0.5913916230201721, 0.601834774017334, 0.5962632298469543, 0.5945307612419128, 0.5971795916557312, 0.5916119813919067, 0.5854834914207458, 0.5887170433998108, 0.5870274305343628, 0.580400288105011, 0.5817697644233704, 0.5820140838623047, 0.5751639604568481, 0.5736185312271118, 0.5786681175231934, 0.5735388994216919, 0.5667197704315186, 0.5672323703765869, 0.5662159323692322, 0.5586376786231995, 0.5731772184371948, 0.5604333281517029, 0.5589333176612854, 0.5591573119163513, 0.5582454204559326, 0.5652244687080383], 'accuracy': [0.6710668206214905, 0.6740301847457886, 0.6904633641242981, 0.6772629022598267, 0.6926185488700867, 0.6904633641242981, 0.6853448152542114, 0.693965494632721, 0.6931573152542114, 0.6955819129943848, 0.693965494632721, 0.6918103694915771, 0.7033944129943848, 0.6901939511299133, 0.6866918206214905, 0.6931573152542114, 0.6977370977401733, 0.704472005367279, 0.7001616358757019, 0.7060883641242981, 0.712284505367279, 0.709590494632721, 0.7066271305084229, 0.6998922228813171, 0.7117456793785095, 0.7090517282485962, 0.7155172228813171, 0.7079741358757019, 0.709590494632721, 0.7238685488700867, 0.720097005367279, 0.7230603694915771, 0.7195581793785095, 0.7165948152542114, 0.7292564511299133, 0.7214439511299133, 0.7214439511299133, 0.7300646305084229, 0.7265625, 0.724946141242981, 0.7268319129943848, 0.728178858757019, 0.7268319129943848, 0.7324892282485962, 0.7271012663841248, 0.7306034564971924, 0.7295258641242981, 0.7351831793785095, 0.7392241358757019, 0.7314116358757019, 0.7349137663841248, 0.7419180870056152, 0.7411099076271057, 0.7303340435028076, 0.7338362336158752, 0.7389547228813171, 0.740571141242981, 0.7316810488700867, 0.7311422228813171, 0.7491918206214905, 0.7367995977401733, 0.7440732717514038, 0.7443426847457886, 0.7394935488700867, 0.7532327771186829, 0.7599676847457886, 0.7588900923728943, 0.7575430870056152, 0.7413793206214905, 0.751616358757019, 0.7634698152542114, 0.7578125, 0.748383641242981, 0.7467672228813171, 0.7621228694915771, 0.7583512663841248, 0.7572737336158752, 0.7535021305084229, 0.7548491358757019, 0.7588900923728943, 0.7605064511299133, 0.7570043206214905, 0.7586206793785095, 0.7677801847457886, 0.7702047228813171, 0.7650862336158752, 0.7753232717514038, 0.771821141242981, 0.7683189511299133, 0.7667025923728943, 0.7715517282485962, 0.7742456793785095, 0.7764008641242981, 0.7817887663841248, 0.7745150923728943, 0.7804418206214905, 0.7847521305084229, 0.7847521305084229, 0.7764008641242981, 0.7737069129943848], 'val_loss': [0.8820484280586243, 0.8831344246864319, 0.8845622539520264, 0.8813238739967346, 0.8692212700843811, 0.8677513003349304, 0.8818194270133972, 0.8901551961898804, 0.8663393259048462, 0.8732149004936218, 0.871665894985199, 0.8483772873878479, 0.8463367223739624, 0.8519778847694397, 0.8388251662254333, 0.837380051612854, 0.8500040769577026, 0.8409478664398193, 0.8496108055114746, 0.8458719849586487, 0.839995801448822, 0.85492342710495, 0.8567153215408325, 0.8646080493927002, 0.8613607287406921, 0.8614199161529541, 0.8780779838562012, 0.8559547066688538, 0.8637851476669312, 0.8781265616416931, 0.8756152391433716, 0.9052731990814209, 0.8933041095733643, 0.8753872513771057, 0.8932341933250427, 0.9118131995201111, 0.8984626531600952, 0.8908328413963318, 0.8979865312576294, 0.8908279538154602, 0.9691036343574524, 0.9158414006233215, 0.917305588722229, 0.9084250926971436, 0.9039791226387024, 0.897507905960083, 0.9122666120529175, 0.9100712537765503, 0.9039575457572937, 0.9283264875411987, 0.9341761469841003, 0.9085246920585632, 1.0212955474853516, 1.0351645946502686, 0.9607277512550354, 0.9256871342658997, 0.9396993517875671, 0.9525282382965088, 0.9696208238601685, 0.9274781942367554, 0.9458160996437073, 0.938226580619812, 0.9609923362731934, 0.9493401050567627, 0.9260618090629578, 0.9355896711349487, 0.932466983795166, 0.9809049367904663, 0.9436262845993042, 0.9595375657081604, 0.9455174803733826, 0.9417044520378113, 0.99808669090271, 0.9980611205101013, 0.9665163159370422, 0.9791983366012573, 0.9733754992485046, 0.9510501623153687, 0.9763767123222351, 1.0054959058761597, 0.9737212657928467, 1.0089036226272583, 0.9823030233383179, 0.9932633638381958, 0.9872750639915466, 1.0122709274291992, 1.0059365034103394, 1.1005244255065918, 1.0184273719787598, 1.0017542839050293, 1.0350120067596436, 1.149078130722046, 1.0566000938415527, 1.0225306749343872, 1.027097463607788, 1.0005991458892822, 1.0129733085632324, 1.015945553779602, 1.0387272834777832, 1.0501954555511475], 'val_accuracy': [0.517241358757019, 0.517241358757019, 0.517241358757019, 0.5183189511299133, 0.5366379022598267, 0.5398706793785095, 0.5280172228813171, 0.5269396305084229, 0.537715494632721, 0.5398706793785095, 0.5431034564971924, 0.6066810488700867, 0.6023706793785095, 0.5840517282485962, 0.600215494632721, 0.6163793206214905, 0.6303879022598267, 0.6174569129943848, 0.6260775923728943, 0.639008641242981, 0.618534505367279, 0.59375, 0.6293103694915771, 0.6142241358757019, 0.6379310488700867, 0.639008641242981, 0.607758641242981, 0.6368534564971924, 0.6336206793785095, 0.6336206793785095, 0.6443965435028076, 0.5991379022598267, 0.6088362336158752, 0.6476293206214905, 0.625, 0.5991379022598267, 0.6066810488700867, 0.642241358757019, 0.6454741358757019, 0.6325430870056152, 0.568965494632721, 0.6056034564971924, 0.6153017282485962, 0.6099137663841248, 0.631465494632721, 0.6346982717514038, 0.6303879022598267, 0.6303879022598267, 0.639008641242981, 0.6325430870056152, 0.6034482717514038, 0.639008641242981, 0.5614224076271057, 0.5614224076271057, 0.6056034564971924, 0.6325430870056152, 0.6260775923728943, 0.6260775923728943, 0.5959051847457886, 0.639008641242981, 0.6120689511299133, 0.6400862336158752, 0.5969827771186829, 0.6163793206214905, 0.6400862336158752, 0.642241358757019, 0.6336206793785095, 0.6379310488700867, 0.6271551847457886, 0.6206896305084229, 0.6282327771186829, 0.6379310488700867, 0.6228448152542114, 0.6271551847457886, 0.6368534564971924, 0.6303879022598267, 0.6282327771186829, 0.6508620977401733, 0.6325430870056152, 0.6260775923728943, 0.6379310488700867, 0.6045258641242981, 0.6379310488700867, 0.6196120977401733, 0.631465494632721, 0.639008641242981, 0.6293103694915771, 0.568965494632721, 0.6120689511299133, 0.6293103694915771, 0.600215494632721, 0.5571120977401733, 0.5948275923728943, 0.6099137663841248, 0.6142241358757019, 0.6379310488700867, 0.639008641242981, 0.6325430870056152, 0.6336206793785095, 0.6196120977401733]}\n","38/38 [==============================] - 4s 6ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - ETA: 0s - loss: 0.7561 - accuracy: 0.6780"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 13s 90ms/step - loss: 0.7561 - accuracy: 0.6780 - val_loss: 0.8804 - val_accuracy: 0.5068\n","Epoch 2/100\n","28/28 [==============================] - 1s 38ms/step - loss: 0.7405 - accuracy: 0.6927 - val_loss: 0.8821 - val_accuracy: 0.5102\n","Epoch 3/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7491 - accuracy: 0.6689 - val_loss: 0.8910 - val_accuracy: 0.5057\n","Epoch 4/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.7432 - accuracy: 0.6802 - val_loss: 0.8694 - val_accuracy: 0.5170\n","Epoch 5/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.7335 - accuracy: 0.6870 - val_loss: 0.8639 - val_accuracy: 0.5317\n","Epoch 6/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7334 - accuracy: 0.6978 - val_loss: 0.8752 - val_accuracy: 0.5113\n","Epoch 7/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7279 - accuracy: 0.6899 - val_loss: 0.8651 - val_accuracy: 0.5204\n","Epoch 8/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7323 - accuracy: 0.6865 - val_loss: 0.8747 - val_accuracy: 0.5170\n","Epoch 9/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.7245 - accuracy: 0.6865 - val_loss: 0.8538 - val_accuracy: 0.5724\n","Epoch 10/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7176 - accuracy: 0.6992 - val_loss: 0.8555 - val_accuracy: 0.5419\n","Epoch 11/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7157 - accuracy: 0.7074 - val_loss: 0.8469 - val_accuracy: 0.5520\n","Epoch 12/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7104 - accuracy: 0.7091 - val_loss: 0.8604 - val_accuracy: 0.5351\n","Epoch 13/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7081 - accuracy: 0.7102 - val_loss: 0.8589 - val_accuracy: 0.5452\n","Epoch 14/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.7145 - accuracy: 0.6998 - val_loss: 0.8320 - val_accuracy: 0.5894\n","Epoch 15/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7039 - accuracy: 0.7074 - val_loss: 0.8350 - val_accuracy: 0.5656\n","Epoch 16/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7021 - accuracy: 0.7085 - val_loss: 0.8367 - val_accuracy: 0.5894\n","Epoch 17/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7063 - accuracy: 0.7035 - val_loss: 0.8305 - val_accuracy: 0.5803\n","Epoch 18/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7045 - accuracy: 0.7097 - val_loss: 0.8401 - val_accuracy: 0.5758\n","Epoch 19/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7023 - accuracy: 0.7091 - val_loss: 0.8315 - val_accuracy: 0.5860\n","Epoch 20/100\n","28/28 [==============================] - 1s 31ms/step - loss: 0.6939 - accuracy: 0.7173 - val_loss: 0.8290 - val_accuracy: 0.6063\n","Epoch 21/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6971 - accuracy: 0.7080 - val_loss: 0.8505 - val_accuracy: 0.5792\n","Epoch 22/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.6986 - accuracy: 0.7080 - val_loss: 0.8454 - val_accuracy: 0.5984\n","Epoch 23/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.6931 - accuracy: 0.7097 - val_loss: 0.8412 - val_accuracy: 0.6176\n","Epoch 24/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6991 - accuracy: 0.6981 - val_loss: 0.8395 - val_accuracy: 0.6109\n","Epoch 25/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.6928 - accuracy: 0.7156 - val_loss: 0.8492 - val_accuracy: 0.6301\n","Epoch 26/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6878 - accuracy: 0.7134 - val_loss: 0.8521 - val_accuracy: 0.6086\n","Epoch 27/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6845 - accuracy: 0.7179 - val_loss: 0.8661 - val_accuracy: 0.6244\n","Epoch 28/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6816 - accuracy: 0.7193 - val_loss: 0.8565 - val_accuracy: 0.6290\n","Epoch 29/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6797 - accuracy: 0.7199 - val_loss: 0.8514 - val_accuracy: 0.6256\n","Epoch 30/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6777 - accuracy: 0.7213 - val_loss: 0.8539 - val_accuracy: 0.6188\n","Epoch 31/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6805 - accuracy: 0.7179 - val_loss: 0.8844 - val_accuracy: 0.5995\n","Epoch 32/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6800 - accuracy: 0.7139 - val_loss: 0.8652 - val_accuracy: 0.6222\n","Epoch 33/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6744 - accuracy: 0.7247 - val_loss: 0.8723 - val_accuracy: 0.6267\n","Epoch 34/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6684 - accuracy: 0.7196 - val_loss: 0.8881 - val_accuracy: 0.6063\n","Epoch 35/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6759 - accuracy: 0.7247 - val_loss: 0.8997 - val_accuracy: 0.6188\n","Epoch 36/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6691 - accuracy: 0.7199 - val_loss: 0.8890 - val_accuracy: 0.6109\n","Epoch 37/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6661 - accuracy: 0.7275 - val_loss: 0.8932 - val_accuracy: 0.6063\n","Epoch 38/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6595 - accuracy: 0.7298 - val_loss: 0.8903 - val_accuracy: 0.6199\n","Epoch 39/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.6651 - accuracy: 0.7315 - val_loss: 0.8916 - val_accuracy: 0.6109\n","Epoch 40/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6556 - accuracy: 0.7354 - val_loss: 0.8878 - val_accuracy: 0.6233\n","Epoch 41/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6596 - accuracy: 0.7419 - val_loss: 0.9117 - val_accuracy: 0.6188\n","Epoch 42/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6583 - accuracy: 0.7235 - val_loss: 0.8876 - val_accuracy: 0.6154\n","Epoch 43/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6611 - accuracy: 0.7252 - val_loss: 0.8926 - val_accuracy: 0.6154\n","Epoch 44/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.6508 - accuracy: 0.7391 - val_loss: 0.9399 - val_accuracy: 0.6041\n","Epoch 45/100\n","28/28 [==============================] - 2s 74ms/step - loss: 0.6486 - accuracy: 0.7301 - val_loss: 0.8891 - val_accuracy: 0.6312\n","Epoch 46/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.6597 - accuracy: 0.7275 - val_loss: 0.9171 - val_accuracy: 0.6233\n","Epoch 47/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6520 - accuracy: 0.7323 - val_loss: 0.8934 - val_accuracy: 0.6199\n","Epoch 48/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.6417 - accuracy: 0.7414 - val_loss: 0.8992 - val_accuracy: 0.6109\n","Epoch 49/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.6466 - accuracy: 0.7385 - val_loss: 0.9085 - val_accuracy: 0.6188\n","Epoch 50/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6444 - accuracy: 0.7405 - val_loss: 0.8968 - val_accuracy: 0.6199\n","Epoch 51/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6420 - accuracy: 0.7459 - val_loss: 0.9228 - val_accuracy: 0.6176\n","Epoch 52/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6370 - accuracy: 0.7434 - val_loss: 0.9249 - val_accuracy: 0.6131\n","Epoch 53/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6337 - accuracy: 0.7380 - val_loss: 0.9133 - val_accuracy: 0.6256\n","Epoch 54/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6293 - accuracy: 0.7504 - val_loss: 0.9313 - val_accuracy: 0.6063\n","Epoch 55/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6336 - accuracy: 0.7467 - val_loss: 0.9294 - val_accuracy: 0.6222\n","Epoch 56/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6342 - accuracy: 0.7450 - val_loss: 0.9350 - val_accuracy: 0.6233\n","Epoch 57/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6358 - accuracy: 0.7465 - val_loss: 0.9342 - val_accuracy: 0.6018\n","Epoch 58/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6363 - accuracy: 0.7414 - val_loss: 0.9376 - val_accuracy: 0.6290\n","Epoch 59/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6266 - accuracy: 0.7462 - val_loss: 0.9302 - val_accuracy: 0.6052\n","Epoch 60/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6329 - accuracy: 0.7397 - val_loss: 0.9536 - val_accuracy: 0.5962\n","Epoch 61/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6250 - accuracy: 0.7504 - val_loss: 0.9342 - val_accuracy: 0.6154\n","Epoch 62/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6221 - accuracy: 0.7552 - val_loss: 0.9343 - val_accuracy: 0.6256\n","Epoch 63/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6239 - accuracy: 0.7535 - val_loss: 0.9503 - val_accuracy: 0.6267\n","Epoch 64/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6288 - accuracy: 0.7462 - val_loss: 0.9401 - val_accuracy: 0.6165\n","Epoch 65/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.6265 - accuracy: 0.7417 - val_loss: 0.9431 - val_accuracy: 0.6324\n","Epoch 66/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6220 - accuracy: 0.7527 - val_loss: 0.9693 - val_accuracy: 0.6222\n","Epoch 67/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6322 - accuracy: 0.7533 - val_loss: 0.9511 - val_accuracy: 0.6120\n","Epoch 68/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6151 - accuracy: 0.7552 - val_loss: 0.9414 - val_accuracy: 0.6290\n","Epoch 69/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6161 - accuracy: 0.7558 - val_loss: 0.9705 - val_accuracy: 0.6063\n","Epoch 70/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.6092 - accuracy: 0.7538 - val_loss: 0.9270 - val_accuracy: 0.6222\n","Epoch 71/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6066 - accuracy: 0.7575 - val_loss: 0.9716 - val_accuracy: 0.6233\n","Epoch 72/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.6111 - accuracy: 0.7541 - val_loss: 1.0115 - val_accuracy: 0.5792\n","Epoch 73/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6131 - accuracy: 0.7561 - val_loss: 1.0335 - val_accuracy: 0.5837\n","Epoch 74/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.6055 - accuracy: 0.7547 - val_loss: 0.9604 - val_accuracy: 0.6165\n","Epoch 75/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6066 - accuracy: 0.7600 - val_loss: 0.9482 - val_accuracy: 0.6210\n","Epoch 76/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.5955 - accuracy: 0.7691 - val_loss: 0.9709 - val_accuracy: 0.6063\n","Epoch 77/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6020 - accuracy: 0.7626 - val_loss: 0.9875 - val_accuracy: 0.6244\n","Epoch 78/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5979 - accuracy: 0.7595 - val_loss: 0.9882 - val_accuracy: 0.6301\n","Epoch 79/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6036 - accuracy: 0.7654 - val_loss: 1.0732 - val_accuracy: 0.5814\n","Epoch 80/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5968 - accuracy: 0.7677 - val_loss: 0.9576 - val_accuracy: 0.6267\n","Epoch 81/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5954 - accuracy: 0.7646 - val_loss: 1.0188 - val_accuracy: 0.6109\n","Epoch 82/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5970 - accuracy: 0.7575 - val_loss: 1.0450 - val_accuracy: 0.5860\n","Epoch 83/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5898 - accuracy: 0.7722 - val_loss: 0.9924 - val_accuracy: 0.6222\n","Epoch 84/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5824 - accuracy: 0.7677 - val_loss: 0.9989 - val_accuracy: 0.6233\n","Epoch 85/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5884 - accuracy: 0.7668 - val_loss: 1.0039 - val_accuracy: 0.6097\n","Epoch 86/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5854 - accuracy: 0.7711 - val_loss: 1.0469 - val_accuracy: 0.5894\n","Epoch 87/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5852 - accuracy: 0.7685 - val_loss: 1.0071 - val_accuracy: 0.6176\n","Epoch 88/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5827 - accuracy: 0.7742 - val_loss: 0.9842 - val_accuracy: 0.6335\n","Epoch 89/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5832 - accuracy: 0.7711 - val_loss: 1.0408 - val_accuracy: 0.6131\n","Epoch 90/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5909 - accuracy: 0.7589 - val_loss: 1.0310 - val_accuracy: 0.6188\n","Epoch 91/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5809 - accuracy: 0.7765 - val_loss: 1.0016 - val_accuracy: 0.6075\n","Epoch 92/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5783 - accuracy: 0.7864 - val_loss: 1.0018 - val_accuracy: 0.6041\n","Epoch 93/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5805 - accuracy: 0.7799 - val_loss: 1.0434 - val_accuracy: 0.6097\n","Epoch 94/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5757 - accuracy: 0.7779 - val_loss: 1.0017 - val_accuracy: 0.6210\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5756 - accuracy: 0.7745 - val_loss: 1.0230 - val_accuracy: 0.6075\n","Epoch 96/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5669 - accuracy: 0.7864 - val_loss: 1.0127 - val_accuracy: 0.6041\n","Epoch 97/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5742 - accuracy: 0.7773 - val_loss: 1.0438 - val_accuracy: 0.6131\n","Epoch 98/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5732 - accuracy: 0.7748 - val_loss: 1.0308 - val_accuracy: 0.6086\n","Epoch 99/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5739 - accuracy: 0.7632 - val_loss: 1.0975 - val_accuracy: 0.5871\n","Epoch 100/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5684 - accuracy: 0.7790 - val_loss: 1.0188 - val_accuracy: 0.6109\n","{'loss': [0.7560796737670898, 0.7404853701591492, 0.7491490840911865, 0.7431811690330505, 0.7334515452384949, 0.733435869216919, 0.7278776168823242, 0.7322929501533508, 0.7245028614997864, 0.7176413536071777, 0.7157341837882996, 0.7104291915893555, 0.7081420421600342, 0.7144537568092346, 0.7038881778717041, 0.702093780040741, 0.706337034702301, 0.7045398354530334, 0.702319860458374, 0.6939339637756348, 0.6971307992935181, 0.6985991597175598, 0.6930803060531616, 0.6991397142410278, 0.6927982568740845, 0.6878256797790527, 0.6845226287841797, 0.6815651655197144, 0.6796956062316895, 0.677749752998352, 0.6805195808410645, 0.6800166368484497, 0.6743667125701904, 0.6684368848800659, 0.6759301424026489, 0.6691157221794128, 0.666122555732727, 0.6594893336296082, 0.6650861501693726, 0.6555854082107544, 0.6596113443374634, 0.6582755446434021, 0.6611131429672241, 0.6507797837257385, 0.6486487984657288, 0.6596806645393372, 0.6520400047302246, 0.6416798830032349, 0.6465834975242615, 0.6443625688552856, 0.6420472264289856, 0.6370035409927368, 0.6336931586265564, 0.6293268203735352, 0.6335688829421997, 0.6341857314109802, 0.6357900500297546, 0.6362596154212952, 0.6265925765037537, 0.6328936815261841, 0.625028669834137, 0.6220729947090149, 0.6238592863082886, 0.6288350820541382, 0.6264649629592896, 0.622046947479248, 0.6322202682495117, 0.6151161193847656, 0.6160539984703064, 0.6092371344566345, 0.6066005229949951, 0.6110745668411255, 0.6130649447441101, 0.6055217981338501, 0.6066078543663025, 0.5955389738082886, 0.6020189523696899, 0.5978999733924866, 0.6035656332969666, 0.5967916250228882, 0.5953622460365295, 0.5969525575637817, 0.5898067355155945, 0.5824356079101562, 0.5883935689926147, 0.5853724479675293, 0.5852069854736328, 0.5826868414878845, 0.5831747055053711, 0.5909186601638794, 0.5808536410331726, 0.5782766938209534, 0.5804503560066223, 0.5756997466087341, 0.5755759477615356, 0.5668778419494629, 0.5742067694664001, 0.5731528997421265, 0.5738584995269775, 0.5683588981628418], 'accuracy': [0.6779853105545044, 0.6926994919776917, 0.6689304113388062, 0.680249035358429, 0.6870402097702026, 0.6977928876876831, 0.6898698210716248, 0.6864742636680603, 0.6864742636680603, 0.6992077231407166, 0.7074136734008789, 0.7091115117073059, 0.7102433443069458, 0.6997736096382141, 0.7074136734008789, 0.7085455656051636, 0.7034521698951721, 0.7096773982048035, 0.7091115117073059, 0.7173174619674683, 0.7079796195030212, 0.7079796195030212, 0.7096773982048035, 0.6980758309364319, 0.715619683265686, 0.7133559584617615, 0.7178834080696106, 0.719298243522644, 0.7198641896247864, 0.7212790250778198, 0.7178834080696106, 0.7139219045639038, 0.7246745824813843, 0.7195811867713928, 0.7246745824813843, 0.7198641896247864, 0.7275042533874512, 0.7297679781913757, 0.731465756893158, 0.7354272603988647, 0.7419354915618896, 0.7235427498817444, 0.7252405285835266, 0.7391058206558228, 0.7300509214401245, 0.7275042533874512, 0.7323146462440491, 0.7413695454597473, 0.7385398745536804, 0.7405206561088562, 0.7458969950675964, 0.7433503270149231, 0.7379739880561829, 0.7504244446754456, 0.7467458844184875, 0.7450481057167053, 0.7464629411697388, 0.7413695454597473, 0.7461799383163452, 0.7396717667579651, 0.7504244446754456, 0.7552348375320435, 0.7535370588302612, 0.7461799383163452, 0.7416524887084961, 0.7526881694793701, 0.7532541155815125, 0.7552348375320435, 0.7558007836341858, 0.7538200616836548, 0.757498562335968, 0.7541030049324036, 0.7560837864875793, 0.7546689510345459, 0.7600452899932861, 0.7691001892089844, 0.7625919580459595, 0.7594793438911438, 0.7654216289520264, 0.7676853537559509, 0.7645727396011353, 0.757498562335968, 0.7722128033638, 0.7676853537559509, 0.7668364644050598, 0.7710809111595154, 0.768534243106842, 0.774193525314331, 0.7710809111595154, 0.7589133977890015, 0.7764572501182556, 0.786361038684845, 0.7798528671264648, 0.7778720855712891, 0.7744765281677246, 0.786361038684845, 0.7773061394691467, 0.7747594714164734, 0.7631579041481018, 0.7790039777755737], 'val_loss': [0.8803749680519104, 0.8820705413818359, 0.8910228610038757, 0.8693825006484985, 0.8639400005340576, 0.875196635723114, 0.8650770783424377, 0.8747461438179016, 0.8538256287574768, 0.8554530143737793, 0.8469053506851196, 0.8603749871253967, 0.8588686585426331, 0.8319980502128601, 0.8349509239196777, 0.8366535305976868, 0.8304653167724609, 0.840081512928009, 0.8315384387969971, 0.8289791941642761, 0.8505069017410278, 0.8453699350357056, 0.8412132263183594, 0.8395145535469055, 0.849158763885498, 0.852081298828125, 0.8660704493522644, 0.8564901947975159, 0.8513805866241455, 0.8539158701896667, 0.8844282627105713, 0.8651989102363586, 0.8722752332687378, 0.888052761554718, 0.8997436761856079, 0.8889539837837219, 0.8932107090950012, 0.8902953863143921, 0.8916491866111755, 0.8878331780433655, 0.9117375016212463, 0.8875722885131836, 0.8925937414169312, 0.9398691654205322, 0.8890554308891296, 0.917078971862793, 0.8933666348457336, 0.8991712331771851, 0.9085143804550171, 0.896807074546814, 0.9227871894836426, 0.924901008605957, 0.9132550954818726, 0.931290864944458, 0.9293513298034668, 0.935028612613678, 0.9341737031936646, 0.9376233816146851, 0.9302067756652832, 0.9536306858062744, 0.9342367649078369, 0.9343062043190002, 0.9503024816513062, 0.9400774836540222, 0.9431233406066895, 0.9693199396133423, 0.9510701298713684, 0.941388726234436, 0.9705488681793213, 0.9270151257514954, 0.9715899229049683, 1.0114846229553223, 1.033523678779602, 0.9603570699691772, 0.9481706023216248, 0.9708709120750427, 0.9875156283378601, 0.9881789088249207, 1.073243260383606, 0.9575666785240173, 1.0187971591949463, 1.0449997186660767, 0.9924123287200928, 0.9989457130432129, 1.0038973093032837, 1.0468851327896118, 1.0071015357971191, 0.9841774106025696, 1.0407763719558716, 1.0309690237045288, 1.0016053915023804, 1.0017951726913452, 1.043436884880066, 1.0016512870788574, 1.0229769945144653, 1.012688159942627, 1.0438319444656372, 1.0307822227478027, 1.0975383520126343, 1.0187697410583496], 'val_accuracy': [0.5067873597145081, 0.5101810097694397, 0.5056561231613159, 0.516968309879303, 0.5316742062568665, 0.5113122463226318, 0.5203620195388794, 0.516968309879303, 0.5723981857299805, 0.5418552160263062, 0.5520362257957458, 0.5350678563117981, 0.5452488660812378, 0.5893664956092834, 0.5656108856201172, 0.5893664956092834, 0.5803167223930359, 0.5757918357849121, 0.5859728455543518, 0.6063348650932312, 0.5791855454444885, 0.598416268825531, 0.6176470518112183, 0.610859751701355, 0.6300904750823975, 0.6085972785949707, 0.6244344115257263, 0.6289592981338501, 0.6255655884742737, 0.6187782883644104, 0.5995475053787231, 0.622171938419342, 0.6266968250274658, 0.6063348650932312, 0.6187782883644104, 0.610859751701355, 0.6063348650932312, 0.6199095249176025, 0.610859751701355, 0.6233031749725342, 0.6187782883644104, 0.6153846383094788, 0.6153846383094788, 0.6040723919868469, 0.6312217116355896, 0.6233031749725342, 0.6199095249176025, 0.610859751701355, 0.6187782883644104, 0.6199095249176025, 0.6176470518112183, 0.6131221652030945, 0.6255655884742737, 0.6063348650932312, 0.622171938419342, 0.6233031749725342, 0.6018099784851074, 0.6289592981338501, 0.6052036285400391, 0.5961538553237915, 0.6153846383094788, 0.6255655884742737, 0.6266968250274658, 0.6165158152580261, 0.6323529481887817, 0.622171938419342, 0.6119909286499023, 0.6289592981338501, 0.6063348650932312, 0.622171938419342, 0.6233031749725342, 0.5791855454444885, 0.5837104320526123, 0.6165158152580261, 0.6210407018661499, 0.6063348650932312, 0.6244344115257263, 0.6300904750823975, 0.581447958946228, 0.6266968250274658, 0.610859751701355, 0.5859728455543518, 0.622171938419342, 0.6233031749725342, 0.6097285151481628, 0.5893664956092834, 0.6176470518112183, 0.6334841847419739, 0.6131221652030945, 0.6187782883644104, 0.6074660420417786, 0.6040723919868469, 0.6097285151481628, 0.6210407018661499, 0.6074660420417786, 0.6040723919868469, 0.6131221652030945, 0.6085972785949707, 0.587104082107544, 0.610859751701355]}\n","45/45 [==============================] - 1s 6ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/31 [==========================>...] - ETA: 0s - loss: 0.7668 - accuracy: 0.6713"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 7s 52ms/step - loss: 0.7640 - accuracy: 0.6765 - val_loss: 0.8734 - val_accuracy: 0.5196\n","Epoch 2/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7529 - accuracy: 0.6762 - val_loss: 0.8800 - val_accuracy: 0.5176\n","Epoch 3/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7513 - accuracy: 0.6814 - val_loss: 0.8780 - val_accuracy: 0.5165\n","Epoch 4/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7493 - accuracy: 0.6760 - val_loss: 0.8756 - val_accuracy: 0.5196\n","Epoch 5/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.7583 - accuracy: 0.6711 - val_loss: 0.8642 - val_accuracy: 0.5351\n","Epoch 6/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7425 - accuracy: 0.6835 - val_loss: 0.8724 - val_accuracy: 0.5238\n","Epoch 7/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.7415 - accuracy: 0.6876 - val_loss: 0.8560 - val_accuracy: 0.5651\n","Epoch 8/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7395 - accuracy: 0.6786 - val_loss: 0.8635 - val_accuracy: 0.5362\n","Epoch 9/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7325 - accuracy: 0.6910 - val_loss: 0.8505 - val_accuracy: 0.5599\n","Epoch 10/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.7292 - accuracy: 0.6881 - val_loss: 0.8459 - val_accuracy: 0.5764\n","Epoch 11/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7309 - accuracy: 0.6941 - val_loss: 0.8531 - val_accuracy: 0.5434\n","Epoch 12/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.7335 - accuracy: 0.6840 - val_loss: 0.8398 - val_accuracy: 0.5868\n","Epoch 13/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.7209 - accuracy: 0.6920 - val_loss: 0.8383 - val_accuracy: 0.5899\n","Epoch 14/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7219 - accuracy: 0.6925 - val_loss: 0.8412 - val_accuracy: 0.5692\n","Epoch 15/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7208 - accuracy: 0.6938 - val_loss: 0.8407 - val_accuracy: 0.5702\n","Epoch 16/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.7171 - accuracy: 0.6948 - val_loss: 0.8321 - val_accuracy: 0.5930\n","Epoch 17/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7246 - accuracy: 0.6946 - val_loss: 0.8280 - val_accuracy: 0.5837\n","Epoch 18/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.7197 - accuracy: 0.6902 - val_loss: 0.8263 - val_accuracy: 0.5764\n","Epoch 19/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.7179 - accuracy: 0.6889 - val_loss: 0.8396 - val_accuracy: 0.5961\n","Epoch 20/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7096 - accuracy: 0.7052 - val_loss: 0.8519 - val_accuracy: 0.5589\n","Epoch 21/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7152 - accuracy: 0.6925 - val_loss: 0.8377 - val_accuracy: 0.5806\n","Epoch 22/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7137 - accuracy: 0.7003 - val_loss: 0.8462 - val_accuracy: 0.5919\n","Epoch 23/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7065 - accuracy: 0.6941 - val_loss: 0.8642 - val_accuracy: 0.5868\n","Epoch 24/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7028 - accuracy: 0.7075 - val_loss: 0.8713 - val_accuracy: 0.5950\n","Epoch 25/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7062 - accuracy: 0.6912 - val_loss: 0.8548 - val_accuracy: 0.5888\n","Epoch 26/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7000 - accuracy: 0.6956 - val_loss: 0.8694 - val_accuracy: 0.5950\n","Epoch 27/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6957 - accuracy: 0.7114 - val_loss: 0.9103 - val_accuracy: 0.5868\n","Epoch 28/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6940 - accuracy: 0.7127 - val_loss: 0.9346 - val_accuracy: 0.5599\n","Epoch 29/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6959 - accuracy: 0.7080 - val_loss: 0.9000 - val_accuracy: 0.5868\n","Epoch 30/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6924 - accuracy: 0.7111 - val_loss: 0.8706 - val_accuracy: 0.5785\n","Epoch 31/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6903 - accuracy: 0.7152 - val_loss: 0.9431 - val_accuracy: 0.5733\n","Epoch 32/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6922 - accuracy: 0.7067 - val_loss: 0.8932 - val_accuracy: 0.5888\n","Epoch 33/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6981 - accuracy: 0.7010 - val_loss: 0.8940 - val_accuracy: 0.5868\n","Epoch 34/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.6857 - accuracy: 0.7124 - val_loss: 0.8797 - val_accuracy: 0.5837\n","Epoch 35/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6830 - accuracy: 0.7119 - val_loss: 0.9294 - val_accuracy: 0.5589\n","Epoch 36/100\n","31/31 [==============================] - 1s 34ms/step - loss: 0.6797 - accuracy: 0.7119 - val_loss: 0.9222 - val_accuracy: 0.5981\n","Epoch 37/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6789 - accuracy: 0.7111 - val_loss: 0.9141 - val_accuracy: 0.5816\n","Epoch 38/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6836 - accuracy: 0.7103 - val_loss: 0.9045 - val_accuracy: 0.5837\n","Epoch 39/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6795 - accuracy: 0.7199 - val_loss: 0.9038 - val_accuracy: 0.5878\n","Epoch 40/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6735 - accuracy: 0.7181 - val_loss: 0.8847 - val_accuracy: 0.5816\n","Epoch 41/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6737 - accuracy: 0.7160 - val_loss: 0.8838 - val_accuracy: 0.5826\n","Epoch 42/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.6651 - accuracy: 0.7261 - val_loss: 0.9287 - val_accuracy: 0.6023\n","Epoch 43/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6684 - accuracy: 0.7165 - val_loss: 0.8800 - val_accuracy: 0.5919\n","Epoch 44/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6656 - accuracy: 0.7256 - val_loss: 0.8991 - val_accuracy: 0.5919\n","Epoch 45/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6607 - accuracy: 0.7331 - val_loss: 0.9201 - val_accuracy: 0.5651\n","Epoch 46/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6666 - accuracy: 0.7243 - val_loss: 0.9517 - val_accuracy: 0.5806\n","Epoch 47/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6654 - accuracy: 0.7240 - val_loss: 0.8973 - val_accuracy: 0.5857\n","Epoch 48/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6602 - accuracy: 0.7225 - val_loss: 0.9084 - val_accuracy: 0.5878\n","Epoch 49/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6634 - accuracy: 0.7132 - val_loss: 0.8951 - val_accuracy: 0.5826\n","Epoch 50/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6571 - accuracy: 0.7243 - val_loss: 0.8981 - val_accuracy: 0.5940\n","Epoch 51/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6543 - accuracy: 0.7214 - val_loss: 0.8882 - val_accuracy: 0.5837\n","Epoch 52/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6536 - accuracy: 0.7245 - val_loss: 0.9093 - val_accuracy: 0.5713\n","Epoch 53/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6483 - accuracy: 0.7274 - val_loss: 0.8970 - val_accuracy: 0.5723\n","Epoch 54/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6498 - accuracy: 0.7282 - val_loss: 0.9363 - val_accuracy: 0.5723\n","Epoch 55/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6583 - accuracy: 0.7240 - val_loss: 0.9261 - val_accuracy: 0.5785\n","Epoch 56/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6411 - accuracy: 0.7341 - val_loss: 0.9342 - val_accuracy: 0.5950\n","Epoch 57/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6472 - accuracy: 0.7284 - val_loss: 0.9151 - val_accuracy: 0.5568\n","Epoch 58/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.6562 - accuracy: 0.7302 - val_loss: 0.8813 - val_accuracy: 0.5899\n","Epoch 59/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6460 - accuracy: 0.7320 - val_loss: 0.9499 - val_accuracy: 0.5692\n","Epoch 60/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.6410 - accuracy: 0.7282 - val_loss: 0.9608 - val_accuracy: 0.5610\n","Epoch 61/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6336 - accuracy: 0.7344 - val_loss: 0.9284 - val_accuracy: 0.5671\n","Epoch 62/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6418 - accuracy: 0.7359 - val_loss: 0.9705 - val_accuracy: 0.5651\n","Epoch 63/100\n","31/31 [==============================] - 1s 32ms/step - loss: 0.6273 - accuracy: 0.7439 - val_loss: 0.9063 - val_accuracy: 0.6054\n","Epoch 64/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6268 - accuracy: 0.7478 - val_loss: 0.9274 - val_accuracy: 0.5806\n","Epoch 65/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6413 - accuracy: 0.7279 - val_loss: 0.9665 - val_accuracy: 0.5847\n","Epoch 66/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6443 - accuracy: 0.7295 - val_loss: 0.9603 - val_accuracy: 0.5806\n","Epoch 67/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6261 - accuracy: 0.7395 - val_loss: 0.9867 - val_accuracy: 0.5775\n","Epoch 68/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6239 - accuracy: 0.7388 - val_loss: 0.9355 - val_accuracy: 0.5723\n","Epoch 69/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6218 - accuracy: 0.7380 - val_loss: 0.9268 - val_accuracy: 0.5795\n","Epoch 70/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6304 - accuracy: 0.7287 - val_loss: 0.9587 - val_accuracy: 0.5723\n","Epoch 71/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6162 - accuracy: 0.7442 - val_loss: 1.0296 - val_accuracy: 0.5754\n","Epoch 72/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6387 - accuracy: 0.7269 - val_loss: 1.0211 - val_accuracy: 0.5589\n","Epoch 73/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6202 - accuracy: 0.7421 - val_loss: 0.9563 - val_accuracy: 0.5764\n","Epoch 74/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6264 - accuracy: 0.7413 - val_loss: 0.9376 - val_accuracy: 0.5868\n","Epoch 75/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6178 - accuracy: 0.7481 - val_loss: 0.9872 - val_accuracy: 0.5795\n","Epoch 76/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6170 - accuracy: 0.7488 - val_loss: 0.9488 - val_accuracy: 0.5992\n","Epoch 77/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6116 - accuracy: 0.7434 - val_loss: 0.9696 - val_accuracy: 0.5806\n","Epoch 78/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6165 - accuracy: 0.7470 - val_loss: 0.9830 - val_accuracy: 0.5785\n","Epoch 79/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6105 - accuracy: 0.7525 - val_loss: 0.9820 - val_accuracy: 0.6033\n","Epoch 80/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5976 - accuracy: 0.7623 - val_loss: 1.0745 - val_accuracy: 0.5610\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6177 - accuracy: 0.7406 - val_loss: 0.9859 - val_accuracy: 0.5548\n","Epoch 82/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6033 - accuracy: 0.7468 - val_loss: 1.0194 - val_accuracy: 0.5868\n","Epoch 83/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6147 - accuracy: 0.7465 - val_loss: 0.9559 - val_accuracy: 0.5764\n","Epoch 84/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6083 - accuracy: 0.7563 - val_loss: 0.9826 - val_accuracy: 0.5806\n","Epoch 85/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6015 - accuracy: 0.7540 - val_loss: 0.9332 - val_accuracy: 0.5816\n","Epoch 86/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5998 - accuracy: 0.7499 - val_loss: 1.1229 - val_accuracy: 0.5589\n","Epoch 87/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.6043 - accuracy: 0.7481 - val_loss: 1.0276 - val_accuracy: 0.5981\n","Epoch 88/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5952 - accuracy: 0.7690 - val_loss: 0.9445 - val_accuracy: 0.5837\n","Epoch 89/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5895 - accuracy: 0.7605 - val_loss: 0.9460 - val_accuracy: 0.5682\n","Epoch 90/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.6036 - accuracy: 0.7561 - val_loss: 0.9606 - val_accuracy: 0.5816\n","Epoch 91/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5918 - accuracy: 0.7597 - val_loss: 0.9654 - val_accuracy: 0.5919\n","Epoch 92/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5897 - accuracy: 0.7677 - val_loss: 1.0345 - val_accuracy: 0.5785\n","Epoch 93/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5875 - accuracy: 0.7592 - val_loss: 1.0499 - val_accuracy: 0.5764\n","Epoch 94/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5940 - accuracy: 0.7568 - val_loss: 0.9741 - val_accuracy: 0.5692\n","Epoch 95/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5966 - accuracy: 0.7599 - val_loss: 1.0331 - val_accuracy: 0.5764\n","Epoch 96/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5902 - accuracy: 0.7599 - val_loss: 1.0456 - val_accuracy: 0.5661\n","Epoch 97/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5910 - accuracy: 0.7636 - val_loss: 1.0045 - val_accuracy: 0.5589\n","Epoch 98/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5775 - accuracy: 0.7775 - val_loss: 0.9930 - val_accuracy: 0.5775\n","Epoch 99/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5768 - accuracy: 0.7669 - val_loss: 1.0610 - val_accuracy: 0.5826\n","Epoch 100/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5925 - accuracy: 0.7571 - val_loss: 1.0112 - val_accuracy: 0.5857\n","{'loss': [0.7639893293380737, 0.7529287338256836, 0.7512808442115784, 0.7492998838424683, 0.7582660913467407, 0.7425476908683777, 0.7415410876274109, 0.7395451664924622, 0.7325040698051453, 0.7291708588600159, 0.730856716632843, 0.733539879322052, 0.7209146022796631, 0.721935510635376, 0.7207567095756531, 0.7170832753181458, 0.7246320843696594, 0.71970134973526, 0.7179396748542786, 0.7095831036567688, 0.7151661515235901, 0.7136858105659485, 0.7065486311912537, 0.7027760148048401, 0.706219494342804, 0.7000091075897217, 0.6957327723503113, 0.6939741969108582, 0.6958569288253784, 0.692361056804657, 0.6903228759765625, 0.692223310470581, 0.6980549693107605, 0.6857312321662903, 0.682997465133667, 0.679651141166687, 0.6788563132286072, 0.6835814714431763, 0.6794931888580322, 0.6734650731086731, 0.6736992001533508, 0.6651115417480469, 0.6684117913246155, 0.6655623316764832, 0.6606725454330444, 0.666630208492279, 0.6654070615768433, 0.6602416038513184, 0.6634085178375244, 0.6570918560028076, 0.6542505621910095, 0.6535701751708984, 0.6483006477355957, 0.6498470306396484, 0.6582561731338501, 0.6411006450653076, 0.6471995711326599, 0.6562185287475586, 0.6460420489311218, 0.6409751176834106, 0.6336086392402649, 0.6418360471725464, 0.6273173093795776, 0.6268466711044312, 0.6412582993507385, 0.6442616581916809, 0.6260595321655273, 0.6239057183265686, 0.6217864155769348, 0.6304095983505249, 0.616248369216919, 0.6387463808059692, 0.6201936602592468, 0.6263710260391235, 0.6177515387535095, 0.6169937252998352, 0.6116306185722351, 0.616517961025238, 0.6105107665061951, 0.5975837707519531, 0.6177400946617126, 0.6033011078834534, 0.6147112250328064, 0.6083230376243591, 0.6015239953994751, 0.599773645401001, 0.6042723059654236, 0.5951668620109558, 0.5894938707351685, 0.6036115884780884, 0.5917907357215881, 0.589737057685852, 0.5874897837638855, 0.5939728021621704, 0.5966400504112244, 0.5902336239814758, 0.5909562706947327, 0.5774734020233154, 0.5767632126808167, 0.5925106406211853], 'accuracy': [0.6764857769012451, 0.6762273907661438, 0.6813953518867493, 0.6759690046310425, 0.6710594296455383, 0.6834625601768494, 0.6875969171524048, 0.6785529851913452, 0.6909560561180115, 0.6881136894226074, 0.6940568685531616, 0.683979332447052, 0.6919896602630615, 0.6925064325332642, 0.6937984228134155, 0.6948320269584656, 0.6945736408233643, 0.6901808977127075, 0.6888889074325562, 0.7051679491996765, 0.6925064325332642, 0.7002583742141724, 0.6940568685531616, 0.7074935436248779, 0.6912144422531128, 0.6956072449684143, 0.711369514465332, 0.7126615047454834, 0.7080103158950806, 0.7111111283302307, 0.7152454853057861, 0.7067183256149292, 0.7010335922241211, 0.7124031186103821, 0.7118862867355347, 0.7118862867355347, 0.7111111283302307, 0.710335910320282, 0.7198966145515442, 0.7180878520011902, 0.7160206437110901, 0.7260981798171997, 0.7165374755859375, 0.7255814075469971, 0.733074963092804, 0.7242894172668457, 0.7240310311317444, 0.7224805951118469, 0.713178277015686, 0.7242894172668457, 0.7214470505714417, 0.724547803401947, 0.7273901700973511, 0.7281653881072998, 0.7240310311317444, 0.7341085076332092, 0.7284237742424011, 0.7302325367927551, 0.7320413589477539, 0.7281653881072998, 0.7343669533729553, 0.735917329788208, 0.7439276576042175, 0.7478036284446716, 0.7279070019721985, 0.7294573783874512, 0.739534854888916, 0.7387596964836121, 0.7379844784736633, 0.7286821603775024, 0.7441860437393188, 0.7268733978271484, 0.7421188354492188, 0.7413436770439148, 0.748062014579773, 0.7488372325897217, 0.7434108257293701, 0.7470284104347229, 0.7524547576904297, 0.762273907661438, 0.7405684590339661, 0.7467700242996216, 0.7465116381645203, 0.7563307285308838, 0.7540051937103271, 0.749870777130127, 0.748062014579773, 0.7689922451972961, 0.760465145111084, 0.7560723423957825, 0.7596899271011353, 0.7677002549171448, 0.7591731548309326, 0.7568475604057312, 0.7599483132362366, 0.7599483132362366, 0.7635658979415894, 0.7775194048881531, 0.766925036907196, 0.7571059465408325], 'val_loss': [0.8733799457550049, 0.8800119161605835, 0.8779622912406921, 0.8756328821182251, 0.864227294921875, 0.8724036812782288, 0.8559613823890686, 0.8635319471359253, 0.8505152463912964, 0.8458566665649414, 0.8531395792961121, 0.8398491740226746, 0.8383241891860962, 0.8412107825279236, 0.8406989574432373, 0.8320799469947815, 0.8280186653137207, 0.8262537121772766, 0.8396106958389282, 0.8519391417503357, 0.837729811668396, 0.8462405800819397, 0.8642139434814453, 0.8712870478630066, 0.8547751307487488, 0.8693563342094421, 0.9102583527565002, 0.9345957040786743, 0.9000295996665955, 0.8706233501434326, 0.9431390166282654, 0.8932471871376038, 0.893969714641571, 0.8797203302383423, 0.9294149875640869, 0.9221572875976562, 0.9140506982803345, 0.9044626951217651, 0.9038416147232056, 0.8846808075904846, 0.8838180303573608, 0.9286963939666748, 0.8800015449523926, 0.8991277813911438, 0.9201260805130005, 0.9517178535461426, 0.8972998261451721, 0.9083709716796875, 0.8950850963592529, 0.8980575203895569, 0.888159990310669, 0.9092711210250854, 0.8969780206680298, 0.936263918876648, 0.9261005520820618, 0.9342224597930908, 0.9150566458702087, 0.8812814354896545, 0.9498713612556458, 0.960831344127655, 0.9283618330955505, 0.9704532027244568, 0.9062918424606323, 0.9273799061775208, 0.9664840698242188, 0.9602982997894287, 0.9867400527000427, 0.9354717135429382, 0.9267671704292297, 0.9587485194206238, 1.0295623540878296, 1.0211398601531982, 0.9562788605690002, 0.9376237392425537, 0.9871750473976135, 0.9488369226455688, 0.9695579409599304, 0.9830073714256287, 0.9819509983062744, 1.0744562149047852, 0.9859083890914917, 1.0194023847579956, 0.9559413194656372, 0.9826326966285706, 0.9332035779953003, 1.1228772401809692, 1.027556300163269, 0.9444507956504822, 0.9460182785987854, 0.960593044757843, 0.965379536151886, 1.0345395803451538, 1.0499083995819092, 0.9740856289863586, 1.0331405401229858, 1.0456217527389526, 1.0044870376586914, 0.9929969310760498, 1.0610370635986328, 1.011248230934143], 'val_accuracy': [0.51962810754776, 0.5175619721412659, 0.5165289044380188, 0.51962810754776, 0.5351239442825317, 0.5237603187561035, 0.5650826692581177, 0.5361570119857788, 0.5599173307418823, 0.5764462947845459, 0.5433884263038635, 0.586776852607727, 0.5898760557174683, 0.5692148804664612, 0.5702479481697083, 0.5929751992225647, 0.5836777091026306, 0.5764462947845459, 0.5960744023323059, 0.55888432264328, 0.5805785059928894, 0.5919421315193176, 0.586776852607727, 0.5950413346290588, 0.5888429880142212, 0.5950413346290588, 0.586776852607727, 0.5599173307418823, 0.586776852607727, 0.5785123705863953, 0.5733470916748047, 0.5888429880142212, 0.586776852607727, 0.5836777091026306, 0.55888432264328, 0.5981404781341553, 0.5816115736961365, 0.5836777091026306, 0.5878099203109741, 0.5816115736961365, 0.5826446413993835, 0.6022727489471436, 0.5919421315193176, 0.5919421315193176, 0.5650826692581177, 0.5805785059928894, 0.58574378490448, 0.5878099203109741, 0.5826446413993835, 0.5940082669258118, 0.5836777091026306, 0.5712810158729553, 0.5723140239715576, 0.5723140239715576, 0.5785123705863953, 0.5950413346290588, 0.5568181872367859, 0.5898760557174683, 0.5692148804664612, 0.5609503984451294, 0.567148745059967, 0.5650826692581177, 0.60537189245224, 0.5805785059928894, 0.5847107172012329, 0.5805785059928894, 0.577479362487793, 0.5723140239715576, 0.5795454382896423, 0.5723140239715576, 0.5754132270812988, 0.55888432264328, 0.5764462947845459, 0.586776852607727, 0.5795454382896423, 0.5991735458374023, 0.5805785059928894, 0.5785123705863953, 0.6033057570457458, 0.5609503984451294, 0.5547520518302917, 0.586776852607727, 0.5764462947845459, 0.5805785059928894, 0.5816115736961365, 0.55888432264328, 0.5981404781341553, 0.5836777091026306, 0.5681818127632141, 0.5816115736961365, 0.5919421315193176, 0.5785123705863953, 0.5764462947845459, 0.5692148804664612, 0.5764462947845459, 0.56611567735672, 0.55888432264328, 0.577479362487793, 0.5826446413993835, 0.58574378490448]}\n","32/32 [==============================] - 1s 5ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/29 [===========================>..] - ETA: 0s - loss: 0.6469 - accuracy: 0.7324"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 7s 56ms/step - loss: 0.6434 - accuracy: 0.7352 - val_loss: 0.8212 - val_accuracy: 0.5032\n","Epoch 2/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.6201 - accuracy: 0.7373 - val_loss: 0.8159 - val_accuracy: 0.5862\n","Epoch 3/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6216 - accuracy: 0.7381 - val_loss: 0.8155 - val_accuracy: 0.5841\n","Epoch 4/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6046 - accuracy: 0.7497 - val_loss: 0.8134 - val_accuracy: 0.5550\n","Epoch 5/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6047 - accuracy: 0.7414 - val_loss: 0.8164 - val_accuracy: 0.5129\n","Epoch 6/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.5976 - accuracy: 0.7619 - val_loss: 0.8108 - val_accuracy: 0.6164\n","Epoch 7/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5944 - accuracy: 0.7562 - val_loss: 0.8118 - val_accuracy: 0.6013\n","Epoch 8/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.6075 - accuracy: 0.7586 - val_loss: 0.8100 - val_accuracy: 0.5884\n","Epoch 9/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5929 - accuracy: 0.7546 - val_loss: 0.7990 - val_accuracy: 0.6013\n","Epoch 10/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5860 - accuracy: 0.7670 - val_loss: 0.7999 - val_accuracy: 0.5959\n","Epoch 11/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5989 - accuracy: 0.7503 - val_loss: 0.8200 - val_accuracy: 0.5787\n","Epoch 12/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.5898 - accuracy: 0.7602 - val_loss: 0.8143 - val_accuracy: 0.6261\n","Epoch 13/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5835 - accuracy: 0.7581 - val_loss: 0.8098 - val_accuracy: 0.6013\n","Epoch 14/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5841 - accuracy: 0.7680 - val_loss: 0.8159 - val_accuracy: 0.5938\n","Epoch 15/100\n","29/29 [==============================] - 1s 29ms/step - loss: 0.5776 - accuracy: 0.7586 - val_loss: 0.7857 - val_accuracy: 0.6466\n","Epoch 16/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5741 - accuracy: 0.7645 - val_loss: 0.8078 - val_accuracy: 0.6390\n","Epoch 17/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.5687 - accuracy: 0.7772 - val_loss: 0.7986 - val_accuracy: 0.6519\n","Epoch 18/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5674 - accuracy: 0.7734 - val_loss: 0.8192 - val_accuracy: 0.6401\n","Epoch 19/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.5783 - accuracy: 0.7627 - val_loss: 0.8026 - val_accuracy: 0.6670\n","Epoch 20/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5763 - accuracy: 0.7713 - val_loss: 0.8943 - val_accuracy: 0.5765\n","Epoch 21/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5728 - accuracy: 0.7664 - val_loss: 0.8396 - val_accuracy: 0.6131\n","Epoch 22/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5743 - accuracy: 0.7654 - val_loss: 0.8184 - val_accuracy: 0.6616\n","Epoch 23/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5687 - accuracy: 0.7678 - val_loss: 0.8179 - val_accuracy: 0.6530\n","Epoch 24/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5673 - accuracy: 0.7734 - val_loss: 0.8640 - val_accuracy: 0.6282\n","Epoch 25/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5581 - accuracy: 0.7872 - val_loss: 0.8621 - val_accuracy: 0.6476\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5598 - accuracy: 0.7788 - val_loss: 0.8599 - val_accuracy: 0.6670\n","Epoch 27/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.5657 - accuracy: 0.7753 - val_loss: 0.8453 - val_accuracy: 0.6713\n","Epoch 28/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5632 - accuracy: 0.7705 - val_loss: 0.8662 - val_accuracy: 0.6681\n","Epoch 29/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5586 - accuracy: 0.7794 - val_loss: 0.8855 - val_accuracy: 0.6476\n","Epoch 30/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5633 - accuracy: 0.7718 - val_loss: 0.8686 - val_accuracy: 0.6692\n","Epoch 31/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5520 - accuracy: 0.7796 - val_loss: 0.9298 - val_accuracy: 0.6606\n","Epoch 32/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5533 - accuracy: 0.7794 - val_loss: 0.8813 - val_accuracy: 0.6681\n","Epoch 33/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5479 - accuracy: 0.7802 - val_loss: 0.9040 - val_accuracy: 0.6444\n","Epoch 34/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5393 - accuracy: 0.7883 - val_loss: 0.9104 - val_accuracy: 0.6315\n","Epoch 35/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.5444 - accuracy: 0.7856 - val_loss: 0.8889 - val_accuracy: 0.6735\n","Epoch 36/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5555 - accuracy: 0.7788 - val_loss: 0.8840 - val_accuracy: 0.6692\n","Epoch 37/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5520 - accuracy: 0.7858 - val_loss: 0.8866 - val_accuracy: 0.6724\n","Epoch 38/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5411 - accuracy: 0.7877 - val_loss: 0.8952 - val_accuracy: 0.6692\n","Epoch 39/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5448 - accuracy: 0.7821 - val_loss: 0.9185 - val_accuracy: 0.6401\n","Epoch 40/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5366 - accuracy: 0.7904 - val_loss: 0.9018 - val_accuracy: 0.6724\n","Epoch 41/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5396 - accuracy: 0.7786 - val_loss: 0.9289 - val_accuracy: 0.6412\n","Epoch 42/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5367 - accuracy: 0.7912 - val_loss: 0.9216 - val_accuracy: 0.6681\n","Epoch 43/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5566 - accuracy: 0.7707 - val_loss: 1.1634 - val_accuracy: 0.5700\n","Epoch 44/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5303 - accuracy: 0.7934 - val_loss: 0.8827 - val_accuracy: 0.6649\n","Epoch 45/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5248 - accuracy: 0.8071 - val_loss: 0.9339 - val_accuracy: 0.6455\n","Epoch 46/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5299 - accuracy: 0.7996 - val_loss: 0.9524 - val_accuracy: 0.6616\n","Epoch 47/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5278 - accuracy: 0.7955 - val_loss: 0.9204 - val_accuracy: 0.6703\n","Epoch 48/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5358 - accuracy: 0.7842 - val_loss: 1.0156 - val_accuracy: 0.6412\n","Epoch 49/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5194 - accuracy: 0.8025 - val_loss: 0.9396 - val_accuracy: 0.6681\n","Epoch 50/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5250 - accuracy: 0.7934 - val_loss: 1.0623 - val_accuracy: 0.5905\n","Epoch 51/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.5368 - accuracy: 0.7864 - val_loss: 1.0872 - val_accuracy: 0.5991\n","Epoch 52/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5336 - accuracy: 0.7853 - val_loss: 0.9392 - val_accuracy: 0.6616\n","Epoch 53/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5233 - accuracy: 0.7901 - val_loss: 1.1015 - val_accuracy: 0.5959\n","Epoch 54/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5199 - accuracy: 0.8047 - val_loss: 0.9407 - val_accuracy: 0.6670\n","Epoch 55/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5123 - accuracy: 0.8020 - val_loss: 0.9437 - val_accuracy: 0.6606\n","Epoch 56/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5200 - accuracy: 0.8012 - val_loss: 1.0565 - val_accuracy: 0.5927\n","Epoch 57/100\n","29/29 [==============================] - 2s 62ms/step - loss: 0.5118 - accuracy: 0.8055 - val_loss: 0.9337 - val_accuracy: 0.6756\n","Epoch 58/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5174 - accuracy: 0.8006 - val_loss: 0.9760 - val_accuracy: 0.6498\n","Epoch 59/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5147 - accuracy: 0.8017 - val_loss: 1.0274 - val_accuracy: 0.6498\n","Epoch 60/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5054 - accuracy: 0.8120 - val_loss: 0.9667 - val_accuracy: 0.6552\n","Epoch 61/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5084 - accuracy: 0.8120 - val_loss: 1.0033 - val_accuracy: 0.6638\n","Epoch 62/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5061 - accuracy: 0.8060 - val_loss: 0.9428 - val_accuracy: 0.6595\n","Epoch 63/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5088 - accuracy: 0.8093 - val_loss: 1.0454 - val_accuracy: 0.6196\n","Epoch 64/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5197 - accuracy: 0.7923 - val_loss: 0.9346 - val_accuracy: 0.6724\n","Epoch 65/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4923 - accuracy: 0.8160 - val_loss: 0.9504 - val_accuracy: 0.6552\n","Epoch 66/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5098 - accuracy: 0.8052 - val_loss: 1.0155 - val_accuracy: 0.6562\n","Epoch 67/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5158 - accuracy: 0.8028 - val_loss: 1.1423 - val_accuracy: 0.5830\n","Epoch 68/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5187 - accuracy: 0.7947 - val_loss: 0.9644 - val_accuracy: 0.6573\n","Epoch 69/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.5114 - accuracy: 0.7950 - val_loss: 0.9653 - val_accuracy: 0.6703\n","Epoch 70/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.5053 - accuracy: 0.8031 - val_loss: 1.0609 - val_accuracy: 0.5991\n","Epoch 71/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4960 - accuracy: 0.8106 - val_loss: 0.9642 - val_accuracy: 0.6562\n","Epoch 72/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4981 - accuracy: 0.8165 - val_loss: 1.2145 - val_accuracy: 0.5690\n","Epoch 73/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4919 - accuracy: 0.8117 - val_loss: 0.9949 - val_accuracy: 0.6369\n","Epoch 74/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4883 - accuracy: 0.8168 - val_loss: 0.9880 - val_accuracy: 0.6692\n","Epoch 75/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4977 - accuracy: 0.8122 - val_loss: 1.0314 - val_accuracy: 0.6573\n","Epoch 76/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4864 - accuracy: 0.8238 - val_loss: 0.9852 - val_accuracy: 0.6713\n","Epoch 77/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4875 - accuracy: 0.8163 - val_loss: 0.9723 - val_accuracy: 0.6541\n","Epoch 78/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4965 - accuracy: 0.8165 - val_loss: 1.0251 - val_accuracy: 0.6703\n","Epoch 79/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4869 - accuracy: 0.8176 - val_loss: 1.0102 - val_accuracy: 0.6724\n","Epoch 80/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4799 - accuracy: 0.8289 - val_loss: 1.0622 - val_accuracy: 0.6131\n","Epoch 81/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4897 - accuracy: 0.8165 - val_loss: 1.0268 - val_accuracy: 0.6519\n","Epoch 82/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4892 - accuracy: 0.8176 - val_loss: 1.0891 - val_accuracy: 0.6067\n","Epoch 83/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4806 - accuracy: 0.8179 - val_loss: 1.1323 - val_accuracy: 0.5970\n","Epoch 84/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4718 - accuracy: 0.8257 - val_loss: 1.0376 - val_accuracy: 0.6390\n","Epoch 85/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4862 - accuracy: 0.8074 - val_loss: 1.0709 - val_accuracy: 0.6444\n","Epoch 86/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4857 - accuracy: 0.8144 - val_loss: 1.0503 - val_accuracy: 0.6422\n","Epoch 87/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4905 - accuracy: 0.8114 - val_loss: 1.1717 - val_accuracy: 0.6293\n","Epoch 88/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4964 - accuracy: 0.8101 - val_loss: 1.0240 - val_accuracy: 0.6724\n","Epoch 89/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4754 - accuracy: 0.8214 - val_loss: 1.0136 - val_accuracy: 0.6670\n","Epoch 90/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4762 - accuracy: 0.8303 - val_loss: 1.1551 - val_accuracy: 0.6455\n","Epoch 91/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4665 - accuracy: 0.8332 - val_loss: 1.0941 - val_accuracy: 0.6196\n","Epoch 92/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4594 - accuracy: 0.8338 - val_loss: 1.1062 - val_accuracy: 0.6153\n","Epoch 93/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4709 - accuracy: 0.8314 - val_loss: 1.1340 - val_accuracy: 0.6519\n","Epoch 94/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4734 - accuracy: 0.8273 - val_loss: 1.1219 - val_accuracy: 0.6562\n","Epoch 95/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4690 - accuracy: 0.8322 - val_loss: 1.0882 - val_accuracy: 0.6325\n","Epoch 96/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4667 - accuracy: 0.8308 - val_loss: 1.0321 - val_accuracy: 0.6638\n","Epoch 97/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4578 - accuracy: 0.8281 - val_loss: 1.0821 - val_accuracy: 0.6455\n","Epoch 98/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4545 - accuracy: 0.8357 - val_loss: 1.0706 - val_accuracy: 0.6735\n","Epoch 99/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4773 - accuracy: 0.8200 - val_loss: 1.0497 - val_accuracy: 0.6584\n","Epoch 100/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4555 - accuracy: 0.8378 - val_loss: 1.0462 - val_accuracy: 0.6606\n","{'loss': [0.6434428691864014, 0.6200600266456604, 0.6215614080429077, 0.6045644879341125, 0.6046952605247498, 0.5976282358169556, 0.5944259762763977, 0.6075238585472107, 0.5928521156311035, 0.5860366225242615, 0.5988589525222778, 0.5898005366325378, 0.583480179309845, 0.58409184217453, 0.57756507396698, 0.5740978717803955, 0.5686765909194946, 0.5673732161521912, 0.5783370733261108, 0.576320469379425, 0.5728371739387512, 0.5743306875228882, 0.5686856508255005, 0.5672850012779236, 0.5581194162368774, 0.5597884058952332, 0.5657124519348145, 0.5631614327430725, 0.5585786700248718, 0.5633072257041931, 0.5519954562187195, 0.5533112287521362, 0.5478630065917969, 0.5392591953277588, 0.544393002986908, 0.5554884672164917, 0.551953911781311, 0.5411301255226135, 0.5448423624038696, 0.5366265177726746, 0.5395602583885193, 0.5366719961166382, 0.5566083788871765, 0.5302596688270569, 0.524849534034729, 0.5299153327941895, 0.5278136730194092, 0.5358078479766846, 0.5194171667098999, 0.5250291228294373, 0.5368187427520752, 0.5335721373558044, 0.5232663154602051, 0.5198856592178345, 0.5122839212417603, 0.5200340747833252, 0.5117976665496826, 0.5174033045768738, 0.5147297382354736, 0.5054003596305847, 0.5084071159362793, 0.5061435103416443, 0.5087794065475464, 0.5197480320930481, 0.49234262108802795, 0.5098437666893005, 0.5157516002655029, 0.5186662077903748, 0.5113843083381653, 0.5052935481071472, 0.49596959352493286, 0.49813616275787354, 0.49187394976615906, 0.48830410838127136, 0.49770936369895935, 0.48638468980789185, 0.4874766767024994, 0.49653494358062744, 0.48687389492988586, 0.47994816303253174, 0.4897395968437195, 0.48919832706451416, 0.48055049777030945, 0.47180694341659546, 0.486182302236557, 0.4857166111469269, 0.49047383666038513, 0.49644139409065247, 0.47543761134147644, 0.4761747419834137, 0.466497004032135, 0.4594205319881439, 0.4709363877773285, 0.4733602702617645, 0.46895381808280945, 0.4667120575904846, 0.4577674865722656, 0.45452597737312317, 0.4772605895996094, 0.45545902848243713], 'accuracy': [0.7351831793785095, 0.7373383641242981, 0.7381465435028076, 0.7497305870056152, 0.7413793206214905, 0.7618534564971924, 0.756196141242981, 0.7586206793785095, 0.7545797228813171, 0.766972005367279, 0.7502694129943848, 0.7602370977401733, 0.7580819129943848, 0.7680495977401733, 0.7586206793785095, 0.7645474076271057, 0.7772090435028076, 0.7734375, 0.7626616358757019, 0.7712823152542114, 0.7664331793785095, 0.7653555870056152, 0.7677801847457886, 0.7734375, 0.7871767282485962, 0.7788254022598267, 0.7753232717514038, 0.7704741358757019, 0.7793642282485962, 0.771821141242981, 0.779633641242981, 0.7793642282485962, 0.7801724076271057, 0.7882543206214905, 0.7855603694915771, 0.7788254022598267, 0.7858297228813171, 0.787715494632721, 0.7820581793785095, 0.790409505367279, 0.7785560488700867, 0.7912176847457886, 0.7707435488700867, 0.7933728694915771, 0.8071120977401733, 0.7995689511299133, 0.795527994632721, 0.7842133641242981, 0.8025323152542114, 0.7933728694915771, 0.7863685488700867, 0.7852909564971924, 0.7901400923728943, 0.8046875, 0.8019935488700867, 0.8011853694915771, 0.8054956793785095, 0.8006465435028076, 0.8017241358757019, 0.8119612336158752, 0.8119612336158752, 0.806034505367279, 0.8092672228813171, 0.7922952771186829, 0.8160021305084229, 0.8052262663841248, 0.8028017282485962, 0.7947198152542114, 0.7949892282485962, 0.803071141242981, 0.8106142282485962, 0.8165409564971924, 0.8116918206214905, 0.8168103694915771, 0.8122305870056152, 0.8238146305084229, 0.8162715435028076, 0.8165409564971924, 0.8176185488700867, 0.8289331793785095, 0.8165409564971924, 0.8176185488700867, 0.8178879022598267, 0.8257004022598267, 0.8073814511299133, 0.8143857717514038, 0.8114224076271057, 0.8100754022598267, 0.8213900923728943, 0.8302801847457886, 0.8332435488700867, 0.8337823152542114, 0.8313577771186829, 0.8273168206214905, 0.8321659564971924, 0.8308189511299133, 0.828125, 0.8356680870056152, 0.8200430870056152, 0.8378232717514038], 'val_loss': [0.8212098479270935, 0.8158923387527466, 0.8155261874198914, 0.8133997917175293, 0.816439688205719, 0.8107930421829224, 0.8117884993553162, 0.8099581003189087, 0.7989641427993774, 0.7999086976051331, 0.8199918270111084, 0.8143203854560852, 0.8098222017288208, 0.8159345388412476, 0.7857402563095093, 0.8077788352966309, 0.7985763549804688, 0.8192448616027832, 0.8025790452957153, 0.8942897319793701, 0.8395931124687195, 0.8183760046958923, 0.8179227709770203, 0.8639748096466064, 0.8621005415916443, 0.8599289655685425, 0.8453421592712402, 0.8661668300628662, 0.8854890465736389, 0.8685798048973083, 0.9297758340835571, 0.8813090920448303, 0.9039664268493652, 0.9103811979293823, 0.8889203071594238, 0.8840025067329407, 0.8865694403648376, 0.8952010869979858, 0.9185436367988586, 0.9017545580863953, 0.9288839101791382, 0.921601414680481, 1.1633881330490112, 0.8827363848686218, 0.9339064955711365, 0.9524397850036621, 0.9204220771789551, 1.015648365020752, 0.9395986199378967, 1.062287449836731, 1.0872279405593872, 0.9392090439796448, 1.1014646291732788, 0.9406795501708984, 0.9436817765235901, 1.056457281112671, 0.9337281584739685, 0.9760040044784546, 1.027416706085205, 0.9666550755500793, 1.0033354759216309, 0.9427733421325684, 1.0454273223876953, 0.9346497654914856, 0.9503895044326782, 1.015515923500061, 1.1423171758651733, 0.9643622636795044, 0.9652608633041382, 1.060902714729309, 0.9641695618629456, 1.2145278453826904, 0.9949198961257935, 0.9880185127258301, 1.031441569328308, 0.9851696491241455, 0.9723036289215088, 1.0250743627548218, 1.0102026462554932, 1.0622262954711914, 1.0267913341522217, 1.0891109704971313, 1.1323132514953613, 1.0375888347625732, 1.0709034204483032, 1.0503071546554565, 1.1717140674591064, 1.02399480342865, 1.0135527849197388, 1.155056357383728, 1.0941290855407715, 1.106188416481018, 1.1340054273605347, 1.1218911409378052, 1.0882301330566406, 1.032069206237793, 1.0820921659469604, 1.0706335306167603, 1.0497326850891113, 1.046196460723877], 'val_accuracy': [0.5032327771186829, 0.5862069129943848, 0.5840517282485962, 0.5549569129943848, 0.5129310488700867, 0.6163793206214905, 0.6012930870056152, 0.5883620977401733, 0.6012930870056152, 0.5959051847457886, 0.5786637663841248, 0.6260775923728943, 0.6012930870056152, 0.59375, 0.6465517282485962, 0.639008641242981, 0.6519396305084229, 0.6400862336158752, 0.6670258641242981, 0.576508641242981, 0.6131465435028076, 0.6616379022598267, 0.6530172228813171, 0.6282327771186829, 0.6476293206214905, 0.6670258641242981, 0.6713362336158752, 0.6681034564971924, 0.6476293206214905, 0.6691810488700867, 0.6605603694915771, 0.6681034564971924, 0.6443965435028076, 0.631465494632721, 0.673491358757019, 0.6691810488700867, 0.6724137663841248, 0.6691810488700867, 0.6400862336158752, 0.6724137663841248, 0.6411637663841248, 0.6681034564971924, 0.5700430870056152, 0.6648706793785095, 0.6454741358757019, 0.6616379022598267, 0.670258641242981, 0.6411637663841248, 0.6681034564971924, 0.5905172228813171, 0.5991379022598267, 0.6616379022598267, 0.5959051847457886, 0.6670258641242981, 0.6605603694915771, 0.5926724076271057, 0.6756465435028076, 0.649784505367279, 0.649784505367279, 0.6551724076271057, 0.6637930870056152, 0.6594827771186829, 0.6196120977401733, 0.6724137663841248, 0.6551724076271057, 0.65625, 0.5829741358757019, 0.6573275923728943, 0.670258641242981, 0.5991379022598267, 0.65625, 0.568965494632721, 0.6368534564971924, 0.6691810488700867, 0.6573275923728943, 0.6713362336158752, 0.6540948152542114, 0.670258641242981, 0.6724137663841248, 0.6131465435028076, 0.6519396305084229, 0.6066810488700867, 0.5969827771186829, 0.639008641242981, 0.6443965435028076, 0.642241358757019, 0.6293103694915771, 0.6724137663841248, 0.6670258641242981, 0.6454741358757019, 0.6196120977401733, 0.6153017282485962, 0.6519396305084229, 0.65625, 0.6325430870056152, 0.6637930870056152, 0.6454741358757019, 0.673491358757019, 0.6584051847457886, 0.6605603694915771]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - ETA: 0s - loss: 0.6301 - accuracy: 0.7487"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 9s 58ms/step - loss: 0.6301 - accuracy: 0.7487 - val_loss: 0.8117 - val_accuracy: 0.5566\n","Epoch 2/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.6083 - accuracy: 0.7439 - val_loss: 0.8086 - val_accuracy: 0.5837\n","Epoch 3/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6098 - accuracy: 0.7564 - val_loss: 0.8284 - val_accuracy: 0.5204\n","Epoch 4/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.6067 - accuracy: 0.7612 - val_loss: 0.8054 - val_accuracy: 0.6109\n","Epoch 5/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6023 - accuracy: 0.7620 - val_loss: 0.8012 - val_accuracy: 0.5701\n","Epoch 6/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5975 - accuracy: 0.7629 - val_loss: 0.7971 - val_accuracy: 0.5554\n","Epoch 7/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5982 - accuracy: 0.7552 - val_loss: 0.8116 - val_accuracy: 0.5294\n","Epoch 8/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5946 - accuracy: 0.7586 - val_loss: 0.7869 - val_accuracy: 0.5995\n","Epoch 9/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6014 - accuracy: 0.7578 - val_loss: 0.7831 - val_accuracy: 0.5962\n","Epoch 10/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5915 - accuracy: 0.7651 - val_loss: 0.7859 - val_accuracy: 0.6335\n","Epoch 11/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5858 - accuracy: 0.7779 - val_loss: 0.7951 - val_accuracy: 0.5803\n","Epoch 12/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5814 - accuracy: 0.7722 - val_loss: 0.7701 - val_accuracy: 0.6244\n","Epoch 13/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5810 - accuracy: 0.7728 - val_loss: 0.7606 - val_accuracy: 0.6267\n","Epoch 14/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5899 - accuracy: 0.7708 - val_loss: 0.7700 - val_accuracy: 0.6063\n","Epoch 15/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5856 - accuracy: 0.7680 - val_loss: 0.7548 - val_accuracy: 0.6301\n","Epoch 16/100\n","28/28 [==============================] - 1s 30ms/step - loss: 0.5742 - accuracy: 0.7759 - val_loss: 0.7514 - val_accuracy: 0.6493\n","Epoch 17/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.5691 - accuracy: 0.7782 - val_loss: 0.7487 - val_accuracy: 0.6505\n","Epoch 18/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5664 - accuracy: 0.7779 - val_loss: 0.7599 - val_accuracy: 0.6233\n","Epoch 19/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5627 - accuracy: 0.7807 - val_loss: 0.7480 - val_accuracy: 0.6414\n","Epoch 20/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5839 - accuracy: 0.7736 - val_loss: 0.7786 - val_accuracy: 0.6290\n","Epoch 21/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5748 - accuracy: 0.7716 - val_loss: 0.7565 - val_accuracy: 0.6471\n","Epoch 22/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.5616 - accuracy: 0.7895 - val_loss: 0.7562 - val_accuracy: 0.6527\n","Epoch 23/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5683 - accuracy: 0.7804 - val_loss: 0.7793 - val_accuracy: 0.6538\n","Epoch 24/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5812 - accuracy: 0.7691 - val_loss: 0.8063 - val_accuracy: 0.6505\n","Epoch 25/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.5794 - accuracy: 0.7722 - val_loss: 0.7809 - val_accuracy: 0.6595\n","Epoch 26/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.5632 - accuracy: 0.7762 - val_loss: 0.7871 - val_accuracy: 0.6663\n","Epoch 27/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5646 - accuracy: 0.7762 - val_loss: 0.8404 - val_accuracy: 0.6663\n","Epoch 28/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5649 - accuracy: 0.7838 - val_loss: 0.8725 - val_accuracy: 0.6482\n","Epoch 29/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5595 - accuracy: 0.7915 - val_loss: 0.8628 - val_accuracy: 0.6663\n","Epoch 30/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5638 - accuracy: 0.7810 - val_loss: 0.8550 - val_accuracy: 0.6618\n","Epoch 31/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5601 - accuracy: 0.7844 - val_loss: 0.8404 - val_accuracy: 0.6652\n","Epoch 32/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5547 - accuracy: 0.7886 - val_loss: 0.8531 - val_accuracy: 0.6686\n","Epoch 33/100\n","28/28 [==============================] - 1s 17ms/step - loss: 0.5584 - accuracy: 0.7770 - val_loss: 0.8336 - val_accuracy: 0.6652\n","Epoch 34/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5534 - accuracy: 0.7847 - val_loss: 0.8838 - val_accuracy: 0.6335\n","Epoch 35/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5602 - accuracy: 0.7813 - val_loss: 0.8459 - val_accuracy: 0.6516\n","Epoch 36/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5528 - accuracy: 0.7889 - val_loss: 0.8227 - val_accuracy: 0.6606\n","Epoch 37/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5552 - accuracy: 0.7821 - val_loss: 0.9308 - val_accuracy: 0.6154\n","Epoch 38/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5505 - accuracy: 0.7968 - val_loss: 0.8587 - val_accuracy: 0.6550\n","Epoch 39/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5520 - accuracy: 0.7946 - val_loss: 0.8826 - val_accuracy: 0.6527\n","Epoch 40/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5460 - accuracy: 0.7917 - val_loss: 0.9292 - val_accuracy: 0.6459\n","Epoch 41/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5525 - accuracy: 0.7827 - val_loss: 0.8598 - val_accuracy: 0.6505\n","Epoch 42/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5390 - accuracy: 0.8016 - val_loss: 0.8633 - val_accuracy: 0.6640\n","Epoch 43/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5355 - accuracy: 0.7903 - val_loss: 0.8457 - val_accuracy: 0.6640\n","Epoch 44/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5451 - accuracy: 0.7965 - val_loss: 0.9081 - val_accuracy: 0.6516\n","Epoch 45/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5398 - accuracy: 0.7980 - val_loss: 0.8887 - val_accuracy: 0.6505\n","Epoch 46/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5436 - accuracy: 0.7847 - val_loss: 0.8783 - val_accuracy: 0.6572\n","Epoch 47/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5416 - accuracy: 0.7923 - val_loss: 0.9124 - val_accuracy: 0.6256\n","Epoch 48/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5451 - accuracy: 0.7917 - val_loss: 0.8561 - val_accuracy: 0.6538\n","Epoch 49/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5283 - accuracy: 0.7974 - val_loss: 0.8455 - val_accuracy: 0.6584\n","Epoch 50/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5449 - accuracy: 0.7940 - val_loss: 0.9336 - val_accuracy: 0.6561\n","Epoch 51/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5540 - accuracy: 0.7852 - val_loss: 0.9912 - val_accuracy: 0.6143\n","Epoch 52/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5584 - accuracy: 0.7742 - val_loss: 1.0982 - val_accuracy: 0.6007\n","Epoch 53/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5363 - accuracy: 0.7932 - val_loss: 1.0240 - val_accuracy: 0.6131\n","Epoch 54/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5368 - accuracy: 0.7971 - val_loss: 0.9405 - val_accuracy: 0.6154\n","Epoch 55/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5292 - accuracy: 0.7946 - val_loss: 0.8810 - val_accuracy: 0.6606\n","Epoch 56/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5220 - accuracy: 0.7965 - val_loss: 0.9031 - val_accuracy: 0.6538\n","Epoch 57/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5172 - accuracy: 0.8070 - val_loss: 0.9276 - val_accuracy: 0.6505\n","Epoch 58/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5312 - accuracy: 0.7999 - val_loss: 0.8702 - val_accuracy: 0.6459\n","Epoch 59/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5136 - accuracy: 0.8067 - val_loss: 0.9252 - val_accuracy: 0.6346\n","Epoch 60/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5122 - accuracy: 0.8115 - val_loss: 0.9092 - val_accuracy: 0.6584\n","Epoch 61/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5195 - accuracy: 0.8036 - val_loss: 0.9916 - val_accuracy: 0.6380\n","Epoch 62/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5134 - accuracy: 0.8121 - val_loss: 0.9103 - val_accuracy: 0.6640\n","Epoch 63/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5166 - accuracy: 0.8113 - val_loss: 0.9348 - val_accuracy: 0.6256\n","Epoch 64/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5053 - accuracy: 0.8149 - val_loss: 0.9291 - val_accuracy: 0.6493\n","Epoch 65/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5157 - accuracy: 0.8019 - val_loss: 0.9210 - val_accuracy: 0.6403\n","Epoch 66/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5103 - accuracy: 0.8081 - val_loss: 1.0455 - val_accuracy: 0.6256\n","Epoch 67/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5061 - accuracy: 0.8113 - val_loss: 0.9162 - val_accuracy: 0.6459\n","Epoch 68/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5111 - accuracy: 0.8076 - val_loss: 0.9296 - val_accuracy: 0.6674\n","Epoch 69/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5176 - accuracy: 0.8039 - val_loss: 0.9428 - val_accuracy: 0.6448\n","Epoch 70/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5073 - accuracy: 0.8138 - val_loss: 1.1678 - val_accuracy: 0.5984\n","Epoch 71/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5133 - accuracy: 0.8087 - val_loss: 0.9370 - val_accuracy: 0.6505\n","Epoch 72/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5149 - accuracy: 0.8098 - val_loss: 0.9033 - val_accuracy: 0.6595\n","Epoch 73/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5064 - accuracy: 0.8149 - val_loss: 1.0099 - val_accuracy: 0.6471\n","Epoch 74/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5155 - accuracy: 0.7994 - val_loss: 0.9478 - val_accuracy: 0.6448\n","Epoch 75/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5004 - accuracy: 0.8200 - val_loss: 1.0654 - val_accuracy: 0.6120\n","Epoch 76/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4963 - accuracy: 0.8152 - val_loss: 0.9843 - val_accuracy: 0.6652\n","Epoch 77/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5038 - accuracy: 0.8079 - val_loss: 0.9248 - val_accuracy: 0.6505\n","Epoch 78/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5050 - accuracy: 0.8093 - val_loss: 0.9248 - val_accuracy: 0.6527\n","Epoch 79/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5084 - accuracy: 0.8158 - val_loss: 0.9682 - val_accuracy: 0.6471\n","Epoch 80/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5009 - accuracy: 0.8186 - val_loss: 0.9212 - val_accuracy: 0.6516\n","Epoch 81/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4951 - accuracy: 0.8265 - val_loss: 0.9418 - val_accuracy: 0.6584\n","Epoch 82/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5044 - accuracy: 0.8149 - val_loss: 0.9845 - val_accuracy: 0.6414\n","Epoch 83/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4941 - accuracy: 0.8217 - val_loss: 0.9284 - val_accuracy: 0.6618\n","Epoch 84/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5060 - accuracy: 0.8175 - val_loss: 0.9440 - val_accuracy: 0.6538\n","Epoch 85/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5030 - accuracy: 0.8127 - val_loss: 0.9091 - val_accuracy: 0.6425\n","Epoch 86/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4828 - accuracy: 0.8263 - val_loss: 0.9396 - val_accuracy: 0.6448\n","Epoch 87/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4898 - accuracy: 0.8166 - val_loss: 1.0006 - val_accuracy: 0.6346\n","Epoch 88/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4902 - accuracy: 0.8237 - val_loss: 0.9585 - val_accuracy: 0.6572\n","Epoch 89/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4911 - accuracy: 0.8195 - val_loss: 1.0411 - val_accuracy: 0.6267\n","Epoch 90/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4855 - accuracy: 0.8234 - val_loss: 1.1019 - val_accuracy: 0.6109\n","Epoch 91/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4916 - accuracy: 0.8237 - val_loss: 1.0623 - val_accuracy: 0.6199\n","Epoch 92/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4951 - accuracy: 0.8178 - val_loss: 0.9598 - val_accuracy: 0.6391\n","Epoch 93/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.4823 - accuracy: 0.8291 - val_loss: 0.9547 - val_accuracy: 0.6471\n","Epoch 94/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4803 - accuracy: 0.8234 - val_loss: 1.0607 - val_accuracy: 0.6414\n","Epoch 95/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4714 - accuracy: 0.8339 - val_loss: 1.0006 - val_accuracy: 0.6380\n","Epoch 96/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4733 - accuracy: 0.8381 - val_loss: 1.0096 - val_accuracy: 0.6550\n","Epoch 97/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4830 - accuracy: 0.8285 - val_loss: 1.2956 - val_accuracy: 0.5916\n","Epoch 98/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4788 - accuracy: 0.8220 - val_loss: 0.9749 - val_accuracy: 0.6369\n","Epoch 99/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4627 - accuracy: 0.8314 - val_loss: 1.0140 - val_accuracy: 0.6493\n","Epoch 100/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4672 - accuracy: 0.8299 - val_loss: 1.0988 - val_accuracy: 0.6312\n","{'loss': [0.6301054954528809, 0.6083270907402039, 0.6097804307937622, 0.6066964268684387, 0.6023028492927551, 0.5975037217140198, 0.5981954336166382, 0.5945962071418762, 0.6013720631599426, 0.591469407081604, 0.5858262777328491, 0.5813575983047485, 0.5809766054153442, 0.5899019241333008, 0.5856086015701294, 0.5742321610450745, 0.5690885782241821, 0.5664061903953552, 0.5626617074012756, 0.5838963389396667, 0.5747733116149902, 0.5615648627281189, 0.5682925581932068, 0.5812093019485474, 0.5793634653091431, 0.5632128119468689, 0.5645714998245239, 0.564857542514801, 0.5595495104789734, 0.5638421177864075, 0.5601295232772827, 0.5546824336051941, 0.5583516359329224, 0.5534437298774719, 0.560210645198822, 0.5527969002723694, 0.5552112460136414, 0.5505126714706421, 0.551999032497406, 0.5459603071212769, 0.5524694323539734, 0.5390094518661499, 0.5355419516563416, 0.5451194047927856, 0.5398002862930298, 0.5436239242553711, 0.5416076183319092, 0.5451011657714844, 0.5282673835754395, 0.5449304580688477, 0.5539701581001282, 0.5583678483963013, 0.5363213419914246, 0.5368043780326843, 0.5292266011238098, 0.5220310091972351, 0.5172058343887329, 0.5312375426292419, 0.513567328453064, 0.5122154951095581, 0.5194633603096008, 0.5134067535400391, 0.5166091918945312, 0.5052629709243774, 0.5157132744789124, 0.5102857351303101, 0.5061324834823608, 0.5110734105110168, 0.5175896883010864, 0.5072875618934631, 0.5132821798324585, 0.5149211287498474, 0.5063576698303223, 0.5154615640640259, 0.5003882646560669, 0.49631187319755554, 0.5037676095962524, 0.5050230622291565, 0.5084127187728882, 0.5008955001831055, 0.49511051177978516, 0.5043664574623108, 0.4940713047981262, 0.5059705972671509, 0.5029650330543518, 0.48279353976249695, 0.4898439049720764, 0.4901864230632782, 0.49106141924858093, 0.48554110527038574, 0.49161770939826965, 0.4950995147228241, 0.4822816252708435, 0.4802626371383667, 0.4713621437549591, 0.473303884267807, 0.4829683005809784, 0.4787895679473877, 0.46269670128822327, 0.4671967327594757], 'accuracy': [0.7487266659736633, 0.7439162135124207, 0.7563667297363281, 0.761177122592926, 0.7620260119438171, 0.7628749012947083, 0.7552348375320435, 0.7586304545402527, 0.7577815651893616, 0.7651386260986328, 0.7778720855712891, 0.7722128033638, 0.7727787494659424, 0.7707979679107666, 0.7679682970046997, 0.7758913636207581, 0.7781550884246826, 0.7778720855712891, 0.780701756477356, 0.7736276388168335, 0.7716468572616577, 0.7894737124443054, 0.7804188132286072, 0.7691001892089844, 0.7722128033638, 0.7761743068695068, 0.7761743068695068, 0.7838143706321716, 0.7914544343948364, 0.7809846997261047, 0.784380316734314, 0.7886247634887695, 0.777023196220398, 0.7846632599830627, 0.7812677025794983, 0.7889077663421631, 0.7821165919303894, 0.7968307733535767, 0.7945670485496521, 0.79173743724823, 0.7826825380325317, 0.8016412258148193, 0.7903226017951965, 0.7965478301048279, 0.7979626655578613, 0.7846632599830627, 0.7923033237457275, 0.79173743724823, 0.797396719455719, 0.7940011024475098, 0.7852292060852051, 0.774193525314331, 0.7931522130966187, 0.7971137762069702, 0.7945670485496521, 0.7965478301048279, 0.8070175647735596, 0.7999433875083923, 0.806734561920166, 0.8115450143814087, 0.8036219477653503, 0.8121109008789062, 0.8112620115280151, 0.8149405717849731, 0.8019241690635681, 0.8081493973731995, 0.8112620115280151, 0.8075834512710571, 0.8039049506187439, 0.8138087391853333, 0.8087153434753418, 0.8098471760749817, 0.8149405717849731, 0.7993775010108948, 0.8200339674949646, 0.8152235150337219, 0.8078664541244507, 0.8092812895774841, 0.8157894611358643, 0.8186191320419312, 0.8265421390533447, 0.8149405717849731, 0.8217317461967468, 0.8174872398376465, 0.8126768469810486, 0.826259195804596, 0.8166383504867554, 0.8237125277519226, 0.8194680213928223, 0.823429524898529, 0.8237125277519226, 0.81777024269104, 0.8290888667106628, 0.823429524898529, 0.8338992595672607, 0.8381437659263611, 0.8285229206085205, 0.8220146894454956, 0.8313525915145874, 0.829937756061554], 'val_loss': [0.8117437362670898, 0.8086014986038208, 0.8283663988113403, 0.8053527474403381, 0.8011721968650818, 0.7971344590187073, 0.81156325340271, 0.7868775725364685, 0.7830626368522644, 0.7858706116676331, 0.7951139807701111, 0.7700955271720886, 0.760610044002533, 0.7700272798538208, 0.7548340559005737, 0.7513763904571533, 0.7487351894378662, 0.7599130272865295, 0.7479914426803589, 0.7785887122154236, 0.7565121650695801, 0.7561842203140259, 0.7793270349502563, 0.8063337802886963, 0.7809151411056519, 0.7870975732803345, 0.8404319882392883, 0.872519850730896, 0.8628085255622864, 0.8550336956977844, 0.8403542637825012, 0.8531228303909302, 0.8335567116737366, 0.8837549686431885, 0.8459386229515076, 0.822677731513977, 0.9307686686515808, 0.8587386608123779, 0.8826044797897339, 0.9292287230491638, 0.859793484210968, 0.8633437752723694, 0.8457368612289429, 0.9080564975738525, 0.8886702060699463, 0.8782737255096436, 0.9124481081962585, 0.8560698628425598, 0.8454839587211609, 0.9336299896240234, 0.9911862015724182, 1.0981550216674805, 1.0240329504013062, 0.9404861927032471, 0.8809890151023865, 0.9031425714492798, 0.9275674223899841, 0.8701606392860413, 0.9252108931541443, 0.9092385172843933, 0.991561770439148, 0.9102936387062073, 0.9347896575927734, 0.9291463494300842, 0.9210394024848938, 1.0455242395401, 0.9162413477897644, 0.9295772314071655, 0.942834734916687, 1.1677743196487427, 0.9369584918022156, 0.9032608866691589, 1.0099067687988281, 0.9477863311767578, 1.0654288530349731, 0.9843183755874634, 0.9248448610305786, 0.9248325824737549, 0.9682394862174988, 0.9211627244949341, 0.9418008327484131, 0.9845302104949951, 0.9283978343009949, 0.9440075755119324, 0.909084677696228, 0.939559817314148, 1.0005838871002197, 0.9585449695587158, 1.0410653352737427, 1.1019327640533447, 1.0623164176940918, 0.9597976207733154, 0.9547008275985718, 1.0607157945632935, 1.0006201267242432, 1.009636402130127, 1.2955868244171143, 0.9748972654342651, 1.014019250869751, 1.0988140106201172], 'val_accuracy': [0.5565611124038696, 0.5837104320526123, 0.5203620195388794, 0.610859751701355, 0.570135772228241, 0.5554298758506775, 0.529411792755127, 0.5995475053787231, 0.5961538553237915, 0.6334841847419739, 0.5803167223930359, 0.6244344115257263, 0.6266968250274658, 0.6063348650932312, 0.6300904750823975, 0.6493212580680847, 0.6504524946212769, 0.6233031749725342, 0.6414027214050293, 0.6289592981338501, 0.6470588445663452, 0.6527149081230164, 0.6538461446762085, 0.6504524946212769, 0.6595022678375244, 0.6662895679473877, 0.6662895679473877, 0.6481900215148926, 0.6662895679473877, 0.6617646813392639, 0.6651583909988403, 0.668552041053772, 0.6651583909988403, 0.6334841847419739, 0.651583731174469, 0.6606335043907166, 0.6153846383094788, 0.6549773812294006, 0.6527149081230164, 0.6459276080131531, 0.6504524946212769, 0.6640271544456482, 0.6640271544456482, 0.651583731174469, 0.6504524946212769, 0.6572397947311401, 0.6255655884742737, 0.6538461446762085, 0.6583710312843323, 0.6561086177825928, 0.6142534017562866, 0.6006787419319153, 0.6131221652030945, 0.6153846383094788, 0.6606335043907166, 0.6538461446762085, 0.6504524946212769, 0.6459276080131531, 0.6346153616905212, 0.6583710312843323, 0.6380090713500977, 0.6640271544456482, 0.6255655884742737, 0.6493212580680847, 0.6402714848518372, 0.6255655884742737, 0.6459276080131531, 0.6674208045005798, 0.6447963714599609, 0.598416268825531, 0.6504524946212769, 0.6595022678375244, 0.6470588445663452, 0.6447963714599609, 0.6119909286499023, 0.6651583909988403, 0.6504524946212769, 0.6527149081230164, 0.6470588445663452, 0.651583731174469, 0.6583710312843323, 0.6414027214050293, 0.6617646813392639, 0.6538461446762085, 0.6425339579582214, 0.6447963714599609, 0.6346153616905212, 0.6572397947311401, 0.6266968250274658, 0.610859751701355, 0.6199095249176025, 0.639140248298645, 0.6470588445663452, 0.6414027214050293, 0.6380090713500977, 0.6549773812294006, 0.5916289687156677, 0.6368778347969055, 0.6493212580680847, 0.6312217116355896]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - ETA: 0s - loss: 0.6517 - accuracy: 0.7266"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 9s 60ms/step - loss: 0.6517 - accuracy: 0.7266 - val_loss: 0.8136 - val_accuracy: 0.5444\n","Epoch 2/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6288 - accuracy: 0.7375 - val_loss: 0.8363 - val_accuracy: 0.5010\n","Epoch 3/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6216 - accuracy: 0.7465 - val_loss: 0.8149 - val_accuracy: 0.5186\n","Epoch 4/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6269 - accuracy: 0.7305 - val_loss: 0.8075 - val_accuracy: 0.5320\n","Epoch 5/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6159 - accuracy: 0.7424 - val_loss: 0.8139 - val_accuracy: 0.5186\n","Epoch 6/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6056 - accuracy: 0.7584 - val_loss: 0.8156 - val_accuracy: 0.5238\n","Epoch 7/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6138 - accuracy: 0.7522 - val_loss: 0.8091 - val_accuracy: 0.5310\n","Epoch 8/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.6127 - accuracy: 0.7362 - val_loss: 0.7907 - val_accuracy: 0.5837\n","Epoch 9/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6081 - accuracy: 0.7452 - val_loss: 0.7918 - val_accuracy: 0.5702\n","Epoch 10/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6033 - accuracy: 0.7460 - val_loss: 0.7917 - val_accuracy: 0.5806\n","Epoch 11/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6029 - accuracy: 0.7465 - val_loss: 0.7939 - val_accuracy: 0.5589\n","Epoch 12/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.6051 - accuracy: 0.7610 - val_loss: 0.7736 - val_accuracy: 0.6198\n","Epoch 13/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6040 - accuracy: 0.7561 - val_loss: 0.7786 - val_accuracy: 0.6043\n","Epoch 14/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5953 - accuracy: 0.7602 - val_loss: 0.7670 - val_accuracy: 0.6333\n","Epoch 15/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5988 - accuracy: 0.7553 - val_loss: 0.7985 - val_accuracy: 0.5620\n","Epoch 16/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5999 - accuracy: 0.7550 - val_loss: 0.7777 - val_accuracy: 0.5857\n","Epoch 17/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5940 - accuracy: 0.7584 - val_loss: 0.7870 - val_accuracy: 0.6012\n","Epoch 18/100\n","31/31 [==============================] - 1s 31ms/step - loss: 0.6024 - accuracy: 0.7540 - val_loss: 0.7685 - val_accuracy: 0.6436\n","Epoch 19/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5930 - accuracy: 0.7646 - val_loss: 0.7696 - val_accuracy: 0.6198\n","Epoch 20/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5872 - accuracy: 0.7553 - val_loss: 0.7865 - val_accuracy: 0.6281\n","Epoch 21/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5880 - accuracy: 0.7563 - val_loss: 0.7836 - val_accuracy: 0.6209\n","Epoch 22/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5986 - accuracy: 0.7483 - val_loss: 0.7846 - val_accuracy: 0.6343\n","Epoch 23/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5900 - accuracy: 0.7558 - val_loss: 0.8383 - val_accuracy: 0.6126\n","Epoch 24/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5781 - accuracy: 0.7667 - val_loss: 0.8785 - val_accuracy: 0.6023\n","Epoch 25/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5771 - accuracy: 0.7672 - val_loss: 0.8517 - val_accuracy: 0.6012\n","Epoch 26/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5856 - accuracy: 0.7615 - val_loss: 0.8372 - val_accuracy: 0.6178\n","Epoch 27/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5777 - accuracy: 0.7661 - val_loss: 0.8664 - val_accuracy: 0.6343\n","Epoch 28/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5755 - accuracy: 0.7760 - val_loss: 0.8644 - val_accuracy: 0.6167\n","Epoch 29/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5859 - accuracy: 0.7623 - val_loss: 0.8625 - val_accuracy: 0.6302\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5793 - accuracy: 0.7724 - val_loss: 0.9056 - val_accuracy: 0.6147\n","Epoch 31/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5787 - accuracy: 0.7659 - val_loss: 0.8534 - val_accuracy: 0.6043\n","Epoch 32/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5731 - accuracy: 0.7739 - val_loss: 0.8865 - val_accuracy: 0.5930\n","Epoch 33/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5669 - accuracy: 0.7628 - val_loss: 0.9374 - val_accuracy: 0.5992\n","Epoch 34/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5757 - accuracy: 0.7625 - val_loss: 0.9099 - val_accuracy: 0.6116\n","Epoch 35/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5820 - accuracy: 0.7649 - val_loss: 1.0035 - val_accuracy: 0.6281\n","Epoch 36/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5816 - accuracy: 0.7514 - val_loss: 1.0074 - val_accuracy: 0.5940\n","Epoch 37/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5698 - accuracy: 0.7806 - val_loss: 0.8882 - val_accuracy: 0.6043\n","Epoch 38/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5558 - accuracy: 0.7904 - val_loss: 0.8670 - val_accuracy: 0.6064\n","Epoch 39/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5599 - accuracy: 0.7798 - val_loss: 0.9010 - val_accuracy: 0.6415\n","Epoch 40/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5588 - accuracy: 0.7817 - val_loss: 0.8570 - val_accuracy: 0.6384\n","Epoch 41/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5802 - accuracy: 0.7687 - val_loss: 0.8798 - val_accuracy: 0.6033\n","Epoch 42/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5608 - accuracy: 0.7736 - val_loss: 0.9803 - val_accuracy: 0.5930\n","Epoch 43/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5477 - accuracy: 0.7884 - val_loss: 0.9102 - val_accuracy: 0.6054\n","Epoch 44/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5598 - accuracy: 0.7721 - val_loss: 0.9071 - val_accuracy: 0.6240\n","Epoch 45/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5517 - accuracy: 0.7778 - val_loss: 0.9158 - val_accuracy: 0.6209\n","Epoch 46/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5520 - accuracy: 0.7778 - val_loss: 1.0573 - val_accuracy: 0.5919\n","Epoch 47/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5836 - accuracy: 0.7633 - val_loss: 0.9653 - val_accuracy: 0.6333\n","Epoch 48/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5760 - accuracy: 0.7636 - val_loss: 0.9445 - val_accuracy: 0.6043\n","Epoch 49/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5673 - accuracy: 0.7695 - val_loss: 0.9660 - val_accuracy: 0.6023\n","Epoch 50/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5584 - accuracy: 0.7767 - val_loss: 0.9476 - val_accuracy: 0.6116\n","Epoch 51/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5487 - accuracy: 0.7817 - val_loss: 0.9477 - val_accuracy: 0.6178\n","Epoch 52/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5443 - accuracy: 0.7904 - val_loss: 1.0302 - val_accuracy: 0.6188\n","Epoch 53/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5382 - accuracy: 0.7876 - val_loss: 0.9456 - val_accuracy: 0.6384\n","Epoch 54/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5393 - accuracy: 0.7814 - val_loss: 0.9655 - val_accuracy: 0.6178\n","Epoch 55/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5487 - accuracy: 0.7770 - val_loss: 0.9215 - val_accuracy: 0.6002\n","Epoch 56/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5465 - accuracy: 0.7881 - val_loss: 0.9436 - val_accuracy: 0.6395\n","Epoch 57/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5385 - accuracy: 0.7866 - val_loss: 0.9492 - val_accuracy: 0.6333\n","Epoch 58/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5391 - accuracy: 0.7894 - val_loss: 0.9595 - val_accuracy: 0.6395\n","Epoch 59/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5282 - accuracy: 0.7920 - val_loss: 0.9841 - val_accuracy: 0.5961\n","Epoch 60/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5370 - accuracy: 0.7894 - val_loss: 1.0036 - val_accuracy: 0.6198\n","Epoch 61/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5434 - accuracy: 0.7871 - val_loss: 1.0883 - val_accuracy: 0.5950\n","Epoch 62/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5558 - accuracy: 0.7819 - val_loss: 1.0158 - val_accuracy: 0.6312\n","Epoch 63/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5332 - accuracy: 0.8018 - val_loss: 0.9619 - val_accuracy: 0.5992\n","Epoch 64/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5251 - accuracy: 0.7997 - val_loss: 0.8983 - val_accuracy: 0.6271\n","Epoch 65/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5221 - accuracy: 0.7961 - val_loss: 1.1584 - val_accuracy: 0.5733\n","Epoch 66/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5348 - accuracy: 0.7907 - val_loss: 0.9993 - val_accuracy: 0.6043\n","Epoch 67/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5317 - accuracy: 0.7928 - val_loss: 0.9264 - val_accuracy: 0.6271\n","Epoch 68/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5299 - accuracy: 0.7933 - val_loss: 1.0100 - val_accuracy: 0.6074\n","Epoch 69/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5395 - accuracy: 0.7899 - val_loss: 0.9628 - val_accuracy: 0.5950\n","Epoch 70/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5198 - accuracy: 0.8039 - val_loss: 0.9829 - val_accuracy: 0.6002\n","Epoch 71/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5207 - accuracy: 0.8109 - val_loss: 0.9462 - val_accuracy: 0.6198\n","Epoch 72/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5171 - accuracy: 0.8008 - val_loss: 1.1186 - val_accuracy: 0.5837\n","Epoch 73/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5300 - accuracy: 0.7995 - val_loss: 1.0380 - val_accuracy: 0.6343\n","Epoch 74/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5218 - accuracy: 0.7990 - val_loss: 0.9489 - val_accuracy: 0.6147\n","Epoch 75/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5215 - accuracy: 0.7907 - val_loss: 1.0324 - val_accuracy: 0.5930\n","Epoch 76/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5181 - accuracy: 0.8010 - val_loss: 1.0865 - val_accuracy: 0.6002\n","Epoch 77/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5186 - accuracy: 0.7961 - val_loss: 0.9254 - val_accuracy: 0.6002\n","Epoch 78/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5245 - accuracy: 0.7997 - val_loss: 0.9807 - val_accuracy: 0.6281\n","Epoch 79/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5035 - accuracy: 0.8085 - val_loss: 1.0255 - val_accuracy: 0.5930\n","Epoch 80/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5144 - accuracy: 0.8044 - val_loss: 1.0150 - val_accuracy: 0.6333\n","Epoch 81/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5140 - accuracy: 0.8013 - val_loss: 0.9558 - val_accuracy: 0.6250\n","Epoch 82/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5064 - accuracy: 0.8171 - val_loss: 0.9543 - val_accuracy: 0.5961\n","Epoch 83/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5130 - accuracy: 0.7979 - val_loss: 1.0470 - val_accuracy: 0.5971\n","Epoch 84/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5056 - accuracy: 0.8062 - val_loss: 0.9733 - val_accuracy: 0.5950\n","Epoch 85/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5110 - accuracy: 0.8008 - val_loss: 1.0579 - val_accuracy: 0.5909\n","Epoch 86/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5043 - accuracy: 0.8005 - val_loss: 1.1005 - val_accuracy: 0.5971\n","Epoch 87/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5032 - accuracy: 0.8103 - val_loss: 1.0846 - val_accuracy: 0.6229\n","Epoch 88/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5141 - accuracy: 0.8044 - val_loss: 0.9732 - val_accuracy: 0.6126\n","Epoch 89/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5149 - accuracy: 0.7995 - val_loss: 1.0273 - val_accuracy: 0.6002\n","Epoch 90/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5168 - accuracy: 0.7997 - val_loss: 0.9792 - val_accuracy: 0.6405\n","Epoch 91/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5059 - accuracy: 0.8031 - val_loss: 1.0081 - val_accuracy: 0.6260\n","Epoch 92/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4925 - accuracy: 0.8147 - val_loss: 1.1037 - val_accuracy: 0.5888\n","Epoch 93/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4919 - accuracy: 0.8134 - val_loss: 0.9910 - val_accuracy: 0.6157\n","Epoch 94/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4855 - accuracy: 0.8171 - val_loss: 1.0500 - val_accuracy: 0.6085\n","Epoch 95/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4961 - accuracy: 0.8059 - val_loss: 1.1058 - val_accuracy: 0.5919\n","Epoch 96/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4963 - accuracy: 0.8173 - val_loss: 1.1037 - val_accuracy: 0.6002\n","Epoch 97/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4889 - accuracy: 0.8176 - val_loss: 0.9878 - val_accuracy: 0.6240\n","Epoch 98/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4883 - accuracy: 0.8155 - val_loss: 1.0726 - val_accuracy: 0.6198\n","Epoch 99/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5033 - accuracy: 0.8098 - val_loss: 1.0090 - val_accuracy: 0.6105\n","Epoch 100/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.5017 - accuracy: 0.8124 - val_loss: 1.0687 - val_accuracy: 0.5909\n","{'loss': [0.6516623497009277, 0.6287921071052551, 0.6216270327568054, 0.6269492506980896, 0.615890383720398, 0.6056482791900635, 0.613841712474823, 0.6126747727394104, 0.6081408262252808, 0.6033007502555847, 0.6029406785964966, 0.6051202416419983, 0.6039996147155762, 0.5952937602996826, 0.5987950563430786, 0.5999078154563904, 0.5940404534339905, 0.6024000644683838, 0.5930211544036865, 0.5872094035148621, 0.5879627466201782, 0.5986359119415283, 0.5900226831436157, 0.5781068801879883, 0.5770891904830933, 0.585574746131897, 0.577688455581665, 0.5754857063293457, 0.5858843326568604, 0.5792842507362366, 0.5786932110786438, 0.573064386844635, 0.5669131278991699, 0.5757019519805908, 0.5820450186729431, 0.5815614461898804, 0.5697851777076721, 0.5558151602745056, 0.5598580241203308, 0.5587897300720215, 0.580221951007843, 0.5607596635818481, 0.5477114915847778, 0.5598004460334778, 0.5516947507858276, 0.551996111869812, 0.5835629105567932, 0.5759775638580322, 0.567317008972168, 0.5583673119544983, 0.5487281084060669, 0.544256865978241, 0.5382448434829712, 0.5392981171607971, 0.5487046837806702, 0.5464674830436707, 0.5385140776634216, 0.5391458868980408, 0.5282445549964905, 0.5369861125946045, 0.5433716177940369, 0.555794358253479, 0.5331798195838928, 0.52508145570755, 0.5220915675163269, 0.5347521901130676, 0.5317208170890808, 0.5298747420310974, 0.5394838452339172, 0.5198227167129517, 0.5206905007362366, 0.5171101689338684, 0.5299975872039795, 0.5218238234519958, 0.5214616656303406, 0.5181133151054382, 0.5186126232147217, 0.5245113372802734, 0.5034859776496887, 0.5144042372703552, 0.5140492916107178, 0.5064020156860352, 0.5130282640457153, 0.5056105852127075, 0.5109749436378479, 0.5042710304260254, 0.5032261610031128, 0.51406329870224, 0.5148584246635437, 0.5168245434761047, 0.5058668255805969, 0.4925368130207062, 0.49190884828567505, 0.48548123240470886, 0.49613308906555176, 0.4963180720806122, 0.48885378241539, 0.4883219301700592, 0.5032865405082703, 0.5016883611679077], 'accuracy': [0.7266150116920471, 0.7374677062034607, 0.7465116381645203, 0.7304909825325012, 0.7423772811889648, 0.7583979368209839, 0.7521963715553284, 0.7361757159233093, 0.7452196478843689, 0.7459948062896729, 0.7465116381645203, 0.7609819173812866, 0.7560723423957825, 0.7602066993713379, 0.7552971839904785, 0.7550387382507324, 0.7583979368209839, 0.7540051937103271, 0.7645995020866394, 0.7552971839904785, 0.7563307285308838, 0.7483204007148743, 0.7558139562606812, 0.7666666507720947, 0.7671834826469421, 0.7614986896514893, 0.7661498785018921, 0.7759689688682556, 0.762273907661438, 0.7723514437675476, 0.7658914923667908, 0.7739018201828003, 0.7627906799316406, 0.7625322937965393, 0.7648578882217407, 0.7514212131500244, 0.7806201577186584, 0.790439248085022, 0.7798449397087097, 0.7816537618637085, 0.7687338590621948, 0.773643434047699, 0.7883720993995667, 0.7720929980278015, 0.7777777910232544, 0.7777777910232544, 0.763307511806488, 0.7635658979415894, 0.7695090174674988, 0.7767441868782043, 0.7816537618637085, 0.790439248085022, 0.7875968813896179, 0.7813953757286072, 0.7770025730133057, 0.7881137132644653, 0.7865633368492126, 0.7894057035446167, 0.7919896841049194, 0.7894057035446167, 0.7870801091194153, 0.7819121479988098, 0.801808774471283, 0.7997416257858276, 0.7961240410804749, 0.7906976938247681, 0.7927648425102234, 0.7932816743850708, 0.7899224758148193, 0.8038759827613831, 0.8108527064323425, 0.8007751703262329, 0.7994831800460815, 0.7989664077758789, 0.7906976938247681, 0.801033616065979, 0.7961240410804749, 0.7997416257858276, 0.8085271120071411, 0.8043927550315857, 0.8012920022010803, 0.817054271697998, 0.7979328036308289, 0.8062015771865845, 0.8007751703262329, 0.8005167841911316, 0.8103359341621399, 0.8043927550315857, 0.7994831800460815, 0.7997416257858276, 0.8031007647514343, 0.8147286772727966, 0.8134366869926453, 0.817054271697998, 0.8059431314468384, 0.8173126578330994, 0.8175710439682007, 0.8155038952827454, 0.8098191022872925, 0.8124030828475952], 'val_loss': [0.8135747313499451, 0.836307942867279, 0.8149375319480896, 0.8075257539749146, 0.8139134049415588, 0.8156479001045227, 0.8090543150901794, 0.7907238006591797, 0.791771650314331, 0.7917101383209229, 0.7938898801803589, 0.773638904094696, 0.7786315679550171, 0.7670275568962097, 0.7984592914581299, 0.7776692509651184, 0.7870244979858398, 0.7685208320617676, 0.7696056962013245, 0.7865188121795654, 0.7836273312568665, 0.7846482396125793, 0.8383013010025024, 0.8785414099693298, 0.8516662120819092, 0.8371613621711731, 0.866422176361084, 0.8643638491630554, 0.862516462802887, 0.9055930972099304, 0.8534462451934814, 0.8865051865577698, 0.9373911619186401, 0.9099035263061523, 1.003515362739563, 1.00742506980896, 0.8881555795669556, 0.8669995665550232, 0.9010454416275024, 0.856986939907074, 0.8798211812973022, 0.9802923202514648, 0.9102345705032349, 0.9071105718612671, 0.9157984852790833, 1.0572611093521118, 0.9652743935585022, 0.9444848895072937, 0.9659587740898132, 0.9475704431533813, 0.9477139115333557, 1.0301518440246582, 0.9456397294998169, 0.9655005931854248, 0.9214592576026917, 0.9435777068138123, 0.949158787727356, 0.9595261216163635, 0.9840679168701172, 1.0035663843154907, 1.0882970094680786, 1.015759825706482, 0.9618741273880005, 0.8983064889907837, 1.1583555936813354, 0.999291718006134, 0.9264349937438965, 1.0100232362747192, 0.9628198742866516, 0.9829416871070862, 0.9462192058563232, 1.118577003479004, 1.0379918813705444, 0.9489357471466064, 1.0324428081512451, 1.086544156074524, 0.9253750443458557, 0.9806628227233887, 1.0254517793655396, 1.0150446891784668, 0.9557642936706543, 0.9543180465698242, 1.046985149383545, 0.9733264446258545, 1.057860016822815, 1.100471019744873, 1.0846383571624756, 0.9732117056846619, 1.027334213256836, 0.9791596531867981, 1.0081160068511963, 1.1036691665649414, 0.9910434484481812, 1.0499732494354248, 1.1058272123336792, 1.1037354469299316, 0.9877761602401733, 1.0726444721221924, 1.0089713335037231, 1.0687110424041748], 'val_accuracy': [0.5444214940071106, 0.5010330677032471, 0.5185950398445129, 0.5320248007774353, 0.5185950398445129, 0.5237603187561035, 0.5309917330741882, 0.5836777091026306, 0.5702479481697083, 0.5805785059928894, 0.55888432264328, 0.6198347210884094, 0.6043388247489929, 0.6332644820213318, 0.5619834661483765, 0.58574378490448, 0.6012396812438965, 0.6435950398445129, 0.6198347210884094, 0.6280992031097412, 0.6208677887916565, 0.6342975497245789, 0.6126033067703247, 0.6022727489471436, 0.6012396812438965, 0.6177685856819153, 0.6342975497245789, 0.6167355179786682, 0.6301652789115906, 0.6146694421768188, 0.6043388247489929, 0.5929751992225647, 0.5991735458374023, 0.6115702390670776, 0.6280992031097412, 0.5940082669258118, 0.6043388247489929, 0.6064049601554871, 0.6415289044380188, 0.6384297609329224, 0.6033057570457458, 0.5929751992225647, 0.60537189245224, 0.6239669322967529, 0.6208677887916565, 0.5919421315193176, 0.6332644820213318, 0.6043388247489929, 0.6022727489471436, 0.6115702390670776, 0.6177685856819153, 0.6188016533851624, 0.6384297609329224, 0.6177685856819153, 0.6002066135406494, 0.6394628286361694, 0.6332644820213318, 0.6394628286361694, 0.5960744023323059, 0.6198347210884094, 0.5950413346290588, 0.6311983466148376, 0.5991735458374023, 0.6270661354064941, 0.5733470916748047, 0.6043388247489929, 0.6270661354064941, 0.6074380278587341, 0.5950413346290588, 0.6002066135406494, 0.6198347210884094, 0.5836777091026306, 0.6342975497245789, 0.6146694421768188, 0.5929751992225647, 0.6002066135406494, 0.6002066135406494, 0.6280992031097412, 0.5929751992225647, 0.6332644820213318, 0.625, 0.5960744023323059, 0.5971074104309082, 0.5950413346290588, 0.5909090638160706, 0.5971074104309082, 0.6229338645935059, 0.6126033067703247, 0.6002066135406494, 0.6404958963394165, 0.6260330677032471, 0.5888429880142212, 0.6157024502754211, 0.6084710955619812, 0.5919421315193176, 0.6002066135406494, 0.6239669322967529, 0.6198347210884094, 0.6105371713638306, 0.5909090638160706]}\n","32/32 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","27/29 [==========================>...] - ETA: 0s - loss: 0.5328 - accuracy: 0.7839"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 8s 63ms/step - loss: 0.5325 - accuracy: 0.7858 - val_loss: 0.8284 - val_accuracy: 0.4817\n","Epoch 2/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5423 - accuracy: 0.7826 - val_loss: 0.9039 - val_accuracy: 0.4892\n","Epoch 3/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.5055 - accuracy: 0.8109 - val_loss: 0.8712 - val_accuracy: 0.4881\n","Epoch 4/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5059 - accuracy: 0.8071 - val_loss: 0.8207 - val_accuracy: 0.4935\n","Epoch 5/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4992 - accuracy: 0.8190 - val_loss: 0.8850 - val_accuracy: 0.4925\n","Epoch 6/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.5052 - accuracy: 0.8136 - val_loss: 0.8115 - val_accuracy: 0.5140\n","Epoch 7/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.5004 - accuracy: 0.8074 - val_loss: 0.8030 - val_accuracy: 0.5388\n","Epoch 8/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4953 - accuracy: 0.8138 - val_loss: 0.8067 - val_accuracy: 0.5172\n","Epoch 9/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.5100 - accuracy: 0.8012 - val_loss: 0.8087 - val_accuracy: 0.6228\n","Epoch 10/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5040 - accuracy: 0.8017 - val_loss: 0.7821 - val_accuracy: 0.5948\n","Epoch 11/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.4955 - accuracy: 0.8101 - val_loss: 0.8168 - val_accuracy: 0.6272\n","Epoch 12/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5060 - accuracy: 0.8079 - val_loss: 0.8006 - val_accuracy: 0.5582\n","Epoch 13/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5085 - accuracy: 0.8109 - val_loss: 0.8013 - val_accuracy: 0.6358\n","Epoch 14/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.5039 - accuracy: 0.8009 - val_loss: 0.7673 - val_accuracy: 0.6498\n","Epoch 15/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.4907 - accuracy: 0.8144 - val_loss: 0.7690 - val_accuracy: 0.6627\n","Epoch 16/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.4817 - accuracy: 0.8211 - val_loss: 0.7704 - val_accuracy: 0.6659\n","Epoch 17/100\n","29/29 [==============================] - 1s 40ms/step - loss: 0.4870 - accuracy: 0.8219 - val_loss: 0.7863 - val_accuracy: 0.6681\n","Epoch 18/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.4739 - accuracy: 0.8241 - val_loss: 0.7523 - val_accuracy: 0.6778\n","Epoch 19/100\n","29/29 [==============================] - 1s 31ms/step - loss: 0.4861 - accuracy: 0.8279 - val_loss: 0.7552 - val_accuracy: 0.6886\n","Epoch 20/100\n","29/29 [==============================] - 1s 30ms/step - loss: 0.4793 - accuracy: 0.8200 - val_loss: 0.7627 - val_accuracy: 0.6918\n","Epoch 21/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.4703 - accuracy: 0.8238 - val_loss: 0.8008 - val_accuracy: 0.6810\n","Epoch 22/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4740 - accuracy: 0.8252 - val_loss: 0.8372 - val_accuracy: 0.6767\n","Epoch 23/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.4735 - accuracy: 0.8273 - val_loss: 0.7700 - val_accuracy: 0.7101\n","Epoch 24/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4788 - accuracy: 0.8238 - val_loss: 0.8104 - val_accuracy: 0.6983\n","Epoch 25/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4652 - accuracy: 0.8276 - val_loss: 0.8617 - val_accuracy: 0.6735\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4702 - accuracy: 0.8338 - val_loss: 0.8442 - val_accuracy: 0.6789\n","Epoch 27/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4784 - accuracy: 0.8209 - val_loss: 0.8986 - val_accuracy: 0.6746\n","Epoch 28/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4799 - accuracy: 0.8122 - val_loss: 0.8613 - val_accuracy: 0.6950\n","Epoch 29/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4859 - accuracy: 0.8074 - val_loss: 0.9435 - val_accuracy: 0.6552\n","Epoch 30/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4785 - accuracy: 0.8241 - val_loss: 0.9526 - val_accuracy: 0.6562\n","Epoch 31/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4848 - accuracy: 0.8133 - val_loss: 0.8858 - val_accuracy: 0.6940\n","Epoch 32/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4782 - accuracy: 0.8305 - val_loss: 0.8430 - val_accuracy: 0.6994\n","Epoch 33/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4581 - accuracy: 0.8314 - val_loss: 0.8864 - val_accuracy: 0.6994\n","Epoch 34/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4559 - accuracy: 0.8303 - val_loss: 0.9084 - val_accuracy: 0.6735\n","Epoch 35/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4601 - accuracy: 0.8335 - val_loss: 0.9072 - val_accuracy: 0.7058\n","Epoch 36/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4625 - accuracy: 0.8314 - val_loss: 0.8946 - val_accuracy: 0.6961\n","Epoch 37/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4525 - accuracy: 0.8324 - val_loss: 0.9058 - val_accuracy: 0.7037\n","Epoch 38/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4609 - accuracy: 0.8332 - val_loss: 1.0878 - val_accuracy: 0.6692\n","Epoch 39/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4563 - accuracy: 0.8311 - val_loss: 0.8865 - val_accuracy: 0.7026\n","Epoch 40/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4493 - accuracy: 0.8357 - val_loss: 1.0602 - val_accuracy: 0.6239\n","Epoch 41/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.4510 - accuracy: 0.8400 - val_loss: 0.9240 - val_accuracy: 0.6789\n","Epoch 42/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4574 - accuracy: 0.8314 - val_loss: 0.9007 - val_accuracy: 0.7080\n","Epoch 43/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4591 - accuracy: 0.8446 - val_loss: 0.9317 - val_accuracy: 0.6929\n","Epoch 44/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.4470 - accuracy: 0.8343 - val_loss: 1.0638 - val_accuracy: 0.6390\n","Epoch 45/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4559 - accuracy: 0.8349 - val_loss: 0.9021 - val_accuracy: 0.6832\n","Epoch 46/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4345 - accuracy: 0.8491 - val_loss: 0.9174 - val_accuracy: 0.6929\n","Epoch 47/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4479 - accuracy: 0.8284 - val_loss: 0.9162 - val_accuracy: 0.7037\n","Epoch 48/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.4394 - accuracy: 0.8438 - val_loss: 1.0610 - val_accuracy: 0.6422\n","Epoch 49/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4463 - accuracy: 0.8389 - val_loss: 0.9515 - val_accuracy: 0.6800\n","Epoch 50/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4418 - accuracy: 0.8392 - val_loss: 0.9021 - val_accuracy: 0.7037\n","Epoch 51/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4421 - accuracy: 0.8370 - val_loss: 0.9799 - val_accuracy: 0.6778\n","Epoch 52/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4342 - accuracy: 0.8489 - val_loss: 0.9625 - val_accuracy: 0.6692\n","Epoch 53/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4461 - accuracy: 0.8373 - val_loss: 0.9242 - val_accuracy: 0.6972\n","Epoch 54/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4460 - accuracy: 0.8464 - val_loss: 0.9795 - val_accuracy: 0.6692\n","Epoch 55/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4375 - accuracy: 0.8435 - val_loss: 1.0230 - val_accuracy: 0.6433\n","Epoch 56/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4340 - accuracy: 0.8475 - val_loss: 0.9887 - val_accuracy: 0.6552\n","Epoch 57/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4261 - accuracy: 0.8481 - val_loss: 1.0676 - val_accuracy: 0.6433\n","Epoch 58/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4272 - accuracy: 0.8518 - val_loss: 0.9441 - val_accuracy: 0.7047\n","Epoch 59/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4412 - accuracy: 0.8322 - val_loss: 0.9436 - val_accuracy: 0.7026\n","Epoch 60/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4155 - accuracy: 0.8588 - val_loss: 0.9566 - val_accuracy: 0.7069\n","Epoch 61/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4189 - accuracy: 0.8561 - val_loss: 0.9951 - val_accuracy: 0.6789\n","Epoch 62/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4354 - accuracy: 0.8464 - val_loss: 1.0486 - val_accuracy: 0.6466\n","Epoch 63/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4295 - accuracy: 0.8427 - val_loss: 0.9797 - val_accuracy: 0.6864\n","Epoch 64/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4299 - accuracy: 0.8413 - val_loss: 1.0771 - val_accuracy: 0.6541\n","Epoch 65/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4370 - accuracy: 0.8416 - val_loss: 0.9705 - val_accuracy: 0.6832\n","Epoch 66/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4171 - accuracy: 0.8578 - val_loss: 0.9624 - val_accuracy: 0.6853\n","Epoch 67/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4230 - accuracy: 0.8561 - val_loss: 0.9922 - val_accuracy: 0.6929\n","Epoch 68/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4264 - accuracy: 0.8518 - val_loss: 0.9583 - val_accuracy: 0.6961\n","Epoch 69/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4197 - accuracy: 0.8534 - val_loss: 0.9878 - val_accuracy: 0.6950\n","Epoch 70/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4103 - accuracy: 0.8661 - val_loss: 1.0757 - val_accuracy: 0.6519\n","Epoch 71/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4333 - accuracy: 0.8438 - val_loss: 0.9682 - val_accuracy: 0.7015\n","Epoch 72/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4423 - accuracy: 0.8343 - val_loss: 0.9891 - val_accuracy: 0.7047\n","Epoch 73/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4107 - accuracy: 0.8629 - val_loss: 1.0889 - val_accuracy: 0.6649\n","Epoch 74/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4157 - accuracy: 0.8596 - val_loss: 1.1867 - val_accuracy: 0.6250\n","Epoch 75/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4142 - accuracy: 0.8518 - val_loss: 1.1272 - val_accuracy: 0.6444\n","Epoch 76/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.4059 - accuracy: 0.8583 - val_loss: 1.1521 - val_accuracy: 0.6347\n","Epoch 77/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4183 - accuracy: 0.8540 - val_loss: 1.0273 - val_accuracy: 0.7004\n","Epoch 78/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4072 - accuracy: 0.8615 - val_loss: 1.0703 - val_accuracy: 0.6541\n","Epoch 79/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4226 - accuracy: 0.8521 - val_loss: 1.0670 - val_accuracy: 0.6800\n","Epoch 80/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4006 - accuracy: 0.8650 - val_loss: 1.0679 - val_accuracy: 0.6886\n","Epoch 81/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4500 - accuracy: 0.8518 - val_loss: 1.2102 - val_accuracy: 0.6703\n","Epoch 82/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4341 - accuracy: 0.8467 - val_loss: 0.9494 - val_accuracy: 0.6875\n","Epoch 83/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.3997 - accuracy: 0.8688 - val_loss: 1.2013 - val_accuracy: 0.6369\n","Epoch 84/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4067 - accuracy: 0.8613 - val_loss: 1.0435 - val_accuracy: 0.6800\n","Epoch 85/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3986 - accuracy: 0.8607 - val_loss: 1.1567 - val_accuracy: 0.6422\n","Epoch 86/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4104 - accuracy: 0.8605 - val_loss: 1.0987 - val_accuracy: 0.7026\n","Epoch 87/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4194 - accuracy: 0.8551 - val_loss: 1.0988 - val_accuracy: 0.6487\n","Epoch 88/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4071 - accuracy: 0.8656 - val_loss: 1.0013 - val_accuracy: 0.6950\n","Epoch 89/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3972 - accuracy: 0.8666 - val_loss: 1.0449 - val_accuracy: 0.6832\n","Epoch 90/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4005 - accuracy: 0.8631 - val_loss: 1.3792 - val_accuracy: 0.5991\n","Epoch 91/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3983 - accuracy: 0.8629 - val_loss: 1.2954 - val_accuracy: 0.6078\n","Epoch 92/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4013 - accuracy: 0.8648 - val_loss: 1.0624 - val_accuracy: 0.6789\n","Epoch 93/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3986 - accuracy: 0.8675 - val_loss: 1.0056 - val_accuracy: 0.6929\n","Epoch 94/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3933 - accuracy: 0.8666 - val_loss: 1.1413 - val_accuracy: 0.6422\n","Epoch 95/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4012 - accuracy: 0.8650 - val_loss: 1.1000 - val_accuracy: 0.6843\n","Epoch 96/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4013 - accuracy: 0.8642 - val_loss: 1.2355 - val_accuracy: 0.6175\n","Epoch 97/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3908 - accuracy: 0.8712 - val_loss: 1.0087 - val_accuracy: 0.6907\n","Epoch 98/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3969 - accuracy: 0.8623 - val_loss: 1.1704 - val_accuracy: 0.6800\n","Epoch 99/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3908 - accuracy: 0.8774 - val_loss: 1.0389 - val_accuracy: 0.6843\n","Epoch 100/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3868 - accuracy: 0.8715 - val_loss: 1.5781 - val_accuracy: 0.5905\n","{'loss': [0.5324859023094177, 0.542271077632904, 0.5055094361305237, 0.5058896541595459, 0.4992437958717346, 0.5052469372749329, 0.5003901124000549, 0.4952632784843445, 0.5100377798080444, 0.5039730668067932, 0.49546805024147034, 0.5060010552406311, 0.5085465312004089, 0.503913164138794, 0.49068382382392883, 0.48167186975479126, 0.4869944155216217, 0.47391825914382935, 0.48610547184944153, 0.47929826378822327, 0.4703389108181, 0.4740457236766815, 0.47345465421676636, 0.47879356145858765, 0.4652480483055115, 0.47016313672065735, 0.4783923625946045, 0.4799213707447052, 0.4858792722225189, 0.47846662998199463, 0.48477452993392944, 0.4781759977340698, 0.4581372141838074, 0.45588600635528564, 0.4601430892944336, 0.4625086486339569, 0.4525367021560669, 0.4609188139438629, 0.4563048183917999, 0.4492577612400055, 0.4509652554988861, 0.45736148953437805, 0.45912498235702515, 0.4469628930091858, 0.45594796538352966, 0.434471994638443, 0.4479161202907562, 0.4394373595714569, 0.4462753236293793, 0.4417874813079834, 0.4420947730541229, 0.43424221873283386, 0.4460815191268921, 0.44603827595710754, 0.4374917149543762, 0.43400752544403076, 0.42613422870635986, 0.4272477924823761, 0.4412204921245575, 0.4155316948890686, 0.4188571870326996, 0.4354225993156433, 0.4295467436313629, 0.42985016107559204, 0.43704721331596375, 0.41705650091171265, 0.42298421263694763, 0.42637899518013, 0.4196721315383911, 0.41032472252845764, 0.433343768119812, 0.4422816336154938, 0.4106675386428833, 0.41567981243133545, 0.41415926814079285, 0.40587127208709717, 0.41834357380867004, 0.40716221928596497, 0.42257726192474365, 0.4006234407424927, 0.45003581047058105, 0.4340799152851105, 0.3997318148612976, 0.4066762626171112, 0.39864763617515564, 0.4104211926460266, 0.4194299578666687, 0.4070999026298523, 0.39718639850616455, 0.40045464038848877, 0.3982630670070648, 0.40131890773773193, 0.3986313045024872, 0.39325928688049316, 0.401215523481369, 0.4013154208660126, 0.39084747433662415, 0.3968731462955475, 0.39080220460891724, 0.3868243992328644], 'accuracy': [0.7858297228813171, 0.782597005367279, 0.810883641242981, 0.8071120977401733, 0.818965494632721, 0.8135775923728943, 0.8073814511299133, 0.813847005367279, 0.8011853694915771, 0.8017241358757019, 0.8100754022598267, 0.8079202771186829, 0.810883641242981, 0.8009159564971924, 0.8143857717514038, 0.8211206793785095, 0.821928858757019, 0.8240840435028076, 0.8278555870056152, 0.8200430870056152, 0.8238146305084229, 0.8251616358757019, 0.8273168206214905, 0.8238146305084229, 0.8275862336158752, 0.8337823152542114, 0.8208512663841248, 0.8122305870056152, 0.8073814511299133, 0.8240840435028076, 0.8133081793785095, 0.8305495977401733, 0.8313577771186829, 0.8302801847457886, 0.8335129022598267, 0.8313577771186829, 0.8324353694915771, 0.8332435488700867, 0.8310883641242981, 0.8356680870056152, 0.8399784564971924, 0.8313577771186829, 0.8445581793785095, 0.834321141242981, 0.8348599076271057, 0.8491379022598267, 0.8283944129943848, 0.84375, 0.8389008641242981, 0.8391702771186829, 0.8370150923728943, 0.8488685488700867, 0.837284505367279, 0.8464439511299133, 0.8434805870056152, 0.8475215435028076, 0.8480603694915771, 0.8518319129943848, 0.8321659564971924, 0.8588362336158752, 0.8561422228813171, 0.8464439511299133, 0.8426724076271057, 0.8413254022598267, 0.8415948152542114, 0.857758641242981, 0.8561422228813171, 0.8518319129943848, 0.8534482717514038, 0.8661099076271057, 0.84375, 0.834321141242981, 0.8628771305084229, 0.8596444129943848, 0.8518319129943848, 0.8582974076271057, 0.8539870977401733, 0.8615301847457886, 0.8521012663841248, 0.8650323152542114, 0.8518319129943848, 0.8467133641242981, 0.868803858757019, 0.8612607717514038, 0.860722005367279, 0.8604525923728943, 0.8550646305084229, 0.865571141242981, 0.8666487336158752, 0.8631465435028076, 0.8628771305084229, 0.8647629022598267, 0.8674569129943848, 0.8666487336158752, 0.8650323152542114, 0.8642241358757019, 0.8712284564971924, 0.8623383641242981, 0.8774245977401733, 0.8714978694915771], 'val_loss': [0.82843017578125, 0.9039022326469421, 0.8712401986122131, 0.8207244873046875, 0.8849518299102783, 0.8115153908729553, 0.8029670715332031, 0.806747555732727, 0.8086506128311157, 0.7820911407470703, 0.8168088793754578, 0.8006417751312256, 0.8012973666191101, 0.7673243284225464, 0.7690384984016418, 0.7703850269317627, 0.7863365411758423, 0.7523399591445923, 0.755248486995697, 0.7627227902412415, 0.8008378744125366, 0.8372063040733337, 0.7700092196464539, 0.8103504776954651, 0.861687421798706, 0.8441758155822754, 0.8985773921012878, 0.8613273501396179, 0.9434828758239746, 0.9526121020317078, 0.8857591152191162, 0.8429823517799377, 0.8863894939422607, 0.9084095358848572, 0.9072095155715942, 0.8946251273155212, 0.9057675004005432, 1.087792992591858, 0.8864781856536865, 1.0601638555526733, 0.9239892363548279, 0.9006817936897278, 0.9317067861557007, 1.0637879371643066, 0.9020984172821045, 0.9174373745918274, 0.9161949157714844, 1.0610471963882446, 0.9515103101730347, 0.9020876884460449, 0.9798868298530579, 0.9625135064125061, 0.924187958240509, 0.9795413017272949, 1.023024320602417, 0.9887425899505615, 1.0675594806671143, 0.9440648555755615, 0.9435986280441284, 0.9565727710723877, 0.9950940608978271, 1.0485644340515137, 0.9796623587608337, 1.077081322669983, 0.9704518914222717, 0.9624225497245789, 0.9922446012496948, 0.9582614302635193, 0.9877520799636841, 1.0756882429122925, 0.9681618213653564, 0.9890969395637512, 1.088852047920227, 1.1867430210113525, 1.1272366046905518, 1.1521239280700684, 1.0272583961486816, 1.0703418254852295, 1.067020297050476, 1.0678976774215698, 1.2101733684539795, 0.9493757486343384, 1.2013498544692993, 1.043473482131958, 1.156679630279541, 1.0987292528152466, 1.0987874269485474, 1.0012778043746948, 1.0449159145355225, 1.3791940212249756, 1.2953870296478271, 1.0623642206192017, 1.0056102275848389, 1.1412564516067505, 1.0999575853347778, 1.2355101108551025, 1.0086807012557983, 1.170353651046753, 1.038907766342163, 1.5780820846557617], 'val_accuracy': [0.48168104887008667, 0.4892241358757019, 0.4881465435028076, 0.49353447556495667, 0.4924568831920624, 0.514008641242981, 0.5387930870056152, 0.517241358757019, 0.6228448152542114, 0.5948275923728943, 0.6271551847457886, 0.5581896305084229, 0.6357758641242981, 0.649784505367279, 0.662715494632721, 0.6659482717514038, 0.6681034564971924, 0.6778017282485962, 0.6885775923728943, 0.6918103694915771, 0.681034505367279, 0.6767241358757019, 0.7101293206214905, 0.6982758641242981, 0.673491358757019, 0.6788793206214905, 0.6745689511299133, 0.6950430870056152, 0.6551724076271057, 0.65625, 0.693965494632721, 0.6993534564971924, 0.6993534564971924, 0.673491358757019, 0.7058189511299133, 0.6961206793785095, 0.7036637663841248, 0.6691810488700867, 0.7025862336158752, 0.6239224076271057, 0.6788793206214905, 0.7079741358757019, 0.6928879022598267, 0.639008641242981, 0.6831896305084229, 0.6928879022598267, 0.7036637663841248, 0.642241358757019, 0.6799569129943848, 0.7036637663841248, 0.6778017282485962, 0.6691810488700867, 0.6971982717514038, 0.6691810488700867, 0.6433189511299133, 0.6551724076271057, 0.6433189511299133, 0.704741358757019, 0.7025862336158752, 0.7068965435028076, 0.6788793206214905, 0.6465517282485962, 0.6864224076271057, 0.6540948152542114, 0.6831896305084229, 0.6853448152542114, 0.6928879022598267, 0.6961206793785095, 0.6950430870056152, 0.6519396305084229, 0.701508641242981, 0.704741358757019, 0.6648706793785095, 0.625, 0.6443965435028076, 0.6346982717514038, 0.7004310488700867, 0.6540948152542114, 0.6799569129943848, 0.6885775923728943, 0.670258641242981, 0.6875, 0.6368534564971924, 0.6799569129943848, 0.642241358757019, 0.7025862336158752, 0.6487069129943848, 0.6950430870056152, 0.6831896305084229, 0.5991379022598267, 0.607758641242981, 0.6788793206214905, 0.6928879022598267, 0.642241358757019, 0.6842672228813171, 0.6174569129943848, 0.6907327771186829, 0.6799569129943848, 0.6842672228813171, 0.5905172228813171]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","25/28 [=========================>....] - ETA: 0s - loss: 0.5660 - accuracy: 0.7788"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 7s 58ms/step - loss: 0.5628 - accuracy: 0.7818 - val_loss: 0.8956 - val_accuracy: 0.5057\n","Epoch 2/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.5173 - accuracy: 0.8014 - val_loss: 0.8636 - val_accuracy: 0.5124\n","Epoch 3/100\n","28/28 [==============================] - 1s 32ms/step - loss: 0.5176 - accuracy: 0.8093 - val_loss: 0.7946 - val_accuracy: 0.5362\n","Epoch 4/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.5098 - accuracy: 0.8033 - val_loss: 0.7757 - val_accuracy: 0.6131\n","Epoch 5/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5052 - accuracy: 0.8070 - val_loss: 0.7735 - val_accuracy: 0.5588\n","Epoch 6/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.5036 - accuracy: 0.8147 - val_loss: 0.7661 - val_accuracy: 0.6290\n","Epoch 7/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5101 - accuracy: 0.8039 - val_loss: 0.7617 - val_accuracy: 0.5905\n","Epoch 8/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5058 - accuracy: 0.8155 - val_loss: 0.7773 - val_accuracy: 0.5520\n","Epoch 9/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.5007 - accuracy: 0.8138 - val_loss: 0.7445 - val_accuracy: 0.6391\n","Epoch 10/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4964 - accuracy: 0.8130 - val_loss: 0.7490 - val_accuracy: 0.6335\n","Epoch 11/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4888 - accuracy: 0.8161 - val_loss: 0.7638 - val_accuracy: 0.5701\n","Epoch 12/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.5186 - accuracy: 0.8019 - val_loss: 0.7346 - val_accuracy: 0.6663\n","Epoch 13/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4972 - accuracy: 0.8169 - val_loss: 0.7198 - val_accuracy: 0.6471\n","Epoch 14/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4958 - accuracy: 0.8169 - val_loss: 0.7230 - val_accuracy: 0.6640\n","Epoch 15/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.5001 - accuracy: 0.8138 - val_loss: 0.7100 - val_accuracy: 0.6731\n","Epoch 16/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.4941 - accuracy: 0.8285 - val_loss: 0.7060 - val_accuracy: 0.6957\n","Epoch 17/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4839 - accuracy: 0.8263 - val_loss: 0.7415 - val_accuracy: 0.6165\n","Epoch 18/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4931 - accuracy: 0.8155 - val_loss: 0.7603 - val_accuracy: 0.6154\n","Epoch 19/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4812 - accuracy: 0.8195 - val_loss: 0.7740 - val_accuracy: 0.6176\n","Epoch 20/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.4779 - accuracy: 0.8164 - val_loss: 0.7079 - val_accuracy: 0.6980\n","Epoch 21/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4708 - accuracy: 0.8356 - val_loss: 0.7244 - val_accuracy: 0.6980\n","Epoch 22/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4739 - accuracy: 0.8333 - val_loss: 0.7324 - val_accuracy: 0.6821\n","Epoch 23/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4787 - accuracy: 0.8212 - val_loss: 0.7692 - val_accuracy: 0.6663\n","Epoch 24/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.4717 - accuracy: 0.8333 - val_loss: 0.7497 - val_accuracy: 0.7093\n","Epoch 25/100\n","28/28 [==============================] - 1s 17ms/step - loss: 0.4802 - accuracy: 0.8192 - val_loss: 0.7289 - val_accuracy: 0.6991\n","Epoch 26/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4785 - accuracy: 0.8198 - val_loss: 0.7384 - val_accuracy: 0.7002\n","Epoch 27/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4839 - accuracy: 0.8271 - val_loss: 0.9172 - val_accuracy: 0.6618\n","Epoch 28/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4779 - accuracy: 0.8294 - val_loss: 0.8398 - val_accuracy: 0.7093\n","Epoch 29/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4740 - accuracy: 0.8274 - val_loss: 0.9332 - val_accuracy: 0.6584\n","Epoch 30/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4677 - accuracy: 0.8364 - val_loss: 0.8339 - val_accuracy: 0.6889\n","Epoch 31/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4643 - accuracy: 0.8297 - val_loss: 0.8314 - val_accuracy: 0.6934\n","Epoch 32/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.4597 - accuracy: 0.8364 - val_loss: 0.8456 - val_accuracy: 0.6776\n","Epoch 33/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.4572 - accuracy: 0.8299 - val_loss: 0.8250 - val_accuracy: 0.6957\n","Epoch 34/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.4669 - accuracy: 0.8268 - val_loss: 0.8058 - val_accuracy: 0.7115\n","Epoch 35/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4961 - accuracy: 0.8209 - val_loss: 0.9328 - val_accuracy: 0.6708\n","Epoch 36/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4751 - accuracy: 0.8277 - val_loss: 0.8367 - val_accuracy: 0.6946\n","Epoch 37/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4660 - accuracy: 0.8314 - val_loss: 0.8562 - val_accuracy: 0.7002\n","Epoch 38/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4651 - accuracy: 0.8319 - val_loss: 0.8269 - val_accuracy: 0.6787\n","Epoch 39/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4566 - accuracy: 0.8342 - val_loss: 0.8102 - val_accuracy: 0.6991\n","Epoch 40/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4551 - accuracy: 0.8404 - val_loss: 0.8855 - val_accuracy: 0.6855\n","Epoch 41/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4619 - accuracy: 0.8285 - val_loss: 0.8815 - val_accuracy: 0.6674\n","Epoch 42/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4616 - accuracy: 0.8359 - val_loss: 0.9807 - val_accuracy: 0.6595\n","Epoch 43/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4609 - accuracy: 0.8294 - val_loss: 0.8152 - val_accuracy: 0.7036\n","Epoch 44/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4623 - accuracy: 0.8265 - val_loss: 0.8465 - val_accuracy: 0.7115\n","Epoch 45/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4474 - accuracy: 0.8424 - val_loss: 0.8683 - val_accuracy: 0.7104\n","Epoch 46/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4496 - accuracy: 0.8370 - val_loss: 0.9052 - val_accuracy: 0.6867\n","Epoch 47/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4520 - accuracy: 0.8379 - val_loss: 0.8493 - val_accuracy: 0.7036\n","Epoch 48/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4599 - accuracy: 0.8356 - val_loss: 0.9091 - val_accuracy: 0.6753\n","Epoch 49/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.4429 - accuracy: 0.8410 - val_loss: 0.9144 - val_accuracy: 0.6968\n","Epoch 50/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4527 - accuracy: 0.8347 - val_loss: 0.8900 - val_accuracy: 0.6946\n","Epoch 51/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4480 - accuracy: 0.8418 - val_loss: 0.9173 - val_accuracy: 0.6810\n","Epoch 52/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4460 - accuracy: 0.8469 - val_loss: 0.9030 - val_accuracy: 0.6742\n","Epoch 53/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4583 - accuracy: 0.8424 - val_loss: 1.0961 - val_accuracy: 0.6595\n","Epoch 54/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4468 - accuracy: 0.8509 - val_loss: 0.9446 - val_accuracy: 0.6799\n","Epoch 55/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.4460 - accuracy: 0.8418 - val_loss: 1.0089 - val_accuracy: 0.6448\n","Epoch 56/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.4510 - accuracy: 0.8328 - val_loss: 0.8721 - val_accuracy: 0.6968\n","Epoch 57/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4316 - accuracy: 0.8543 - val_loss: 1.0926 - val_accuracy: 0.6516\n","Epoch 58/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4314 - accuracy: 0.8506 - val_loss: 0.8859 - val_accuracy: 0.6991\n","Epoch 59/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4353 - accuracy: 0.8506 - val_loss: 0.9433 - val_accuracy: 0.6833\n","Epoch 60/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.4320 - accuracy: 0.8540 - val_loss: 0.9079 - val_accuracy: 0.7002\n","Epoch 61/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.4355 - accuracy: 0.8514 - val_loss: 1.0783 - val_accuracy: 0.6561\n","Epoch 62/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.4325 - accuracy: 0.8577 - val_loss: 0.9090 - val_accuracy: 0.6731\n","Epoch 63/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4329 - accuracy: 0.8512 - val_loss: 0.9205 - val_accuracy: 0.6833\n","Epoch 64/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4288 - accuracy: 0.8537 - val_loss: 0.9435 - val_accuracy: 0.6889\n","Epoch 65/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4304 - accuracy: 0.8540 - val_loss: 0.9967 - val_accuracy: 0.6821\n","Epoch 66/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4353 - accuracy: 0.8480 - val_loss: 0.8860 - val_accuracy: 0.6844\n","Epoch 67/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4350 - accuracy: 0.8475 - val_loss: 1.0106 - val_accuracy: 0.6606\n","Epoch 68/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4350 - accuracy: 0.8568 - val_loss: 0.9695 - val_accuracy: 0.6663\n","Epoch 69/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4272 - accuracy: 0.8605 - val_loss: 1.1734 - val_accuracy: 0.6290\n","Epoch 70/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4234 - accuracy: 0.8591 - val_loss: 0.9067 - val_accuracy: 0.6900\n","Epoch 71/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4165 - accuracy: 0.8582 - val_loss: 0.9138 - val_accuracy: 0.6878\n","Epoch 72/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4238 - accuracy: 0.8543 - val_loss: 0.9921 - val_accuracy: 0.6663\n","Epoch 73/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4187 - accuracy: 0.8543 - val_loss: 1.0567 - val_accuracy: 0.6606\n","Epoch 74/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4190 - accuracy: 0.8613 - val_loss: 0.8947 - val_accuracy: 0.6878\n","Epoch 75/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4160 - accuracy: 0.8546 - val_loss: 1.0883 - val_accuracy: 0.6516\n","Epoch 76/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4410 - accuracy: 0.8410 - val_loss: 1.1260 - val_accuracy: 0.6878\n","Epoch 77/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.4418 - accuracy: 0.8466 - val_loss: 0.9097 - val_accuracy: 0.6889\n","Epoch 78/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4588 - accuracy: 0.8291 - val_loss: 1.5942 - val_accuracy: 0.5882\n","Epoch 79/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4181 - accuracy: 0.8560 - val_loss: 0.9331 - val_accuracy: 0.6968\n","Epoch 80/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4236 - accuracy: 0.8568 - val_loss: 0.9445 - val_accuracy: 0.6946\n","Epoch 81/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4094 - accuracy: 0.8667 - val_loss: 0.9816 - val_accuracy: 0.6686\n","Epoch 82/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4081 - accuracy: 0.8611 - val_loss: 1.0089 - val_accuracy: 0.6810\n","Epoch 83/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4124 - accuracy: 0.8608 - val_loss: 0.9822 - val_accuracy: 0.6912\n","Epoch 84/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4284 - accuracy: 0.8492 - val_loss: 0.9199 - val_accuracy: 0.6810\n","Epoch 85/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4152 - accuracy: 0.8517 - val_loss: 0.9866 - val_accuracy: 0.6889\n","Epoch 86/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4419 - accuracy: 0.8444 - val_loss: 1.2809 - val_accuracy: 0.6143\n","Epoch 87/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4320 - accuracy: 0.8628 - val_loss: 1.0202 - val_accuracy: 0.6923\n","Epoch 88/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4030 - accuracy: 0.8650 - val_loss: 0.9485 - val_accuracy: 0.6991\n","Epoch 89/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4046 - accuracy: 0.8662 - val_loss: 1.0143 - val_accuracy: 0.6686\n","Epoch 90/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.3930 - accuracy: 0.8752 - val_loss: 1.1564 - val_accuracy: 0.6516\n","Epoch 91/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4082 - accuracy: 0.8698 - val_loss: 1.0163 - val_accuracy: 0.6934\n","Epoch 92/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.4123 - accuracy: 0.8656 - val_loss: 1.0020 - val_accuracy: 0.6765\n","Epoch 93/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4196 - accuracy: 0.8574 - val_loss: 1.0116 - val_accuracy: 0.6629\n","Epoch 94/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4104 - accuracy: 0.8656 - val_loss: 1.0550 - val_accuracy: 0.6686\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4054 - accuracy: 0.8594 - val_loss: 0.9806 - val_accuracy: 0.6799\n","Epoch 96/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4221 - accuracy: 0.8625 - val_loss: 1.0150 - val_accuracy: 0.6753\n","Epoch 97/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3966 - accuracy: 0.8721 - val_loss: 1.0946 - val_accuracy: 0.6595\n","Epoch 98/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3947 - accuracy: 0.8687 - val_loss: 1.3452 - val_accuracy: 0.6165\n","Epoch 99/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4012 - accuracy: 0.8628 - val_loss: 0.9996 - val_accuracy: 0.6674\n","Epoch 100/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3928 - accuracy: 0.8718 - val_loss: 1.0949 - val_accuracy: 0.6652\n","{'loss': [0.5627819895744324, 0.517311155796051, 0.517594039440155, 0.5098205208778381, 0.5052217245101929, 0.503600537776947, 0.510052502155304, 0.5058271288871765, 0.5006932020187378, 0.4963989555835724, 0.48880428075790405, 0.5185806751251221, 0.4971734881401062, 0.49584895372390747, 0.5001027584075928, 0.4941088557243347, 0.4838799238204956, 0.4930853545665741, 0.4811554253101349, 0.4778791666030884, 0.4707733690738678, 0.47385480999946594, 0.478730708360672, 0.47170349955558777, 0.48017847537994385, 0.4784781336784363, 0.48386457562446594, 0.47789856791496277, 0.47400709986686707, 0.46769359707832336, 0.4642997086048126, 0.45966076850891113, 0.45722126960754395, 0.46691060066223145, 0.49611610174179077, 0.4751242995262146, 0.4659993648529053, 0.46509116888046265, 0.45657026767730713, 0.45508283376693726, 0.4618588984012604, 0.461605042219162, 0.4608759582042694, 0.46225759387016296, 0.4473789632320404, 0.4496484100818634, 0.4519927203655243, 0.4599483609199524, 0.4429186284542084, 0.45266929268836975, 0.448003888130188, 0.44601312279701233, 0.45830777287483215, 0.4467952847480774, 0.4459749758243561, 0.4510411024093628, 0.4315749406814575, 0.4314323961734772, 0.43530410528182983, 0.4320013225078583, 0.4355139434337616, 0.4325270652770996, 0.43287473917007446, 0.42878133058547974, 0.43040138483047485, 0.4352531433105469, 0.43503355979919434, 0.43502694368362427, 0.4271950125694275, 0.42337551712989807, 0.4165264368057251, 0.42378655076026917, 0.4187370538711548, 0.41900649666786194, 0.4160495102405548, 0.4409896731376648, 0.44183245301246643, 0.4587593078613281, 0.4181242883205414, 0.42359644174575806, 0.40940552949905396, 0.4081154465675354, 0.41242676973342896, 0.4284190535545349, 0.41518381237983704, 0.44190648198127747, 0.43195509910583496, 0.4030092656612396, 0.40462198853492737, 0.3930235505104065, 0.40817078948020935, 0.4122861921787262, 0.4196450710296631, 0.41040733456611633, 0.4053998589515686, 0.4220826029777527, 0.39659979939460754, 0.3946975767612457, 0.4012247323989868, 0.3928331732749939], 'accuracy': [0.7818335890769958, 0.8013582229614258, 0.8092812895774841, 0.8033390045166016, 0.8070175647735596, 0.8146576285362244, 0.8039049506187439, 0.8155065178871155, 0.8138087391853333, 0.8129597902297974, 0.8160724639892578, 0.8019241690635681, 0.8169213533401489, 0.8169213533401489, 0.8138087391853333, 0.8285229206085205, 0.826259195804596, 0.8155065178871155, 0.8194680213928223, 0.8163554072380066, 0.835597038269043, 0.8333333134651184, 0.8211658000946045, 0.8333333134651184, 0.8191850781440735, 0.819750964641571, 0.8271080851554871, 0.8293718099594116, 0.8273910880088806, 0.8364459276199341, 0.8296547532081604, 0.8364459276199341, 0.829937756061554, 0.8268251419067383, 0.8208828568458557, 0.8276740312576294, 0.8313525915145874, 0.831918478012085, 0.8341822028160095, 0.8404074907302856, 0.8285229206085205, 0.8358800411224365, 0.8293718099594116, 0.8265421390533447, 0.8423882126808167, 0.8370118737220764, 0.8378607630729675, 0.835597038269043, 0.8409733772277832, 0.8347481489181519, 0.8418223261833191, 0.8469156622886658, 0.8423882126808167, 0.8508771657943726, 0.8418223261833191, 0.8327674269676208, 0.8542727828025818, 0.8505942225456238, 0.8505942225456238, 0.853989839553833, 0.8514431118965149, 0.8576683402061462, 0.8511601686477661, 0.8537068367004395, 0.853989839553833, 0.8480475544929504, 0.8474816083908081, 0.8568194508552551, 0.8604980111122131, 0.8590831756591797, 0.8582342863082886, 0.8542727828025818, 0.8542727828025818, 0.8613469004631042, 0.8545557260513306, 0.8409733772277832, 0.846632719039917, 0.8290888667106628, 0.855970561504364, 0.8568194508552551, 0.8667232394218445, 0.8610639572143555, 0.8607810139656067, 0.8491793870925903, 0.8517261147499084, 0.8443689942359924, 0.8627617359161377, 0.8650254607200623, 0.8661573529243469, 0.8752122521400452, 0.8698358535766602, 0.8655914068222046, 0.8573853969573975, 0.8655914068222046, 0.8593661785125732, 0.8624787926673889, 0.8720995783805847, 0.8687040209770203, 0.8627617359161377, 0.8718166351318359], 'val_loss': [0.8956265449523926, 0.8636128902435303, 0.794556200504303, 0.7757202386856079, 0.773549497127533, 0.7661471962928772, 0.761653482913971, 0.7772793769836426, 0.7445006966590881, 0.7490191459655762, 0.7637871503829956, 0.7346308827400208, 0.7198319435119629, 0.7229611277580261, 0.7100021839141846, 0.7060062885284424, 0.7414780855178833, 0.7603246569633484, 0.7740383744239807, 0.7078831195831299, 0.7244477272033691, 0.7323755025863647, 0.7691639065742493, 0.7497444152832031, 0.728855311870575, 0.7383914589881897, 0.9171733856201172, 0.8397592306137085, 0.9331973195075989, 0.8338940739631653, 0.8313755393028259, 0.8455564975738525, 0.8250389695167542, 0.8058062195777893, 0.9327616095542908, 0.8367091417312622, 0.8561574220657349, 0.8269293308258057, 0.8101879954338074, 0.885473906993866, 0.8814657330513, 0.9806872010231018, 0.8152399659156799, 0.846472978591919, 0.8683104515075684, 0.9051532745361328, 0.8492732644081116, 0.9090923070907593, 0.9143819212913513, 0.8900146484375, 0.9173448085784912, 0.9029771089553833, 1.096097469329834, 0.9446157813072205, 1.0088777542114258, 0.8720611929893494, 1.0925590991973877, 0.8858591914176941, 0.9432882070541382, 0.907944917678833, 1.0783140659332275, 0.9089833498001099, 0.9205111265182495, 0.9434822201728821, 0.9967222809791565, 0.8860408663749695, 1.0105782747268677, 0.9694753289222717, 1.1733793020248413, 0.9067208766937256, 0.9137557148933411, 0.9921481013298035, 1.0566798448562622, 0.8946768641471863, 1.0882904529571533, 1.125954031944275, 0.9096649885177612, 1.5941827297210693, 0.9331364631652832, 0.9444929361343384, 0.9815823435783386, 1.0088938474655151, 0.9822081327438354, 0.919940173625946, 0.9866127371788025, 1.2808802127838135, 1.0202374458312988, 0.9485117197036743, 1.0142914056777954, 1.1563893556594849, 1.0162688493728638, 1.0020005702972412, 1.0115758180618286, 1.0549579858779907, 0.9806201457977295, 1.0149831771850586, 1.0946404933929443, 1.345172643661499, 0.9996092319488525, 1.0948872566223145], 'val_accuracy': [0.5056561231613159, 0.5124434232711792, 0.5361990928649902, 0.6131221652030945, 0.5588235259056091, 0.6289592981338501, 0.5904977321624756, 0.5520362257957458, 0.639140248298645, 0.6334841847419739, 0.570135772228241, 0.6662895679473877, 0.6470588445663452, 0.6640271544456482, 0.6730769276618958, 0.6957013607025146, 0.6165158152580261, 0.6153846383094788, 0.6176470518112183, 0.6979637742042542, 0.6979637742042542, 0.6821267008781433, 0.6662895679473877, 0.709276020526886, 0.6990950107574463, 0.7002262473106384, 0.6617646813392639, 0.709276020526886, 0.6583710312843323, 0.6889140009880066, 0.6934388875961304, 0.6776018142700195, 0.6957013607025146, 0.7115384340286255, 0.6708144545555115, 0.6945701241493225, 0.7002262473106384, 0.6787330508232117, 0.6990950107574463, 0.685520350933075, 0.6674208045005798, 0.6595022678375244, 0.7036198973655701, 0.7115384340286255, 0.7104072570800781, 0.6866515874862671, 0.7036198973655701, 0.6753393411636353, 0.6968325972557068, 0.6945701241493225, 0.6809954643249512, 0.6742081642150879, 0.6595022678375244, 0.679864227771759, 0.6447963714599609, 0.6968325972557068, 0.651583731174469, 0.6990950107574463, 0.6832579374313354, 0.7002262473106384, 0.6561086177825928, 0.6730769276618958, 0.6832579374313354, 0.6889140009880066, 0.6821267008781433, 0.6843891143798828, 0.6606335043907166, 0.6662895679473877, 0.6289592981338501, 0.6900452375411987, 0.6877828240394592, 0.6662895679473877, 0.6606335043907166, 0.6877828240394592, 0.651583731174469, 0.6877828240394592, 0.6889140009880066, 0.5882353186607361, 0.6968325972557068, 0.6945701241493225, 0.668552041053772, 0.6809954643249512, 0.6911764740943909, 0.6809954643249512, 0.6889140009880066, 0.6142534017562866, 0.692307710647583, 0.6990950107574463, 0.668552041053772, 0.651583731174469, 0.6934388875961304, 0.6764705777168274, 0.662895917892456, 0.668552041053772, 0.679864227771759, 0.6753393411636353, 0.6595022678375244, 0.6165158152580261, 0.6674208045005798, 0.6651583909988403]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 37, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 37, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 19, 256)           0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 19, 256)           196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 10, 256)           0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 10, 512)           393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 5, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 3, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 3, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 3, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 3, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               393728    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4100673 (15.64 MB)\n","Trainable params: 4100161 (15.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/31 [==========================>...] - ETA: 0s - loss: 0.5751 - accuracy: 0.7709"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 9s 53ms/step - loss: 0.5726 - accuracy: 0.7698 - val_loss: 0.8102 - val_accuracy: 0.5021\n","Epoch 2/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5333 - accuracy: 0.7941 - val_loss: 0.8207 - val_accuracy: 0.5031\n","Epoch 3/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5488 - accuracy: 0.7744 - val_loss: 0.8601 - val_accuracy: 0.4990\n","Epoch 4/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5466 - accuracy: 0.7907 - val_loss: 0.8184 - val_accuracy: 0.5134\n","Epoch 5/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5406 - accuracy: 0.7845 - val_loss: 0.7792 - val_accuracy: 0.6105\n","Epoch 6/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5200 - accuracy: 0.7946 - val_loss: 0.7760 - val_accuracy: 0.6085\n","Epoch 7/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5245 - accuracy: 0.7866 - val_loss: 0.7690 - val_accuracy: 0.6126\n","Epoch 8/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5195 - accuracy: 0.7984 - val_loss: 0.7650 - val_accuracy: 0.6105\n","Epoch 9/100\n","31/31 [==============================] - 1s 30ms/step - loss: 0.5605 - accuracy: 0.7731 - val_loss: 0.7661 - val_accuracy: 0.6260\n","Epoch 10/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5546 - accuracy: 0.7855 - val_loss: 0.7617 - val_accuracy: 0.6188\n","Epoch 11/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5255 - accuracy: 0.7935 - val_loss: 0.7635 - val_accuracy: 0.6209\n","Epoch 12/100\n","31/31 [==============================] - 1s 31ms/step - loss: 0.5114 - accuracy: 0.8059 - val_loss: 0.7474 - val_accuracy: 0.6405\n","Epoch 13/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5058 - accuracy: 0.8119 - val_loss: 0.7390 - val_accuracy: 0.6405\n","Epoch 14/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5187 - accuracy: 0.7935 - val_loss: 0.7508 - val_accuracy: 0.6405\n","Epoch 15/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5059 - accuracy: 0.7982 - val_loss: 0.7480 - val_accuracy: 0.6498\n","Epoch 16/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5154 - accuracy: 0.8010 - val_loss: 0.7511 - val_accuracy: 0.6653\n","Epoch 17/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5102 - accuracy: 0.8096 - val_loss: 0.7400 - val_accuracy: 0.6426\n","Epoch 18/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5196 - accuracy: 0.8013 - val_loss: 0.7612 - val_accuracy: 0.6260\n","Epoch 19/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.5101 - accuracy: 0.8134 - val_loss: 0.7517 - val_accuracy: 0.6674\n","Epoch 20/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5052 - accuracy: 0.8085 - val_loss: 0.7912 - val_accuracy: 0.6601\n","Epoch 21/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5014 - accuracy: 0.8090 - val_loss: 0.7891 - val_accuracy: 0.6343\n","Epoch 22/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.5051 - accuracy: 0.8132 - val_loss: 0.8074 - val_accuracy: 0.6725\n","Epoch 23/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5067 - accuracy: 0.8096 - val_loss: 0.8140 - val_accuracy: 0.6477\n","Epoch 24/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5134 - accuracy: 0.7992 - val_loss: 0.8347 - val_accuracy: 0.6353\n","Epoch 25/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5014 - accuracy: 0.8085 - val_loss: 0.8682 - val_accuracy: 0.6694\n","Epoch 26/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4933 - accuracy: 0.8142 - val_loss: 0.8347 - val_accuracy: 0.6705\n","Epoch 27/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4992 - accuracy: 0.8085 - val_loss: 0.8431 - val_accuracy: 0.6725\n","Epoch 28/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.5023 - accuracy: 0.8147 - val_loss: 0.8459 - val_accuracy: 0.6756\n","Epoch 29/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5054 - accuracy: 0.8016 - val_loss: 1.0786 - val_accuracy: 0.6002\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4928 - accuracy: 0.8093 - val_loss: 0.8916 - val_accuracy: 0.6343\n","Epoch 31/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4894 - accuracy: 0.8098 - val_loss: 0.9313 - val_accuracy: 0.6353\n","Epoch 32/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4919 - accuracy: 0.8145 - val_loss: 0.9566 - val_accuracy: 0.6446\n","Epoch 33/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4851 - accuracy: 0.8160 - val_loss: 0.8572 - val_accuracy: 0.6488\n","Epoch 34/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4814 - accuracy: 0.8165 - val_loss: 0.9657 - val_accuracy: 0.6250\n","Epoch 35/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.4868 - accuracy: 0.8186 - val_loss: 0.8454 - val_accuracy: 0.6808\n","Epoch 36/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5029 - accuracy: 0.8145 - val_loss: 0.8474 - val_accuracy: 0.6488\n","Epoch 37/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5074 - accuracy: 0.8013 - val_loss: 1.0928 - val_accuracy: 0.6384\n","Epoch 38/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4956 - accuracy: 0.8114 - val_loss: 1.0263 - val_accuracy: 0.6550\n","Epoch 39/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4879 - accuracy: 0.8163 - val_loss: 0.9787 - val_accuracy: 0.6426\n","Epoch 40/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4821 - accuracy: 0.8222 - val_loss: 0.9318 - val_accuracy: 0.6312\n","Epoch 41/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4650 - accuracy: 0.8297 - val_loss: 0.9045 - val_accuracy: 0.6395\n","Epoch 42/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4712 - accuracy: 0.8186 - val_loss: 0.8573 - val_accuracy: 0.6715\n","Epoch 43/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4808 - accuracy: 0.8168 - val_loss: 1.0138 - val_accuracy: 0.6271\n","Epoch 44/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4806 - accuracy: 0.8199 - val_loss: 0.9518 - val_accuracy: 0.6643\n","Epoch 45/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4853 - accuracy: 0.8196 - val_loss: 0.8939 - val_accuracy: 0.6643\n","Epoch 46/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4829 - accuracy: 0.8142 - val_loss: 0.8934 - val_accuracy: 0.6426\n","Epoch 47/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4784 - accuracy: 0.8225 - val_loss: 0.9226 - val_accuracy: 0.6591\n","Epoch 48/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4808 - accuracy: 0.8196 - val_loss: 0.9855 - val_accuracy: 0.6663\n","Epoch 49/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4806 - accuracy: 0.8220 - val_loss: 0.9665 - val_accuracy: 0.6353\n","Epoch 50/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.4889 - accuracy: 0.8186 - val_loss: 1.0187 - val_accuracy: 0.6198\n","Epoch 51/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4675 - accuracy: 0.8326 - val_loss: 0.9549 - val_accuracy: 0.6508\n","Epoch 52/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4692 - accuracy: 0.8367 - val_loss: 0.9607 - val_accuracy: 0.6229\n","Epoch 53/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4765 - accuracy: 0.8212 - val_loss: 0.9454 - val_accuracy: 0.6622\n","Epoch 54/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4695 - accuracy: 0.8243 - val_loss: 0.9179 - val_accuracy: 0.6302\n","Epoch 55/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4601 - accuracy: 0.8333 - val_loss: 0.9565 - val_accuracy: 0.6591\n","Epoch 56/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4763 - accuracy: 0.8256 - val_loss: 0.9082 - val_accuracy: 0.6663\n","Epoch 57/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4948 - accuracy: 0.8168 - val_loss: 0.9069 - val_accuracy: 0.6663\n","Epoch 58/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4611 - accuracy: 0.8354 - val_loss: 1.0041 - val_accuracy: 0.6539\n","Epoch 59/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4609 - accuracy: 0.8313 - val_loss: 0.9293 - val_accuracy: 0.6643\n","Epoch 60/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4568 - accuracy: 0.8323 - val_loss: 1.0087 - val_accuracy: 0.6508\n","Epoch 61/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4560 - accuracy: 0.8364 - val_loss: 0.8987 - val_accuracy: 0.6694\n","Epoch 62/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4667 - accuracy: 0.8336 - val_loss: 0.9927 - val_accuracy: 0.6477\n","Epoch 63/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4671 - accuracy: 0.8276 - val_loss: 0.9893 - val_accuracy: 0.6446\n","Epoch 64/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4548 - accuracy: 0.8395 - val_loss: 1.0112 - val_accuracy: 0.6229\n","Epoch 65/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4586 - accuracy: 0.8323 - val_loss: 1.1357 - val_accuracy: 0.6095\n","Epoch 66/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4527 - accuracy: 0.8419 - val_loss: 0.9576 - val_accuracy: 0.6632\n","Epoch 67/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4587 - accuracy: 0.8367 - val_loss: 0.9550 - val_accuracy: 0.6426\n","Epoch 68/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4535 - accuracy: 0.8336 - val_loss: 0.9873 - val_accuracy: 0.6591\n","Epoch 69/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4620 - accuracy: 0.8341 - val_loss: 0.9746 - val_accuracy: 0.6271\n","Epoch 70/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4500 - accuracy: 0.8442 - val_loss: 0.9025 - val_accuracy: 0.6674\n","Epoch 71/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4469 - accuracy: 0.8475 - val_loss: 0.9410 - val_accuracy: 0.6725\n","Epoch 72/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4531 - accuracy: 0.8390 - val_loss: 1.0036 - val_accuracy: 0.6333\n","Epoch 73/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4634 - accuracy: 0.8295 - val_loss: 0.9938 - val_accuracy: 0.6240\n","Epoch 74/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4500 - accuracy: 0.8364 - val_loss: 0.9344 - val_accuracy: 0.6601\n","Epoch 75/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4470 - accuracy: 0.8395 - val_loss: 0.9731 - val_accuracy: 0.6560\n","Epoch 76/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4400 - accuracy: 0.8460 - val_loss: 1.0370 - val_accuracy: 0.6415\n","Epoch 77/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4525 - accuracy: 0.8380 - val_loss: 1.0023 - val_accuracy: 0.6229\n","Epoch 78/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4397 - accuracy: 0.8395 - val_loss: 1.0089 - val_accuracy: 0.6333\n","Epoch 79/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4419 - accuracy: 0.8372 - val_loss: 0.9943 - val_accuracy: 0.6312\n","Epoch 80/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4372 - accuracy: 0.8457 - val_loss: 0.9813 - val_accuracy: 0.6508\n","Epoch 81/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4360 - accuracy: 0.8509 - val_loss: 1.0185 - val_accuracy: 0.6457\n","Epoch 82/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4384 - accuracy: 0.8473 - val_loss: 1.0855 - val_accuracy: 0.6240\n","Epoch 83/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4478 - accuracy: 0.8388 - val_loss: 1.5203 - val_accuracy: 0.5785\n","Epoch 84/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4488 - accuracy: 0.8403 - val_loss: 1.0075 - val_accuracy: 0.6260\n","Epoch 85/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4335 - accuracy: 0.8432 - val_loss: 1.2160 - val_accuracy: 0.6271\n","Epoch 86/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4267 - accuracy: 0.8475 - val_loss: 0.9690 - val_accuracy: 0.6612\n","Epoch 87/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4388 - accuracy: 0.8470 - val_loss: 1.0440 - val_accuracy: 0.6643\n","Epoch 88/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4256 - accuracy: 0.8514 - val_loss: 0.9805 - val_accuracy: 0.6333\n","Epoch 89/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4335 - accuracy: 0.8468 - val_loss: 1.0515 - val_accuracy: 0.6446\n","Epoch 90/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4354 - accuracy: 0.8411 - val_loss: 1.0246 - val_accuracy: 0.6581\n","Epoch 91/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4295 - accuracy: 0.8501 - val_loss: 0.9627 - val_accuracy: 0.6426\n","Epoch 92/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4428 - accuracy: 0.8447 - val_loss: 1.2276 - val_accuracy: 0.6260\n","Epoch 93/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4536 - accuracy: 0.8372 - val_loss: 0.9417 - val_accuracy: 0.6539\n","Epoch 94/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4180 - accuracy: 0.8597 - val_loss: 1.0007 - val_accuracy: 0.6539\n","Epoch 95/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4305 - accuracy: 0.8488 - val_loss: 1.1334 - val_accuracy: 0.6198\n","Epoch 96/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4251 - accuracy: 0.8522 - val_loss: 1.0023 - val_accuracy: 0.6591\n","Epoch 97/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4168 - accuracy: 0.8525 - val_loss: 0.9996 - val_accuracy: 0.6570\n","Epoch 98/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4321 - accuracy: 0.8432 - val_loss: 1.0324 - val_accuracy: 0.6426\n","Epoch 99/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4181 - accuracy: 0.8527 - val_loss: 1.2361 - val_accuracy: 0.5981\n","Epoch 100/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4317 - accuracy: 0.8434 - val_loss: 1.2320 - val_accuracy: 0.6074\n","{'loss': [0.5726378560066223, 0.5332856178283691, 0.5487725734710693, 0.5466094613075256, 0.5405709743499756, 0.5199680924415588, 0.5244891047477722, 0.5194588899612427, 0.5604847073554993, 0.5546072721481323, 0.5255496501922607, 0.5114157795906067, 0.5057757496833801, 0.5186637043952942, 0.5058872103691101, 0.5153634548187256, 0.5102067589759827, 0.5195947289466858, 0.5101494193077087, 0.5052372217178345, 0.501420259475708, 0.5050928592681885, 0.5066941380500793, 0.5134081244468689, 0.5014059543609619, 0.4932885468006134, 0.49922576546669006, 0.502315878868103, 0.5054115056991577, 0.49276280403137207, 0.4894331693649292, 0.4918520450592041, 0.4850735664367676, 0.4813511371612549, 0.48678115010261536, 0.5029481053352356, 0.5073792934417725, 0.49556824564933777, 0.4879138469696045, 0.4820876717567444, 0.46504613757133484, 0.471163272857666, 0.4808211028575897, 0.48055580258369446, 0.4852871894836426, 0.4829033613204956, 0.4783698320388794, 0.4808180034160614, 0.48056501150131226, 0.48892587423324585, 0.46747133135795593, 0.4691588282585144, 0.4764614999294281, 0.4695468246936798, 0.46006685495376587, 0.4762924909591675, 0.4948357343673706, 0.46114200353622437, 0.4608900249004364, 0.4568362832069397, 0.4559980630874634, 0.46670088171958923, 0.46710366010665894, 0.45484066009521484, 0.4585951864719391, 0.45270031690597534, 0.45872795581817627, 0.45351311564445496, 0.4620451033115387, 0.450002521276474, 0.4469304382801056, 0.453056275844574, 0.46338963508605957, 0.45004576444625854, 0.44702860713005066, 0.44001084566116333, 0.45245736837387085, 0.43967628479003906, 0.4418829679489136, 0.43720531463623047, 0.43601930141448975, 0.4384062588214874, 0.4478156566619873, 0.4488465487957001, 0.43354448676109314, 0.4267464578151703, 0.4388105869293213, 0.42560875415802, 0.4335364103317261, 0.43535298109054565, 0.42948004603385925, 0.4427751898765564, 0.4536437690258026, 0.41796067357063293, 0.43054789304733276, 0.4250556230545044, 0.41677916049957275, 0.432111531496048, 0.4180614948272705, 0.4317163825035095], 'accuracy': [0.7697674632072449, 0.7940568327903748, 0.7744185924530029, 0.7906976938247681, 0.7844961285591125, 0.7945736646652222, 0.7865633368492126, 0.7984496355056763, 0.7731266021728516, 0.7855297327041626, 0.7935400605201721, 0.8059431314468384, 0.8118863105773926, 0.7935400605201721, 0.7981911897659302, 0.801033616065979, 0.8095607161521912, 0.8012920022010803, 0.8134366869926453, 0.8085271120071411, 0.8090439438819885, 0.813178300857544, 0.8095607161521912, 0.7992247939109802, 0.8085271120071411, 0.814211905002594, 0.8085271120071411, 0.8147286772727966, 0.8015503883361816, 0.8093023300170898, 0.8098191022872925, 0.8144702911376953, 0.816020667552948, 0.8165374398231506, 0.8186046481132507, 0.8144702911376953, 0.8012920022010803, 0.8113695383071899, 0.8162790536880493, 0.8222222328186035, 0.8297157883644104, 0.8186046481132507, 0.8167958855628967, 0.8198966383934021, 0.8196382522583008, 0.814211905002594, 0.8224806189537048, 0.8196382522583008, 0.8219638466835022, 0.8186046481132507, 0.8325581550598145, 0.8366925120353699, 0.8211886286735535, 0.8242893815040588, 0.8333333134651184, 0.8255813717842102, 0.8167958855628967, 0.8354005217552185, 0.8312661647796631, 0.8322997689247131, 0.8364341259002686, 0.8335917592048645, 0.8276485800743103, 0.8395348787307739, 0.8322997689247131, 0.8418604731559753, 0.8366925120353699, 0.8335917592048645, 0.8341085314750671, 0.8441860675811768, 0.8475452065467834, 0.8390181064605713, 0.8294573426246643, 0.8364341259002686, 0.8395348787307739, 0.8459948301315308, 0.8379845023155212, 0.8395348787307739, 0.8372092843055725, 0.8457364439964294, 0.8509044051170349, 0.8472868204116821, 0.8387596607208252, 0.8403100967407227, 0.8431524634361267, 0.8475452065467834, 0.8470284342765808, 0.8514211773872375, 0.8467700481414795, 0.8410852551460266, 0.8501291871070862, 0.8447028398513794, 0.8372092843055725, 0.8596899509429932, 0.8488371968269348, 0.8521963953971863, 0.8524547815322876, 0.8431524634361267, 0.8527131676673889, 0.843410849571228], 'val_loss': [0.8101514577865601, 0.820665717124939, 0.8601189851760864, 0.8183935284614563, 0.7791852951049805, 0.775988757610321, 0.7690343856811523, 0.765002965927124, 0.7661037445068359, 0.7616832256317139, 0.7635322213172913, 0.7473999261856079, 0.7389598488807678, 0.7508183121681213, 0.7479670643806458, 0.7511194348335266, 0.7399630546569824, 0.7611950039863586, 0.7517209649085999, 0.7911922931671143, 0.7890713810920715, 0.8074097633361816, 0.8139909505844116, 0.8347311019897461, 0.8681623935699463, 0.8346635699272156, 0.8430993556976318, 0.8459393978118896, 1.0785770416259766, 0.8915587067604065, 0.9312857389450073, 0.95664381980896, 0.8572363257408142, 0.9657469391822815, 0.8453683257102966, 0.8474184274673462, 1.092771291732788, 1.0262902975082397, 0.9786888957023621, 0.9317931532859802, 0.904543936252594, 0.8573018312454224, 1.013848066329956, 0.9518457651138306, 0.8939134478569031, 0.8933979868888855, 0.9226414561271667, 0.9854561686515808, 0.9665114283561707, 1.0187307596206665, 0.9549161195755005, 0.9607387185096741, 0.9454053044319153, 0.9178555011749268, 0.9564666748046875, 0.9082375764846802, 0.9068560004234314, 1.0040571689605713, 0.9292783141136169, 1.0087032318115234, 0.8986692428588867, 0.9927142858505249, 0.9893447756767273, 1.011210322380066, 1.1357311010360718, 0.9575750231742859, 0.9550082087516785, 0.9873093366622925, 0.9745932221412659, 0.9025213122367859, 0.9410253763198853, 1.003584623336792, 0.9937567710876465, 0.9343850612640381, 0.9731301665306091, 1.037015438079834, 1.0023130178451538, 1.008949637413025, 0.9943383932113647, 0.9813360571861267, 1.0185115337371826, 1.0855079889297485, 1.5203415155410767, 1.007536768913269, 1.2159708738327026, 0.9690253734588623, 1.0440465211868286, 0.9805475473403931, 1.0514507293701172, 1.024619221687317, 0.9626812934875488, 1.2276169061660767, 0.9416559934616089, 1.0006897449493408, 1.133410930633545, 1.0022684335708618, 0.9995978474617004, 1.0323915481567383, 1.2361372709274292, 1.2319831848144531], 'val_accuracy': [0.5020661354064941, 0.5030992031097412, 0.49896693229675293, 0.5134297609329224, 0.6105371713638306, 0.6084710955619812, 0.6126033067703247, 0.6105371713638306, 0.6260330677032471, 0.6188016533851624, 0.6208677887916565, 0.6404958963394165, 0.6404958963394165, 0.6404958963394165, 0.6497933864593506, 0.6652892827987671, 0.6425619721412659, 0.6260330677032471, 0.6673553586006165, 0.6601239442825317, 0.6342975497245789, 0.672520637512207, 0.6477272510528564, 0.6353305578231812, 0.6694214940071106, 0.6704545617103577, 0.672520637512207, 0.6756198406219482, 0.6002066135406494, 0.6342975497245789, 0.6353305578231812, 0.64462810754776, 0.6487603187561035, 0.625, 0.6807851195335388, 0.6487603187561035, 0.6384297609329224, 0.6549586653709412, 0.6425619721412659, 0.6311983466148376, 0.6394628286361694, 0.6714876294136047, 0.6270661354064941, 0.66425621509552, 0.66425621509552, 0.6425619721412659, 0.6590909361839294, 0.6663222908973694, 0.6353305578231812, 0.6198347210884094, 0.6508264541625977, 0.6229338645935059, 0.6621900796890259, 0.6301652789115906, 0.6590909361839294, 0.6663222908973694, 0.6663222908973694, 0.6539255976676941, 0.66425621509552, 0.6508264541625977, 0.6694214940071106, 0.6477272510528564, 0.64462810754776, 0.6229338645935059, 0.6095041036605835, 0.663223147392273, 0.6425619721412659, 0.6590909361839294, 0.6270661354064941, 0.6673553586006165, 0.672520637512207, 0.6332644820213318, 0.6239669322967529, 0.6601239442825317, 0.6559917330741882, 0.6415289044380188, 0.6229338645935059, 0.6332644820213318, 0.6311983466148376, 0.6508264541625977, 0.6456611752510071, 0.6239669322967529, 0.5785123705863953, 0.6260330677032471, 0.6270661354064941, 0.6611570119857788, 0.66425621509552, 0.6332644820213318, 0.64462810754776, 0.6580578684806824, 0.6425619721412659, 0.6260330677032471, 0.6539255976676941, 0.6539255976676941, 0.6198347210884094, 0.6590909361839294, 0.6570248007774353, 0.6425619721412659, 0.5981404781341553, 0.6074380278587341]}\n","32/32 [==============================] - 1s 4ms/step\n"]}]},{"cell_type":"code","source":["metrics_df_gru"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"collapsed":true,"id":"kleLoWSV5B7Y","executionInfo":{"status":"ok","timestamp":1717506995237,"user_tz":-360,"elapsed":39,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}},"outputId":"552a6d5a-a805-460b-d85a-61a829b81248"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Client  Accuracy  Precision    Recall        F1  Sensitivity  Specificity  \\\n","0        0  0.556114   0.603715  0.326633  0.423913     0.326633     0.785595   \n","1        1  0.550847   0.577922  0.377119  0.456410     0.377119     0.724576   \n","2        2  0.550201   0.565104  0.435743  0.492063     0.435743     0.664659   \n","3        0  0.565327   0.558735  0.621441  0.588422     0.621441     0.509213   \n","4        1  0.569209   0.606987  0.392655  0.476844     0.392655     0.745763   \n","5        2  0.567269   0.652968  0.287149  0.398884     0.287149     0.847390   \n","6        0  0.601340   0.596184  0.628141  0.611746     0.628141     0.574539   \n","7        1  0.593927   0.595409  0.586158  0.590747     0.586158     0.601695   \n","8        2  0.632530   0.653488  0.564257  0.605603     0.564257     0.700803   \n","9        0  0.633166   0.661914  0.544389  0.597426     0.544389     0.721943   \n","10       1  0.626412   0.606421  0.720339  0.658489     0.720339     0.532486   \n","11       2  0.672691   0.727513  0.552209  0.627854     0.552209     0.793173   \n","12       0  0.664154   0.686312  0.604690  0.642921     0.604690     0.723618   \n","13       1  0.663842   0.710145  0.553672  0.622222     0.553672     0.774011   \n","14       2  0.717871   0.743820  0.664659  0.702015     0.664659     0.771084   \n","\n","       Kappa  \n","0   0.112228  \n","1   0.101695  \n","2   0.100402  \n","3   0.130653  \n","4   0.138418  \n","5   0.134538  \n","6   0.202680  \n","7   0.187853  \n","8   0.265060  \n","9   0.266332  \n","10  0.252825  \n","11  0.345382  \n","12  0.328308  \n","13  0.327684  \n","14  0.435743  "],"text/html":["\n","  <div id=\"df-2fc933af-1f88-4e23-bf9b-61425e248978\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Client</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Sensitivity</th>\n","      <th>Specificity</th>\n","      <th>Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.556114</td>\n","      <td>0.603715</td>\n","      <td>0.326633</td>\n","      <td>0.423913</td>\n","      <td>0.326633</td>\n","      <td>0.785595</td>\n","      <td>0.112228</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.550847</td>\n","      <td>0.577922</td>\n","      <td>0.377119</td>\n","      <td>0.456410</td>\n","      <td>0.377119</td>\n","      <td>0.724576</td>\n","      <td>0.101695</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.550201</td>\n","      <td>0.565104</td>\n","      <td>0.435743</td>\n","      <td>0.492063</td>\n","      <td>0.435743</td>\n","      <td>0.664659</td>\n","      <td>0.100402</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.565327</td>\n","      <td>0.558735</td>\n","      <td>0.621441</td>\n","      <td>0.588422</td>\n","      <td>0.621441</td>\n","      <td>0.509213</td>\n","      <td>0.130653</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.569209</td>\n","      <td>0.606987</td>\n","      <td>0.392655</td>\n","      <td>0.476844</td>\n","      <td>0.392655</td>\n","      <td>0.745763</td>\n","      <td>0.138418</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2</td>\n","      <td>0.567269</td>\n","      <td>0.652968</td>\n","      <td>0.287149</td>\n","      <td>0.398884</td>\n","      <td>0.287149</td>\n","      <td>0.847390</td>\n","      <td>0.134538</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0.601340</td>\n","      <td>0.596184</td>\n","      <td>0.628141</td>\n","      <td>0.611746</td>\n","      <td>0.628141</td>\n","      <td>0.574539</td>\n","      <td>0.202680</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0.593927</td>\n","      <td>0.595409</td>\n","      <td>0.586158</td>\n","      <td>0.590747</td>\n","      <td>0.586158</td>\n","      <td>0.601695</td>\n","      <td>0.187853</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>0.632530</td>\n","      <td>0.653488</td>\n","      <td>0.564257</td>\n","      <td>0.605603</td>\n","      <td>0.564257</td>\n","      <td>0.700803</td>\n","      <td>0.265060</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>0.633166</td>\n","      <td>0.661914</td>\n","      <td>0.544389</td>\n","      <td>0.597426</td>\n","      <td>0.544389</td>\n","      <td>0.721943</td>\n","      <td>0.266332</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0.626412</td>\n","      <td>0.606421</td>\n","      <td>0.720339</td>\n","      <td>0.658489</td>\n","      <td>0.720339</td>\n","      <td>0.532486</td>\n","      <td>0.252825</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2</td>\n","      <td>0.672691</td>\n","      <td>0.727513</td>\n","      <td>0.552209</td>\n","      <td>0.627854</td>\n","      <td>0.552209</td>\n","      <td>0.793173</td>\n","      <td>0.345382</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>0.664154</td>\n","      <td>0.686312</td>\n","      <td>0.604690</td>\n","      <td>0.642921</td>\n","      <td>0.604690</td>\n","      <td>0.723618</td>\n","      <td>0.328308</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0.663842</td>\n","      <td>0.710145</td>\n","      <td>0.553672</td>\n","      <td>0.622222</td>\n","      <td>0.553672</td>\n","      <td>0.774011</td>\n","      <td>0.327684</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2</td>\n","      <td>0.717871</td>\n","      <td>0.743820</td>\n","      <td>0.664659</td>\n","      <td>0.702015</td>\n","      <td>0.664659</td>\n","      <td>0.771084</td>\n","      <td>0.435743</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fc933af-1f88-4e23-bf9b-61425e248978')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2fc933af-1f88-4e23-bf9b-61425e248978 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2fc933af-1f88-4e23-bf9b-61425e248978');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-08c5b4e4-4dcf-4849-a280-46db8be952bb\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-08c5b4e4-4dcf-4849-a280-46db8be952bb')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-08c5b4e4-4dcf-4849-a280-46db8be952bb button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"metrics_df_gru","summary":"{\n  \"name\": \"metrics_df_gru\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Client\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05243662768636695,\n        \"min\": 0.5502008032128514,\n        \"max\": 0.7178714859437751,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.6331658291457286,\n          0.6726907630522089,\n          0.5561139028475712\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05955313369449473,\n        \"min\": 0.5587349397590361,\n        \"max\": 0.7438202247191011,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.6619144602851323,\n          0.7275132275132276,\n          0.6037151702786377\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12940189413860717,\n        \"min\": 0.28714859437751006,\n        \"max\": 0.7203389830508474,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.5443886097152428,\n          0.5522088353413654,\n          0.32663316582914576\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09229668247612399,\n        \"min\": 0.398884239888424,\n        \"max\": 0.7020148462354189,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.5974264705882352,\n          0.6278538812785388,\n          0.4239130434782608\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sensitivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12940189413860717,\n        \"min\": 0.28714859437751006,\n        \"max\": 0.7203389830508474,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.5443886097152428,\n          0.5522088353413654,\n          0.32663316582914576\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Specificity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10108564700113166,\n        \"min\": 0.509212730318258,\n        \"max\": 0.8473895582329317,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.7219430485762144,\n          0.7931726907630522,\n          0.7855946398659966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kappa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10487325537273386,\n        \"min\": 0.10040160642570284,\n        \"max\": 0.4357429718875502,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.26633165829145733,\n          0.34538152610441764,\n          0.11222780569514235\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["metrics_df_gru.round(4).to_csv('/content/drive/MyDrive/EEG Signal /results/FLresults/TF Domain/GRU/Delta_tf_GRU.csv', index = False)"],"metadata":{"id":"QWSx6aHR5Bwr","executionInfo":{"status":"ok","timestamp":1717506995238,"user_tz":-360,"elapsed":20,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["print('hello')"],"metadata":{"id":"JOIXJgmr5vGB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717508264736,"user_tz":-360,"elapsed":800,"user":{"displayName":"Fazla Rabby Raihan","userId":"02045780865667266745"}},"outputId":"ae804ac6-26e8-4ec3-8405-3753382fb733"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["hello\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"5EAZ0_faeEy4"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}