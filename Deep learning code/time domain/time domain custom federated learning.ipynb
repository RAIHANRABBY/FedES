{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Mbfw8uvdftFM"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INiFJfLjgOkx"},"outputs":[],"source":["from sklearn import metrics\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, cohen_kappa_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYo2Uq77gQSH"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.tree import ExtraTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score, cross_val_predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g66575_xgVzz"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout,LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22210,"status":"ok","timestamp":1717396969946,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"MOCNWlamfr3v","outputId":"42a54867-fe55-455c-9cd5-d11bcc1a5725"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNy9eOGMf2qO"},"outputs":[],"source":["# Raw_data = '/content/drive/MyDrive/EEG Signal /Epileptic seizure/data/only data/raw_data.npz'\n","\n","Theta_data = '/content/drive/MyDrive/EEG Signal /Epileptic seizure/data/time domain /RAW/Total_time.npz'\n","\n","\n","\n","# RAW = np.load(Raw_data, allow_pickle=True)  # Allow loading pickled object arrays\n","# X = RAW['X'].astype('float64')\n","# Y = RAW['Y'].astype('float64')\n","# group = RAW['Group'].astype('float64')"]},{"cell_type":"markdown","metadata":{"id":"PzeABzSyHgKg"},"source":["This is a good performing model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Os2jd5SO1jf"},"outputs":[],"source":["# %%capture\n","# !pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbr8rHq9PnM4"},"outputs":[],"source":["# import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iwbCosHcPohF"},"outputs":[],"source":["# !wandb login"]},{"cell_type":"markdown","metadata":{"id":"6DhAYwXUSE9z"},"source":["# CNN-LSTM"]},{"cell_type":"code","source":["%%capture\n","!pip install tensorflow_addons"],"metadata":{"id":"lLmS_BM_vgab"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VvjC2xCQNHLP"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, LSTM, Dropout\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","import tensorflow_addons as tfa\n","\n","class FederatedData:\n","    def __init__(self, data_path, num_clients, scaler_type='MinMax', test_size=0.2):\n","        self.data_path = data_path\n","        self.num_clients = num_clients\n","        self.scaler_type = scaler_type\n","        self.test_size = test_size\n","        self.partitions = []  # Initialize partitions attribute\n","        self.load_data()\n","        self.scale_data()\n","        self.create_partitions()\n","\n","    def load_data(self):\n","        try:\n","            data = np.load(self.data_path, allow_pickle=True)\n","            self.X = np.moveaxis(data['X'], 1, 2)  # Move axis here\n","            self.Y = data['Y']\n","            self.groups = data['Group']  # Assuming 'Group' contains group identifiers\n","        except KeyError as e:\n","            raise ValueError(f\"Missing expected data field: {e}\")\n","        except FileNotFoundError as e:\n","            raise ValueError(f\"Data file not found: {e}\")\n","\n","    def scale_data(self):\n","        # Reshape data to 2D array for scaling\n","        X_reshaped = self.X.reshape(-1, self.X.shape[-1])\n","\n","        # Select scaler based on input\n","        if self.scaler_type == 'Standard':\n","            scaler = StandardScaler()\n","        elif self.scaler_type == 'MinMax':\n","            scaler = MinMaxScaler(feature_range=(0, 1))\n","        else:\n","            raise ValueError(\"Unsupported scaler type. Choose either 'Standard' or 'MinMax'.\")\n","\n","        # Fit and transform the data\n","        scaled_data_reshaped = scaler.fit_transform(X_reshaped)\n","\n","        # Reshape back to original shape\n","        self.X = scaled_data_reshaped.reshape(self.X.shape)\n","\n","    def create_partitions(self):\n","        gss = GroupShuffleSplit(n_splits=self.num_clients, test_size=self.test_size, random_state=42)\n","        for train_index, test_index in gss.split(self.X, self.Y, self.groups):\n","            X_train, X_test = self.X[train_index], self.X[test_index]\n","            Y_train, Y_test = self.Y[train_index], self.Y[test_index]\n","            self.partitions.append((X_train, Y_train, X_test, Y_test))\n","            print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n","\n","    def get_training_and_validation_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        partition_X_train, partition_Y_train, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        X_train, X_val, Y_train, Y_val = train_test_split(partition_X_train, partition_Y_train, test_size=0.2, random_state=42)\n","        return X_train, X_val, Y_train, Y_val, partition_X_test, partition_Y_test\n","\n","    def get_testing_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        _, _, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        return partition_X_test, partition_Y_test\n","\n","def build_sequential_model(input_shape):\n","    clear_session()\n","    model = Sequential()\n","\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\", input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(512, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(filters=1024, kernel_size=3, strides=1, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(LSTM(256, return_sequences=True))\n","    model.add(LSTM(256, return_sequences=True))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(256, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n","\n","    # opt = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","    opt = tfa.optimizers.RectifiedAdam(learning_rate=0.00009)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = recall\n","\n","    return accuracy, precision, recall, f1, sensitivity, specificity, kappa\n","\n","def federated_learning(data_path):\n","    federated_data = FederatedData(data_path, num_clients=3, scaler_type='MinMax')\n","    federated_data.create_partitions()\n","\n","    # Get the input shape from the data\n","    input_shape = federated_data.X.shape[1:]\n","    global_model = build_sequential_model(input_shape)\n","\n","    num_clients = 3\n","    local_epochs = 5\n","    global_optimizer = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","\n","    # Initialize m and v for Adam optimizer\n","    m = [np.zeros_like(w) for w in global_model.get_weights()]\n","    v = [np.zeros_like(w) for w in global_model.get_weights()]\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 1e-7\n","    t = 0\n","\n","    client_data = []\n","    for client_idx in range(num_clients):\n","        x_train, x_val, y_train, y_val, x_test, y_test = federated_data.get_training_and_validation_data(client_idx)\n","        client_data.append((x_train, x_val, y_train, y_val, x_test, y_test))\n","\n","    metrics_list = []\n","\n","    for epoch in range(local_epochs):\n","        client_models = []\n","\n","        for client in range(num_clients):\n","            x_train, x_val, y_train, y_val, x_test, y_test = client_data[client]\n","            client_model = build_sequential_model(input_shape)\n","            client_model.set_weights(global_model.get_weights())  # Initialize with global weights\n","            opt = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","            client_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","            # Set up callbacks\n","            run_name = f\"epoch_{epoch}_client_{client}\"\n","            checkpoint_filepath = f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time_domain/Total/CNN_Lstm/best_model_{run_name}.h5'\n","            checkpoint_callback = ModelCheckpoint(\n","                checkpoint_filepath,\n","                monitor='val_accuracy',\n","                save_best_only=True,\n","                mode='max'\n","            )\n","            csv_logger = CSVLogger(f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time_domain/Total/CNN_Lstm/training_log_{run_name}.csv', append=True, separator=';')\n","\n","            history = client_model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_val, y_val), callbacks=[checkpoint_callback, csv_logger])\n","            client_models.append(client_model)\n","\n","            print(history.history)\n","\n","            # Load the best model before evaluation\n","            best_model = load_model(checkpoint_filepath)\n","\n","            # Evaluate client model\n","            y_pred = (best_model.predict(x_test) > 0.5).astype(\"int32\")\n","            accuracy, precision, recall, f1, sensitivity, specificity, kappa = compute_metrics(y_test, y_pred)\n","            metrics_list.append({\n","                'Client': client,\n","                'Accuracy': accuracy,\n","                'Precision': precision,\n","                'Recall': recall,\n","                'F1': f1,\n","                'Sensitivity': sensitivity,\n","                'Specificity': specificity,\n","                'Kappa': kappa\n","            })\n","\n","        global_weights = global_model.get_weights()\n","        layer_start_idx = 0\n","        for layer in global_model.layers:\n","            layer_weights = []\n","            layer_biases = []\n","            num_params = len(layer.get_weights())\n","            for i in range(num_clients):\n","                layer_params = client_models[i].get_weights()[layer_start_idx:layer_start_idx + num_params]\n","                if num_params > 0:\n","                    layer_weights.append(layer_params[0])\n","                if num_params > 1:\n","                    layer_biases.append(layer_params[1])\n","\n","            if len(layer_weights) > 0:\n","                averaged_layer_weights = np.mean(layer_weights, axis=0)\n","                if averaged_layer_weights.shape == global_weights[layer_start_idx].shape:\n","                    global_weights[layer_start_idx] = averaged_layer_weights\n","                else:\n","                    print(f\"Warning: Shape mismatch for weights at layer {layer.name}, expected {global_weights[layer_start_idx].shape} but got {averaged_layer_weights.shape}\")\n","            if len(layer_biases) > 0:\n","                averaged_layer_biases = np.mean(layer_biases, axis=0)\n","                if averaged_layer_biases.shape == global_weights[layer_start_idx + 1].shape:\n","                    global_weights[layer_start_idx + 1] = averaged_layer_biases\n","                else:\n","                    print(f\"Warning: Shape mismatch for biases at layer {layer.name}, expected {global_weights[layer_start_idx + 1].shape} but got {averaged_layer_biases.shape}\")\n","\n","            layer_start_idx += num_params\n","\n","        # Apply FedOpt (Adam) update to global weights\n","        t += 1\n","        for i in range(len(global_weights)):\n","            g_t = global_weights[i] - global_model.get_weights()[i]  # Gradient\n","            m[i] = beta1 * m[i] + (1 - beta1) * g_t\n","            v[i] = beta2 * v[i] + (1 - beta2) * (g_t ** 2)\n","            m_hat = m[i] / (1 - beta1 ** t)\n","            v_hat = v[i] / (1 - beta2 ** t)\n","            global_weights[i] -= global_optimizer.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n","\n","        global_model.set_weights(global_weights)\n","\n","    metrics_df = pd.DataFrame(metrics_list)\n","    return global_model, metrics_df\n","\n","# Example usage\n","# data_path = 'path/to/your/data.npz'\n","# global_model, metrics_df = federated_learning(data_path)\n","# print(metrics_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVURUnmYNNNg","outputId":"a6e605e3-83ca-44b5-ffce-7e24abbe778f","executionInfo":{"status":"ok","timestamp":1717401990261,"user_tz":-360,"elapsed":1210533,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (4640, 18, 29), Test shape: (1194, 18, 29)\n","Train shape: (4418, 18, 29), Test shape: (1416, 18, 29)\n","Train shape: (4838, 18, 29), Test shape: (996, 18, 29)\n","Train shape: (4640, 18, 29), Test shape: (1194, 18, 29)\n","Train shape: (4418, 18, 29), Test shape: (1416, 18, 29)\n","Train shape: (4838, 18, 29), Test shape: (996, 18, 29)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","26/29 [=========================>....] - ETA: 0s - loss: 1.4001 - accuracy: 0.6274"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 8s 85ms/step - loss: 1.3944 - accuracy: 0.6385 - val_loss: 1.3743 - val_accuracy: 0.7144\n","Epoch 2/100\n","29/29 [==============================] - 1s 44ms/step - loss: 1.2441 - accuracy: 0.7355 - val_loss: 1.3077 - val_accuracy: 0.7565\n","Epoch 3/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.1431 - accuracy: 0.7527 - val_loss: 1.2675 - val_accuracy: 0.7791\n","Epoch 4/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.0843 - accuracy: 0.7686 - val_loss: 1.2246 - val_accuracy: 0.7812\n","Epoch 5/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.0227 - accuracy: 0.7858 - val_loss: 1.1840 - val_accuracy: 0.8157\n","Epoch 6/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9805 - accuracy: 0.7920 - val_loss: 1.1416 - val_accuracy: 0.8050\n","Epoch 7/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9368 - accuracy: 0.7982 - val_loss: 1.1053 - val_accuracy: 0.7834\n","Epoch 8/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.9169 - accuracy: 0.7918 - val_loss: 1.0810 - val_accuracy: 0.7737\n","Epoch 9/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8712 - accuracy: 0.8063 - val_loss: 1.0319 - val_accuracy: 0.8039\n","Epoch 10/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8394 - accuracy: 0.8112 - val_loss: 0.9943 - val_accuracy: 0.8028\n","Epoch 11/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8000 - accuracy: 0.8157 - val_loss: 0.9534 - val_accuracy: 0.8179\n","Epoch 12/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7771 - accuracy: 0.8233 - val_loss: 0.9206 - val_accuracy: 0.8297\n","Epoch 13/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.7482 - accuracy: 0.8195 - val_loss: 0.8776 - val_accuracy: 0.8384\n","Epoch 14/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7360 - accuracy: 0.8209 - val_loss: 0.8567 - val_accuracy: 0.8082\n","Epoch 15/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7075 - accuracy: 0.8270 - val_loss: 0.7990 - val_accuracy: 0.8157\n","Epoch 16/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6928 - accuracy: 0.8295 - val_loss: 0.7994 - val_accuracy: 0.8006\n","Epoch 17/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6634 - accuracy: 0.8351 - val_loss: 0.7180 - val_accuracy: 0.8351\n","Epoch 18/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6615 - accuracy: 0.8327 - val_loss: 0.7158 - val_accuracy: 0.8351\n","Epoch 19/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6202 - accuracy: 0.8446 - val_loss: 0.6561 - val_accuracy: 0.8244\n","Epoch 20/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.6045 - accuracy: 0.8467 - val_loss: 0.6261 - val_accuracy: 0.8394\n","Epoch 21/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5976 - accuracy: 0.8456 - val_loss: 0.6369 - val_accuracy: 0.8244\n","Epoch 22/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5931 - accuracy: 0.8411 - val_loss: 0.6108 - val_accuracy: 0.8448\n","Epoch 23/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5843 - accuracy: 0.8429 - val_loss: 0.5934 - val_accuracy: 0.8470\n","Epoch 24/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.5465 - accuracy: 0.8575 - val_loss: 0.5629 - val_accuracy: 0.8481\n","Epoch 25/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5518 - accuracy: 0.8516 - val_loss: 0.5797 - val_accuracy: 0.8308\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5306 - accuracy: 0.8613 - val_loss: 0.5439 - val_accuracy: 0.8459\n","Epoch 27/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5302 - accuracy: 0.8518 - val_loss: 0.5501 - val_accuracy: 0.8394\n","Epoch 28/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.5256 - accuracy: 0.8494 - val_loss: 0.5295 - val_accuracy: 0.8534\n","Epoch 29/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4905 - accuracy: 0.8696 - val_loss: 0.5224 - val_accuracy: 0.8470\n","Epoch 30/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4697 - accuracy: 0.8742 - val_loss: 0.5353 - val_accuracy: 0.8394\n","Epoch 31/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4670 - accuracy: 0.8726 - val_loss: 0.5331 - val_accuracy: 0.8373\n","Epoch 32/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4500 - accuracy: 0.8860 - val_loss: 0.6282 - val_accuracy: 0.8114\n","Epoch 33/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4900 - accuracy: 0.8594 - val_loss: 0.5151 - val_accuracy: 0.8438\n","Epoch 34/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.4407 - accuracy: 0.8852 - val_loss: 0.5030 - val_accuracy: 0.8621\n","Epoch 35/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4214 - accuracy: 0.8898 - val_loss: 0.4927 - val_accuracy: 0.8524\n","Epoch 36/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4093 - accuracy: 0.8952 - val_loss: 0.4952 - val_accuracy: 0.8567\n","Epoch 37/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4099 - accuracy: 0.8922 - val_loss: 0.5329 - val_accuracy: 0.8502\n","Epoch 38/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3925 - accuracy: 0.8974 - val_loss: 0.5396 - val_accuracy: 0.8610\n","Epoch 39/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.3854 - accuracy: 0.9027 - val_loss: 0.4816 - val_accuracy: 0.8664\n","Epoch 40/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3663 - accuracy: 0.9060 - val_loss: 0.4855 - val_accuracy: 0.8567\n","Epoch 41/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3645 - accuracy: 0.9095 - val_loss: 0.5091 - val_accuracy: 0.8524\n","Epoch 42/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3614 - accuracy: 0.9092 - val_loss: 0.4767 - val_accuracy: 0.8610\n","Epoch 43/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3451 - accuracy: 0.9087 - val_loss: 0.4912 - val_accuracy: 0.8502\n","Epoch 44/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3243 - accuracy: 0.9200 - val_loss: 0.5416 - val_accuracy: 0.8524\n","Epoch 45/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3544 - accuracy: 0.9138 - val_loss: 0.5750 - val_accuracy: 0.8265\n","Epoch 46/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3135 - accuracy: 0.9248 - val_loss: 0.4895 - val_accuracy: 0.8631\n","Epoch 47/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3076 - accuracy: 0.9278 - val_loss: 0.4796 - val_accuracy: 0.8567\n","Epoch 48/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2819 - accuracy: 0.9370 - val_loss: 0.5232 - val_accuracy: 0.8534\n","Epoch 49/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3141 - accuracy: 0.9203 - val_loss: 0.5889 - val_accuracy: 0.8244\n","Epoch 50/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2899 - accuracy: 0.9318 - val_loss: 0.5281 - val_accuracy: 0.8534\n","Epoch 51/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2892 - accuracy: 0.9310 - val_loss: 0.4862 - val_accuracy: 0.8588\n","Epoch 52/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2718 - accuracy: 0.9372 - val_loss: 0.5019 - val_accuracy: 0.8567\n","Epoch 53/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2748 - accuracy: 0.9318 - val_loss: 0.4889 - val_accuracy: 0.8610\n","Epoch 54/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2574 - accuracy: 0.9413 - val_loss: 0.5685 - val_accuracy: 0.8524\n","Epoch 55/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.2441 - accuracy: 0.9477 - val_loss: 0.7416 - val_accuracy: 0.8265\n","Epoch 56/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.2696 - accuracy: 0.9351 - val_loss: 0.5025 - val_accuracy: 0.8675\n","Epoch 57/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2410 - accuracy: 0.9461 - val_loss: 0.6028 - val_accuracy: 0.8567\n","Epoch 58/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2467 - accuracy: 0.9434 - val_loss: 0.5685 - val_accuracy: 0.8631\n","Epoch 59/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.2163 - accuracy: 0.9569 - val_loss: 0.6149 - val_accuracy: 0.8265\n","Epoch 60/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2220 - accuracy: 0.9491 - val_loss: 0.6415 - val_accuracy: 0.8297\n","Epoch 61/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.2139 - accuracy: 0.9566 - val_loss: 0.5966 - val_accuracy: 0.8685\n","Epoch 62/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2153 - accuracy: 0.9564 - val_loss: 0.5725 - val_accuracy: 0.8459\n","Epoch 63/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2021 - accuracy: 0.9609 - val_loss: 0.5367 - val_accuracy: 0.8513\n","Epoch 64/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2019 - accuracy: 0.9626 - val_loss: 0.5805 - val_accuracy: 0.8685\n","Epoch 65/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.1872 - accuracy: 0.9647 - val_loss: 0.5894 - val_accuracy: 0.8664\n","Epoch 66/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.1748 - accuracy: 0.9706 - val_loss: 0.6208 - val_accuracy: 0.8675\n","Epoch 67/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1700 - accuracy: 0.9733 - val_loss: 0.6584 - val_accuracy: 0.8599\n","Epoch 68/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1977 - accuracy: 0.9588 - val_loss: 0.5594 - val_accuracy: 0.8545\n","Epoch 69/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1767 - accuracy: 0.9690 - val_loss: 0.6417 - val_accuracy: 0.8588\n","Epoch 70/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1736 - accuracy: 0.9674 - val_loss: 0.7247 - val_accuracy: 0.8394\n","Epoch 71/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1739 - accuracy: 0.9685 - val_loss: 0.6006 - val_accuracy: 0.8631\n","Epoch 72/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1849 - accuracy: 0.9628 - val_loss: 0.7263 - val_accuracy: 0.8416\n","Epoch 73/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2562 - accuracy: 0.9313 - val_loss: 0.6373 - val_accuracy: 0.8265\n","Epoch 74/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.1695 - accuracy: 0.9690 - val_loss: 0.5926 - val_accuracy: 0.8578\n","Epoch 75/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1342 - accuracy: 0.9844 - val_loss: 0.6498 - val_accuracy: 0.8599\n","Epoch 76/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1247 - accuracy: 0.9863 - val_loss: 0.6792 - val_accuracy: 0.8491\n","Epoch 77/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1291 - accuracy: 0.9846 - val_loss: 0.6949 - val_accuracy: 0.8664\n","Epoch 78/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.1591 - accuracy: 0.9704 - val_loss: 0.6745 - val_accuracy: 0.8481\n","Epoch 79/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.1422 - accuracy: 0.9784 - val_loss: 0.6961 - val_accuracy: 0.8125\n","Epoch 80/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1305 - accuracy: 0.9820 - val_loss: 0.8161 - val_accuracy: 0.8448\n","Epoch 81/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1413 - accuracy: 0.9776 - val_loss: 0.7489 - val_accuracy: 0.8438\n","Epoch 82/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1278 - accuracy: 0.9836 - val_loss: 0.7042 - val_accuracy: 0.8416\n","Epoch 83/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1103 - accuracy: 0.9890 - val_loss: 0.7598 - val_accuracy: 0.8578\n","Epoch 84/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1307 - accuracy: 0.9793 - val_loss: 0.7048 - val_accuracy: 0.8578\n","Epoch 85/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1420 - accuracy: 0.9749 - val_loss: 0.7772 - val_accuracy: 0.8556\n","Epoch 86/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1282 - accuracy: 0.9787 - val_loss: 0.7277 - val_accuracy: 0.8470\n","Epoch 87/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1092 - accuracy: 0.9892 - val_loss: 0.7613 - val_accuracy: 0.8534\n","Epoch 88/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1925 - accuracy: 0.9558 - val_loss: 0.5682 - val_accuracy: 0.8481\n","Epoch 89/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1301 - accuracy: 0.9806 - val_loss: 0.7164 - val_accuracy: 0.8545\n","Epoch 90/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1098 - accuracy: 0.9881 - val_loss: 0.7060 - val_accuracy: 0.8394\n","Epoch 91/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1059 - accuracy: 0.9876 - val_loss: 0.7388 - val_accuracy: 0.8394\n","Epoch 92/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1065 - accuracy: 0.9887 - val_loss: 0.7165 - val_accuracy: 0.8459\n","Epoch 93/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0940 - accuracy: 0.9922 - val_loss: 0.7596 - val_accuracy: 0.8599\n","Epoch 94/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0986 - accuracy: 0.9890 - val_loss: 0.6801 - val_accuracy: 0.8545\n","Epoch 95/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0891 - accuracy: 0.9935 - val_loss: 0.7779 - val_accuracy: 0.8621\n","Epoch 96/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0876 - accuracy: 0.9930 - val_loss: 0.7737 - val_accuracy: 0.8502\n","Epoch 97/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.1150 - accuracy: 0.9806 - val_loss: 0.7173 - val_accuracy: 0.8556\n","Epoch 98/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.1171 - accuracy: 0.9801 - val_loss: 0.7028 - val_accuracy: 0.8416\n","Epoch 99/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0873 - accuracy: 0.9922 - val_loss: 0.7675 - val_accuracy: 0.8524\n","Epoch 100/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0794 - accuracy: 0.9960 - val_loss: 0.8280 - val_accuracy: 0.8438\n","{'loss': [1.3943674564361572, 1.2441481351852417, 1.1431092023849487, 1.0842663049697876, 1.0226925611495972, 0.9804904460906982, 0.9367865324020386, 0.9169386625289917, 0.8712327480316162, 0.8393929600715637, 0.7999623417854309, 0.7771188020706177, 0.748167872428894, 0.7359934449195862, 0.7074828147888184, 0.6928369998931885, 0.6633715033531189, 0.6614949703216553, 0.6201508045196533, 0.6044506430625916, 0.597557008266449, 0.5930970907211304, 0.5843371152877808, 0.5465417504310608, 0.5517656207084656, 0.5305743217468262, 0.5302339196205139, 0.5255634784698486, 0.49048033356666565, 0.46967813372612, 0.46698179841041565, 0.4500122666358948, 0.48999154567718506, 0.44068267941474915, 0.4214152991771698, 0.40933772921562195, 0.4098570942878723, 0.39248573780059814, 0.3854183852672577, 0.36633163690567017, 0.3645438253879547, 0.361392080783844, 0.34505823254585266, 0.3242785334587097, 0.3544066250324249, 0.31347423791885376, 0.3075619637966156, 0.2818772494792938, 0.314132422208786, 0.2899113893508911, 0.28923946619033813, 0.27184218168258667, 0.27481815218925476, 0.25738900899887085, 0.2441137731075287, 0.2696402072906494, 0.2410263866186142, 0.24666082859039307, 0.2162640392780304, 0.22196461260318756, 0.213889017701149, 0.2153000682592392, 0.20206254720687866, 0.20186378061771393, 0.18719588220119476, 0.17475715279579163, 0.16997335851192474, 0.19767723977565765, 0.17669086158275604, 0.1736270785331726, 0.17385916411876678, 0.18488872051239014, 0.2561970055103302, 0.1694895327091217, 0.13417759537696838, 0.12472621351480484, 0.12905174493789673, 0.15911626815795898, 0.14215169847011566, 0.1305079311132431, 0.14125509560108185, 0.1278042048215866, 0.1103077083826065, 0.13074669241905212, 0.1419864147901535, 0.1282019317150116, 0.10920720547437668, 0.19253981113433838, 0.13005737960338593, 0.10978693515062332, 0.10585247725248337, 0.10646624118089676, 0.09396211057901382, 0.09855623543262482, 0.08913297206163406, 0.08757700771093369, 0.11498542129993439, 0.1171356663107872, 0.08733845502138138, 0.07940973341464996], 'accuracy': [0.6384698152542114, 0.7354525923728943, 0.7526939511299133, 0.7685883641242981, 0.7858297228813171, 0.7920258641242981, 0.798222005367279, 0.7917564511299133, 0.806303858757019, 0.811152994632721, 0.8157327771186829, 0.8232758641242981, 0.8195043206214905, 0.8208512663841248, 0.8270474076271057, 0.829472005367279, 0.8351293206214905, 0.8327047228813171, 0.8445581793785095, 0.8467133641242981, 0.8456357717514038, 0.8410560488700867, 0.8429418206214905, 0.8574892282485962, 0.8515625, 0.8612607717514038, 0.8518319129943848, 0.8494073152542114, 0.8696120977401733, 0.8741918206214905, 0.8725754022598267, 0.8860452771186829, 0.859375, 0.8852370977401733, 0.8898168206214905, 0.8952047228813171, 0.892241358757019, 0.8973599076271057, 0.9027478694915771, 0.9059805870056152, 0.9094827771186829, 0.9092133641242981, 0.9086745977401733, 0.9199892282485962, 0.9137930870056152, 0.9248383641242981, 0.9278017282485962, 0.9369612336158752, 0.920258641242981, 0.9318426847457886, 0.931034505367279, 0.9372305870056152, 0.9318426847457886, 0.9412715435028076, 0.9477370977401733, 0.9350754022598267, 0.9461206793785095, 0.9434267282485962, 0.9568965435028076, 0.9490840435028076, 0.9566271305084229, 0.9563577771186829, 0.9609375, 0.962553858757019, 0.9647090435028076, 0.9706357717514038, 0.9733297228813171, 0.9587823152542114, 0.9690194129943848, 0.967402994632721, 0.9684805870056152, 0.9628232717514038, 0.931303858757019, 0.9690194129943848, 0.984375, 0.9862607717514038, 0.9846444129943848, 0.970366358757019, 0.9784482717514038, 0.9819504022598267, 0.9776400923728943, 0.9835668206214905, 0.9889547228813171, 0.9792564511299133, 0.974946141242981, 0.9787176847457886, 0.9892241358757019, 0.9558189511299133, 0.9806034564971924, 0.9881465435028076, 0.9876077771186829, 0.9886853694915771, 0.9921875, 0.9889547228813171, 0.993534505367279, 0.9929956793785095, 0.9806034564971924, 0.9800646305084229, 0.9921875, 0.9959590435028076], 'val_loss': [1.3743349313735962, 1.3076612949371338, 1.267536997795105, 1.224594235420227, 1.1839882135391235, 1.141577959060669, 1.105283498764038, 1.0809886455535889, 1.0319221019744873, 0.9943186640739441, 0.9534466862678528, 0.9206070899963379, 0.8776161074638367, 0.8567384481430054, 0.7989872694015503, 0.7994245290756226, 0.7180059552192688, 0.7157589197158813, 0.656093180179596, 0.6260542869567871, 0.6368962526321411, 0.6108371019363403, 0.5933927893638611, 0.5628886818885803, 0.5796886086463928, 0.5439496636390686, 0.5500551462173462, 0.5294861197471619, 0.5223530530929565, 0.535298764705658, 0.5331386923789978, 0.6281748414039612, 0.5151105523109436, 0.5030249953269958, 0.4927368462085724, 0.49515342712402344, 0.5329016447067261, 0.539552628993988, 0.48155340552330017, 0.4855077564716339, 0.5091466307640076, 0.47666844725608826, 0.49120989441871643, 0.5415655374526978, 0.5749639272689819, 0.48954328894615173, 0.47957804799079895, 0.5231673717498779, 0.588894784450531, 0.5281380414962769, 0.4862448573112488, 0.5018759965896606, 0.4889076054096222, 0.5685256719589233, 0.7416069507598877, 0.5024945735931396, 0.6027846932411194, 0.5685437917709351, 0.6149156093597412, 0.6414996981620789, 0.5965946912765503, 0.5724984407424927, 0.5367478728294373, 0.5804540514945984, 0.5894272327423096, 0.6208212375640869, 0.6584389209747314, 0.5594392418861389, 0.6416665315628052, 0.7246668338775635, 0.6005851626396179, 0.7263116240501404, 0.6373341083526611, 0.5926074981689453, 0.6497513055801392, 0.6792140007019043, 0.6949315667152405, 0.6744515895843506, 0.6960752606391907, 0.8161422610282898, 0.7489346861839294, 0.7042047381401062, 0.759753942489624, 0.7047960162162781, 0.7771885395050049, 0.7277388572692871, 0.7613364458084106, 0.5682089328765869, 0.7163757681846619, 0.7060291171073914, 0.7387506365776062, 0.7165130972862244, 0.7596099376678467, 0.680065929889679, 0.7778670787811279, 0.7736562490463257, 0.7172889709472656, 0.7028335332870483, 0.7675327658653259, 0.828041672706604], 'val_accuracy': [0.7144396305084229, 0.756465494632721, 0.7790948152542114, 0.78125, 0.8157327771186829, 0.8049569129943848, 0.7834051847457886, 0.7737069129943848, 0.8038793206214905, 0.8028017282485962, 0.8178879022598267, 0.829741358757019, 0.8383620977401733, 0.8081896305084229, 0.8157327771186829, 0.8006465435028076, 0.8351293206214905, 0.8351293206214905, 0.8243534564971924, 0.8394396305084229, 0.8243534564971924, 0.8448275923728943, 0.8469827771186829, 0.8480603694915771, 0.8308189511299133, 0.8459051847457886, 0.8394396305084229, 0.8534482717514038, 0.8469827771186829, 0.8394396305084229, 0.837284505367279, 0.8114224076271057, 0.84375, 0.8620689511299133, 0.8523706793785095, 0.8566810488700867, 0.850215494632721, 0.860991358757019, 0.8663793206214905, 0.8566810488700867, 0.8523706793785095, 0.860991358757019, 0.850215494632721, 0.8523706793785095, 0.826508641242981, 0.8631465435028076, 0.8566810488700867, 0.8534482717514038, 0.8243534564971924, 0.8534482717514038, 0.8588362336158752, 0.8566810488700867, 0.860991358757019, 0.8523706793785095, 0.826508641242981, 0.8674569129943848, 0.8566810488700867, 0.8631465435028076, 0.826508641242981, 0.829741358757019, 0.868534505367279, 0.8459051847457886, 0.8512930870056152, 0.868534505367279, 0.8663793206214905, 0.8674569129943848, 0.8599137663841248, 0.8545258641242981, 0.8588362336158752, 0.8394396305084229, 0.8631465435028076, 0.8415948152542114, 0.826508641242981, 0.857758641242981, 0.8599137663841248, 0.8491379022598267, 0.8663793206214905, 0.8480603694915771, 0.8125, 0.8448275923728943, 0.84375, 0.8415948152542114, 0.857758641242981, 0.857758641242981, 0.8556034564971924, 0.8469827771186829, 0.8534482717514038, 0.8480603694915771, 0.8545258641242981, 0.8394396305084229, 0.8394396305084229, 0.8459051847457886, 0.8599137663841248, 0.8545258641242981, 0.8620689511299133, 0.850215494632721, 0.8556034564971924, 0.8415948152542114, 0.8523706793785095, 0.84375]}\n","38/38 [==============================] - 1s 5ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","24/28 [========================>.....] - ETA: 0s - loss: 1.4063 - accuracy: 0.6152"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 8s 81ms/step - loss: 1.4006 - accuracy: 0.6254 - val_loss: 1.3766 - val_accuracy: 0.6855\n","Epoch 2/100\n","28/28 [==============================] - 1s 29ms/step - loss: 1.2838 - accuracy: 0.7083 - val_loss: 1.3180 - val_accuracy: 0.7149\n","Epoch 3/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.1754 - accuracy: 0.7284 - val_loss: 1.2733 - val_accuracy: 0.7398\n","Epoch 4/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.0983 - accuracy: 0.7550 - val_loss: 1.2302 - val_accuracy: 0.7738\n","Epoch 5/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.0449 - accuracy: 0.7663 - val_loss: 1.1941 - val_accuracy: 0.6742\n","Epoch 6/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.9866 - accuracy: 0.7906 - val_loss: 1.1560 - val_accuracy: 0.7590\n","Epoch 7/100\n","28/28 [==============================] - 1s 31ms/step - loss: 0.9424 - accuracy: 0.7999 - val_loss: 1.1174 - val_accuracy: 0.7851\n","Epoch 8/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.9046 - accuracy: 0.7997 - val_loss: 1.0879 - val_accuracy: 0.7839\n","Epoch 9/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8890 - accuracy: 0.7912 - val_loss: 1.0612 - val_accuracy: 0.7726\n","Epoch 10/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.8486 - accuracy: 0.8096 - val_loss: 1.0130 - val_accuracy: 0.7873\n","Epoch 11/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8154 - accuracy: 0.8067 - val_loss: 0.9872 - val_accuracy: 0.7794\n","Epoch 12/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.7874 - accuracy: 0.8135 - val_loss: 0.9554 - val_accuracy: 0.7704\n","Epoch 13/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7552 - accuracy: 0.8200 - val_loss: 0.9203 - val_accuracy: 0.7794\n","Epoch 14/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.7504 - accuracy: 0.8169 - val_loss: 0.8905 - val_accuracy: 0.8032\n","Epoch 15/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7321 - accuracy: 0.8161 - val_loss: 0.8413 - val_accuracy: 0.7975\n","Epoch 16/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7081 - accuracy: 0.8231 - val_loss: 0.8283 - val_accuracy: 0.7760\n","Epoch 17/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6914 - accuracy: 0.8243 - val_loss: 0.7994 - val_accuracy: 0.7930\n","Epoch 18/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.6627 - accuracy: 0.8353 - val_loss: 0.7473 - val_accuracy: 0.8145\n","Epoch 19/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6305 - accuracy: 0.8480 - val_loss: 0.7218 - val_accuracy: 0.8032\n","Epoch 20/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6326 - accuracy: 0.8353 - val_loss: 0.7300 - val_accuracy: 0.7738\n","Epoch 21/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5947 - accuracy: 0.8582 - val_loss: 0.6730 - val_accuracy: 0.8156\n","Epoch 22/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5808 - accuracy: 0.8540 - val_loss: 0.6588 - val_accuracy: 0.7975\n","Epoch 23/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5908 - accuracy: 0.8447 - val_loss: 0.6543 - val_accuracy: 0.8077\n","Epoch 24/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5602 - accuracy: 0.8551 - val_loss: 0.6127 - val_accuracy: 0.8201\n","Epoch 25/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5705 - accuracy: 0.8458 - val_loss: 0.6248 - val_accuracy: 0.8145\n","Epoch 26/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5243 - accuracy: 0.8662 - val_loss: 0.6238 - val_accuracy: 0.8100\n","Epoch 27/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5258 - accuracy: 0.8633 - val_loss: 0.6007 - val_accuracy: 0.8190\n","Epoch 28/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5070 - accuracy: 0.8662 - val_loss: 0.6315 - val_accuracy: 0.8066\n","Epoch 29/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4948 - accuracy: 0.8752 - val_loss: 0.6047 - val_accuracy: 0.8043\n","Epoch 30/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.4855 - accuracy: 0.8696 - val_loss: 0.5883 - val_accuracy: 0.8258\n","Epoch 31/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4853 - accuracy: 0.8642 - val_loss: 0.6190 - val_accuracy: 0.7998\n","Epoch 32/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4623 - accuracy: 0.8789 - val_loss: 0.5657 - val_accuracy: 0.8224\n","Epoch 33/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4502 - accuracy: 0.8899 - val_loss: 0.5897 - val_accuracy: 0.8133\n","Epoch 34/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4218 - accuracy: 0.9012 - val_loss: 0.5711 - val_accuracy: 0.8156\n","Epoch 35/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4395 - accuracy: 0.8814 - val_loss: 0.5421 - val_accuracy: 0.8224\n","Epoch 36/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4066 - accuracy: 0.9018 - val_loss: 0.5703 - val_accuracy: 0.8303\n","Epoch 37/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4295 - accuracy: 0.8817 - val_loss: 0.5700 - val_accuracy: 0.8145\n","Epoch 38/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3998 - accuracy: 0.9012 - val_loss: 0.5744 - val_accuracy: 0.8167\n","Epoch 39/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3956 - accuracy: 0.8945 - val_loss: 0.5995 - val_accuracy: 0.7975\n","Epoch 40/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3937 - accuracy: 0.8976 - val_loss: 0.6009 - val_accuracy: 0.7952\n","Epoch 41/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3917 - accuracy: 0.8928 - val_loss: 0.5369 - val_accuracy: 0.8235\n","Epoch 42/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3890 - accuracy: 0.8913 - val_loss: 0.5717 - val_accuracy: 0.8054\n","Epoch 43/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3879 - accuracy: 0.8862 - val_loss: 0.5358 - val_accuracy: 0.8111\n","Epoch 44/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3321 - accuracy: 0.9259 - val_loss: 0.5545 - val_accuracy: 0.8133\n","Epoch 45/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.3232 - accuracy: 0.9222 - val_loss: 0.5437 - val_accuracy: 0.8337\n","Epoch 46/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3108 - accuracy: 0.9273 - val_loss: 0.5884 - val_accuracy: 0.8122\n","Epoch 47/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3510 - accuracy: 0.9038 - val_loss: 0.7423 - val_accuracy: 0.6708\n","Epoch 48/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3921 - accuracy: 0.8826 - val_loss: 0.5716 - val_accuracy: 0.8281\n","Epoch 49/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3217 - accuracy: 0.9202 - val_loss: 0.5805 - val_accuracy: 0.8258\n","Epoch 50/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3348 - accuracy: 0.9095 - val_loss: 0.5920 - val_accuracy: 0.8066\n","Epoch 51/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3390 - accuracy: 0.9066 - val_loss: 0.5688 - val_accuracy: 0.8111\n","Epoch 52/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2966 - accuracy: 0.9270 - val_loss: 0.7477 - val_accuracy: 0.7534\n","Epoch 53/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2753 - accuracy: 0.9377 - val_loss: 0.8006 - val_accuracy: 0.7749\n","Epoch 54/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2542 - accuracy: 0.9457 - val_loss: 0.5992 - val_accuracy: 0.8133\n","Epoch 55/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2481 - accuracy: 0.9471 - val_loss: 0.6714 - val_accuracy: 0.8269\n","Epoch 56/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2659 - accuracy: 0.9392 - val_loss: 0.6719 - val_accuracy: 0.8190\n","Epoch 57/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2398 - accuracy: 0.9519 - val_loss: 0.8488 - val_accuracy: 0.7873\n","Epoch 58/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3155 - accuracy: 0.9120 - val_loss: 0.5532 - val_accuracy: 0.8066\n","Epoch 59/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2413 - accuracy: 0.9485 - val_loss: 0.6621 - val_accuracy: 0.8235\n","Epoch 60/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2245 - accuracy: 0.9550 - val_loss: 0.6731 - val_accuracy: 0.8077\n","Epoch 61/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2103 - accuracy: 0.9612 - val_loss: 0.6976 - val_accuracy: 0.8043\n","Epoch 62/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2245 - accuracy: 0.9493 - val_loss: 0.7873 - val_accuracy: 0.7398\n","Epoch 63/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.2344 - accuracy: 0.9454 - val_loss: 0.6209 - val_accuracy: 0.8360\n","Epoch 64/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2020 - accuracy: 0.9595 - val_loss: 0.9479 - val_accuracy: 0.7500\n","Epoch 65/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2365 - accuracy: 0.9406 - val_loss: 0.6290 - val_accuracy: 0.8258\n","Epoch 66/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2216 - accuracy: 0.9457 - val_loss: 0.7697 - val_accuracy: 0.7862\n","Epoch 67/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2103 - accuracy: 0.9564 - val_loss: 0.7552 - val_accuracy: 0.8145\n","Epoch 68/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1656 - accuracy: 0.9745 - val_loss: 0.7071 - val_accuracy: 0.8281\n","Epoch 69/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1597 - accuracy: 0.9751 - val_loss: 0.7738 - val_accuracy: 0.8190\n","Epoch 70/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.1548 - accuracy: 0.9759 - val_loss: 0.8430 - val_accuracy: 0.8100\n","Epoch 71/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.1504 - accuracy: 0.9776 - val_loss: 0.8055 - val_accuracy: 0.8179\n","Epoch 72/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.1845 - accuracy: 0.9621 - val_loss: 0.6642 - val_accuracy: 0.8133\n","Epoch 73/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.1678 - accuracy: 0.9683 - val_loss: 0.8436 - val_accuracy: 0.8281\n","Epoch 74/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2193 - accuracy: 0.9426 - val_loss: 0.6326 - val_accuracy: 0.8360\n","Epoch 75/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.1431 - accuracy: 0.9802 - val_loss: 0.8078 - val_accuracy: 0.8247\n","Epoch 76/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.1307 - accuracy: 0.9847 - val_loss: 0.8847 - val_accuracy: 0.8133\n","Epoch 77/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.1469 - accuracy: 0.9774 - val_loss: 0.7869 - val_accuracy: 0.7952\n","Epoch 78/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1524 - accuracy: 0.9759 - val_loss: 0.8353 - val_accuracy: 0.8190\n","Epoch 79/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2455 - accuracy: 0.9312 - val_loss: 0.6813 - val_accuracy: 0.8269\n","Epoch 80/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.1693 - accuracy: 0.9643 - val_loss: 0.9649 - val_accuracy: 0.7579\n","Epoch 81/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.1284 - accuracy: 0.9822 - val_loss: 0.8486 - val_accuracy: 0.7941\n","Epoch 82/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1358 - accuracy: 0.9774 - val_loss: 0.7669 - val_accuracy: 0.8122\n","Epoch 83/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.1101 - accuracy: 0.9904 - val_loss: 0.8838 - val_accuracy: 0.8269\n","Epoch 84/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1125 - accuracy: 0.9875 - val_loss: 0.8875 - val_accuracy: 0.8281\n","Epoch 85/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1210 - accuracy: 0.9830 - val_loss: 0.8791 - val_accuracy: 0.8167\n","Epoch 86/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1223 - accuracy: 0.9856 - val_loss: 0.8178 - val_accuracy: 0.8167\n","Epoch 87/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1117 - accuracy: 0.9878 - val_loss: 0.9110 - val_accuracy: 0.8247\n","Epoch 88/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1655 - accuracy: 0.9570 - val_loss: 0.7074 - val_accuracy: 0.7975\n","Epoch 89/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1603 - accuracy: 0.9677 - val_loss: 0.7509 - val_accuracy: 0.8111\n","Epoch 90/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1018 - accuracy: 0.9901 - val_loss: 0.9596 - val_accuracy: 0.8281\n","Epoch 91/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0952 - accuracy: 0.9929 - val_loss: 0.9928 - val_accuracy: 0.7941\n","Epoch 92/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1171 - accuracy: 0.9816 - val_loss: 0.9549 - val_accuracy: 0.8088\n","Epoch 93/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1001 - accuracy: 0.9884 - val_loss: 0.9894 - val_accuracy: 0.8122\n","Epoch 94/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0904 - accuracy: 0.9938 - val_loss: 0.9227 - val_accuracy: 0.8258\n","Epoch 95/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0868 - accuracy: 0.9949 - val_loss: 0.9737 - val_accuracy: 0.8269\n","Epoch 96/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0964 - accuracy: 0.9895 - val_loss: 0.9607 - val_accuracy: 0.8258\n","Epoch 97/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1007 - accuracy: 0.9887 - val_loss: 0.9168 - val_accuracy: 0.8066\n","Epoch 98/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0895 - accuracy: 0.9915 - val_loss: 1.1073 - val_accuracy: 0.8179\n","Epoch 99/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1617 - accuracy: 0.9624 - val_loss: 0.7813 - val_accuracy: 0.7523\n","Epoch 100/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1207 - accuracy: 0.9813 - val_loss: 0.9406 - val_accuracy: 0.8032\n","{'loss': [1.4006328582763672, 1.2837868928909302, 1.1753613948822021, 1.098293423652649, 1.0449209213256836, 0.9866332411766052, 0.942397952079773, 0.9046452641487122, 0.8890298008918762, 0.8485928773880005, 0.815361738204956, 0.787413477897644, 0.7551676034927368, 0.7503986358642578, 0.7320570349693298, 0.7081198692321777, 0.6914163827896118, 0.6627367734909058, 0.6304932832717896, 0.6325565576553345, 0.5947113633155823, 0.5807636976242065, 0.5907677412033081, 0.5602105259895325, 0.5704569816589355, 0.5243492126464844, 0.5257951617240906, 0.506984293460846, 0.49480846524238586, 0.48550236225128174, 0.48527753353118896, 0.4623349606990814, 0.4501725435256958, 0.42182156443595886, 0.4395325481891632, 0.40659889578819275, 0.4294723868370056, 0.3997817933559418, 0.39562249183654785, 0.3937091827392578, 0.3916611671447754, 0.38902902603149414, 0.3878766596317291, 0.332114577293396, 0.3232274055480957, 0.3107558488845825, 0.35098910331726074, 0.39210107922554016, 0.32168900966644287, 0.3348294794559479, 0.3389721214771271, 0.2966456115245819, 0.27528655529022217, 0.25421637296676636, 0.24813024699687958, 0.2659000754356384, 0.23980943858623505, 0.31551650166511536, 0.2412591576576233, 0.2245185673236847, 0.2103024274110794, 0.2245231419801712, 0.23442690074443817, 0.20201586186885834, 0.23654483258724213, 0.22162258625030518, 0.21031039953231812, 0.16562151908874512, 0.15972746908664703, 0.1547793596982956, 0.15036922693252563, 0.18447233736515045, 0.16781732439994812, 0.21934853494167328, 0.14309343695640564, 0.1307407021522522, 0.14690245687961578, 0.15235742926597595, 0.24551960825920105, 0.1693100780248642, 0.12842576205730438, 0.13583922386169434, 0.11008678376674652, 0.11249931156635284, 0.12104933708906174, 0.12232349812984467, 0.11173583567142487, 0.16547197103500366, 0.1602659672498703, 0.10179482400417328, 0.09515409916639328, 0.11714749783277512, 0.10006862133741379, 0.09040310978889465, 0.08677820861339569, 0.09643516689538956, 0.10068526864051819, 0.08952690660953522, 0.16171762347221375, 0.12066315859556198], 'accuracy': [0.6253536939620972, 0.70826256275177, 0.7283531427383423, 0.7549518942832947, 0.7662705183029175, 0.7906055450439453, 0.7999433875083923, 0.7996604442596436, 0.7911714911460876, 0.8095642328262329, 0.806734561920166, 0.8135257363319397, 0.8200339674949646, 0.8169213533401489, 0.8160724639892578, 0.8231465816497803, 0.8242784142494202, 0.8353140950202942, 0.8480475544929504, 0.8353140950202942, 0.8582342863082886, 0.853989839553833, 0.8446519374847412, 0.8551216721534729, 0.8457838296890259, 0.8661573529243469, 0.86332768201828, 0.8661573529243469, 0.8752122521400452, 0.8695529103279114, 0.8641765713691711, 0.8788907527923584, 0.8899264335632324, 0.9012450575828552, 0.8814374804496765, 0.9018110036849976, 0.8817204236984253, 0.9012450575828552, 0.8944538831710815, 0.8975664973258972, 0.8927561044692993, 0.8913412690162659, 0.8862478733062744, 0.9258630275726318, 0.9221844673156738, 0.9272778630256653, 0.9037917256355286, 0.8825693130493164, 0.9202037453651428, 0.9094510674476624, 0.9066213965415955, 0.9269949197769165, 0.937747597694397, 0.9456706047058105, 0.947085440158844, 0.9391624331474304, 0.9518958926200867, 0.9119977355003357, 0.9485002756118774, 0.9550085067749023, 0.9612337350845337, 0.9493491649627686, 0.9453876614570618, 0.9595359563827515, 0.9405772686004639, 0.9456706047058105, 0.9564233422279358, 0.9745330810546875, 0.9750990271568298, 0.975947916507721, 0.977645754814148, 0.9620826244354248, 0.9683078527450562, 0.9425579905509949, 0.9801924228668213, 0.9847198724746704, 0.9773627519607544, 0.975947916507721, 0.9312393665313721, 0.9643463492393494, 0.9821732044219971, 0.9773627519607544, 0.9903791546821594, 0.9875495433807373, 0.9830220937728882, 0.9855687618255615, 0.9878324866294861, 0.9569892287254333, 0.9677419066429138, 0.9900962114334106, 0.9929258823394775, 0.9816072583198547, 0.9883984327316284, 0.9937747716903687, 0.9949066042900085, 0.9895302653312683, 0.9886813759803772, 0.9915110468864441, 0.9623655676841736, 0.9813242554664612], 'val_loss': [1.3766201734542847, 1.3180335760116577, 1.2733293771743774, 1.2301918268203735, 1.1940935850143433, 1.1560320854187012, 1.1173818111419678, 1.0879353284835815, 1.0612281560897827, 1.0130445957183838, 0.987156867980957, 0.9554280638694763, 0.9202768802642822, 0.8905336260795593, 0.8412562608718872, 0.8282716274261475, 0.7994328141212463, 0.7473440766334534, 0.7217593789100647, 0.7300432920455933, 0.6729828715324402, 0.6587972044944763, 0.6543413400650024, 0.6126503348350525, 0.6248022317886353, 0.623827338218689, 0.6007131338119507, 0.631475031375885, 0.604682445526123, 0.588302731513977, 0.618994414806366, 0.5656790733337402, 0.5896720886230469, 0.571094810962677, 0.5420941710472107, 0.5703006386756897, 0.5699940323829651, 0.5744163393974304, 0.5995023846626282, 0.600921630859375, 0.5368993878364563, 0.5717438459396362, 0.5358454585075378, 0.5544796586036682, 0.5437156558036804, 0.5884328484535217, 0.7422996163368225, 0.571645200252533, 0.5804815292358398, 0.5920378565788269, 0.5688462853431702, 0.7477030158042908, 0.8005924224853516, 0.5992101430892944, 0.6713628172874451, 0.671949565410614, 0.8487679958343506, 0.5531684756278992, 0.6620521545410156, 0.6730861663818359, 0.6975866556167603, 0.787269115447998, 0.6209049820899963, 0.9479421973228455, 0.6290330290794373, 0.7696790099143982, 0.7551841735839844, 0.7071007490158081, 0.773796558380127, 0.8429996967315674, 0.8055163621902466, 0.664170503616333, 0.8436431288719177, 0.6325583457946777, 0.8078200221061707, 0.8847464323043823, 0.7868780493736267, 0.8353453874588013, 0.6813496947288513, 0.9648699164390564, 0.8485761284828186, 0.7669060230255127, 0.8837886452674866, 0.8875231146812439, 0.8790566325187683, 0.8177884221076965, 0.9110470414161682, 0.7074361443519592, 0.7508751153945923, 0.9595736265182495, 0.9928172826766968, 0.9549137949943542, 0.9893898963928223, 0.9227398037910461, 0.9736688137054443, 0.9606873989105225, 0.9167730808258057, 1.107267141342163, 0.7813308238983154, 0.9405566453933716], 'val_accuracy': [0.685520350933075, 0.7149321436882019, 0.7398189902305603, 0.773755669593811, 0.6742081642150879, 0.7590497732162476, 0.7850678563117981, 0.7839366793632507, 0.7726244330406189, 0.7873303294181824, 0.779411792755127, 0.7703620195388794, 0.779411792755127, 0.8031674027442932, 0.7975113391876221, 0.7760180830955505, 0.7929864525794983, 0.814479649066925, 0.8031674027442932, 0.773755669593811, 0.8156108856201172, 0.7975113391876221, 0.807692289352417, 0.820135772228241, 0.814479649066925, 0.8099547624588013, 0.8190045356750488, 0.8065611124038696, 0.8042986392974854, 0.8257918357849121, 0.7997737526893616, 0.8223981857299805, 0.8133484125137329, 0.8156108856201172, 0.8223981857299805, 0.8303167223930359, 0.814479649066925, 0.8167420625686646, 0.7975113391876221, 0.7952488660812378, 0.8235294222831726, 0.8054298758506775, 0.8110859990119934, 0.8133484125137329, 0.8337104320526123, 0.8122171759605408, 0.6708144545555115, 0.8280543088912964, 0.8257918357849121, 0.8065611124038696, 0.8110859990119934, 0.7533936500549316, 0.7748869061470032, 0.8133484125137329, 0.8269230723381042, 0.8190045356750488, 0.7873303294181824, 0.8065611124038696, 0.8235294222831726, 0.807692289352417, 0.8042986392974854, 0.7398189902305603, 0.8359728455543518, 0.75, 0.8257918357849121, 0.7861990928649902, 0.814479649066925, 0.8280543088912964, 0.8190045356750488, 0.8099547624588013, 0.8178732991218567, 0.8133484125137329, 0.8280543088912964, 0.8359728455543518, 0.8246606588363647, 0.8133484125137329, 0.7952488660812378, 0.8190045356750488, 0.8269230723381042, 0.7579185366630554, 0.7941176295280457, 0.8122171759605408, 0.8269230723381042, 0.8280543088912964, 0.8167420625686646, 0.8167420625686646, 0.8246606588363647, 0.7975113391876221, 0.8110859990119934, 0.8280543088912964, 0.7941176295280457, 0.8088235259056091, 0.8122171759605408, 0.8257918357849121, 0.8269230723381042, 0.8257918357849121, 0.8065611124038696, 0.8178732991218567, 0.7522624731063843, 0.8031674027442932]}\n","45/45 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","30/31 [============================>.] - ETA: 0s - loss: 1.3983 - accuracy: 0.5641"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 10s 55ms/step - loss: 1.3979 - accuracy: 0.5649 - val_loss: 1.3705 - val_accuracy: 0.6343\n","Epoch 2/100\n","31/31 [==============================] - 1s 24ms/step - loss: 1.2685 - accuracy: 0.6855 - val_loss: 1.3089 - val_accuracy: 0.7035\n","Epoch 3/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.1459 - accuracy: 0.7380 - val_loss: 1.2616 - val_accuracy: 0.7128\n","Epoch 4/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.0645 - accuracy: 0.7667 - val_loss: 1.2152 - val_accuracy: 0.7562\n","Epoch 5/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.9977 - accuracy: 0.7837 - val_loss: 1.1727 - val_accuracy: 0.7944\n","Epoch 6/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.9598 - accuracy: 0.7791 - val_loss: 1.1321 - val_accuracy: 0.7872\n","Epoch 7/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9059 - accuracy: 0.7977 - val_loss: 1.0907 - val_accuracy: 0.7469\n","Epoch 8/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8794 - accuracy: 0.7984 - val_loss: 1.0590 - val_accuracy: 0.7738\n","Epoch 9/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.8256 - accuracy: 0.8127 - val_loss: 1.0101 - val_accuracy: 0.8068\n","Epoch 10/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.8037 - accuracy: 0.8142 - val_loss: 0.9838 - val_accuracy: 0.7862\n","Epoch 11/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7758 - accuracy: 0.8145 - val_loss: 0.9680 - val_accuracy: 0.7045\n","Epoch 12/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7735 - accuracy: 0.8109 - val_loss: 0.9124 - val_accuracy: 0.8006\n","Epoch 13/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7205 - accuracy: 0.8243 - val_loss: 0.8738 - val_accuracy: 0.7841\n","Epoch 14/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6869 - accuracy: 0.8419 - val_loss: 0.8108 - val_accuracy: 0.8017\n","Epoch 15/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6754 - accuracy: 0.8390 - val_loss: 0.7867 - val_accuracy: 0.8006\n","Epoch 16/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6494 - accuracy: 0.8380 - val_loss: 0.7692 - val_accuracy: 0.7934\n","Epoch 17/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6228 - accuracy: 0.8455 - val_loss: 0.7228 - val_accuracy: 0.7944\n","Epoch 18/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6022 - accuracy: 0.8501 - val_loss: 0.6929 - val_accuracy: 0.7944\n","Epoch 19/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5868 - accuracy: 0.8540 - val_loss: 0.7525 - val_accuracy: 0.7438\n","Epoch 20/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.5912 - accuracy: 0.8496 - val_loss: 0.6312 - val_accuracy: 0.8285\n","Epoch 21/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5515 - accuracy: 0.8620 - val_loss: 0.6270 - val_accuracy: 0.8151\n","Epoch 22/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5473 - accuracy: 0.8553 - val_loss: 0.6207 - val_accuracy: 0.8130\n","Epoch 23/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5244 - accuracy: 0.8669 - val_loss: 0.5815 - val_accuracy: 0.8388\n","Epoch 24/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5175 - accuracy: 0.8607 - val_loss: 0.5816 - val_accuracy: 0.8347\n","Epoch 25/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5013 - accuracy: 0.8687 - val_loss: 0.6200 - val_accuracy: 0.8058\n","Epoch 26/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4932 - accuracy: 0.8656 - val_loss: 0.5652 - val_accuracy: 0.8316\n","Epoch 27/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4621 - accuracy: 0.8788 - val_loss: 0.5581 - val_accuracy: 0.8306\n","Epoch 28/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4583 - accuracy: 0.8788 - val_loss: 0.6377 - val_accuracy: 0.7955\n","Epoch 29/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4508 - accuracy: 0.8804 - val_loss: 0.7302 - val_accuracy: 0.7789\n","Epoch 30/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.4541 - accuracy: 0.8757 - val_loss: 0.5450 - val_accuracy: 0.8450\n","Epoch 31/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4099 - accuracy: 0.8964 - val_loss: 0.5786 - val_accuracy: 0.8254\n","Epoch 32/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4219 - accuracy: 0.8840 - val_loss: 0.5246 - val_accuracy: 0.8409\n","Epoch 33/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4007 - accuracy: 0.8972 - val_loss: 0.5802 - val_accuracy: 0.8048\n","Epoch 34/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3872 - accuracy: 0.8987 - val_loss: 0.5394 - val_accuracy: 0.8450\n","Epoch 35/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3772 - accuracy: 0.9018 - val_loss: 0.6044 - val_accuracy: 0.8306\n","Epoch 36/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4503 - accuracy: 0.8610 - val_loss: 0.5165 - val_accuracy: 0.8244\n","Epoch 37/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3686 - accuracy: 0.9028 - val_loss: 0.5212 - val_accuracy: 0.8461\n","Epoch 38/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3929 - accuracy: 0.8886 - val_loss: 0.6077 - val_accuracy: 0.7965\n","Epoch 39/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.3642 - accuracy: 0.9000 - val_loss: 0.5771 - val_accuracy: 0.8244\n","Epoch 40/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.3519 - accuracy: 0.9090 - val_loss: 0.5449 - val_accuracy: 0.8471\n","Epoch 41/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3124 - accuracy: 0.9248 - val_loss: 0.5385 - val_accuracy: 0.8388\n","Epoch 42/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3136 - accuracy: 0.9217 - val_loss: 0.5391 - val_accuracy: 0.8450\n","Epoch 43/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3058 - accuracy: 0.9282 - val_loss: 0.5767 - val_accuracy: 0.8368\n","Epoch 44/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3045 - accuracy: 0.9274 - val_loss: 0.6472 - val_accuracy: 0.8027\n","Epoch 45/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2926 - accuracy: 0.9307 - val_loss: 0.6032 - val_accuracy: 0.8275\n","Epoch 46/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3317 - accuracy: 0.9072 - val_loss: 0.6178 - val_accuracy: 0.8017\n","Epoch 47/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2824 - accuracy: 0.9339 - val_loss: 0.6015 - val_accuracy: 0.8399\n","Epoch 48/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2582 - accuracy: 0.9424 - val_loss: 0.5736 - val_accuracy: 0.8419\n","Epoch 49/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2741 - accuracy: 0.9351 - val_loss: 0.6264 - val_accuracy: 0.8213\n","Epoch 50/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2616 - accuracy: 0.9393 - val_loss: 0.5590 - val_accuracy: 0.8430\n","Epoch 51/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2781 - accuracy: 0.9292 - val_loss: 0.5373 - val_accuracy: 0.8492\n","Epoch 52/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2424 - accuracy: 0.9465 - val_loss: 0.5836 - val_accuracy: 0.8450\n","Epoch 53/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2648 - accuracy: 0.9302 - val_loss: 0.5728 - val_accuracy: 0.8461\n","Epoch 54/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2549 - accuracy: 0.9426 - val_loss: 0.6538 - val_accuracy: 0.8254\n","Epoch 55/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2368 - accuracy: 0.9473 - val_loss: 0.5888 - val_accuracy: 0.8399\n","Epoch 56/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2222 - accuracy: 0.9571 - val_loss: 0.6837 - val_accuracy: 0.8017\n","Epoch 57/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2031 - accuracy: 0.9610 - val_loss: 0.6256 - val_accuracy: 0.8399\n","Epoch 58/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2159 - accuracy: 0.9579 - val_loss: 0.6578 - val_accuracy: 0.8326\n","Epoch 59/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2138 - accuracy: 0.9525 - val_loss: 0.6467 - val_accuracy: 0.8254\n","Epoch 60/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1951 - accuracy: 0.9610 - val_loss: 0.6512 - val_accuracy: 0.8357\n","Epoch 61/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2447 - accuracy: 0.9357 - val_loss: 0.5838 - val_accuracy: 0.8233\n","Epoch 62/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2114 - accuracy: 0.9514 - val_loss: 1.0337 - val_accuracy: 0.7841\n","Epoch 63/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2480 - accuracy: 0.9382 - val_loss: 0.6324 - val_accuracy: 0.8306\n","Epoch 64/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1766 - accuracy: 0.9700 - val_loss: 0.6533 - val_accuracy: 0.8161\n","Epoch 65/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1696 - accuracy: 0.9693 - val_loss: 0.6917 - val_accuracy: 0.8264\n","Epoch 66/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2149 - accuracy: 0.9491 - val_loss: 0.6355 - val_accuracy: 0.8233\n","Epoch 67/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1897 - accuracy: 0.9607 - val_loss: 0.6316 - val_accuracy: 0.8347\n","Epoch 68/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1739 - accuracy: 0.9664 - val_loss: 0.7024 - val_accuracy: 0.8161\n","Epoch 69/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.1587 - accuracy: 0.9716 - val_loss: 0.8092 - val_accuracy: 0.8068\n","Epoch 70/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.1635 - accuracy: 0.9664 - val_loss: 0.7392 - val_accuracy: 0.8326\n","Epoch 71/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.1491 - accuracy: 0.9757 - val_loss: 0.7614 - val_accuracy: 0.8275\n","Epoch 72/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2260 - accuracy: 0.9434 - val_loss: 0.6276 - val_accuracy: 0.8399\n","Epoch 73/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.1802 - accuracy: 0.9618 - val_loss: 0.7399 - val_accuracy: 0.8079\n","Epoch 74/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.1636 - accuracy: 0.9685 - val_loss: 0.8150 - val_accuracy: 0.7944\n","Epoch 75/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.1347 - accuracy: 0.9778 - val_loss: 0.7161 - val_accuracy: 0.8202\n","Epoch 76/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.1226 - accuracy: 0.9840 - val_loss: 0.7108 - val_accuracy: 0.8419\n","Epoch 77/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1384 - accuracy: 0.9752 - val_loss: 0.6932 - val_accuracy: 0.8388\n","Epoch 78/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1247 - accuracy: 0.9827 - val_loss: 0.7764 - val_accuracy: 0.8368\n","Epoch 79/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1356 - accuracy: 0.9775 - val_loss: 0.7987 - val_accuracy: 0.8337\n","Epoch 80/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1278 - accuracy: 0.9798 - val_loss: 0.7979 - val_accuracy: 0.8388\n","Epoch 81/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1166 - accuracy: 0.9840 - val_loss: 0.7793 - val_accuracy: 0.8347\n","Epoch 82/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1109 - accuracy: 0.9860 - val_loss: 0.8762 - val_accuracy: 0.8099\n","Epoch 83/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1337 - accuracy: 0.9739 - val_loss: 0.7770 - val_accuracy: 0.8388\n","Epoch 84/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1386 - accuracy: 0.9739 - val_loss: 0.6697 - val_accuracy: 0.8450\n","Epoch 85/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1770 - accuracy: 0.9527 - val_loss: 0.6488 - val_accuracy: 0.7944\n","Epoch 86/100\n","31/31 [==============================] - 1s 37ms/step - loss: 0.2617 - accuracy: 0.9191 - val_loss: 0.5511 - val_accuracy: 0.8512\n","Epoch 87/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.1320 - accuracy: 0.9773 - val_loss: 0.7005 - val_accuracy: 0.8523\n","Epoch 88/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1065 - accuracy: 0.9858 - val_loss: 0.8164 - val_accuracy: 0.8326\n","Epoch 89/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0949 - accuracy: 0.9907 - val_loss: 0.8801 - val_accuracy: 0.8171\n","Epoch 90/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0917 - accuracy: 0.9915 - val_loss: 0.8689 - val_accuracy: 0.8430\n","Epoch 91/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.1127 - accuracy: 0.9809 - val_loss: 0.7882 - val_accuracy: 0.8264\n","Epoch 92/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0869 - accuracy: 0.9930 - val_loss: 0.9417 - val_accuracy: 0.8223\n","Epoch 93/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0901 - accuracy: 0.9912 - val_loss: 1.0628 - val_accuracy: 0.8048\n","Epoch 94/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0983 - accuracy: 0.9855 - val_loss: 0.8601 - val_accuracy: 0.8471\n","Epoch 95/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.1030 - accuracy: 0.9842 - val_loss: 0.8223 - val_accuracy: 0.8295\n","Epoch 96/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.0923 - accuracy: 0.9897 - val_loss: 0.9181 - val_accuracy: 0.8378\n","Epoch 97/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.1237 - accuracy: 0.9742 - val_loss: 0.8113 - val_accuracy: 0.8399\n","Epoch 98/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.1009 - accuracy: 0.9853 - val_loss: 1.0791 - val_accuracy: 0.7707\n","Epoch 99/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.0929 - accuracy: 0.9860 - val_loss: 0.8080 - val_accuracy: 0.8409\n","Epoch 100/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.1114 - accuracy: 0.9796 - val_loss: 0.9030 - val_accuracy: 0.8202\n","{'loss': [1.3978955745697021, 1.2685450315475464, 1.1458797454833984, 1.0645217895507812, 0.9976589679718018, 0.9597506523132324, 0.9058759212493896, 0.879423975944519, 0.8255591988563538, 0.803707480430603, 0.7758369445800781, 0.7735115885734558, 0.720504105091095, 0.6869456768035889, 0.6753591299057007, 0.6493662595748901, 0.6228308081626892, 0.6021511554718018, 0.5867642760276794, 0.5911856293678284, 0.5514929294586182, 0.5473154187202454, 0.524376630783081, 0.5175153017044067, 0.5013138651847839, 0.4932429790496826, 0.4620841443538666, 0.4583480656147003, 0.45082923769950867, 0.4541180729866028, 0.4099080264568329, 0.42193976044654846, 0.4007258713245392, 0.38721543550491333, 0.3771880567073822, 0.4503255784511566, 0.36855778098106384, 0.3929298222064972, 0.36418211460113525, 0.3518989682197571, 0.31235042214393616, 0.3135579228401184, 0.30584707856178284, 0.30450621247291565, 0.29259544610977173, 0.33170586824417114, 0.2823747992515564, 0.2582493722438812, 0.27405229210853577, 0.26157140731811523, 0.2781003713607788, 0.24235421419143677, 0.264780730009079, 0.2548670172691345, 0.23676538467407227, 0.2222309708595276, 0.2030789852142334, 0.21588532626628876, 0.21376672387123108, 0.1951400488615036, 0.24467208981513977, 0.21140386164188385, 0.2480211853981018, 0.17656169831752777, 0.16958081722259521, 0.21488706767559052, 0.18968401849269867, 0.1738857924938202, 0.15867511928081512, 0.16347068548202515, 0.14914360642433167, 0.22600802779197693, 0.1802062690258026, 0.1636197417974472, 0.13473840057849884, 0.12255599349737167, 0.13840678334236145, 0.12472541630268097, 0.13557013869285583, 0.12781809270381927, 0.11664348095655441, 0.11094997823238373, 0.13374726474285126, 0.13858722150325775, 0.1769932210445404, 0.2617376744747162, 0.1319582164287567, 0.10648883134126663, 0.09492764621973038, 0.09166456013917923, 0.11270452290773392, 0.08692718297243118, 0.09012101590633392, 0.09829112142324448, 0.10295671224594116, 0.09228299558162689, 0.12371423095464706, 0.1008816510438919, 0.09287447482347488, 0.11140748113393784], 'accuracy': [0.5648579001426697, 0.6855297088623047, 0.7379844784736633, 0.7666666507720947, 0.7837209105491638, 0.7790697813034058, 0.7976744174957275, 0.7984496355056763, 0.8126614689826965, 0.814211905002594, 0.8144702911376953, 0.8108527064323425, 0.8242893815040588, 0.8418604731559753, 0.8390181064605713, 0.8379845023155212, 0.8454780578613281, 0.8501291871070862, 0.8540051579475403, 0.8496124148368835, 0.8620154857635498, 0.8552971482276917, 0.866925060749054, 0.8607234954833984, 0.868733823299408, 0.8656330704689026, 0.8788113594055176, 0.8788113594055176, 0.8803617358207703, 0.8757106065750122, 0.8963824510574341, 0.883979320526123, 0.897157609462738, 0.8987079858779907, 0.9018087983131409, 0.8609819412231445, 0.9028424024581909, 0.8886305093765259, 0.8999999761581421, 0.9090439081192017, 0.9248061776161194, 0.921705424785614, 0.9281653761863708, 0.9273901581764221, 0.9307493567466736, 0.9072351455688477, 0.933850109577179, 0.9423772692680359, 0.9351420998573303, 0.9392764568328857, 0.9291989803314209, 0.9465116262435913, 0.930232584476471, 0.9426356554031372, 0.94728684425354, 0.9571059346199036, 0.9609819054603577, 0.9578811526298523, 0.9524548053741455, 0.9609819054603577, 0.9356589317321777, 0.9514212012290955, 0.9382429122924805, 0.9700258374214172, 0.9692506194114685, 0.949095606803894, 0.9607235193252563, 0.9664082527160645, 0.9715762138366699, 0.9664082527160645, 0.9757105708122253, 0.9434108734130859, 0.9617571234703064, 0.9684754610061646, 0.9777777791023254, 0.983979344367981, 0.9751937985420227, 0.9826873540878296, 0.9775193929672241, 0.9798449873924255, 0.983979344367981, 0.9860464930534363, 0.9739018082618713, 0.9739018082618713, 0.9527131915092468, 0.9191214442253113, 0.9772610068321228, 0.985788106918335, 0.9906976819038391, 0.9914728403091431, 0.9808785319328308, 0.9930232763290405, 0.9912144541740417, 0.9855297207832336, 0.9842377305030823, 0.9896640777587891, 0.9741601943969727, 0.9852713346481323, 0.9860464930534363, 0.9795865416526794], 'val_loss': [1.370458722114563, 1.3089476823806763, 1.2616242170333862, 1.2152159214019775, 1.172741174697876, 1.132070779800415, 1.0907458066940308, 1.0589600801467896, 1.010087013244629, 0.983797550201416, 0.9679543972015381, 0.912377119064331, 0.8737872242927551, 0.8108453154563904, 0.7866617441177368, 0.769156277179718, 0.7228299975395203, 0.6928566098213196, 0.7524905204772949, 0.6312381625175476, 0.6269504427909851, 0.6207154393196106, 0.5814629793167114, 0.5816246867179871, 0.6199929714202881, 0.5651551485061646, 0.5580564141273499, 0.6376739740371704, 0.7301523685455322, 0.5450388193130493, 0.5785818099975586, 0.5245832800865173, 0.5801726579666138, 0.5394312143325806, 0.6044421195983887, 0.5165019035339355, 0.5212349891662598, 0.607702374458313, 0.5770562887191772, 0.5448945760726929, 0.5384675860404968, 0.5391257405281067, 0.5766787528991699, 0.6472233533859253, 0.603192150592804, 0.6177926063537598, 0.6014824509620667, 0.5736170411109924, 0.6263810992240906, 0.5589708089828491, 0.5373005270957947, 0.5836077332496643, 0.5728419423103333, 0.6537702679634094, 0.58881014585495, 0.6836808323860168, 0.6255943179130554, 0.6577988862991333, 0.6467393040657043, 0.651181161403656, 0.5837889313697815, 1.0337249040603638, 0.6324461102485657, 0.653274655342102, 0.6916975378990173, 0.6354745030403137, 0.6316051483154297, 0.7024298906326294, 0.8091650009155273, 0.7392258644104004, 0.7614083886146545, 0.6276423335075378, 0.7399211525917053, 0.8150002360343933, 0.7160741090774536, 0.7107542753219604, 0.6931770443916321, 0.7764186263084412, 0.7986541986465454, 0.7978846430778503, 0.7793282866477966, 0.8762401938438416, 0.7770049571990967, 0.6696893572807312, 0.6488088965415955, 0.5511437058448792, 0.7004544734954834, 0.8164474964141846, 0.8801441192626953, 0.8688707947731018, 0.7882250547409058, 0.9417420625686646, 1.0627784729003906, 0.8601217865943909, 0.8223364949226379, 0.9180832505226135, 0.8112698197364807, 1.0791072845458984, 0.8080154657363892, 0.9030064344406128], 'val_accuracy': [0.6342975497245789, 0.7035123705863953, 0.7128099203109741, 0.7561983466148376, 0.7944214940071106, 0.7871900796890259, 0.7469007968902588, 0.7737603187561035, 0.8068181872367859, 0.7861570119857788, 0.7045454382896423, 0.8006198406219482, 0.7840909361839294, 0.8016529083251953, 0.8006198406219482, 0.7933884263038635, 0.7944214940071106, 0.7944214940071106, 0.7438016533851624, 0.8285123705863953, 0.8150826692581177, 0.8130165338516235, 0.8388429880142212, 0.8347107172012329, 0.8057851195335388, 0.8316115736961365, 0.8305785059928894, 0.7954545617103577, 0.7789255976676941, 0.8450413346290588, 0.8254132270812988, 0.8409090638160706, 0.8047520518302917, 0.8450413346290588, 0.8305785059928894, 0.8243801593780518, 0.8460744023323059, 0.7964876294136047, 0.8243801593780518, 0.8471074104309082, 0.8388429880142212, 0.8450413346290588, 0.836776852607727, 0.8026859760284424, 0.827479362487793, 0.8016529083251953, 0.8398760557174683, 0.8419421315193176, 0.8212810158729553, 0.8429751992225647, 0.8491735458374023, 0.8450413346290588, 0.8460744023323059, 0.8254132270812988, 0.8398760557174683, 0.8016529083251953, 0.8398760557174683, 0.8326446413993835, 0.8254132270812988, 0.83574378490448, 0.8233470916748047, 0.7840909361839294, 0.8305785059928894, 0.81611567735672, 0.8264462947845459, 0.8233470916748047, 0.8347107172012329, 0.81611567735672, 0.8068181872367859, 0.8326446413993835, 0.827479362487793, 0.8398760557174683, 0.807851254940033, 0.7944214940071106, 0.8202479481697083, 0.8419421315193176, 0.8388429880142212, 0.836776852607727, 0.8336777091026306, 0.8388429880142212, 0.8347107172012329, 0.8099173307418823, 0.8388429880142212, 0.8450413346290588, 0.7944214940071106, 0.8512396812438965, 0.8522727489471436, 0.8326446413993835, 0.817148745059967, 0.8429751992225647, 0.8264462947845459, 0.8223140239715576, 0.8047520518302917, 0.8471074104309082, 0.8295454382896423, 0.8378099203109741, 0.8398760557174683, 0.7706611752510071, 0.8409090638160706, 0.8202479481697083]}\n","32/32 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - ETA: 0s - loss: 0.3853 - accuracy: 0.8518"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 7s 79ms/step - loss: 0.3853 - accuracy: 0.8518 - val_loss: 0.7228 - val_accuracy: 0.4849\n","Epoch 2/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.2790 - accuracy: 0.9027 - val_loss: 0.7156 - val_accuracy: 0.5603\n","Epoch 3/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.2483 - accuracy: 0.9165 - val_loss: 0.7124 - val_accuracy: 0.8039\n","Epoch 4/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.2241 - accuracy: 0.9305 - val_loss: 0.6973 - val_accuracy: 0.8610\n","Epoch 5/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.2328 - accuracy: 0.9224 - val_loss: 0.6876 - val_accuracy: 0.8502\n","Epoch 6/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.1876 - accuracy: 0.9456 - val_loss: 0.6771 - val_accuracy: 0.8125\n","Epoch 7/100\n","29/29 [==============================] - 1s 29ms/step - loss: 0.1680 - accuracy: 0.9512 - val_loss: 0.6536 - val_accuracy: 0.8739\n","Epoch 8/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1401 - accuracy: 0.9639 - val_loss: 0.6329 - val_accuracy: 0.7953\n","Epoch 9/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.1394 - accuracy: 0.9644 - val_loss: 0.6119 - val_accuracy: 0.8459\n","Epoch 10/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.1343 - accuracy: 0.9634 - val_loss: 0.5655 - val_accuracy: 0.8901\n","Epoch 11/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1235 - accuracy: 0.9731 - val_loss: 0.5476 - val_accuracy: 0.8728\n","Epoch 12/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.1253 - accuracy: 0.9690 - val_loss: 0.5184 - val_accuracy: 0.8761\n","Epoch 13/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1580 - accuracy: 0.9520 - val_loss: 0.5235 - val_accuracy: 0.8297\n","Epoch 14/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1058 - accuracy: 0.9771 - val_loss: 0.4725 - val_accuracy: 0.8556\n","Epoch 15/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.1415 - accuracy: 0.9588 - val_loss: 0.4192 - val_accuracy: 0.8976\n","Epoch 16/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0927 - accuracy: 0.9833 - val_loss: 0.3846 - val_accuracy: 0.8825\n","Epoch 17/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1151 - accuracy: 0.9741 - val_loss: 0.3587 - val_accuracy: 0.8782\n","Epoch 18/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1181 - accuracy: 0.9725 - val_loss: 0.3728 - val_accuracy: 0.8502\n","Epoch 19/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0901 - accuracy: 0.9838 - val_loss: 0.3443 - val_accuracy: 0.8847\n","Epoch 20/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0703 - accuracy: 0.9919 - val_loss: 0.3411 - val_accuracy: 0.8879\n","Epoch 21/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0677 - accuracy: 0.9922 - val_loss: 0.3509 - val_accuracy: 0.8869\n","Epoch 22/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0679 - accuracy: 0.9925 - val_loss: 0.3945 - val_accuracy: 0.8890\n","Epoch 23/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0856 - accuracy: 0.9833 - val_loss: 0.4602 - val_accuracy: 0.8631\n","Epoch 24/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0791 - accuracy: 0.9857 - val_loss: 0.5990 - val_accuracy: 0.8017\n","Epoch 25/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0953 - accuracy: 0.9790 - val_loss: 0.4087 - val_accuracy: 0.8912\n","Epoch 26/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0723 - accuracy: 0.9895 - val_loss: 0.4206 - val_accuracy: 0.8944\n","Epoch 27/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0883 - accuracy: 0.9825 - val_loss: 0.6080 - val_accuracy: 0.7942\n","Epoch 28/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0807 - accuracy: 0.9852 - val_loss: 0.4405 - val_accuracy: 0.8912\n","Epoch 29/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0614 - accuracy: 0.9935 - val_loss: 0.4971 - val_accuracy: 0.8922\n","Epoch 30/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0538 - accuracy: 0.9957 - val_loss: 0.5459 - val_accuracy: 0.8739\n","Epoch 31/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0623 - accuracy: 0.9911 - val_loss: 0.5988 - val_accuracy: 0.8804\n","Epoch 32/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0656 - accuracy: 0.9908 - val_loss: 0.6423 - val_accuracy: 0.8664\n","Epoch 33/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0601 - accuracy: 0.9925 - val_loss: 0.5175 - val_accuracy: 0.8933\n","Epoch 34/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.0520 - accuracy: 0.9954 - val_loss: 0.5592 - val_accuracy: 0.8901\n","Epoch 35/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0556 - accuracy: 0.9943 - val_loss: 0.6670 - val_accuracy: 0.8804\n","Epoch 36/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0700 - accuracy: 0.9879 - val_loss: 0.5897 - val_accuracy: 0.8782\n","Epoch 37/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0715 - accuracy: 0.9887 - val_loss: 0.9401 - val_accuracy: 0.8157\n","Epoch 38/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0819 - accuracy: 0.9830 - val_loss: 0.7190 - val_accuracy: 0.8394\n","Epoch 39/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0778 - accuracy: 0.9857 - val_loss: 0.4855 - val_accuracy: 0.8869\n","Epoch 40/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0772 - accuracy: 0.9860 - val_loss: 0.7277 - val_accuracy: 0.8125\n","Epoch 41/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0662 - accuracy: 0.9876 - val_loss: 0.5816 - val_accuracy: 0.8761\n","Epoch 42/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.1326 - accuracy: 0.9655 - val_loss: 0.5679 - val_accuracy: 0.8470\n","Epoch 43/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1081 - accuracy: 0.9747 - val_loss: 0.4223 - val_accuracy: 0.8858\n","Epoch 44/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0605 - accuracy: 0.9914 - val_loss: 0.5742 - val_accuracy: 0.8739\n","Epoch 45/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0471 - accuracy: 0.9965 - val_loss: 0.6201 - val_accuracy: 0.8858\n","Epoch 46/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0399 - accuracy: 0.9992 - val_loss: 0.5846 - val_accuracy: 0.8879\n","Epoch 47/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0377 - accuracy: 0.9995 - val_loss: 0.6401 - val_accuracy: 0.8750\n","Epoch 48/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0371 - accuracy: 0.9997 - val_loss: 0.6548 - val_accuracy: 0.8804\n","Epoch 49/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0378 - accuracy: 0.9989 - val_loss: 0.6827 - val_accuracy: 0.8653\n","Epoch 50/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0355 - accuracy: 0.9995 - val_loss: 0.6397 - val_accuracy: 0.8836\n","Epoch 51/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 0.8836\n","Epoch 52/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.6553 - val_accuracy: 0.8879\n","Epoch 53/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0339 - accuracy: 0.9997 - val_loss: 0.6418 - val_accuracy: 0.8858\n","Epoch 54/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.6384 - val_accuracy: 0.8912\n","Epoch 55/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0335 - accuracy: 0.9995 - val_loss: 0.6768 - val_accuracy: 0.8869\n","Epoch 56/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0340 - accuracy: 0.9995 - val_loss: 0.6293 - val_accuracy: 0.8793\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0331 - accuracy: 0.9997 - val_loss: 0.6256 - val_accuracy: 0.8858\n","Epoch 58/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0321 - accuracy: 0.9997 - val_loss: 0.6492 - val_accuracy: 0.8858\n","Epoch 59/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0328 - accuracy: 0.9995 - val_loss: 0.6250 - val_accuracy: 0.8922\n","Epoch 60/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0330 - accuracy: 0.9992 - val_loss: 0.6262 - val_accuracy: 0.8890\n","Epoch 61/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.6483 - val_accuracy: 0.8890\n","Epoch 62/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0307 - accuracy: 1.0000 - val_loss: 0.6596 - val_accuracy: 0.8966\n","Epoch 63/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.1112 - accuracy: 0.9688 - val_loss: 0.4753 - val_accuracy: 0.8664\n","Epoch 64/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0737 - accuracy: 0.9844 - val_loss: 0.4658 - val_accuracy: 0.8750\n","Epoch 65/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0478 - accuracy: 0.9943 - val_loss: 0.5039 - val_accuracy: 0.8836\n","Epoch 66/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0414 - accuracy: 0.9962 - val_loss: 0.5940 - val_accuracy: 0.8793\n","Epoch 67/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0382 - accuracy: 0.9968 - val_loss: 0.6357 - val_accuracy: 0.8836\n","Epoch 68/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0377 - accuracy: 0.9978 - val_loss: 0.5724 - val_accuracy: 0.8815\n","Epoch 69/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0318 - accuracy: 0.9997 - val_loss: 0.6279 - val_accuracy: 0.8858\n","Epoch 70/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0305 - accuracy: 0.9997 - val_loss: 0.6128 - val_accuracy: 0.8901\n","Epoch 71/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0295 - accuracy: 0.9995 - val_loss: 0.5820 - val_accuracy: 0.8901\n","Epoch 72/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0305 - accuracy: 0.9997 - val_loss: 0.7705 - val_accuracy: 0.8416\n","Epoch 73/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0410 - accuracy: 0.9954 - val_loss: 0.8630 - val_accuracy: 0.8696\n","Epoch 74/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2252 - accuracy: 0.9273 - val_loss: 0.4727 - val_accuracy: 0.8211\n","Epoch 75/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0922 - accuracy: 0.9798 - val_loss: 0.4198 - val_accuracy: 0.8750\n","Epoch 76/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0386 - accuracy: 0.9978 - val_loss: 0.5736 - val_accuracy: 0.8847\n","Epoch 77/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0332 - accuracy: 0.9984 - val_loss: 0.5747 - val_accuracy: 0.8944\n","Epoch 78/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0308 - accuracy: 0.9987 - val_loss: 0.5764 - val_accuracy: 0.8922\n","Epoch 79/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0285 - accuracy: 0.9997 - val_loss: 0.6581 - val_accuracy: 0.8664\n","Epoch 80/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0293 - accuracy: 0.9989 - val_loss: 0.6132 - val_accuracy: 0.8804\n","Epoch 81/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0283 - accuracy: 0.9995 - val_loss: 0.6307 - val_accuracy: 0.8707\n","Epoch 82/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.6288 - val_accuracy: 0.8825\n","Epoch 83/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.6376 - val_accuracy: 0.8847\n","Epoch 84/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 0.8836\n","Epoch 85/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.6486 - val_accuracy: 0.8901\n","Epoch 86/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.6397 - val_accuracy: 0.8890\n","Epoch 87/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.6333 - val_accuracy: 0.8879\n","Epoch 88/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 0.8890\n","Epoch 89/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 0.6301 - val_accuracy: 0.8879\n","Epoch 90/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.6279 - val_accuracy: 0.8869\n","Epoch 91/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.6363 - val_accuracy: 0.8879\n","Epoch 92/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.6205 - val_accuracy: 0.8890\n","Epoch 93/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 0.8858\n","Epoch 94/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.6194 - val_accuracy: 0.8890\n","Epoch 95/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 0.8879\n","Epoch 96/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.6239 - val_accuracy: 0.8858\n","Epoch 97/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.6148 - val_accuracy: 0.8901\n","Epoch 98/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.6081 - val_accuracy: 0.8890\n","Epoch 99/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.6008 - val_accuracy: 0.8912\n","Epoch 100/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.5999 - val_accuracy: 0.8901\n","{'loss': [0.38528183102607727, 0.2790157198905945, 0.2483241707086563, 0.22406452894210815, 0.23277781903743744, 0.18756182491779327, 0.1679607629776001, 0.14008493721485138, 0.13939163088798523, 0.13430561125278473, 0.12347228825092316, 0.1253490000963211, 0.1579897254705429, 0.10575704276561737, 0.1414940506219864, 0.09268832206726074, 0.11511261016130447, 0.1181037500500679, 0.09009826928377151, 0.07031923532485962, 0.06768926978111267, 0.06793379038572311, 0.08560360968112946, 0.07905696332454681, 0.09526681900024414, 0.07230406999588013, 0.08830699324607849, 0.08065878599882126, 0.06136973947286606, 0.0538230836391449, 0.06225014850497246, 0.06563804298639297, 0.06013670191168785, 0.05198919400572777, 0.055552057921886444, 0.07001111656427383, 0.07152216881513596, 0.08189623802900314, 0.07781814783811569, 0.077227883040905, 0.06616182625293732, 0.13261160254478455, 0.10814588516950607, 0.060498036444187164, 0.04714660346508026, 0.03985176980495453, 0.03773849830031395, 0.03709786757826805, 0.03779585286974907, 0.03552933409810066, 0.03466647118330002, 0.03380439430475235, 0.033934399485588074, 0.0333760641515255, 0.03346450999379158, 0.03396226093173027, 0.03312600776553154, 0.03207653760910034, 0.03283068537712097, 0.03302450850605965, 0.031046133488416672, 0.030724450945854187, 0.11117683351039886, 0.07371275871992111, 0.047817412763834, 0.04141946882009506, 0.03821694478392601, 0.03766341879963875, 0.03176253288984299, 0.03047136403620243, 0.029496271163225174, 0.03049492835998535, 0.04096106439828873, 0.2252185195684433, 0.09218087047338486, 0.038599852472543716, 0.0332011841237545, 0.030838724225759506, 0.028451962396502495, 0.02930467016994953, 0.02834513410925865, 0.027143310755491257, 0.026467660441994667, 0.02600906416773796, 0.025671662762761116, 0.025272807106375694, 0.02500929683446884, 0.024909837171435356, 0.02464737370610237, 0.024322116747498512, 0.024089401587843895, 0.02397773787379265, 0.023665964603424072, 0.023470010608434677, 0.023260964080691338, 0.023121695965528488, 0.022813398391008377, 0.022669684141874313, 0.02240654081106186, 0.022240716964006424], 'accuracy': [0.8518319129943848, 0.9027478694915771, 0.9164870977401733, 0.9304956793785095, 0.9224137663841248, 0.9455819129943848, 0.9512392282485962, 0.9639008641242981, 0.9644396305084229, 0.9633620977401733, 0.9730603694915771, 0.9690194129943848, 0.9520474076271057, 0.9771012663841248, 0.9587823152542114, 0.9832974076271057, 0.9741379022598267, 0.9725215435028076, 0.9838362336158752, 0.9919180870056152, 0.9921875, 0.9924569129943848, 0.9832974076271057, 0.985722005367279, 0.9789870977401733, 0.9894935488700867, 0.9824892282485962, 0.9851831793785095, 0.993534505367279, 0.9956896305084229, 0.9911099076271057, 0.990840494632721, 0.9924569129943848, 0.9954202771186829, 0.9943426847457886, 0.9878771305084229, 0.9886853694915771, 0.983027994632721, 0.985722005367279, 0.985991358757019, 0.9876077771186829, 0.9655172228813171, 0.9746767282485962, 0.9913793206214905, 0.9964978694915771, 0.9991918206214905, 0.9994612336158752, 0.9997305870056152, 0.9989224076271057, 0.9994612336158752, 1.0, 1.0, 0.9997305870056152, 1.0, 0.9994612336158752, 0.9994612336158752, 0.9997305870056152, 0.9997305870056152, 0.9994612336158752, 0.9991918206214905, 1.0, 1.0, 0.96875, 0.984375, 0.9943426847457886, 0.9962284564971924, 0.9967672228813171, 0.9978448152542114, 0.9997305870056152, 0.9997305870056152, 0.9994612336158752, 0.9997305870056152, 0.9954202771186829, 0.9272629022598267, 0.9797952771186829, 0.9978448152542114, 0.998383641242981, 0.998652994632721, 0.9997305870056152, 0.9989224076271057, 0.9994612336158752, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.7227691411972046, 0.7156083583831787, 0.7124488949775696, 0.6973488330841064, 0.6875911355018616, 0.6771073341369629, 0.6536197066307068, 0.632922887802124, 0.6119384765625, 0.565474271774292, 0.5476098656654358, 0.5184053778648376, 0.5235059261322021, 0.4724915325641632, 0.419171005487442, 0.38463789224624634, 0.3586672842502594, 0.3727531433105469, 0.34431877732276917, 0.3410646915435791, 0.35085830092430115, 0.394453763961792, 0.4601985812187195, 0.5989953279495239, 0.4087073802947998, 0.42057791352272034, 0.6080397963523865, 0.4404684007167816, 0.4970717132091522, 0.5459015965461731, 0.5988317728042603, 0.6423166990280151, 0.5174630284309387, 0.5591832399368286, 0.6669641733169556, 0.5896573662757874, 0.9400986433029175, 0.7190372943878174, 0.48549628257751465, 0.7277237176895142, 0.5815898776054382, 0.5679494142532349, 0.4223242700099945, 0.5742337703704834, 0.620093047618866, 0.5845655798912048, 0.6400991082191467, 0.6547589898109436, 0.6827448606491089, 0.6397379636764526, 0.6638767719268799, 0.6553143858909607, 0.6417636871337891, 0.6383653283119202, 0.6768444180488586, 0.6293498277664185, 0.625553548336029, 0.6492436528205872, 0.6250200867652893, 0.626217246055603, 0.6483497619628906, 0.6596124172210693, 0.47529441118240356, 0.4657546877861023, 0.5039048790931702, 0.5939754843711853, 0.635688304901123, 0.5724214911460876, 0.6278780102729797, 0.6128255724906921, 0.5820006132125854, 0.7705156803131104, 0.8630254864692688, 0.4727359712123871, 0.4198310077190399, 0.5736021995544434, 0.574735701084137, 0.5764055848121643, 0.6580827236175537, 0.6131975650787354, 0.6307492256164551, 0.6287736296653748, 0.6376128196716309, 0.6388829350471497, 0.6486428380012512, 0.6396710872650146, 0.6333005428314209, 0.6366235613822937, 0.6301265358924866, 0.6279385685920715, 0.636338472366333, 0.6204709410667419, 0.6181007623672485, 0.6193770170211792, 0.6127153038978577, 0.6238590478897095, 0.6147509813308716, 0.6080586910247803, 0.6008251905441284, 0.5999034643173218], 'val_accuracy': [0.48491379618644714, 0.5603448152542114, 0.8038793206214905, 0.860991358757019, 0.850215494632721, 0.8125, 0.8739224076271057, 0.795258641242981, 0.8459051847457886, 0.8900862336158752, 0.8728448152542114, 0.8760775923728943, 0.829741358757019, 0.8556034564971924, 0.8976293206214905, 0.8825430870056152, 0.8782327771186829, 0.850215494632721, 0.8846982717514038, 0.8879310488700867, 0.8868534564971924, 0.889008641242981, 0.8631465435028076, 0.8017241358757019, 0.8911637663841248, 0.8943965435028076, 0.7941810488700867, 0.8911637663841248, 0.892241358757019, 0.8739224076271057, 0.8803879022598267, 0.8663793206214905, 0.8933189511299133, 0.8900862336158752, 0.8803879022598267, 0.8782327771186829, 0.8157327771186829, 0.8394396305084229, 0.8868534564971924, 0.8125, 0.8760775923728943, 0.8469827771186829, 0.8857758641242981, 0.8739224076271057, 0.8857758641242981, 0.8879310488700867, 0.875, 0.8803879022598267, 0.8653017282485962, 0.8836206793785095, 0.8836206793785095, 0.8879310488700867, 0.8857758641242981, 0.8911637663841248, 0.8868534564971924, 0.8793103694915771, 0.8857758641242981, 0.8857758641242981, 0.892241358757019, 0.889008641242981, 0.889008641242981, 0.8965517282485962, 0.8663793206214905, 0.875, 0.8836206793785095, 0.8793103694915771, 0.8836206793785095, 0.881465494632721, 0.8857758641242981, 0.8900862336158752, 0.8900862336158752, 0.8415948152542114, 0.8696120977401733, 0.8211206793785095, 0.875, 0.8846982717514038, 0.8943965435028076, 0.892241358757019, 0.8663793206214905, 0.8803879022598267, 0.8706896305084229, 0.8825430870056152, 0.8846982717514038, 0.8836206793785095, 0.8900862336158752, 0.889008641242981, 0.8879310488700867, 0.889008641242981, 0.8879310488700867, 0.8868534564971924, 0.8879310488700867, 0.889008641242981, 0.8857758641242981, 0.889008641242981, 0.8879310488700867, 0.8857758641242981, 0.8900862336158752, 0.889008641242981, 0.8911637663841248, 0.8900862336158752]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - ETA: 0s - loss: 0.3841 - accuracy: 0.8548"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 7s 60ms/step - loss: 0.3841 - accuracy: 0.8548 - val_loss: 0.7242 - val_accuracy: 0.4955\n","Epoch 2/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3055 - accuracy: 0.8945 - val_loss: 0.7183 - val_accuracy: 0.5973\n","Epoch 3/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2443 - accuracy: 0.9222 - val_loss: 0.7128 - val_accuracy: 0.5781\n","Epoch 4/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.2333 - accuracy: 0.9264 - val_loss: 0.7024 - val_accuracy: 0.8043\n","Epoch 5/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.1909 - accuracy: 0.9423 - val_loss: 0.6889 - val_accuracy: 0.8281\n","Epoch 6/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1734 - accuracy: 0.9510 - val_loss: 0.6771 - val_accuracy: 0.7217\n","Epoch 7/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2147 - accuracy: 0.9293 - val_loss: 0.6798 - val_accuracy: 0.7771\n","Epoch 8/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.2028 - accuracy: 0.9369 - val_loss: 0.6564 - val_accuracy: 0.8416\n","Epoch 9/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.1424 - accuracy: 0.9643 - val_loss: 0.6208 - val_accuracy: 0.8235\n","Epoch 10/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.1295 - accuracy: 0.9646 - val_loss: 0.6033 - val_accuracy: 0.8563\n","Epoch 11/100\n","28/28 [==============================] - 1s 51ms/step - loss: 0.1480 - accuracy: 0.9584 - val_loss: 0.5846 - val_accuracy: 0.8609\n","Epoch 12/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.1293 - accuracy: 0.9672 - val_loss: 0.5609 - val_accuracy: 0.8416\n","Epoch 13/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1065 - accuracy: 0.9774 - val_loss: 0.5149 - val_accuracy: 0.8382\n","Epoch 14/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0860 - accuracy: 0.9853 - val_loss: 0.4649 - val_accuracy: 0.8473\n","Epoch 15/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1273 - accuracy: 0.9638 - val_loss: 0.4670 - val_accuracy: 0.8371\n","Epoch 16/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1077 - accuracy: 0.9748 - val_loss: 0.4669 - val_accuracy: 0.8133\n","Epoch 17/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0948 - accuracy: 0.9813 - val_loss: 0.4242 - val_accuracy: 0.8473\n","Epoch 18/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0820 - accuracy: 0.9861 - val_loss: 0.3863 - val_accuracy: 0.8473\n","Epoch 19/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0999 - accuracy: 0.9768 - val_loss: 0.3806 - val_accuracy: 0.8597\n","Epoch 20/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0959 - accuracy: 0.9808 - val_loss: 0.3949 - val_accuracy: 0.8563\n","Epoch 21/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.0785 - accuracy: 0.9881 - val_loss: 0.3991 - val_accuracy: 0.8631\n","Epoch 22/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0844 - accuracy: 0.9842 - val_loss: 0.4040 - val_accuracy: 0.8575\n","Epoch 23/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0741 - accuracy: 0.9878 - val_loss: 0.4343 - val_accuracy: 0.8597\n","Epoch 24/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0662 - accuracy: 0.9909 - val_loss: 0.4464 - val_accuracy: 0.8541\n","Epoch 25/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1121 - accuracy: 0.9720 - val_loss: 0.6220 - val_accuracy: 0.8405\n","Epoch 26/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.1736 - accuracy: 0.9525 - val_loss: 0.3972 - val_accuracy: 0.8609\n","Epoch 27/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0769 - accuracy: 0.9887 - val_loss: 0.5381 - val_accuracy: 0.8462\n","Epoch 28/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.0767 - accuracy: 0.9867 - val_loss: 0.5286 - val_accuracy: 0.8699\n","Epoch 29/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0612 - accuracy: 0.9935 - val_loss: 0.6838 - val_accuracy: 0.8360\n","Epoch 30/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0589 - accuracy: 0.9921 - val_loss: 0.5811 - val_accuracy: 0.8676\n","Epoch 31/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0644 - accuracy: 0.9904 - val_loss: 1.0988 - val_accuracy: 0.7523\n","Epoch 32/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0625 - accuracy: 0.9932 - val_loss: 0.6355 - val_accuracy: 0.8563\n","Epoch 33/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0627 - accuracy: 0.9912 - val_loss: 0.6878 - val_accuracy: 0.8563\n","Epoch 34/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1942 - accuracy: 0.9420 - val_loss: 0.5323 - val_accuracy: 0.8179\n","Epoch 35/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0949 - accuracy: 0.9827 - val_loss: 0.5438 - val_accuracy: 0.8484\n","Epoch 36/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0693 - accuracy: 0.9884 - val_loss: 0.6803 - val_accuracy: 0.8337\n","Epoch 37/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0593 - accuracy: 0.9929 - val_loss: 0.6831 - val_accuracy: 0.8382\n","Epoch 38/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0482 - accuracy: 0.9977 - val_loss: 0.6382 - val_accuracy: 0.8563\n","Epoch 39/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.0431 - accuracy: 0.9989 - val_loss: 0.6678 - val_accuracy: 0.8609\n","Epoch 40/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.0416 - accuracy: 0.9994 - val_loss: 0.6831 - val_accuracy: 0.8710\n","Epoch 41/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0397 - accuracy: 0.9994 - val_loss: 0.6760 - val_accuracy: 0.8609\n","Epoch 42/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0418 - accuracy: 0.9986 - val_loss: 0.7529 - val_accuracy: 0.8654\n","Epoch 43/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0426 - accuracy: 0.9975 - val_loss: 0.8038 - val_accuracy: 0.8439\n","Epoch 44/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0444 - accuracy: 0.9977 - val_loss: 0.6890 - val_accuracy: 0.8620\n","Epoch 45/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0410 - accuracy: 0.9986 - val_loss: 0.7170 - val_accuracy: 0.8620\n","Epoch 46/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0406 - accuracy: 0.9989 - val_loss: 0.7088 - val_accuracy: 0.8609\n","Epoch 47/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 0.7388 - val_accuracy: 0.8563\n","Epoch 48/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0369 - accuracy: 0.9997 - val_loss: 0.7539 - val_accuracy: 0.8563\n","Epoch 49/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 0.7434 - val_accuracy: 0.8744\n","Epoch 50/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0363 - accuracy: 0.9997 - val_loss: 0.7295 - val_accuracy: 0.8710\n","Epoch 51/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.7550 - val_accuracy: 0.8722\n","Epoch 52/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0364 - accuracy: 0.9989 - val_loss: 0.7324 - val_accuracy: 0.8529\n","Epoch 53/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0353 - accuracy: 0.9997 - val_loss: 0.7957 - val_accuracy: 0.8529\n","Epoch 54/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0390 - accuracy: 0.9977 - val_loss: 0.8494 - val_accuracy: 0.8360\n","Epoch 55/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0395 - accuracy: 0.9975 - val_loss: 0.7679 - val_accuracy: 0.8597\n","Epoch 56/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0730 - accuracy: 0.9850 - val_loss: 0.7295 - val_accuracy: 0.8518\n","Epoch 57/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2706 - accuracy: 0.9185 - val_loss: 0.4529 - val_accuracy: 0.8020\n","Epoch 58/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0994 - accuracy: 0.9827 - val_loss: 0.5030 - val_accuracy: 0.8609\n","Epoch 59/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0554 - accuracy: 0.9926 - val_loss: 0.6450 - val_accuracy: 0.8416\n","Epoch 60/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0423 - accuracy: 0.9966 - val_loss: 0.6245 - val_accuracy: 0.8416\n","Epoch 61/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0455 - accuracy: 0.9949 - val_loss: 0.6682 - val_accuracy: 0.8541\n","Epoch 62/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0408 - accuracy: 0.9972 - val_loss: 0.7586 - val_accuracy: 0.8235\n","Epoch 63/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0359 - accuracy: 0.9989 - val_loss: 0.7227 - val_accuracy: 0.8597\n","Epoch 64/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0377 - accuracy: 0.9977 - val_loss: 0.6952 - val_accuracy: 0.8631\n","Epoch 65/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0341 - accuracy: 0.9992 - val_loss: 0.7336 - val_accuracy: 0.8609\n","Epoch 66/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0365 - accuracy: 0.9983 - val_loss: 0.7962 - val_accuracy: 0.8541\n","Epoch 67/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0890 - accuracy: 0.9808 - val_loss: 0.7496 - val_accuracy: 0.8371\n","Epoch 68/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1050 - accuracy: 0.9740 - val_loss: 0.4775 - val_accuracy: 0.8529\n","Epoch 69/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0920 - accuracy: 0.9759 - val_loss: 0.6389 - val_accuracy: 0.8190\n","Epoch 70/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0515 - accuracy: 0.9926 - val_loss: 0.6128 - val_accuracy: 0.8620\n","Epoch 71/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0346 - accuracy: 0.9992 - val_loss: 0.5951 - val_accuracy: 0.8699\n","Epoch 72/100\n","28/28 [==============================] - 1s 50ms/step - loss: 0.0310 - accuracy: 0.9997 - val_loss: 0.6390 - val_accuracy: 0.8756\n","Epoch 73/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 0.6532 - val_accuracy: 0.8710\n","Epoch 74/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.6652 - val_accuracy: 0.8722\n","Epoch 75/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 0.6702 - val_accuracy: 0.8733\n","Epoch 76/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 0.6688 - val_accuracy: 0.8744\n","Epoch 77/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.8699\n","Epoch 78/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 0.6733 - val_accuracy: 0.8744\n","Epoch 79/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 0.6668 - val_accuracy: 0.8710\n","Epoch 80/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.6816 - val_accuracy: 0.8699\n","Epoch 81/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.6874 - val_accuracy: 0.8710\n","Epoch 82/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.6849 - val_accuracy: 0.8756\n","Epoch 83/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.6702 - val_accuracy: 0.8756\n","Epoch 84/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.6707 - val_accuracy: 0.8722\n","Epoch 85/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.6706 - val_accuracy: 0.8722\n","Epoch 86/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0259 - accuracy: 1.0000 - val_loss: 0.6681 - val_accuracy: 0.8744\n","Epoch 87/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.6611 - val_accuracy: 0.8733\n","Epoch 88/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 0.6619 - val_accuracy: 0.8710\n","Epoch 89/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 0.8710\n","Epoch 90/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.6478 - val_accuracy: 0.8733\n","Epoch 91/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.6410 - val_accuracy: 0.8688\n","Epoch 92/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 0.8733\n","Epoch 93/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.6494 - val_accuracy: 0.8722\n","Epoch 94/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.8733\n","Epoch 95/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.6505 - val_accuracy: 0.8710\n","Epoch 96/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 0.6385 - val_accuracy: 0.8710\n","Epoch 97/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.6297 - val_accuracy: 0.8710\n","Epoch 98/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.6537 - val_accuracy: 0.8688\n","Epoch 99/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.6405 - val_accuracy: 0.8710\n","Epoch 100/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.6413 - val_accuracy: 0.8722\n","{'loss': [0.38409626483917236, 0.3055366277694702, 0.24428071081638336, 0.23328399658203125, 0.19094818830490112, 0.1733803153038025, 0.21465544402599335, 0.20276623964309692, 0.1423787623643875, 0.12951891124248505, 0.14802563190460205, 0.12928266823291779, 0.10648935288190842, 0.08603379130363464, 0.12734343111515045, 0.10772660374641418, 0.09476824849843979, 0.08197727054357529, 0.09993090480566025, 0.09591882675886154, 0.0784633606672287, 0.08442110568284988, 0.07411998510360718, 0.06615181267261505, 0.11211980879306793, 0.17356941103935242, 0.07691879570484161, 0.07672765105962753, 0.06116168573498726, 0.058866582810878754, 0.0643673986196518, 0.06251254677772522, 0.06271667778491974, 0.1941855549812317, 0.09490808844566345, 0.06933337450027466, 0.05930459499359131, 0.04824642091989517, 0.0431225411593914, 0.041562147438526154, 0.039715807884931564, 0.04178996756672859, 0.042631663382053375, 0.044445306062698364, 0.04099181294441223, 0.04062231630086899, 0.03781653568148613, 0.036897946149110794, 0.03579562529921532, 0.036317892372608185, 0.03620942682027817, 0.03639283403754234, 0.03533347696065903, 0.03902408853173256, 0.039497122168540955, 0.07298776507377625, 0.27059611678123474, 0.0994248241186142, 0.055416204035282135, 0.04233911633491516, 0.04545753449201584, 0.040759120136499405, 0.035943008959293365, 0.03765192627906799, 0.03410626947879791, 0.036543577909469604, 0.08897806704044342, 0.10500458627939224, 0.09199730306863785, 0.05149958282709122, 0.034638695418834686, 0.03098927065730095, 0.029804248362779617, 0.02936689555644989, 0.02897636964917183, 0.028557423502206802, 0.028319470584392548, 0.028139380738139153, 0.027862831950187683, 0.027591893449425697, 0.02725103124976158, 0.027015743777155876, 0.026739932596683502, 0.026462053880095482, 0.02624075673520565, 0.02591363713145256, 0.02574426867067814, 0.025558946654200554, 0.025316651910543442, 0.025116723030805588, 0.024855604395270348, 0.024670738726854324, 0.024473562836647034, 0.024212680757045746, 0.02402699738740921, 0.024199016392230988, 0.023611655458807945, 0.023538829758763313, 0.023165054619312286, 0.02289099246263504], 'accuracy': [0.8548387289047241, 0.8944538831710815, 0.9221844673156738, 0.9264289736747742, 0.9422750473022461, 0.9510469436645508, 0.9292586445808411, 0.9368987083435059, 0.9643463492393494, 0.9646292924880981, 0.9584040641784668, 0.9671760201454163, 0.9773627519607544, 0.9852858185768127, 0.963780403137207, 0.974816083908081, 0.9813242554664612, 0.9861347079277039, 0.9767968058586121, 0.9807583689689636, 0.9881154298782349, 0.9841539263725281, 0.9878324866294861, 0.9909451007843018, 0.9719864130020142, 0.9524617791175842, 0.9886813759803772, 0.9867005944252014, 0.9934917688369751, 0.9920769929885864, 0.9903791546821594, 0.9932088255882263, 0.9912280440330505, 0.9419921040534973, 0.9827390909194946, 0.9883984327316284, 0.9929258823394775, 0.9977362751960754, 0.9988681674003601, 0.9994340538978577, 0.9994340538978577, 0.9985851645469666, 0.9974533319473267, 0.9977362751960754, 0.9985851645469666, 0.9988681674003601, 1.0, 0.9997170567512512, 1.0, 0.9997170567512512, 1.0, 0.9988681674003601, 0.9997170567512512, 0.9977362751960754, 0.9974533319473267, 0.9850028157234192, 0.9185059666633606, 0.9827390909194946, 0.992642879486084, 0.9966044425964355, 0.9949066042900085, 0.9971703290939331, 0.9988681674003601, 0.9977362751960754, 0.9991511106491089, 0.9983022212982178, 0.9807583689689636, 0.9739671945571899, 0.975947916507721, 0.992642879486084, 0.9991511106491089, 0.9997170567512512, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.7242380976676941, 0.7182849645614624, 0.7127916812896729, 0.7023714780807495, 0.6889025568962097, 0.6771408915519714, 0.6797689199447632, 0.6563903093338013, 0.620829164981842, 0.6032810211181641, 0.5846025943756104, 0.5608736872673035, 0.5148707032203674, 0.4648570716381073, 0.46699607372283936, 0.4669455289840698, 0.4242480397224426, 0.3862636089324951, 0.3805660009384155, 0.39487317204475403, 0.39910829067230225, 0.4040241539478302, 0.4342961311340332, 0.44639647006988525, 0.6219822764396667, 0.39716586470603943, 0.5380971431732178, 0.528610348701477, 0.6838198900222778, 0.5811420679092407, 1.0988267660140991, 0.6354765892028809, 0.6878190040588379, 0.5322532057762146, 0.5438024401664734, 0.6802995204925537, 0.6830572485923767, 0.6382293105125427, 0.6677577495574951, 0.6831002235412598, 0.6760281324386597, 0.752865195274353, 0.8038462400436401, 0.6890275478363037, 0.7169634103775024, 0.7088464498519897, 0.7388379573822021, 0.7538973689079285, 0.7433708310127258, 0.7295203804969788, 0.7549811601638794, 0.7324001789093018, 0.795678973197937, 0.8493590950965881, 0.7679364681243896, 0.7294871211051941, 0.452921599149704, 0.5030416250228882, 0.6450292468070984, 0.6245155334472656, 0.6681771278381348, 0.758636474609375, 0.7227142453193665, 0.6952140927314758, 0.733550488948822, 0.7961783409118652, 0.7495746612548828, 0.4774717390537262, 0.6389337778091431, 0.612800121307373, 0.595140278339386, 0.6389636397361755, 0.6532490849494934, 0.665212094783783, 0.6702367067337036, 0.6687594652175903, 0.6999267935752869, 0.6732684969902039, 0.6668078899383545, 0.6816340684890747, 0.6873699426651001, 0.684893786907196, 0.6701763272285461, 0.6706675291061401, 0.670640230178833, 0.6680865287780762, 0.6611214876174927, 0.6619499325752258, 0.6621668338775635, 0.6477788090705872, 0.6409781575202942, 0.6469460129737854, 0.6493698954582214, 0.6409088373184204, 0.6505042910575867, 0.6385445594787598, 0.6296992301940918, 0.6537426710128784, 0.6404910683631897, 0.6413277983665466], 'val_accuracy': [0.4954751133918762, 0.5972850918769836, 0.5780543088912964, 0.8042986392974854, 0.8280543088912964, 0.7217194437980652, 0.7771493196487427, 0.8416289687156677, 0.8235294222831726, 0.8563348650932312, 0.860859751701355, 0.8416289687156677, 0.8382353186607361, 0.8472850918769836, 0.837104082107544, 0.8133484125137329, 0.8472850918769836, 0.8472850918769836, 0.8597285151481628, 0.8563348650932312, 0.8631221652030945, 0.8574660420417786, 0.8597285151481628, 0.8540723919868469, 0.8404977321624756, 0.860859751701355, 0.8461538553237915, 0.8699095249176025, 0.8359728455543518, 0.8676470518112183, 0.7522624731063843, 0.8563348650932312, 0.8563348650932312, 0.8178732991218567, 0.848416268825531, 0.8337104320526123, 0.8382353186607361, 0.8563348650932312, 0.860859751701355, 0.8710407018661499, 0.860859751701355, 0.8653846383094788, 0.8438913822174072, 0.8619909286499023, 0.8619909286499023, 0.860859751701355, 0.8563348650932312, 0.8563348650932312, 0.8744344115257263, 0.8710407018661499, 0.872171938419342, 0.8529411554336548, 0.8529411554336548, 0.8359728455543518, 0.8597285151481628, 0.8518099784851074, 0.8020362257957458, 0.860859751701355, 0.8416289687156677, 0.8416289687156677, 0.8540723919868469, 0.8235294222831726, 0.8597285151481628, 0.8631221652030945, 0.860859751701355, 0.8540723919868469, 0.837104082107544, 0.8529411554336548, 0.8190045356750488, 0.8619909286499023, 0.8699095249176025, 0.8755655884742737, 0.8710407018661499, 0.872171938419342, 0.8733031749725342, 0.8744344115257263, 0.8699095249176025, 0.8744344115257263, 0.8710407018661499, 0.8699095249176025, 0.8710407018661499, 0.8755655884742737, 0.8755655884742737, 0.872171938419342, 0.872171938419342, 0.8744344115257263, 0.8733031749725342, 0.8710407018661499, 0.8710407018661499, 0.8733031749725342, 0.8687782883644104, 0.8733031749725342, 0.872171938419342, 0.8733031749725342, 0.8710407018661499, 0.8710407018661499, 0.8710407018661499, 0.8687782883644104, 0.8710407018661499, 0.872171938419342]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - ETA: 0s - loss: 0.3739 - accuracy: 0.8525"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 8s 55ms/step - loss: 0.3739 - accuracy: 0.8525 - val_loss: 0.7237 - val_accuracy: 0.4876\n","Epoch 2/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.3115 - accuracy: 0.8860 - val_loss: 0.7176 - val_accuracy: 0.6374\n","Epoch 3/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.2633 - accuracy: 0.9057 - val_loss: 0.7082 - val_accuracy: 0.8192\n","Epoch 4/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.2262 - accuracy: 0.9300 - val_loss: 0.6956 - val_accuracy: 0.8461\n","Epoch 5/100\n","31/31 [==============================] - 3s 98ms/step - loss: 0.2161 - accuracy: 0.9289 - val_loss: 0.6830 - val_accuracy: 0.8533\n","Epoch 6/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2104 - accuracy: 0.9341 - val_loss: 0.6697 - val_accuracy: 0.8275\n","Epoch 7/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2036 - accuracy: 0.9354 - val_loss: 0.6583 - val_accuracy: 0.8326\n","Epoch 8/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.1604 - accuracy: 0.9597 - val_loss: 0.6305 - val_accuracy: 0.8543\n","Epoch 9/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2145 - accuracy: 0.9307 - val_loss: 0.6192 - val_accuracy: 0.8244\n","Epoch 10/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.2160 - accuracy: 0.9256 - val_loss: 0.6020 - val_accuracy: 0.8605\n","Epoch 11/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.1379 - accuracy: 0.9667 - val_loss: 0.5369 - val_accuracy: 0.8647\n","Epoch 12/100\n","31/31 [==============================] - 1s 30ms/step - loss: 0.1444 - accuracy: 0.9597 - val_loss: 0.5105 - val_accuracy: 0.8688\n","Epoch 13/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1384 - accuracy: 0.9636 - val_loss: 0.4722 - val_accuracy: 0.8636\n","Epoch 14/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1111 - accuracy: 0.9742 - val_loss: 0.4549 - val_accuracy: 0.8399\n","Epoch 15/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1178 - accuracy: 0.9721 - val_loss: 0.4569 - val_accuracy: 0.7955\n","Epoch 16/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1161 - accuracy: 0.9724 - val_loss: 0.4141 - val_accuracy: 0.8357\n","Epoch 17/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1148 - accuracy: 0.9726 - val_loss: 0.3635 - val_accuracy: 0.8647\n","Epoch 18/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1252 - accuracy: 0.9677 - val_loss: 0.4475 - val_accuracy: 0.8171\n","Epoch 19/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.1152 - accuracy: 0.9721 - val_loss: 0.3574 - val_accuracy: 0.8760\n","Epoch 20/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0940 - accuracy: 0.9801 - val_loss: 0.3964 - val_accuracy: 0.8554\n","Epoch 21/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9747 - val_loss: 0.3956 - val_accuracy: 0.8719\n","Epoch 22/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0853 - accuracy: 0.9855 - val_loss: 0.4229 - val_accuracy: 0.8688\n","Epoch 23/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0778 - accuracy: 0.9873 - val_loss: 0.5480 - val_accuracy: 0.8275\n","Epoch 24/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0874 - accuracy: 0.9822 - val_loss: 0.4827 - val_accuracy: 0.8667\n","Epoch 25/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0825 - accuracy: 0.9848 - val_loss: 0.5154 - val_accuracy: 0.8709\n","Epoch 26/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0901 - accuracy: 0.9793 - val_loss: 0.5023 - val_accuracy: 0.8605\n","Epoch 27/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0952 - accuracy: 0.9811 - val_loss: 0.5828 - val_accuracy: 0.8667\n","Epoch 28/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0993 - accuracy: 0.9783 - val_loss: 0.6416 - val_accuracy: 0.8533\n","Epoch 29/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0741 - accuracy: 0.9858 - val_loss: 0.5448 - val_accuracy: 0.8750\n","Epoch 30/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0634 - accuracy: 0.9910 - val_loss: 0.6728 - val_accuracy: 0.8357\n","Epoch 31/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0625 - accuracy: 0.9920 - val_loss: 0.6486 - val_accuracy: 0.8564\n","Epoch 32/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0835 - accuracy: 0.9835 - val_loss: 0.7007 - val_accuracy: 0.8378\n","Epoch 33/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0910 - accuracy: 0.9806 - val_loss: 0.7050 - val_accuracy: 0.8388\n","Epoch 34/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0718 - accuracy: 0.9876 - val_loss: 0.5646 - val_accuracy: 0.8605\n","Epoch 35/100\n","31/31 [==============================] - 1s 31ms/step - loss: 0.0628 - accuracy: 0.9912 - val_loss: 0.6511 - val_accuracy: 0.8781\n","Epoch 36/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0941 - accuracy: 0.9788 - val_loss: 0.6728 - val_accuracy: 0.8595\n","Epoch 37/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0767 - accuracy: 0.9858 - val_loss: 0.6807 - val_accuracy: 0.8678\n","Epoch 38/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0856 - accuracy: 0.9835 - val_loss: 0.5464 - val_accuracy: 0.8698\n","Epoch 39/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0620 - accuracy: 0.9907 - val_loss: 0.5937 - val_accuracy: 0.8750\n","Epoch 40/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0515 - accuracy: 0.9938 - val_loss: 0.8438 - val_accuracy: 0.8523\n","Epoch 41/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0513 - accuracy: 0.9948 - val_loss: 0.7243 - val_accuracy: 0.8667\n","Epoch 42/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0649 - accuracy: 0.9891 - val_loss: 0.6227 - val_accuracy: 0.8605\n","Epoch 43/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0696 - accuracy: 0.9863 - val_loss: 0.6395 - val_accuracy: 0.8678\n","Epoch 44/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0592 - accuracy: 0.9907 - val_loss: 0.6345 - val_accuracy: 0.8564\n","Epoch 45/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0563 - accuracy: 0.9920 - val_loss: 0.6649 - val_accuracy: 0.8709\n","Epoch 46/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0563 - accuracy: 0.9912 - val_loss: 0.7598 - val_accuracy: 0.8450\n","Epoch 47/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1373 - accuracy: 0.9574 - val_loss: 0.7351 - val_accuracy: 0.8120\n","Epoch 48/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0808 - accuracy: 0.9817 - val_loss: 0.5653 - val_accuracy: 0.8750\n","Epoch 49/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0635 - accuracy: 0.9891 - val_loss: 0.5998 - val_accuracy: 0.8709\n","Epoch 50/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0538 - accuracy: 0.9922 - val_loss: 0.7578 - val_accuracy: 0.8605\n","Epoch 51/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0520 - accuracy: 0.9938 - val_loss: 0.7113 - val_accuracy: 0.8409\n","Epoch 52/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0445 - accuracy: 0.9966 - val_loss: 0.8326 - val_accuracy: 0.8450\n","Epoch 53/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0445 - accuracy: 0.9956 - val_loss: 0.7421 - val_accuracy: 0.8492\n","Epoch 54/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0380 - accuracy: 0.9984 - val_loss: 0.6998 - val_accuracy: 0.8688\n","Epoch 55/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0402 - accuracy: 0.9974 - val_loss: 0.8273 - val_accuracy: 0.8585\n","Epoch 56/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.0381 - accuracy: 0.9982 - val_loss: 0.8512 - val_accuracy: 0.8368\n","Epoch 57/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0374 - accuracy: 0.9982 - val_loss: 0.7324 - val_accuracy: 0.8657\n","Epoch 58/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0349 - accuracy: 0.9992 - val_loss: 0.7372 - val_accuracy: 0.8667\n","Epoch 59/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0349 - accuracy: 0.9987 - val_loss: 0.7887 - val_accuracy: 0.8647\n","Epoch 60/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0353 - accuracy: 0.9990 - val_loss: 0.7393 - val_accuracy: 0.8647\n","Epoch 61/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0403 - accuracy: 0.9959 - val_loss: 0.7987 - val_accuracy: 0.8574\n","Epoch 62/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0499 - accuracy: 0.9928 - val_loss: 0.7338 - val_accuracy: 0.8636\n","Epoch 63/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2192 - accuracy: 0.9331 - val_loss: 0.4128 - val_accuracy: 0.8512\n","Epoch 64/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0994 - accuracy: 0.9780 - val_loss: 0.4952 - val_accuracy: 0.8616\n","Epoch 65/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0509 - accuracy: 0.9922 - val_loss: 0.6183 - val_accuracy: 0.8688\n","Epoch 66/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0365 - accuracy: 0.9979 - val_loss: 0.6142 - val_accuracy: 0.8709\n","Epoch 67/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0364 - accuracy: 0.9974 - val_loss: 0.6366 - val_accuracy: 0.8750\n","Epoch 68/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0318 - accuracy: 0.9995 - val_loss: 0.6842 - val_accuracy: 0.8709\n","Epoch 69/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.6841 - val_accuracy: 0.8740\n","Epoch 70/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0289 - accuracy: 0.9997 - val_loss: 0.6907 - val_accuracy: 0.8740\n","Epoch 71/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.7036 - val_accuracy: 0.8760\n","Epoch 72/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.6895 - val_accuracy: 0.8740\n","Epoch 73/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0282 - accuracy: 0.9997 - val_loss: 0.7114 - val_accuracy: 0.8729\n","Epoch 74/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0296 - accuracy: 0.9987 - val_loss: 0.6904 - val_accuracy: 0.8605\n","Epoch 75/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0310 - accuracy: 0.9984 - val_loss: 0.7791 - val_accuracy: 0.8678\n","Epoch 76/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0301 - accuracy: 0.9987 - val_loss: 0.7544 - val_accuracy: 0.8481\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.7801 - val_accuracy: 0.8750\n","Epoch 78/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.7131 - val_accuracy: 0.8750\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.7267 - val_accuracy: 0.8781\n","Epoch 80/100\n","31/31 [==============================] - 1s 37ms/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.6958 - val_accuracy: 0.8791\n","Epoch 81/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.6963 - val_accuracy: 0.8802\n","Epoch 82/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.7008 - val_accuracy: 0.8781\n","Epoch 83/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.7006 - val_accuracy: 0.8812\n","Epoch 84/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.7032 - val_accuracy: 0.8781\n","Epoch 85/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.7002 - val_accuracy: 0.8771\n","Epoch 86/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.6964 - val_accuracy: 0.8771\n","Epoch 87/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.6883 - val_accuracy: 0.8812\n","Epoch 88/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.6745 - val_accuracy: 0.8802\n","Epoch 89/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.6712 - val_accuracy: 0.8771\n","Epoch 90/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.6724 - val_accuracy: 0.8760\n","Epoch 91/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 0.8750\n","Epoch 92/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.6754 - val_accuracy: 0.8833\n","Epoch 93/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.6583 - val_accuracy: 0.8791\n","Epoch 94/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.6639 - val_accuracy: 0.8667\n","Epoch 95/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.7341 - val_accuracy: 0.8678\n","Epoch 96/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.6633 - val_accuracy: 0.8740\n","Epoch 97/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.6722 - val_accuracy: 0.8771\n","Epoch 98/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.6724 - val_accuracy: 0.8729\n","Epoch 99/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.6686 - val_accuracy: 0.8750\n","Epoch 100/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.6897 - val_accuracy: 0.8750\n","{'loss': [0.3738901913166046, 0.3114911913871765, 0.2632828950881958, 0.22621199488639832, 0.21605734527111053, 0.21038006246089935, 0.20357678830623627, 0.16044990718364716, 0.21449869871139526, 0.21604837477207184, 0.13793152570724487, 0.14442631602287292, 0.13839000463485718, 0.11105477809906006, 0.11784864962100983, 0.11607146263122559, 0.11479298770427704, 0.12524624168872833, 0.11517117917537689, 0.0939636081457138, 0.11060192435979843, 0.08531568199396133, 0.07777053862810135, 0.0873744934797287, 0.0825040191411972, 0.09010729193687439, 0.09520121663808823, 0.09933292120695114, 0.07412917912006378, 0.06344607472419739, 0.06247426196932793, 0.0834897980093956, 0.09100901335477829, 0.07176307588815689, 0.062797911465168, 0.09408605843782425, 0.07667103409767151, 0.08558963984251022, 0.06199897080659866, 0.05146252363920212, 0.05125632509589195, 0.0649171993136406, 0.0695866122841835, 0.059213846921920776, 0.056285060942173004, 0.0563482902944088, 0.13730382919311523, 0.08075481653213501, 0.06351051479578018, 0.05380227789282799, 0.05196915566921234, 0.04452989995479584, 0.044480226933956146, 0.03795744478702545, 0.0402170866727829, 0.03806724771857262, 0.03744269534945488, 0.03486732020974159, 0.03486407920718193, 0.03528885543346405, 0.040297895669937134, 0.04986754432320595, 0.2192382514476776, 0.09942591935396194, 0.050873879343271255, 0.036510154604911804, 0.03638175129890442, 0.0318465493619442, 0.029320575296878815, 0.028909124433994293, 0.02870231680572033, 0.028021302074193954, 0.02824082225561142, 0.029613399878144264, 0.031029794365167618, 0.030143508687615395, 0.027805270627141, 0.027080366387963295, 0.026152851060032845, 0.026219012215733528, 0.025505026802420616, 0.02516268938779831, 0.024905450642108917, 0.024685140699148178, 0.02442796155810356, 0.02426944114267826, 0.023899473249912262, 0.023670392110943794, 0.023393502458930016, 0.023149803280830383, 0.022936955094337463, 0.02272193506360054, 0.022494155913591385, 0.02239774912595749, 0.022382192313671112, 0.02207649126648903, 0.021859770640730858, 0.021583912894129753, 0.021216025575995445, 0.020987993106245995], 'accuracy': [0.8524547815322876, 0.8860465288162231, 0.905684769153595, 0.9299741387367249, 0.9289405941963196, 0.934108555316925, 0.9354005455970764, 0.9596899151802063, 0.9307493567466736, 0.9255813956260681, 0.9666666388511658, 0.9596899151802063, 0.9635658860206604, 0.9741601943969727, 0.9720930457115173, 0.9723514318466187, 0.97260981798172, 0.9677002429962158, 0.9720930457115173, 0.9801033735275269, 0.9746770262718201, 0.9855297207832336, 0.9873384833335876, 0.9821705222129822, 0.9847545027732849, 0.9793281555175781, 0.9811369776725769, 0.9782945513725281, 0.985788106918335, 0.9909560680389404, 0.9919896721839905, 0.9834625124931335, 0.9806201457977295, 0.987596869468689, 0.9912144541740417, 0.9788113832473755, 0.985788106918335, 0.9834625124931335, 0.9906976819038391, 0.9937984347343445, 0.9948320388793945, 0.9891473054885864, 0.9863049387931824, 0.9906976819038391, 0.9919896721839905, 0.9912144541740417, 0.9573643207550049, 0.9816537499427795, 0.9891473054885864, 0.9922480583190918, 0.9937984347343445, 0.9966408014297485, 0.9956072568893433, 0.9984496235847473, 0.9974160194396973, 0.998191237449646, 0.998191237449646, 0.9992247819900513, 0.9987080097198486, 0.99896639585495, 0.9958656430244446, 0.9927648305892944, 0.933074951171875, 0.9780361652374268, 0.9922480583190918, 0.9979327917098999, 0.9974160194396973, 0.9994832277297974, 1.0, 0.9997416138648987, 1.0, 1.0, 0.9997416138648987, 0.9987080097198486, 0.9984496235847473, 0.9987080097198486, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.7236754894256592, 0.7175546884536743, 0.7081806063652039, 0.6955992579460144, 0.6830053925514221, 0.6697192192077637, 0.6583212614059448, 0.630545973777771, 0.6191654205322266, 0.6019607782363892, 0.5368558168411255, 0.5104653239250183, 0.47221651673316956, 0.45487648248672485, 0.4568845331668854, 0.41412606835365295, 0.3634808361530304, 0.4474796652793884, 0.35737335681915283, 0.39636844396591187, 0.39559489488601685, 0.42285415530204773, 0.5479879975318909, 0.48270702362060547, 0.5154018402099609, 0.5023156404495239, 0.5828188061714172, 0.6415547728538513, 0.5447606444358826, 0.6728301048278809, 0.6485694646835327, 0.7007463574409485, 0.704983651638031, 0.5646491646766663, 0.65110844373703, 0.6727803349494934, 0.6807182431221008, 0.5464158058166504, 0.5937340259552002, 0.8438282608985901, 0.7243204712867737, 0.6227319240570068, 0.6394521594047546, 0.6345142722129822, 0.6649489402770996, 0.7598010897636414, 0.7350713610649109, 0.5652790665626526, 0.5998493432998657, 0.75782310962677, 0.7112846970558167, 0.8325583338737488, 0.7421036958694458, 0.6997519135475159, 0.8273251056671143, 0.8512190580368042, 0.7324426174163818, 0.7372385859489441, 0.7887460589408875, 0.7392854690551758, 0.798742413520813, 0.7338404655456543, 0.4128308892250061, 0.4951828122138977, 0.6182523965835571, 0.6142190098762512, 0.636591911315918, 0.6841931939125061, 0.6840554475784302, 0.6907345056533813, 0.7036327719688416, 0.6894969940185547, 0.7113524675369263, 0.6903809905052185, 0.7791411876678467, 0.7544198632240295, 0.7800965309143066, 0.7131365537643433, 0.7266924977302551, 0.6957767605781555, 0.696263313293457, 0.700758695602417, 0.7005621790885925, 0.7031559348106384, 0.7001839876174927, 0.6964219808578491, 0.688296377658844, 0.6745197176933289, 0.6711762547492981, 0.6723704934120178, 0.6613320112228394, 0.6753832697868347, 0.6583126187324524, 0.6638895869255066, 0.7341189384460449, 0.6632566452026367, 0.672171413898468, 0.6723588109016418, 0.6686108708381653, 0.6897148489952087], 'val_accuracy': [0.4876033067703247, 0.6373966932296753, 0.8192148804664612, 0.8460744023323059, 0.8533057570457458, 0.827479362487793, 0.8326446413993835, 0.8543388247489929, 0.8243801593780518, 0.8605371713638306, 0.8646694421768188, 0.8688016533851624, 0.8636363744735718, 0.8398760557174683, 0.7954545617103577, 0.83574378490448, 0.8646694421768188, 0.817148745059967, 0.8760330677032471, 0.85537189245224, 0.8719007968902588, 0.8688016533851624, 0.827479362487793, 0.8667355179786682, 0.8708677887916565, 0.8605371713638306, 0.8667355179786682, 0.8533057570457458, 0.875, 0.83574378490448, 0.8564049601554871, 0.8378099203109741, 0.8388429880142212, 0.8605371713638306, 0.8780992031097412, 0.8595041036605835, 0.8677685856819153, 0.8698347210884094, 0.875, 0.8522727489471436, 0.8667355179786682, 0.8605371713638306, 0.8677685856819153, 0.8564049601554871, 0.8708677887916565, 0.8450413346290588, 0.8119834661483765, 0.875, 0.8708677887916565, 0.8605371713638306, 0.8409090638160706, 0.8450413346290588, 0.8491735458374023, 0.8688016533851624, 0.8584710955619812, 0.836776852607727, 0.8657024502754211, 0.8667355179786682, 0.8646694421768188, 0.8646694421768188, 0.8574380278587341, 0.8636363744735718, 0.8512396812438965, 0.8615702390670776, 0.8688016533851624, 0.8708677887916565, 0.875, 0.8708677887916565, 0.8739669322967529, 0.8739669322967529, 0.8760330677032471, 0.8739669322967529, 0.8729338645935059, 0.8605371713638306, 0.8677685856819153, 0.8481404781341553, 0.875, 0.875, 0.8780992031097412, 0.8791322112083435, 0.8801652789115906, 0.8780992031097412, 0.8811983466148376, 0.8780992031097412, 0.8770661354064941, 0.8770661354064941, 0.8811983466148376, 0.8801652789115906, 0.8770661354064941, 0.8760330677032471, 0.875, 0.8832644820213318, 0.8791322112083435, 0.8667355179786682, 0.8677685856819153, 0.8739669322967529, 0.8770661354064941, 0.8729338645935059, 0.875, 0.875]}\n","32/32 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","26/29 [=========================>....] - ETA: 0s - loss: 0.1947 - accuracy: 0.9501"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 9s 103ms/step - loss: 0.1821 - accuracy: 0.9539 - val_loss: 0.6876 - val_accuracy: 0.8147\n","Epoch 2/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.0645 - accuracy: 0.9860 - val_loss: 0.6789 - val_accuracy: 0.8394\n","Epoch 3/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0457 - accuracy: 0.9930 - val_loss: 0.6687 - val_accuracy: 0.8341\n","Epoch 4/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.0364 - accuracy: 0.9965 - val_loss: 0.6466 - val_accuracy: 0.8653\n","Epoch 5/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0405 - accuracy: 0.9927 - val_loss: 0.6366 - val_accuracy: 0.8125\n","Epoch 6/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0478 - accuracy: 0.9903 - val_loss: 0.6155 - val_accuracy: 0.8427\n","Epoch 7/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0457 - accuracy: 0.9919 - val_loss: 0.5893 - val_accuracy: 0.8200\n","Epoch 8/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0457 - accuracy: 0.9916 - val_loss: 0.5528 - val_accuracy: 0.8718\n","Epoch 9/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0464 - accuracy: 0.9908 - val_loss: 0.5371 - val_accuracy: 0.8890\n","Epoch 10/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0626 - accuracy: 0.9855 - val_loss: 0.4940 - val_accuracy: 0.8976\n","Epoch 11/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0606 - accuracy: 0.9852 - val_loss: 0.4829 - val_accuracy: 0.8739\n","Epoch 12/100\n","29/29 [==============================] - 2s 59ms/step - loss: 0.0364 - accuracy: 0.9952 - val_loss: 0.4144 - val_accuracy: 0.9127\n","Epoch 13/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.0402 - accuracy: 0.9946 - val_loss: 0.3678 - val_accuracy: 0.9149\n","Epoch 14/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0550 - accuracy: 0.9857 - val_loss: 0.3771 - val_accuracy: 0.8513\n","Epoch 15/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0558 - accuracy: 0.9863 - val_loss: 0.3465 - val_accuracy: 0.8890\n","Epoch 16/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0295 - accuracy: 0.9984 - val_loss: 0.2948 - val_accuracy: 0.8987\n","Epoch 17/100\n","29/29 [==============================] - 1s 30ms/step - loss: 0.0252 - accuracy: 0.9995 - val_loss: 0.2432 - val_accuracy: 0.9203\n","Epoch 18/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 0.2203 - val_accuracy: 0.9278\n","Epoch 19/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9256\n","Epoch 20/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.2204 - val_accuracy: 0.9267\n","Epoch 21/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.2195 - val_accuracy: 0.9375\n","Epoch 22/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9429\n","Epoch 23/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.2302 - val_accuracy: 0.9407\n","Epoch 24/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.2349 - val_accuracy: 0.9429\n","Epoch 25/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.2454 - val_accuracy: 0.9397\n","Epoch 26/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.2513 - val_accuracy: 0.9450\n","Epoch 27/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9397\n","Epoch 28/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.2599 - val_accuracy: 0.9429\n","Epoch 29/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.2636 - val_accuracy: 0.9397\n","Epoch 30/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.2713 - val_accuracy: 0.9429\n","Epoch 31/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.2643 - val_accuracy: 0.9450\n","Epoch 32/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.2650 - val_accuracy: 0.9397\n","Epoch 33/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9397\n","Epoch 34/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 0.9386\n","Epoch 35/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9418\n","Epoch 36/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.3174 - val_accuracy: 0.9386\n","Epoch 37/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.2877 - val_accuracy: 0.9397\n","Epoch 38/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.2870 - val_accuracy: 0.9407\n","Epoch 39/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.2622 - val_accuracy: 0.9375\n","Epoch 40/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.2629 - val_accuracy: 0.9397\n","Epoch 41/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.2630 - val_accuracy: 0.9429\n","Epoch 42/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.2645 - val_accuracy: 0.9440\n","Epoch 43/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.2690 - val_accuracy: 0.9450\n","Epoch 44/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.2622 - val_accuracy: 0.9397\n","Epoch 45/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.2614 - val_accuracy: 0.9407\n","Epoch 46/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 0.9397\n","Epoch 47/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.2528 - val_accuracy: 0.9364\n","Epoch 48/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.2538 - val_accuracy: 0.9407\n","Epoch 49/100\n","29/29 [==============================] - 1s 42ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.2645 - val_accuracy: 0.9461\n","Epoch 50/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.2642 - val_accuracy: 0.9418\n","Epoch 51/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.2717 - val_accuracy: 0.9364\n","Epoch 52/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.2634 - val_accuracy: 0.9375\n","Epoch 53/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.2610 - val_accuracy: 0.9386\n","Epoch 54/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.2654 - val_accuracy: 0.9397\n","Epoch 55/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.2588 - val_accuracy: 0.9397\n","Epoch 56/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.9397\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9386\n","Epoch 58/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.2604 - val_accuracy: 0.9407\n","Epoch 59/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.2631 - val_accuracy: 0.9418\n","Epoch 60/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.2697 - val_accuracy: 0.9224\n","Epoch 61/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.2827 - val_accuracy: 0.9343\n","Epoch 62/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.2855 - val_accuracy: 0.9440\n","Epoch 63/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.2831 - val_accuracy: 0.9310\n","Epoch 64/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.2897 - val_accuracy: 0.9300\n","Epoch 65/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.2744 - val_accuracy: 0.9321\n","Epoch 66/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.3587 - val_accuracy: 0.9321\n","Epoch 67/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.2732 - val_accuracy: 0.9397\n","Epoch 68/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.2841 - val_accuracy: 0.9375\n","Epoch 69/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.2828 - val_accuracy: 0.9343\n","Epoch 70/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.2881 - val_accuracy: 0.9332\n","Epoch 71/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.2801 - val_accuracy: 0.9343\n","Epoch 72/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.2792 - val_accuracy: 0.9375\n","Epoch 73/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.2825 - val_accuracy: 0.9386\n","Epoch 74/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.2769 - val_accuracy: 0.9321\n","Epoch 75/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.2741 - val_accuracy: 0.9386\n","Epoch 76/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.2729 - val_accuracy: 0.9429\n","Epoch 77/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.2642 - val_accuracy: 0.9364\n","Epoch 78/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.2683 - val_accuracy: 0.9418\n","Epoch 79/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 0.9407\n","Epoch 80/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.2803 - val_accuracy: 0.9397\n","Epoch 81/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.2725 - val_accuracy: 0.9332\n","Epoch 82/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.2819 - val_accuracy: 0.9300\n","Epoch 83/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 0.9300\n","Epoch 84/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.2831 - val_accuracy: 0.9353\n","Epoch 85/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2768 - val_accuracy: 0.9353\n","Epoch 86/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.2867 - val_accuracy: 0.9375\n","Epoch 87/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9289\n","Epoch 88/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.2943 - val_accuracy: 0.9332\n","Epoch 89/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.2872 - val_accuracy: 0.9321\n","Epoch 90/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.2717 - val_accuracy: 0.9321\n","Epoch 91/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.2711 - val_accuracy: 0.9289\n","Epoch 92/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.2855 - val_accuracy: 0.9375\n","Epoch 93/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.2832 - val_accuracy: 0.9332\n","Epoch 94/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9364\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.2704 - val_accuracy: 0.9332\n","Epoch 96/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.2845 - val_accuracy: 0.9353\n","Epoch 97/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.9353\n","Epoch 98/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.2770 - val_accuracy: 0.9343\n","Epoch 99/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9353\n","Epoch 100/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2804 - val_accuracy: 0.9332\n","{'loss': [0.18210028111934662, 0.06448297947645187, 0.04566819593310356, 0.03635264188051224, 0.04052402824163437, 0.04776468127965927, 0.04572392627596855, 0.045730676501989365, 0.04640151187777519, 0.06262673437595367, 0.06062610447406769, 0.036377809941768646, 0.04017547518014908, 0.05504870414733887, 0.05583498626947403, 0.029505355283617973, 0.025164593011140823, 0.022867528721690178, 0.021713687106966972, 0.02130809798836708, 0.021047892048954964, 0.020817773416638374, 0.02063693292438984, 0.020467471331357956, 0.02031378261744976, 0.02030850574374199, 0.0201162900775671, 0.019840557128190994, 0.01975042186677456, 0.01938595063984394, 0.0192633755505085, 0.019109752029180527, 0.018926845863461494, 0.018822569400072098, 0.018628548830747604, 0.01859852485358715, 0.01852133497595787, 0.018247228115797043, 0.017962459474802017, 0.017722783610224724, 0.017570210620760918, 0.017455214634537697, 0.01726234331727028, 0.01705344393849373, 0.01692832261323929, 0.016766872256994247, 0.016662156209349632, 0.016463231295347214, 0.016324086114764214, 0.016102520748972893, 0.01601639948785305, 0.015809882432222366, 0.015636639669537544, 0.015562493354082108, 0.015401975251734257, 0.015332821756601334, 0.01517725270241499, 0.014949781820178032, 0.014813710935413837, 0.01476647611707449, 0.01491603534668684, 0.014707730151712894, 0.014535965397953987, 0.014271875843405724, 0.01406029425561428, 0.014062216505408287, 0.014180796220898628, 0.013721439056098461, 0.013563087210059166, 0.013430946506559849, 0.01327638141810894, 0.01313288975507021, 0.012971245683729649, 0.012854131869971752, 0.012709366157650948, 0.012575319036841393, 0.012481770478188992, 0.012347453273832798, 0.01228050421923399, 0.012121119536459446, 0.012024062685668468, 0.011890606954693794, 0.011765428818762302, 0.011692495085299015, 0.011564978398382664, 0.011461865156888962, 0.011436345055699348, 0.011351248249411583, 0.011163836345076561, 0.011058490723371506, 0.010990909300744534, 0.010856174863874912, 0.010750669054687023, 0.010650350712239742, 0.010568587109446526, 0.010495268739759922, 0.01037520170211792, 0.01029415987432003, 0.010224311612546444, 0.010117834433913231], 'accuracy': [0.9539331793785095, 0.985991358757019, 0.9929956793785095, 0.9964978694915771, 0.9927262663841248, 0.9903017282485962, 0.9919180870056152, 0.9916487336158752, 0.990840494632721, 0.9854525923728943, 0.9851831793785095, 0.9951508641242981, 0.9946120977401733, 0.985722005367279, 0.9862607717514038, 0.998383641242981, 0.9994612336158752, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.6875571012496948, 0.6789430975914001, 0.6687249541282654, 0.6465665102005005, 0.6365552544593811, 0.6154740452766418, 0.5892518162727356, 0.5527963042259216, 0.5370835065841675, 0.49395790696144104, 0.4828673303127289, 0.4144032597541809, 0.36775290966033936, 0.37710970640182495, 0.34647873044013977, 0.29480788111686707, 0.2431737631559372, 0.2203260064125061, 0.2238556146621704, 0.22043199837207794, 0.21952253580093384, 0.22389966249465942, 0.23021408915519714, 0.23488403856754303, 0.24542568624019623, 0.25128108263015747, 0.2567460834980011, 0.25986525416374207, 0.26362428069114685, 0.2712912857532501, 0.2642790377140045, 0.2650453448295593, 0.2682822346687317, 0.2976367473602295, 0.27523377537727356, 0.317407488822937, 0.28771907091140747, 0.28702229261398315, 0.2621513307094574, 0.2629004120826721, 0.2630322575569153, 0.26447731256484985, 0.2690219581127167, 0.262210875749588, 0.26136207580566406, 0.26349693536758423, 0.25276798009872437, 0.2538371682167053, 0.26448771357536316, 0.2642488479614258, 0.27169668674468994, 0.26344794034957886, 0.2609667479991913, 0.2653667628765106, 0.25884801149368286, 0.2776762545108795, 0.25665920972824097, 0.2603757977485657, 0.2631107568740845, 0.2697201371192932, 0.2827128767967224, 0.2855043113231659, 0.28314948081970215, 0.2896568775177002, 0.2744480073451996, 0.3586861491203308, 0.27322888374328613, 0.28406110405921936, 0.2828495502471924, 0.28810250759124756, 0.2800903022289276, 0.2792239487171173, 0.2824878394603729, 0.27688536047935486, 0.27405846118927, 0.27290865778923035, 0.2642354369163513, 0.26832419633865356, 0.26754748821258545, 0.2802979350090027, 0.27248117327690125, 0.28187295794487, 0.2709104120731354, 0.2831074297428131, 0.27682387828826904, 0.28673821687698364, 0.2830039858818054, 0.29433882236480713, 0.28716564178466797, 0.27166804671287537, 0.27107882499694824, 0.28545188903808594, 0.28324705362319946, 0.28303632140159607, 0.2703745365142822, 0.2845108211040497, 0.27772781252861023, 0.2770008444786072, 0.2780892252922058, 0.28042933344841003], 'val_accuracy': [0.8146551847457886, 0.8394396305084229, 0.8340517282485962, 0.8653017282485962, 0.8125, 0.8426724076271057, 0.8200430870056152, 0.8717672228813171, 0.889008641242981, 0.8976293206214905, 0.8739224076271057, 0.912715494632721, 0.9148706793785095, 0.8512930870056152, 0.889008641242981, 0.8987069129943848, 0.920258641242981, 0.9278017282485962, 0.9256465435028076, 0.9267241358757019, 0.9375, 0.9428879022598267, 0.9407327771186829, 0.9428879022598267, 0.9396551847457886, 0.9450430870056152, 0.9396551847457886, 0.9428879022598267, 0.9396551847457886, 0.9428879022598267, 0.9450430870056152, 0.9396551847457886, 0.9396551847457886, 0.9385775923728943, 0.9418103694915771, 0.9385775923728943, 0.9396551847457886, 0.9407327771186829, 0.9375, 0.9396551847457886, 0.9428879022598267, 0.943965494632721, 0.9450430870056152, 0.9396551847457886, 0.9407327771186829, 0.9396551847457886, 0.9364224076271057, 0.9407327771186829, 0.9461206793785095, 0.9418103694915771, 0.9364224076271057, 0.9375, 0.9385775923728943, 0.9396551847457886, 0.9396551847457886, 0.9396551847457886, 0.9385775923728943, 0.9407327771186829, 0.9418103694915771, 0.9224137663841248, 0.9342672228813171, 0.943965494632721, 0.931034505367279, 0.9299569129943848, 0.9321120977401733, 0.9321120977401733, 0.9396551847457886, 0.9375, 0.9342672228813171, 0.9331896305084229, 0.9342672228813171, 0.9375, 0.9385775923728943, 0.9321120977401733, 0.9385775923728943, 0.9428879022598267, 0.9364224076271057, 0.9418103694915771, 0.9407327771186829, 0.9396551847457886, 0.9331896305084229, 0.9299569129943848, 0.9299569129943848, 0.9353448152542114, 0.9353448152542114, 0.9375, 0.9288793206214905, 0.9331896305084229, 0.9321120977401733, 0.9321120977401733, 0.9288793206214905, 0.9375, 0.9331896305084229, 0.9364224076271057, 0.9331896305084229, 0.9353448152542114, 0.9353448152542114, 0.9342672228813171, 0.9353448152542114, 0.9331896305084229]}\n","38/38 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","27/28 [===========================>..] - ETA: 0s - loss: 0.2184 - accuracy: 0.9433"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 9s 63ms/step - loss: 0.2163 - accuracy: 0.9437 - val_loss: 0.6882 - val_accuracy: 0.7647\n","Epoch 2/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0620 - accuracy: 0.9890 - val_loss: 0.6818 - val_accuracy: 0.7998\n","Epoch 3/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0472 - accuracy: 0.9909 - val_loss: 0.6677 - val_accuracy: 0.8156\n","Epoch 4/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0357 - accuracy: 0.9975 - val_loss: 0.6548 - val_accuracy: 0.8247\n","Epoch 5/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0319 - accuracy: 0.9983 - val_loss: 0.6350 - val_accuracy: 0.8394\n","Epoch 6/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0314 - accuracy: 0.9975 - val_loss: 0.6122 - val_accuracy: 0.8462\n","Epoch 7/100\n","28/28 [==============================] - 2s 91ms/step - loss: 0.0317 - accuracy: 0.9963 - val_loss: 0.5765 - val_accuracy: 0.8484\n","Epoch 8/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0432 - accuracy: 0.9926 - val_loss: 0.5393 - val_accuracy: 0.8405\n","Epoch 9/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0758 - accuracy: 0.9805 - val_loss: 0.5397 - val_accuracy: 0.8665\n","Epoch 10/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0630 - accuracy: 0.9833 - val_loss: 0.5187 - val_accuracy: 0.8416\n","Epoch 11/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.0754 - accuracy: 0.9791 - val_loss: 0.5027 - val_accuracy: 0.8733\n","Epoch 12/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0350 - accuracy: 0.9975 - val_loss: 0.4559 - val_accuracy: 0.8914\n","Epoch 13/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0305 - accuracy: 0.9980 - val_loss: 0.3958 - val_accuracy: 0.8756\n","Epoch 14/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0271 - accuracy: 0.9986 - val_loss: 0.3474 - val_accuracy: 0.8801\n","Epoch 15/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0243 - accuracy: 0.9997 - val_loss: 0.3041 - val_accuracy: 0.8903\n","Epoch 16/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.2741 - val_accuracy: 0.9038\n","Epoch 17/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 0.2544 - val_accuracy: 0.9072\n","Epoch 18/100\n","28/28 [==============================] - 1s 32ms/step - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 0.9084\n","Epoch 19/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.2475 - val_accuracy: 0.9140\n","Epoch 20/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.2473 - val_accuracy: 0.9231\n","Epoch 21/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.2673 - val_accuracy: 0.9208\n","Epoch 22/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0220 - accuracy: 0.9997 - val_loss: 0.2707 - val_accuracy: 0.9186\n","Epoch 23/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0234 - accuracy: 0.9997 - val_loss: 0.2787 - val_accuracy: 0.9163\n","Epoch 24/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0256 - accuracy: 0.9977 - val_loss: 0.2950 - val_accuracy: 0.9208\n","Epoch 25/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0319 - accuracy: 0.9958 - val_loss: 0.5815 - val_accuracy: 0.8620\n","Epoch 26/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0504 - accuracy: 0.9867 - val_loss: 0.6974 - val_accuracy: 0.8190\n","Epoch 27/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.1179 - accuracy: 0.9658 - val_loss: 0.4156 - val_accuracy: 0.8620\n","Epoch 28/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0579 - accuracy: 0.9867 - val_loss: 0.3033 - val_accuracy: 0.9129\n","Epoch 29/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0513 - accuracy: 0.9892 - val_loss: 0.3938 - val_accuracy: 0.8846\n","Epoch 30/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0342 - accuracy: 0.9949 - val_loss: 0.3669 - val_accuracy: 0.9005\n","Epoch 31/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0269 - accuracy: 0.9980 - val_loss: 0.3580 - val_accuracy: 0.9152\n","Epoch 32/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0402 - accuracy: 0.9924 - val_loss: 0.6540 - val_accuracy: 0.8462\n","Epoch 33/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0663 - accuracy: 0.9825 - val_loss: 0.4507 - val_accuracy: 0.8744\n","Epoch 34/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0473 - accuracy: 0.9890 - val_loss: 0.3345 - val_accuracy: 0.9084\n","Epoch 35/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0285 - accuracy: 0.9972 - val_loss: 0.3810 - val_accuracy: 0.9005\n","Epoch 36/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0221 - accuracy: 0.9997 - val_loss: 0.5254 - val_accuracy: 0.8790\n","Epoch 37/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.9061\n","Epoch 38/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.3866 - val_accuracy: 0.9197\n","Epoch 39/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.3947 - val_accuracy: 0.9163\n","Epoch 40/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.3996 - val_accuracy: 0.9197\n","Epoch 41/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.4034 - val_accuracy: 0.9174\n","Epoch 42/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.3966 - val_accuracy: 0.9197\n","Epoch 43/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9197\n","Epoch 44/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 0.9231\n","Epoch 45/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.4035 - val_accuracy: 0.9186\n","Epoch 46/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.3987 - val_accuracy: 0.9186\n","Epoch 47/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 0.9197\n","Epoch 48/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.3966 - val_accuracy: 0.9208\n","Epoch 49/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.3929 - val_accuracy: 0.9208\n","Epoch 50/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.3865 - val_accuracy: 0.9219\n","Epoch 51/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.3879 - val_accuracy: 0.9208\n","Epoch 52/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.3982 - val_accuracy: 0.9186\n","Epoch 53/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.4072 - val_accuracy: 0.9140\n","Epoch 54/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 0.3890 - val_accuracy: 0.9219\n","Epoch 55/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.3890 - val_accuracy: 0.9197\n","Epoch 56/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.3874 - val_accuracy: 0.9208\n","Epoch 57/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.3813 - val_accuracy: 0.9219\n","Epoch 58/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 0.9231\n","Epoch 59/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.3883 - val_accuracy: 0.9208\n","Epoch 60/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.3841 - val_accuracy: 0.9219\n","Epoch 61/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.3836 - val_accuracy: 0.9219\n","Epoch 62/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.3736 - val_accuracy: 0.9174\n","Epoch 63/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.3911 - val_accuracy: 0.9208\n","Epoch 64/100\n","28/28 [==============================] - 1s 42ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.3710 - val_accuracy: 0.9253\n","Epoch 65/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.3985 - val_accuracy: 0.9186\n","Epoch 66/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0180 - accuracy: 0.9992 - val_loss: 0.3848 - val_accuracy: 0.9197\n","Epoch 67/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0180 - accuracy: 0.9992 - val_loss: 0.4908 - val_accuracy: 0.9050\n","Epoch 68/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1340 - accuracy: 0.9675 - val_loss: 0.5771 - val_accuracy: 0.8450\n","Epoch 69/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.1193 - accuracy: 0.9618 - val_loss: 0.8938 - val_accuracy: 0.7738\n","Epoch 70/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.1090 - accuracy: 0.9610 - val_loss: 0.3698 - val_accuracy: 0.8801\n","Epoch 71/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0542 - accuracy: 0.9867 - val_loss: 0.3519 - val_accuracy: 0.8982\n","Epoch 72/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0276 - accuracy: 0.9966 - val_loss: 0.3589 - val_accuracy: 0.9050\n","Epoch 73/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0221 - accuracy: 0.9977 - val_loss: 0.3856 - val_accuracy: 0.9005\n","Epoch 74/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0190 - accuracy: 0.9994 - val_loss: 0.5056 - val_accuracy: 0.8993\n","Epoch 75/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0196 - accuracy: 0.9986 - val_loss: 0.4496 - val_accuracy: 0.8903\n","Epoch 76/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.0171 - accuracy: 0.9992 - val_loss: 0.4412 - val_accuracy: 0.9084\n","Epoch 77/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0201 - accuracy: 0.9975 - val_loss: 0.4232 - val_accuracy: 0.9072\n","Epoch 78/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0230 - accuracy: 0.9972 - val_loss: 0.4713 - val_accuracy: 0.8971\n","Epoch 79/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0478 - accuracy: 0.9887 - val_loss: 1.8631 - val_accuracy: 0.6663\n","Epoch 80/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0657 - accuracy: 0.9808 - val_loss: 0.5551 - val_accuracy: 0.8484\n","Epoch 81/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.0272 - accuracy: 0.9969 - val_loss: 0.4137 - val_accuracy: 0.8971\n","Epoch 82/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0181 - accuracy: 0.9994 - val_loss: 0.4975 - val_accuracy: 0.8959\n","Epoch 83/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.4849 - val_accuracy: 0.9016\n","Epoch 84/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.4858 - val_accuracy: 0.9016\n","Epoch 85/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.9061\n","Epoch 86/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.4556 - val_accuracy: 0.9027\n","Epoch 87/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.4888 - val_accuracy: 0.9038\n","Epoch 88/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.4785 - val_accuracy: 0.9084\n","Epoch 89/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.4657 - val_accuracy: 0.9072\n","Epoch 90/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.4632 - val_accuracy: 0.9061\n","Epoch 91/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.4607 - val_accuracy: 0.9072\n","Epoch 92/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.4587 - val_accuracy: 0.9106\n","Epoch 93/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.4577 - val_accuracy: 0.9084\n","Epoch 94/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.4561 - val_accuracy: 0.9072\n","Epoch 95/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.4544 - val_accuracy: 0.9061\n","Epoch 96/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.4530 - val_accuracy: 0.9050\n","Epoch 97/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.9084\n","Epoch 98/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.4481 - val_accuracy: 0.9072\n","Epoch 99/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4482 - val_accuracy: 0.9061\n","Epoch 100/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.4463 - val_accuracy: 0.9072\n","{'loss': [0.21631760895252228, 0.06195205822587013, 0.047207050025463104, 0.035720665007829666, 0.03189399838447571, 0.03144190087914467, 0.03174249827861786, 0.04319165647029877, 0.0757647231221199, 0.06298131495714188, 0.07538332045078278, 0.03499959036707878, 0.030497035011649132, 0.027074232697486877, 0.02433566004037857, 0.023036876693367958, 0.022175772115588188, 0.02176262065768242, 0.02192138135433197, 0.02151326835155487, 0.02161210961639881, 0.022046348080039024, 0.023387610912322998, 0.02555735968053341, 0.03190070763230324, 0.050367437303066254, 0.11793043464422226, 0.05791793018579483, 0.0513179674744606, 0.03416456654667854, 0.02693372592329979, 0.04020630195736885, 0.06628171354532242, 0.047291141003370285, 0.028458401560783386, 0.022117018699645996, 0.020534824579954147, 0.01999012753367424, 0.01943751983344555, 0.01923196204006672, 0.01900404877960682, 0.01885509304702282, 0.018746085464954376, 0.01860915683209896, 0.018449153751134872, 0.018259355798363686, 0.01815907470881939, 0.017944300547242165, 0.017912285402417183, 0.01770375296473503, 0.017586039379239082, 0.01745765097439289, 0.017326394096016884, 0.01721143163740635, 0.01702890731394291, 0.01688663475215435, 0.0167949628084898, 0.01671602949500084, 0.016552483662962914, 0.01641508936882019, 0.016301747411489487, 0.016530944034457207, 0.016326293349266052, 0.01614411361515522, 0.01606176234781742, 0.01800261251628399, 0.018016118556261063, 0.1340448558330536, 0.119290791451931, 0.10901409387588501, 0.05420345813035965, 0.027563655748963356, 0.02208971232175827, 0.019006922841072083, 0.019561031833291054, 0.017146755009889603, 0.020077155902981758, 0.023006102070212364, 0.04782794415950775, 0.06569445878267288, 0.02723024971783161, 0.01811414770781994, 0.01596822217106819, 0.015346292406320572, 0.015087776817381382, 0.01510864682495594, 0.015269234776496887, 0.01493724063038826, 0.014602696523070335, 0.014431350864470005, 0.014313461259007454, 0.014248422347009182, 0.01413415651768446, 0.014077012427151203, 0.013975312933325768, 0.01387579832226038, 0.01378693338483572, 0.013705039396882057, 0.013624431565403938, 0.013582867570221424], 'accuracy': [0.9436898827552795, 0.988964319229126, 0.9909451007843018, 0.9974533319473267, 0.9983022212982178, 0.9974533319473267, 0.996321439743042, 0.992642879486084, 0.9804753661155701, 0.983305037021637, 0.9790605306625366, 0.9974533319473267, 0.9980192184448242, 0.9985851645469666, 0.9997170567512512, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9997170567512512, 0.9997170567512512, 0.9977362751960754, 0.9957554936408997, 0.9867005944252014, 0.9657611846923828, 0.9867005944252014, 0.9892473220825195, 0.9949066042900085, 0.9980192184448242, 0.9923599362373352, 0.9824561476707458, 0.988964319229126, 0.9971703290939331, 0.9997170567512512, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9991511106491089, 0.9991511106491089, 0.967458963394165, 0.961799681186676, 0.9609507918357849, 0.9867005944252014, 0.9966044425964355, 0.9977362751960754, 0.9994340538978577, 0.9985851645469666, 0.9991511106491089, 0.9974533319473267, 0.9971703290939331, 0.9886813759803772, 0.9807583689689636, 0.9968873858451843, 0.9994340538978577, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.688222348690033, 0.6817635297775269, 0.6676647663116455, 0.6547591090202332, 0.6349837779998779, 0.6121805310249329, 0.5764851570129395, 0.5393315553665161, 0.5396741628646851, 0.5186851620674133, 0.5027052164077759, 0.4558624029159546, 0.39577463269233704, 0.3474496006965637, 0.30412179231643677, 0.2740727961063385, 0.2543744742870331, 0.24383427202701569, 0.24748742580413818, 0.24734488129615784, 0.26726460456848145, 0.27070000767707825, 0.2787242829799652, 0.2950231432914734, 0.5815130472183228, 0.6974341869354248, 0.41557225584983826, 0.3033072352409363, 0.39377301931381226, 0.3669133186340332, 0.3580114245414734, 0.6540278196334839, 0.4506816864013672, 0.3345150947570801, 0.38095879554748535, 0.5254215002059937, 0.41696202754974365, 0.3865935802459717, 0.3946596086025238, 0.39961960911750793, 0.4034452438354492, 0.3966045677661896, 0.3968237340450287, 0.3998144567012787, 0.40348243713378906, 0.39866194128990173, 0.39910340309143066, 0.3965725004673004, 0.3929397165775299, 0.3864701986312866, 0.38790592551231384, 0.39819496870040894, 0.4071718156337738, 0.38903677463531494, 0.38896456360816956, 0.38739609718322754, 0.38127151131629944, 0.38493239879608154, 0.38830167055130005, 0.38413864374160767, 0.3836105763912201, 0.37356889247894287, 0.3911239802837372, 0.3710162937641144, 0.39846405386924744, 0.3847762644290924, 0.4907769560813904, 0.5771222710609436, 0.8937784433364868, 0.369767963886261, 0.35185307264328003, 0.35893672704696655, 0.38555681705474854, 0.5056378841400146, 0.4496387243270874, 0.44118791818618774, 0.42318853735923767, 0.4713272452354431, 1.8630626201629639, 0.555062472820282, 0.4136769771575928, 0.4975415766239166, 0.48487597703933716, 0.48578861355781555, 0.459662526845932, 0.45555776357650757, 0.488818883895874, 0.4785037636756897, 0.46574723720550537, 0.46324512362480164, 0.46065986156463623, 0.4587380290031433, 0.45768627524375916, 0.45613157749176025, 0.45436006784439087, 0.45298856496810913, 0.45061981678009033, 0.448106586933136, 0.44817033410072327, 0.4462524652481079], 'val_accuracy': [0.7647058963775635, 0.7997737526893616, 0.8156108856201172, 0.8246606588363647, 0.8393664956092834, 0.8461538553237915, 0.848416268825531, 0.8404977321624756, 0.8665158152580261, 0.8416289687156677, 0.8733031749725342, 0.8914027214050293, 0.8755655884742737, 0.8800904750823975, 0.8902714848518372, 0.9038461446762085, 0.9072397947311401, 0.9083710312843323, 0.9140271544456482, 0.9230769276618958, 0.9208144545555115, 0.918552041053772, 0.9162895679473877, 0.9208144545555115, 0.8619909286499023, 0.8190045356750488, 0.8619909286499023, 0.912895917892456, 0.8846153616905212, 0.9004524946212769, 0.9151583909988403, 0.8461538553237915, 0.8744344115257263, 0.9083710312843323, 0.9004524946212769, 0.8789592981338501, 0.9061086177825928, 0.9196832776069641, 0.9162895679473877, 0.9196832776069641, 0.9174208045005798, 0.9196832776069641, 0.9196832776069641, 0.9230769276618958, 0.918552041053772, 0.918552041053772, 0.9196832776069641, 0.9208144545555115, 0.9208144545555115, 0.9219456911087036, 0.9208144545555115, 0.918552041053772, 0.9140271544456482, 0.9219456911087036, 0.9196832776069641, 0.9208144545555115, 0.9219456911087036, 0.9230769276618958, 0.9208144545555115, 0.9219456911087036, 0.9219456911087036, 0.9174208045005798, 0.9208144545555115, 0.9253393411636353, 0.918552041053772, 0.9196832776069641, 0.9049773812294006, 0.8450226187705994, 0.773755669593811, 0.8800904750823975, 0.8981900215148926, 0.9049773812294006, 0.9004524946212769, 0.8993212580680847, 0.8902714848518372, 0.9083710312843323, 0.9072397947311401, 0.8970588445663452, 0.6662895679473877, 0.848416268825531, 0.8970588445663452, 0.8959276080131531, 0.901583731174469, 0.901583731174469, 0.9061086177825928, 0.9027149081230164, 0.9038461446762085, 0.9083710312843323, 0.9072397947311401, 0.9061086177825928, 0.9072397947311401, 0.9106335043907166, 0.9083710312843323, 0.9072397947311401, 0.9061086177825928, 0.9049773812294006, 0.9083710312843323, 0.9072397947311401, 0.9061086177825928, 0.9072397947311401]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/31 [==========================>...] - ETA: 0s - loss: 0.1723 - accuracy: 0.9523"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 7s 62ms/step - loss: 0.1687 - accuracy: 0.9522 - val_loss: 0.6867 - val_accuracy: 0.8347\n","Epoch 2/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.0613 - accuracy: 0.9858 - val_loss: 0.6763 - val_accuracy: 0.8357\n","Epoch 3/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.0438 - accuracy: 0.9933 - val_loss: 0.6618 - val_accuracy: 0.8430\n","Epoch 4/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0356 - accuracy: 0.9972 - val_loss: 0.6422 - val_accuracy: 0.8399\n","Epoch 5/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.0438 - accuracy: 0.9917 - val_loss: 0.6144 - val_accuracy: 0.8636\n","Epoch 6/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.0428 - accuracy: 0.9915 - val_loss: 0.5833 - val_accuracy: 0.8647\n","Epoch 7/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0538 - accuracy: 0.9866 - val_loss: 0.5643 - val_accuracy: 0.8368\n","Epoch 8/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1546 - accuracy: 0.9527 - val_loss: 0.5988 - val_accuracy: 0.8399\n","Epoch 9/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0702 - accuracy: 0.9848 - val_loss: 0.5407 - val_accuracy: 0.8874\n","Epoch 10/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0452 - accuracy: 0.9920 - val_loss: 0.4859 - val_accuracy: 0.8440\n","Epoch 11/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0383 - accuracy: 0.9938 - val_loss: 0.4278 - val_accuracy: 0.8967\n","Epoch 12/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0375 - accuracy: 0.9951 - val_loss: 0.3938 - val_accuracy: 0.8833\n","Epoch 13/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0367 - accuracy: 0.9943 - val_loss: 0.3908 - val_accuracy: 0.8275\n","Epoch 14/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0536 - accuracy: 0.9889 - val_loss: 0.3080 - val_accuracy: 0.9029\n","Epoch 15/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0323 - accuracy: 0.9956 - val_loss: 0.3308 - val_accuracy: 0.8616\n","Epoch 16/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0312 - accuracy: 0.9966 - val_loss: 0.3092 - val_accuracy: 0.8781\n","Epoch 17/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0422 - accuracy: 0.9915 - val_loss: 0.2709 - val_accuracy: 0.8946\n","Epoch 18/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0592 - accuracy: 0.9853 - val_loss: 0.3435 - val_accuracy: 0.8688\n","Epoch 19/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0593 - accuracy: 0.9850 - val_loss: 0.5095 - val_accuracy: 0.7913\n","Epoch 20/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0760 - accuracy: 0.9788 - val_loss: 0.2780 - val_accuracy: 0.9019\n","Epoch 21/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0421 - accuracy: 0.9912 - val_loss: 0.2808 - val_accuracy: 0.8988\n","Epoch 22/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0274 - accuracy: 0.9982 - val_loss: 0.3778 - val_accuracy: 0.8998\n","Epoch 23/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0573 - accuracy: 0.9863 - val_loss: 0.4118 - val_accuracy: 0.8781\n","Epoch 24/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0894 - accuracy: 0.9739 - val_loss: 0.3045 - val_accuracy: 0.8905\n","Epoch 25/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.0388 - accuracy: 0.9959 - val_loss: 0.3807 - val_accuracy: 0.9070\n","Epoch 26/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0254 - accuracy: 0.9992 - val_loss: 0.5705 - val_accuracy: 0.8709\n","Epoch 27/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.4086 - val_accuracy: 0.9112\n","Epoch 28/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0228 - accuracy: 0.9990 - val_loss: 0.4117 - val_accuracy: 0.9081\n","Epoch 29/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0219 - accuracy: 0.9997 - val_loss: 0.4481 - val_accuracy: 0.9112\n","Epoch 30/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.4201 - val_accuracy: 0.9174\n","Epoch 31/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.4239 - val_accuracy: 0.9163\n","Epoch 32/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.4291 - val_accuracy: 0.9184\n","Epoch 33/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.4311 - val_accuracy: 0.9184\n","Epoch 34/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.4375 - val_accuracy: 0.9174\n","Epoch 35/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.4273 - val_accuracy: 0.9122\n","Epoch 36/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.4277 - val_accuracy: 0.9143\n","Epoch 37/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.4279 - val_accuracy: 0.9132\n","Epoch 38/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.4294 - val_accuracy: 0.9143\n","Epoch 39/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.4234 - val_accuracy: 0.9174\n","Epoch 40/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.4167 - val_accuracy: 0.9153\n","Epoch 41/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.4166 - val_accuracy: 0.9132\n","Epoch 42/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.4155 - val_accuracy: 0.9143\n","Epoch 43/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.4190 - val_accuracy: 0.9143\n","Epoch 44/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.4124 - val_accuracy: 0.9122\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.4195 - val_accuracy: 0.9174\n","Epoch 46/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.4244 - val_accuracy: 0.9163\n","Epoch 47/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.4241 - val_accuracy: 0.9184\n","Epoch 48/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.4113 - val_accuracy: 0.9112\n","Epoch 49/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.4194 - val_accuracy: 0.9153\n","Epoch 50/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.4166 - val_accuracy: 0.9153\n","Epoch 51/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.9153\n","Epoch 52/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.4088 - val_accuracy: 0.9101\n","Epoch 53/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.4078 - val_accuracy: 0.9153\n","Epoch 54/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.4129 - val_accuracy: 0.9174\n","Epoch 55/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 0.4271 - val_accuracy: 0.9163\n","Epoch 56/100\n","31/31 [==============================] - 1s 40ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.9194\n","Epoch 57/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.4156 - val_accuracy: 0.9163\n","Epoch 58/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.4119 - val_accuracy: 0.9163\n","Epoch 59/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.4088 - val_accuracy: 0.9132\n","Epoch 60/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.4042 - val_accuracy: 0.9132\n","Epoch 61/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.4059 - val_accuracy: 0.9153\n","Epoch 62/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 0.9174\n","Epoch 63/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.3992 - val_accuracy: 0.9143\n","Epoch 64/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.4047 - val_accuracy: 0.9153\n","Epoch 65/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.4031 - val_accuracy: 0.9143\n","Epoch 66/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.4056 - val_accuracy: 0.9153\n","Epoch 67/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.4071 - val_accuracy: 0.9122\n","Epoch 68/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.4237 - val_accuracy: 0.9132\n","Epoch 69/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.4043 - val_accuracy: 0.9174\n","Epoch 70/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.4096 - val_accuracy: 0.9122\n","Epoch 71/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.4207 - val_accuracy: 0.9174\n","Epoch 72/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.4507 - val_accuracy: 0.9081\n","Epoch 73/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0175 - accuracy: 0.9987 - val_loss: 0.5151 - val_accuracy: 0.8977\n","Epoch 74/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1086 - accuracy: 0.9667 - val_loss: 0.3272 - val_accuracy: 0.8833\n","Epoch 75/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.1364 - accuracy: 0.9561 - val_loss: 0.5461 - val_accuracy: 0.8399\n","Epoch 76/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.1306 - accuracy: 0.9561 - val_loss: 0.3162 - val_accuracy: 0.8833\n","Epoch 77/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.1037 - accuracy: 0.9680 - val_loss: 0.3223 - val_accuracy: 0.8967\n","Epoch 78/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0416 - accuracy: 0.9910 - val_loss: 0.4304 - val_accuracy: 0.8936\n","Epoch 79/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.0261 - accuracy: 0.9961 - val_loss: 0.4503 - val_accuracy: 0.8967\n","Epoch 80/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0314 - accuracy: 0.9938 - val_loss: 0.4894 - val_accuracy: 0.8926\n","Epoch 81/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0503 - accuracy: 0.9858 - val_loss: 1.0082 - val_accuracy: 0.7490\n","Epoch 82/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0622 - accuracy: 0.9829 - val_loss: 0.4088 - val_accuracy: 0.8843\n","Epoch 83/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0365 - accuracy: 0.9897 - val_loss: 0.5298 - val_accuracy: 0.8781\n","Epoch 84/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0372 - accuracy: 0.9904 - val_loss: 0.4721 - val_accuracy: 0.8853\n","Epoch 85/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0369 - accuracy: 0.9925 - val_loss: 0.5138 - val_accuracy: 0.8915\n","Epoch 86/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0741 - accuracy: 0.9760 - val_loss: 0.6889 - val_accuracy: 0.8450\n","Epoch 87/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0456 - accuracy: 0.9891 - val_loss: 0.4277 - val_accuracy: 0.8864\n","Epoch 88/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0200 - accuracy: 0.9984 - val_loss: 0.4618 - val_accuracy: 0.8926\n","Epoch 89/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0663 - accuracy: 0.9793 - val_loss: 0.4445 - val_accuracy: 0.8791\n","Epoch 90/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0382 - accuracy: 0.9907 - val_loss: 0.4784 - val_accuracy: 0.8843\n","Epoch 91/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0194 - accuracy: 0.9984 - val_loss: 0.4813 - val_accuracy: 0.8853\n","Epoch 92/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0171 - accuracy: 0.9990 - val_loss: 0.6214 - val_accuracy: 0.8760\n","Epoch 93/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0201 - accuracy: 0.9979 - val_loss: 0.5340 - val_accuracy: 0.8874\n","Epoch 94/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0171 - accuracy: 0.9984 - val_loss: 0.6617 - val_accuracy: 0.8760\n","Epoch 95/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0166 - accuracy: 0.9992 - val_loss: 0.5899 - val_accuracy: 0.8915\n","Epoch 96/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0143 - accuracy: 0.9997 - val_loss: 0.5488 - val_accuracy: 0.8874\n","Epoch 97/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0206 - accuracy: 0.9979 - val_loss: 0.6337 - val_accuracy: 0.8822\n","Epoch 98/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0197 - accuracy: 0.9984 - val_loss: 0.5180 - val_accuracy: 0.8915\n","Epoch 99/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0283 - accuracy: 0.9933 - val_loss: 0.6604 - val_accuracy: 0.8760\n","Epoch 100/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0863 - accuracy: 0.9765 - val_loss: 0.4486 - val_accuracy: 0.8719\n","{'loss': [0.16868956387043, 0.06133449822664261, 0.043805353343486786, 0.03556670993566513, 0.04379121586680412, 0.04278316721320152, 0.05378693342208862, 0.15458308160305023, 0.07016415148973465, 0.04520631954073906, 0.0382908396422863, 0.037508267909288406, 0.036679115146398544, 0.05357661843299866, 0.03230483829975128, 0.031198862940073013, 0.04223727434873581, 0.059190601110458374, 0.05932261422276497, 0.07597675174474716, 0.04208679124712944, 0.027444487437605858, 0.05728454887866974, 0.08936796337366104, 0.038832906633615494, 0.02537013404071331, 0.02242087386548519, 0.022845890372991562, 0.021907037124037743, 0.02076132595539093, 0.02020949125289917, 0.019974809139966965, 0.019732115790247917, 0.01953624002635479, 0.01937923952937126, 0.019186757504940033, 0.019007721915841103, 0.01884344406425953, 0.018807781860232353, 0.018558761104941368, 0.01845676265656948, 0.01826271414756775, 0.01811092346906662, 0.017985664308071136, 0.01780504733324051, 0.017667019739747047, 0.017540356144309044, 0.01742333173751831, 0.017255371436476707, 0.01714024879038334, 0.017109759151935577, 0.016963394358754158, 0.01677991822361946, 0.01660776138305664, 0.016543839126825333, 0.016280805692076683, 0.016217967495322227, 0.01601218990981579, 0.015869254246354103, 0.015783248469233513, 0.015602569095790386, 0.0154893659055233, 0.015418295748531818, 0.015270868316292763, 0.015098176896572113, 0.014987561851739883, 0.014881745912134647, 0.014698749408125877, 0.014698928222060204, 0.014474346302449703, 0.01451187301427126, 0.014844716526567936, 0.01752779446542263, 0.10859327018260956, 0.13639281690120697, 0.13058915734291077, 0.10370983928442001, 0.0416441448032856, 0.026118747889995575, 0.03138352558016777, 0.05027594789862633, 0.062233153730630875, 0.036540303379297256, 0.03718387335538864, 0.03685159608721733, 0.07413635402917862, 0.045558393001556396, 0.019974570721387863, 0.06632986664772034, 0.03820282220840454, 0.019440004602074623, 0.0171213261783123, 0.02011715993285179, 0.017083756625652313, 0.01660393364727497, 0.014349980279803276, 0.02057979814708233, 0.019713394343852997, 0.028292665258049965, 0.08628968149423599], 'accuracy': [0.9521963596343994, 0.985788106918335, 0.9932816624641418, 0.997157633304596, 0.9917312860488892, 0.9914728403091431, 0.9865633249282837, 0.9527131915092468, 0.9847545027732849, 0.9919896721839905, 0.9937984347343445, 0.9950904250144958, 0.9943152666091919, 0.9888888597488403, 0.9956072568893433, 0.9966408014297485, 0.9914728403091431, 0.9852713346481323, 0.985012948513031, 0.9788113832473755, 0.9912144541740417, 0.998191237449646, 0.9863049387931824, 0.9739018082618713, 0.9958656430244446, 0.9992247819900513, 1.0, 0.99896639585495, 0.9997416138648987, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9987080097198486, 0.9666666388511658, 0.9560723304748535, 0.9560723304748535, 0.9679586291313171, 0.9909560680389404, 0.9961240291595459, 0.9937984347343445, 0.985788106918335, 0.9829457402229309, 0.9896640777587891, 0.9904392957687378, 0.9925064444541931, 0.9759690165519714, 0.9891473054885864, 0.9984496235847473, 0.9793281555175781, 0.9906976819038391, 0.9984496235847473, 0.99896639585495, 0.9979327917098999, 0.9984496235847473, 0.9992247819900513, 0.9997416138648987, 0.9979327917098999, 0.9984496235847473, 0.9932816624641418, 0.9764857888221741], 'val_loss': [0.6866913437843323, 0.6762773394584656, 0.6617904305458069, 0.6422333121299744, 0.6144155859947205, 0.5832898616790771, 0.5642663836479187, 0.5987934470176697, 0.5407007336616516, 0.485923171043396, 0.42780601978302, 0.39380037784576416, 0.3907541036605835, 0.30796435475349426, 0.3308490812778473, 0.30915367603302, 0.27088838815689087, 0.34348204731941223, 0.5094993114471436, 0.27800893783569336, 0.28080999851226807, 0.37777838110923767, 0.4117611050605774, 0.30451416969299316, 0.3806593418121338, 0.5705452561378479, 0.408605694770813, 0.41167983412742615, 0.44812703132629395, 0.4201200604438782, 0.42386743426322937, 0.4291498064994812, 0.43110257387161255, 0.4375273287296295, 0.4272530972957611, 0.4277154803276062, 0.4278639853000641, 0.4294242858886719, 0.42341020703315735, 0.4167337119579315, 0.4166048765182495, 0.41554954648017883, 0.41896718740463257, 0.41239723563194275, 0.419453889131546, 0.42442384362220764, 0.42412957549095154, 0.4113200306892395, 0.4194484353065491, 0.416645884513855, 0.41696056723594666, 0.4087933599948883, 0.4077658951282501, 0.41285115480422974, 0.4270710051059723, 0.410981148481369, 0.4155983328819275, 0.4119033217430115, 0.4087890088558197, 0.4042104184627533, 0.4058902859687805, 0.40741580724716187, 0.39921221137046814, 0.404692679643631, 0.40306398272514343, 0.4055625796318054, 0.40713977813720703, 0.4237246513366699, 0.40429002046585083, 0.40960583090782166, 0.4207378327846527, 0.4507136344909668, 0.515127956867218, 0.32720625400543213, 0.5460543036460876, 0.3162182867527008, 0.3222515881061554, 0.4303523004055023, 0.4503394365310669, 0.48940372467041016, 1.0081539154052734, 0.4088481366634369, 0.5297619700431824, 0.4720938205718994, 0.5137816667556763, 0.6889288425445557, 0.42768895626068115, 0.461797297000885, 0.44454336166381836, 0.47844523191452026, 0.48132839798927307, 0.6214443445205688, 0.5340235233306885, 0.6616721749305725, 0.5899431109428406, 0.5488414764404297, 0.6337375640869141, 0.5179939866065979, 0.6604256629943848, 0.4485737681388855], 'val_accuracy': [0.8347107172012329, 0.83574378490448, 0.8429751992225647, 0.8398760557174683, 0.8636363744735718, 0.8646694421768188, 0.836776852607727, 0.8398760557174683, 0.8873966932296753, 0.8440082669258118, 0.8966942429542542, 0.8832644820213318, 0.827479362487793, 0.9028925895690918, 0.8615702390670776, 0.8780992031097412, 0.89462810754776, 0.8688016533851624, 0.7913222908973694, 0.9018595218658447, 0.8987603187561035, 0.8997933864593506, 0.8780992031097412, 0.8904958963394165, 0.9070248007774353, 0.8708677887916565, 0.9111570119857788, 0.9080578684806824, 0.9111570119857788, 0.9173553586006165, 0.9163222908973694, 0.9183884263038635, 0.9183884263038635, 0.9173553586006165, 0.9121900796890259, 0.91425621509552, 0.913223147392273, 0.91425621509552, 0.9173553586006165, 0.9152892827987671, 0.913223147392273, 0.91425621509552, 0.91425621509552, 0.9121900796890259, 0.9173553586006165, 0.9163222908973694, 0.9183884263038635, 0.9111570119857788, 0.9152892827987671, 0.9152892827987671, 0.9152892827987671, 0.9101239442825317, 0.9152892827987671, 0.9173553586006165, 0.9163222908973694, 0.9194214940071106, 0.9163222908973694, 0.9163222908973694, 0.913223147392273, 0.913223147392273, 0.9152892827987671, 0.9173553586006165, 0.91425621509552, 0.9152892827987671, 0.91425621509552, 0.9152892827987671, 0.9121900796890259, 0.913223147392273, 0.9173553586006165, 0.9121900796890259, 0.9173553586006165, 0.9080578684806824, 0.8977272510528564, 0.8832644820213318, 0.8398760557174683, 0.8832644820213318, 0.8966942429542542, 0.8935950398445129, 0.8966942429542542, 0.8925619721412659, 0.7489669322967529, 0.8842975497245789, 0.8780992031097412, 0.8853305578231812, 0.8915289044380188, 0.8450413346290588, 0.8863636255264282, 0.8925619721412659, 0.8791322112083435, 0.8842975497245789, 0.8853305578231812, 0.8760330677032471, 0.8873966932296753, 0.8760330677032471, 0.8915289044380188, 0.8873966932296753, 0.8822314143180847, 0.8915289044380188, 0.8760330677032471, 0.8719007968902588]}\n","32/32 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","25/29 [========================>.....] - ETA: 0s - loss: 0.1146 - accuracy: 0.9644"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 10s 56ms/step - loss: 0.1104 - accuracy: 0.9642 - val_loss: 0.6783 - val_accuracy: 0.7672\n","Epoch 2/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0380 - accuracy: 0.9908 - val_loss: 0.6723 - val_accuracy: 0.7317\n","Epoch 3/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0208 - accuracy: 0.9981 - val_loss: 0.6536 - val_accuracy: 0.8276\n","Epoch 4/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0153 - accuracy: 0.9995 - val_loss: 0.6340 - val_accuracy: 0.8448\n","Epoch 5/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.6080 - val_accuracy: 0.8642\n","Epoch 6/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0136 - accuracy: 0.9997 - val_loss: 0.5766 - val_accuracy: 0.8524\n","Epoch 7/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.5361 - val_accuracy: 0.8621\n","Epoch 8/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.4935 - val_accuracy: 0.8793\n","Epoch 9/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.4489 - val_accuracy: 0.8847\n","Epoch 10/100\n","29/29 [==============================] - 3s 110ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.3994 - val_accuracy: 0.9062\n","Epoch 11/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.3516 - val_accuracy: 0.9289\n","Epoch 12/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.3054 - val_accuracy: 0.9353\n","Epoch 13/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.2609 - val_accuracy: 0.9483\n","Epoch 14/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.2259 - val_accuracy: 0.9386\n","Epoch 15/100\n","29/29 [==============================] - 2s 56ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.1901 - val_accuracy: 0.9494\n","Epoch 16/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9580\n","Epoch 17/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.1434 - val_accuracy: 0.9601\n","Epoch 18/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.1304 - val_accuracy: 0.9601\n","Epoch 19/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.1261 - val_accuracy: 0.9623\n","Epoch 20/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.1215 - val_accuracy: 0.9644\n","Epoch 21/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.1216 - val_accuracy: 0.9688\n","Epoch 22/100\n","29/29 [==============================] - 3s 95ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 0.9709\n","Epoch 23/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 0.9698\n","Epoch 24/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9709\n","Epoch 25/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 0.9709\n","Epoch 26/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.1400 - val_accuracy: 0.9709\n","Epoch 27/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.1428 - val_accuracy: 0.9698\n","Epoch 28/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.1451 - val_accuracy: 0.9677\n","Epoch 29/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.1480 - val_accuracy: 0.9688\n","Epoch 30/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.1528 - val_accuracy: 0.9666\n","Epoch 31/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9677\n","Epoch 32/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.1541 - val_accuracy: 0.9677\n","Epoch 33/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.1555 - val_accuracy: 0.9666\n","Epoch 34/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9655\n","Epoch 35/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9688\n","Epoch 36/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9644\n","Epoch 37/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1766 - val_accuracy: 0.9601\n","Epoch 38/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.1688 - val_accuracy: 0.9634\n","Epoch 39/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3227 - accuracy: 0.9025 - val_loss: 0.3300 - val_accuracy: 0.8696\n","Epoch 40/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.1429 - accuracy: 0.9496 - val_loss: 0.2444 - val_accuracy: 0.9149\n","Epoch 41/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.1578 - accuracy: 0.9432 - val_loss: 0.2240 - val_accuracy: 0.9235\n","Epoch 42/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0485 - accuracy: 0.9895 - val_loss: 0.2089 - val_accuracy: 0.9278\n","Epoch 43/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0207 - accuracy: 0.9970 - val_loss: 0.1935 - val_accuracy: 0.9450\n","Epoch 44/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.2093 - val_accuracy: 0.9526\n","Epoch 45/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.1992 - val_accuracy: 0.9504\n","Epoch 46/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.2008 - val_accuracy: 0.9580\n","Epoch 47/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.2199 - val_accuracy: 0.9526\n","Epoch 48/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.1925 - val_accuracy: 0.9547\n","Epoch 49/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.1891 - val_accuracy: 0.9569\n","Epoch 50/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.1899 - val_accuracy: 0.9569\n","Epoch 51/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.1899 - val_accuracy: 0.9580\n","Epoch 52/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1949 - val_accuracy: 0.9558\n","Epoch 53/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.1949 - val_accuracy: 0.9569\n","Epoch 54/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9569\n","Epoch 55/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9537\n","Epoch 56/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.1984 - val_accuracy: 0.9580\n","Epoch 57/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.1902 - val_accuracy: 0.9569\n","Epoch 58/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1941 - val_accuracy: 0.9569\n","Epoch 59/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1949 - val_accuracy: 0.9569\n","Epoch 60/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.1984 - val_accuracy: 0.9558\n","Epoch 61/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.1944 - val_accuracy: 0.9569\n","Epoch 62/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.1946 - val_accuracy: 0.9547\n","Epoch 63/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.1949 - val_accuracy: 0.9558\n","Epoch 64/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9569\n","Epoch 65/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.9537\n","Epoch 66/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9558\n","Epoch 67/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.1946 - val_accuracy: 0.9537\n","Epoch 68/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.1943 - val_accuracy: 0.9547\n","Epoch 69/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2023 - val_accuracy: 0.9558\n","Epoch 70/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9558\n","Epoch 71/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.2030 - val_accuracy: 0.9547\n","Epoch 72/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.1979 - val_accuracy: 0.9591\n","Epoch 73/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2056 - val_accuracy: 0.9547\n","Epoch 74/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2050 - val_accuracy: 0.9547\n","Epoch 75/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9558\n","Epoch 76/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9558\n","Epoch 77/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.9547\n","Epoch 78/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9547\n","Epoch 79/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.9569\n","Epoch 80/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9558\n","Epoch 81/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.2033 - val_accuracy: 0.9558\n","Epoch 82/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.1995 - val_accuracy: 0.9569\n","Epoch 83/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.2018 - val_accuracy: 0.9569\n","Epoch 84/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.9569\n","Epoch 85/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2058 - val_accuracy: 0.9558\n","Epoch 86/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2016 - val_accuracy: 0.9591\n","Epoch 87/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2065 - val_accuracy: 0.9547\n","Epoch 88/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2037 - val_accuracy: 0.9558\n","Epoch 89/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2194 - val_accuracy: 0.9526\n","Epoch 90/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2025 - val_accuracy: 0.9601\n","Epoch 91/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2031 - val_accuracy: 0.9580\n","Epoch 92/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.9558\n","Epoch 93/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9580\n","Epoch 94/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.2123 - val_accuracy: 0.9558\n","Epoch 95/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2039 - val_accuracy: 0.9569\n","Epoch 96/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2049 - val_accuracy: 0.9580\n","Epoch 97/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.9558\n","Epoch 98/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 0.9569\n","Epoch 99/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2006 - val_accuracy: 0.9601\n","Epoch 100/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.2097 - val_accuracy: 0.9569\n","{'loss': [0.11042986810207367, 0.038033775985240936, 0.020793624222278595, 0.015297215431928635, 0.01348915696144104, 0.013644326478242874, 0.012697658501565456, 0.012464065104722977, 0.012238046154379845, 0.012180434539914131, 0.012094388715922832, 0.011999386362731457, 0.012081185355782509, 0.01195442769676447, 0.011868269182741642, 0.011812017299234867, 0.011670145206153393, 0.011678965762257576, 0.011576583608984947, 0.011492254212498665, 0.011407429352402687, 0.011350148357450962, 0.011281478218734264, 0.011234285309910774, 0.011161754839122295, 0.011124461889266968, 0.011043637059628963, 0.010956522077322006, 0.010890942998230457, 0.010822915472090244, 0.010758827440440655, 0.010699652135372162, 0.01062854751944542, 0.010575392283499241, 0.010560196824371815, 0.010511601343750954, 0.010509053245186806, 0.010892501100897789, 0.32271724939346313, 0.14289115369319916, 0.1578279435634613, 0.04854543134570122, 0.020734576508402824, 0.013158770278096199, 0.011381033807992935, 0.01113087311387062, 0.010977029800415039, 0.011005161330103874, 0.010845303535461426, 0.010672740638256073, 0.010629532858729362, 0.010521112941205502, 0.010485503822565079, 0.01041694637387991, 0.01038737315684557, 0.010324041359126568, 0.010303948074579239, 0.010247624479234219, 0.010206966660916805, 0.010201063007116318, 0.010140732862055302, 0.010105166584253311, 0.010069974698126316, 0.01002725400030613, 0.010001607239246368, 0.009967605583369732, 0.009950775653123856, 0.0099116750061512, 0.009895025752484798, 0.009857268072664738, 0.009829862043261528, 0.009776072576642036, 0.009746897965669632, 0.009736942127346992, 0.009696069173514843, 0.009673734195530415, 0.009644987992942333, 0.0096150953322649, 0.009568553417921066, 0.009557426907122135, 0.009523884393274784, 0.009502002969384193, 0.00946425087749958, 0.00944109633564949, 0.009424292482435703, 0.009395393542945385, 0.009368370287120342, 0.009346595034003258, 0.009319008328020573, 0.009283741936087608, 0.009263246320188046, 0.009232845157384872, 0.009200111962854862, 0.009175491519272327, 0.009137949906289577, 0.009116649627685547, 0.009087362326681614, 0.00906622689217329, 0.009052911773324013, 0.00901383813470602], 'accuracy': [0.9641702771186829, 0.990840494632721, 0.9981142282485962, 0.9994612336158752, 1.0, 0.9997305870056152, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9024784564971924, 0.9496228694915771, 0.9431573152542114, 0.9894935488700867, 0.9970366358757019, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.6783351898193359, 0.6722976565361023, 0.6535825133323669, 0.6339501738548279, 0.607964038848877, 0.5765731930732727, 0.536074161529541, 0.49347007274627686, 0.4489278495311737, 0.39938610792160034, 0.3516492545604706, 0.3054274916648865, 0.2609461843967438, 0.2258598953485489, 0.19014102220535278, 0.16037829220294952, 0.14341241121292114, 0.1303601711988449, 0.12607526779174805, 0.12147119641304016, 0.12155143916606903, 0.1223762109875679, 0.12573765218257904, 0.12725785374641418, 0.13346031308174133, 0.14004717767238617, 0.1427684873342514, 0.14505542814731598, 0.14802059531211853, 0.1528099626302719, 0.15475663542747498, 0.1541476547718048, 0.15553312003612518, 0.15727411210536957, 0.1593853235244751, 0.15530790388584137, 0.17664606869220734, 0.16876693069934845, 0.3299980163574219, 0.24443551898002625, 0.22400569915771484, 0.2088616043329239, 0.19354985654354095, 0.20933187007904053, 0.19915474951267242, 0.20077604055404663, 0.21992795169353485, 0.1925053745508194, 0.18911603093147278, 0.1898605227470398, 0.18992245197296143, 0.1948874592781067, 0.19492781162261963, 0.1956247091293335, 0.20340900123119354, 0.19839121401309967, 0.19021078944206238, 0.19410380721092224, 0.19492731988430023, 0.1983814239501953, 0.19435736536979675, 0.19461308419704437, 0.19494158029556274, 0.19797588884830475, 0.20539137721061707, 0.20337015390396118, 0.19457116723060608, 0.19426991045475006, 0.20225510001182556, 0.20476402342319489, 0.20296527445316315, 0.19789426028728485, 0.205598846077919, 0.20496606826782227, 0.19585590064525604, 0.19593337178230286, 0.20537322759628296, 0.20336155593395233, 0.2005491703748703, 0.20547758042812347, 0.20327477157115936, 0.19951996207237244, 0.20181486010551453, 0.20281890034675598, 0.20581738650798798, 0.20158793032169342, 0.20646187663078308, 0.2037345916032791, 0.21938557922840118, 0.20245203375816345, 0.20314162969589233, 0.20541082322597504, 0.20483000576496124, 0.21231026947498322, 0.20389166474342346, 0.20493456721305847, 0.20073947310447693, 0.20414873957633972, 0.20055337250232697, 0.20973792672157288], 'val_accuracy': [0.767241358757019, 0.7316810488700867, 0.8275862336158752, 0.8448275923728943, 0.8642241358757019, 0.8523706793785095, 0.8620689511299133, 0.8793103694915771, 0.8846982717514038, 0.90625, 0.9288793206214905, 0.9353448152542114, 0.9482758641242981, 0.9385775923728943, 0.9493534564971924, 0.9579741358757019, 0.9601293206214905, 0.9601293206214905, 0.962284505367279, 0.9644396305084229, 0.96875, 0.9709051847457886, 0.9698275923728943, 0.9709051847457886, 0.9709051847457886, 0.9709051847457886, 0.9698275923728943, 0.9676724076271057, 0.96875, 0.9665948152542114, 0.9676724076271057, 0.9676724076271057, 0.9665948152542114, 0.9655172228813171, 0.96875, 0.9644396305084229, 0.9601293206214905, 0.9633620977401733, 0.8696120977401733, 0.9148706793785095, 0.923491358757019, 0.9278017282485962, 0.9450430870056152, 0.9525862336158752, 0.9504310488700867, 0.9579741358757019, 0.9525862336158752, 0.954741358757019, 0.9568965435028076, 0.9568965435028076, 0.9579741358757019, 0.9558189511299133, 0.9568965435028076, 0.9568965435028076, 0.9536637663841248, 0.9579741358757019, 0.9568965435028076, 0.9568965435028076, 0.9568965435028076, 0.9558189511299133, 0.9568965435028076, 0.954741358757019, 0.9558189511299133, 0.9568965435028076, 0.9536637663841248, 0.9558189511299133, 0.9536637663841248, 0.954741358757019, 0.9558189511299133, 0.9558189511299133, 0.954741358757019, 0.9590517282485962, 0.954741358757019, 0.954741358757019, 0.9558189511299133, 0.9558189511299133, 0.954741358757019, 0.954741358757019, 0.9568965435028076, 0.9558189511299133, 0.9558189511299133, 0.9568965435028076, 0.9568965435028076, 0.9568965435028076, 0.9558189511299133, 0.9590517282485962, 0.954741358757019, 0.9558189511299133, 0.9525862336158752, 0.9601293206214905, 0.9579741358757019, 0.9558189511299133, 0.9579741358757019, 0.9558189511299133, 0.9568965435028076, 0.9579741358757019, 0.9558189511299133, 0.9568965435028076, 0.9601293206214905, 0.9568965435028076]}\n","38/38 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","26/28 [==========================>...] - ETA: 0s - loss: 0.0881 - accuracy: 0.9775"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 8s 61ms/step - loss: 0.0869 - accuracy: 0.9776 - val_loss: 0.6814 - val_accuracy: 0.6403\n","Epoch 2/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.0277 - accuracy: 0.9952 - val_loss: 0.6712 - val_accuracy: 0.7172\n","Epoch 3/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0185 - accuracy: 0.9992 - val_loss: 0.6605 - val_accuracy: 0.7070\n","Epoch 4/100\n","28/28 [==============================] - 1s 31ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.6366 - val_accuracy: 0.7941\n","Epoch 5/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0155 - accuracy: 0.9989 - val_loss: 0.6191 - val_accuracy: 0.7466\n","Epoch 6/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0152 - accuracy: 0.9989 - val_loss: 0.5793 - val_accuracy: 0.8156\n","Epoch 7/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0531 - accuracy: 0.9864 - val_loss: 0.5683 - val_accuracy: 0.7919\n","Epoch 8/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0496 - accuracy: 0.9853 - val_loss: 0.5656 - val_accuracy: 0.8224\n","Epoch 9/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0513 - accuracy: 0.9850 - val_loss: 0.5437 - val_accuracy: 0.7941\n","Epoch 10/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0415 - accuracy: 0.9887 - val_loss: 0.4975 - val_accuracy: 0.8982\n","Epoch 11/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0422 - accuracy: 0.9898 - val_loss: 0.4700 - val_accuracy: 0.8914\n","Epoch 12/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0306 - accuracy: 0.9941 - val_loss: 0.4078 - val_accuracy: 0.8337\n","Epoch 13/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0259 - accuracy: 0.9946 - val_loss: 0.3754 - val_accuracy: 0.8824\n","Epoch 14/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0166 - accuracy: 0.9983 - val_loss: 0.3156 - val_accuracy: 0.9163\n","Epoch 15/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0147 - accuracy: 0.9994 - val_loss: 0.2764 - val_accuracy: 0.9118\n","Epoch 16/100\n","28/28 [==============================] - 1s 31ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.2426 - val_accuracy: 0.9287\n","Epoch 17/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.2278 - val_accuracy: 0.9186\n","Epoch 18/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.2142 - val_accuracy: 0.9276\n","Epoch 19/100\n","28/28 [==============================] - 1s 30ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.2052 - val_accuracy: 0.9321\n","Epoch 20/100\n","28/28 [==============================] - 1s 37ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.2057 - val_accuracy: 0.9344\n","Epoch 21/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.2096 - val_accuracy: 0.9333\n","Epoch 22/100\n","28/28 [==============================] - 1s 30ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2121 - val_accuracy: 0.9423\n","Epoch 23/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.2220 - val_accuracy: 0.9423\n","Epoch 24/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.2266 - val_accuracy: 0.9446\n","Epoch 25/100\n","28/28 [==============================] - 1s 30ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.2319 - val_accuracy: 0.9468\n","Epoch 26/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.2377 - val_accuracy: 0.9468\n","Epoch 27/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.2465 - val_accuracy: 0.9457\n","Epoch 28/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.2487 - val_accuracy: 0.9468\n","Epoch 29/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.2549 - val_accuracy: 0.9457\n","Epoch 30/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.2600 - val_accuracy: 0.9457\n","Epoch 31/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.2640 - val_accuracy: 0.9457\n","Epoch 32/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.2640 - val_accuracy: 0.9457\n","Epoch 33/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.2651 - val_accuracy: 0.9480\n","Epoch 34/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.2677 - val_accuracy: 0.9457\n","Epoch 35/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.2704 - val_accuracy: 0.9446\n","Epoch 36/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.2799 - val_accuracy: 0.9412\n","Epoch 37/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.2745 - val_accuracy: 0.9457\n","Epoch 38/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.2739 - val_accuracy: 0.9457\n","Epoch 39/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.2728 - val_accuracy: 0.9446\n","Epoch 40/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.2792 - val_accuracy: 0.9423\n","Epoch 41/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.2772 - val_accuracy: 0.9457\n","Epoch 42/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.2762 - val_accuracy: 0.9434\n","Epoch 43/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.2755 - val_accuracy: 0.9434\n","Epoch 44/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.2742 - val_accuracy: 0.9434\n","Epoch 45/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9423\n","Epoch 46/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.2740 - val_accuracy: 0.9446\n","Epoch 47/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9423\n","Epoch 48/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9434\n","Epoch 49/100\n","28/28 [==============================] - 1s 34ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9491\n","Epoch 50/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.2725 - val_accuracy: 0.9434\n","Epoch 51/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2731 - val_accuracy: 0.9446\n","Epoch 52/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.2750 - val_accuracy: 0.9423\n","Epoch 53/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.2759 - val_accuracy: 0.9412\n","Epoch 54/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2766 - val_accuracy: 0.9400\n","Epoch 55/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2746 - val_accuracy: 0.9423\n","Epoch 56/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.2750 - val_accuracy: 0.9468\n","Epoch 57/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.2769 - val_accuracy: 0.9389\n","Epoch 58/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.2770 - val_accuracy: 0.9412\n","Epoch 59/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9434\n","Epoch 60/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2760 - val_accuracy: 0.9423\n","Epoch 61/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.9400\n","Epoch 62/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9446\n","Epoch 63/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2770 - val_accuracy: 0.9446\n","Epoch 64/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.2808 - val_accuracy: 0.9389\n","Epoch 65/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.2804 - val_accuracy: 0.9457\n","Epoch 66/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.2803 - val_accuracy: 0.9412\n","Epoch 67/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2827 - val_accuracy: 0.9389\n","Epoch 68/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.2815 - val_accuracy: 0.9400\n","Epoch 69/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.2820 - val_accuracy: 0.9389\n","Epoch 70/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.2780 - val_accuracy: 0.9423\n","Epoch 71/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.2787 - val_accuracy: 0.9423\n","Epoch 72/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.2772 - val_accuracy: 0.9423\n","Epoch 73/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.2812 - val_accuracy: 0.9400\n","Epoch 74/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.2814 - val_accuracy: 0.9423\n","Epoch 75/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2797 - val_accuracy: 0.9468\n","Epoch 76/100\n","28/28 [==============================] - 1s 17ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2769 - val_accuracy: 0.9423\n","Epoch 77/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2762 - val_accuracy: 0.9412\n","Epoch 78/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2785 - val_accuracy: 0.9423\n","Epoch 79/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2796 - val_accuracy: 0.9434\n","Epoch 80/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.2798 - val_accuracy: 0.9446\n","Epoch 81/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.2784 - val_accuracy: 0.9446\n","Epoch 82/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2823 - val_accuracy: 0.9423\n","Epoch 83/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2834 - val_accuracy: 0.9434\n","Epoch 84/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.2888 - val_accuracy: 0.9400\n","Epoch 85/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2867 - val_accuracy: 0.9423\n","Epoch 86/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2854 - val_accuracy: 0.9423\n","Epoch 87/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2854 - val_accuracy: 0.9423\n","Epoch 88/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2845 - val_accuracy: 0.9434\n","Epoch 89/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2855 - val_accuracy: 0.9423\n","Epoch 90/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2896 - val_accuracy: 0.9400\n","Epoch 91/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9423\n","Epoch 92/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2871 - val_accuracy: 0.9423\n","Epoch 93/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2899 - val_accuracy: 0.9412\n","Epoch 94/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2889 - val_accuracy: 0.9423\n","Epoch 95/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2926 - val_accuracy: 0.9434\n","Epoch 96/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2944 - val_accuracy: 0.9389\n","Epoch 97/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2872 - val_accuracy: 0.9423\n","Epoch 98/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2870 - val_accuracy: 0.9412\n","Epoch 99/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2896 - val_accuracy: 0.9446\n","Epoch 100/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2884 - val_accuracy: 0.9434\n","{'loss': [0.08685373514890671, 0.0276603065431118, 0.01853370852768421, 0.014402288943529129, 0.01553859282284975, 0.015185904689133167, 0.05308590084314346, 0.0496131107211113, 0.05132567510008812, 0.04150692746043205, 0.04216248542070389, 0.030568154528737068, 0.02592862956225872, 0.01658647693693638, 0.01468587201088667, 0.013003689236938953, 0.012242745608091354, 0.011971359141170979, 0.011857200413942337, 0.011820296756923199, 0.011686231009662151, 0.011619224213063717, 0.01154288835823536, 0.011465484276413918, 0.011400213465094566, 0.01133202388882637, 0.011274299584329128, 0.01121852733194828, 0.01113053597509861, 0.011072858236730099, 0.011058015748858452, 0.01097886823117733, 0.010917152278125286, 0.010837052948772907, 0.010787535458803177, 0.010776171460747719, 0.01069869939237833, 0.010618184693157673, 0.010561345145106316, 0.010502486489713192, 0.010448100045323372, 0.010378575883805752, 0.01033240370452404, 0.010276110842823982, 0.010214735753834248, 0.010164761915802956, 0.010112637653946877, 0.010057860985398293, 0.010037094354629517, 0.00995421689003706, 0.00989762507379055, 0.009856813587248325, 0.009799256920814514, 0.00974609050899744, 0.009693664498627186, 0.009665228426456451, 0.00960408803075552, 0.00953215453773737, 0.009493408724665642, 0.009447758086025715, 0.00939215999096632, 0.009348313324153423, 0.009300108067691326, 0.009263343177735806, 0.00920884683728218, 0.00915470439940691, 0.009105384349822998, 0.009057278744876385, 0.009009949862957, 0.008959016762673855, 0.008906287141144276, 0.008861532434821129, 0.008828914724290371, 0.008775303140282631, 0.008748605847358704, 0.008694382384419441, 0.008637657389044762, 0.008601070381700993, 0.008562113158404827, 0.008518708869814873, 0.008473639376461506, 0.008436591364443302, 0.0083931228145957, 0.008354018442332745, 0.00832036416977644, 0.008267535828053951, 0.008234274573624134, 0.008198613300919533, 0.008160081692039967, 0.008126843720674515, 0.008094508200883865, 0.008047935552895069, 0.008007734082639217, 0.007976780645549297, 0.00794273242354393, 0.007914942689239979, 0.007864722050726414, 0.007840069010853767, 0.007807252462953329, 0.007768385577946901], 'accuracy': [0.977645754814148, 0.9951896071434021, 0.9991511106491089, 1.0, 0.9988681674003601, 0.9988681674003601, 0.9864176511764526, 0.9852858185768127, 0.9850028157234192, 0.9886813759803772, 0.9898132681846619, 0.9940577149391174, 0.9946236610412598, 0.9983022212982178, 0.9994340538978577, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.6813898086547852, 0.671177327632904, 0.6605011224746704, 0.6365992426872253, 0.6190888285636902, 0.5792884230613708, 0.5683067440986633, 0.565643310546875, 0.5436615943908691, 0.4974960684776306, 0.4700385332107544, 0.407781720161438, 0.3753877580165863, 0.3156021535396576, 0.2763701379299164, 0.24257923662662506, 0.2277904897928238, 0.21419402956962585, 0.20521597564220428, 0.20569847524166107, 0.20960432291030884, 0.21206803619861603, 0.2220277041196823, 0.22658665478229523, 0.23192068934440613, 0.2377149760723114, 0.24651214480400085, 0.2486519068479538, 0.2548598647117615, 0.2600395977497101, 0.2639503479003906, 0.2639850676059723, 0.26512381434440613, 0.2677117884159088, 0.2704062759876251, 0.2798621654510498, 0.2744593024253845, 0.2739237844944, 0.2728331983089447, 0.2791830599308014, 0.27722084522247314, 0.27615270018577576, 0.27552685141563416, 0.27416929602622986, 0.27434206008911133, 0.27404874563217163, 0.2742868959903717, 0.275155246257782, 0.2751571536064148, 0.2725447714328766, 0.2731158435344696, 0.2750222682952881, 0.2759150564670563, 0.27659985423088074, 0.27456602454185486, 0.2749674916267395, 0.27686402201652527, 0.27695009112358093, 0.27809464931488037, 0.27599385380744934, 0.2776859402656555, 0.277295857667923, 0.27703216671943665, 0.28079065680503845, 0.28044092655181885, 0.28032442927360535, 0.28273889422416687, 0.28149259090423584, 0.2819538414478302, 0.2779921889305115, 0.2786659598350525, 0.2771919369697571, 0.2811620235443115, 0.2813738286495209, 0.27967873215675354, 0.2769491374492645, 0.2761867046356201, 0.2784821391105652, 0.27956223487854004, 0.2797900438308716, 0.2784149646759033, 0.2822599709033966, 0.28335219621658325, 0.2888453006744385, 0.28666722774505615, 0.2854144275188446, 0.28535017371177673, 0.2845258116722107, 0.2854580283164978, 0.28960615396499634, 0.283015638589859, 0.287057101726532, 0.28992292284965515, 0.288935124874115, 0.2925534248352051, 0.2943880259990692, 0.287214070558548, 0.2870163023471832, 0.2896225154399872, 0.2883564829826355], 'val_accuracy': [0.6402714848518372, 0.7171945571899414, 0.7070135474205017, 0.7941176295280457, 0.7466063499450684, 0.8156108856201172, 0.7918552160263062, 0.8223981857299805, 0.7941176295280457, 0.8981900215148926, 0.8914027214050293, 0.8337104320526123, 0.8823529481887817, 0.9162895679473877, 0.9117646813392639, 0.9287330508232117, 0.918552041053772, 0.9276018142700195, 0.9321267008781433, 0.9343891143798828, 0.9332579374313354, 0.942307710647583, 0.942307710647583, 0.9445701241493225, 0.9468325972557068, 0.9468325972557068, 0.9457013607025146, 0.9468325972557068, 0.9457013607025146, 0.9457013607025146, 0.9457013607025146, 0.9457013607025146, 0.9479637742042542, 0.9457013607025146, 0.9445701241493225, 0.9411764740943909, 0.9457013607025146, 0.9457013607025146, 0.9445701241493225, 0.942307710647583, 0.9457013607025146, 0.9434388875961304, 0.9434388875961304, 0.9434388875961304, 0.942307710647583, 0.9445701241493225, 0.942307710647583, 0.9434388875961304, 0.9490950107574463, 0.9434388875961304, 0.9445701241493225, 0.942307710647583, 0.9411764740943909, 0.9400452375411987, 0.942307710647583, 0.9468325972557068, 0.9389140009880066, 0.9411764740943909, 0.9434388875961304, 0.942307710647583, 0.9400452375411987, 0.9445701241493225, 0.9445701241493225, 0.9389140009880066, 0.9457013607025146, 0.9411764740943909, 0.9389140009880066, 0.9400452375411987, 0.9389140009880066, 0.942307710647583, 0.942307710647583, 0.942307710647583, 0.9400452375411987, 0.942307710647583, 0.9468325972557068, 0.942307710647583, 0.9411764740943909, 0.942307710647583, 0.9434388875961304, 0.9445701241493225, 0.9445701241493225, 0.942307710647583, 0.9434388875961304, 0.9400452375411987, 0.942307710647583, 0.942307710647583, 0.942307710647583, 0.9434388875961304, 0.942307710647583, 0.9400452375411987, 0.942307710647583, 0.942307710647583, 0.9411764740943909, 0.942307710647583, 0.9434388875961304, 0.9389140009880066, 0.942307710647583, 0.9411764740943909, 0.9445701241493225, 0.9434388875961304]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/31 [===========================>..] - ETA: 0s - loss: 0.0671 - accuracy: 0.9795"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 8s 74ms/step - loss: 0.0657 - accuracy: 0.9801 - val_loss: 0.6773 - val_accuracy: 0.7676\n","Epoch 2/100\n","31/31 [==============================] - 1s 30ms/step - loss: 0.0376 - accuracy: 0.9910 - val_loss: 0.6651 - val_accuracy: 0.8481\n","Epoch 3/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.0186 - accuracy: 0.9990 - val_loss: 0.6404 - val_accuracy: 0.8554\n","Epoch 4/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0242 - accuracy: 0.9964 - val_loss: 0.6187 - val_accuracy: 0.8006\n","Epoch 5/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0439 - accuracy: 0.9876 - val_loss: 0.6018 - val_accuracy: 0.8450\n","Epoch 6/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0305 - accuracy: 0.9938 - val_loss: 0.5683 - val_accuracy: 0.8616\n","Epoch 7/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0313 - accuracy: 0.9933 - val_loss: 0.5364 - val_accuracy: 0.8182\n","Epoch 8/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.1674 - accuracy: 0.9457 - val_loss: 0.6151 - val_accuracy: 0.8182\n","Epoch 9/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.0473 - accuracy: 0.9899 - val_loss: 0.5181 - val_accuracy: 0.8740\n","Epoch 10/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0184 - accuracy: 0.9987 - val_loss: 0.4490 - val_accuracy: 0.8874\n","Epoch 11/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.3742 - val_accuracy: 0.8946\n","Epoch 12/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.3273 - val_accuracy: 0.9050\n","Epoch 13/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.2881 - val_accuracy: 0.9039\n","Epoch 14/100\n","31/31 [==============================] - 2s 82ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.2604 - val_accuracy: 0.9143\n","Epoch 15/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9122\n","Epoch 16/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.2420 - val_accuracy: 0.9070\n","Epoch 17/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.2263 - val_accuracy: 0.9225\n","Epoch 18/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.2302 - val_accuracy: 0.9277\n","Epoch 19/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.2380 - val_accuracy: 0.9277\n","Epoch 20/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.2477 - val_accuracy: 0.9298\n","Epoch 21/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.2606 - val_accuracy: 0.9277\n","Epoch 22/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.2744 - val_accuracy: 0.9267\n","Epoch 23/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.2878 - val_accuracy: 0.9246\n","Epoch 24/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.2944 - val_accuracy: 0.9277\n","Epoch 25/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.3131 - val_accuracy: 0.9267\n","Epoch 26/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.3110 - val_accuracy: 0.9246\n","Epoch 27/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.3158 - val_accuracy: 0.9236\n","Epoch 28/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 0.9308\n","Epoch 29/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.3308 - val_accuracy: 0.9308\n","Epoch 30/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.3267 - val_accuracy: 0.9277\n","Epoch 31/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.3304 - val_accuracy: 0.9287\n","Epoch 32/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.3281 - val_accuracy: 0.9287\n","Epoch 33/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.3275 - val_accuracy: 0.9287\n","Epoch 34/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3289 - val_accuracy: 0.9267\n","Epoch 35/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.3278 - val_accuracy: 0.9246\n","Epoch 36/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.3261 - val_accuracy: 0.9246\n","Epoch 37/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.3268 - val_accuracy: 0.9277\n","Epoch 38/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 0.9277\n","Epoch 39/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.3286 - val_accuracy: 0.9267\n","Epoch 40/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.3362 - val_accuracy: 0.9298\n","Epoch 41/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3278 - val_accuracy: 0.9246\n","Epoch 42/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9287\n","Epoch 43/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.3331 - val_accuracy: 0.9246\n","Epoch 44/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 0.9277\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.3394 - val_accuracy: 0.9225\n","Epoch 46/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.3353 - val_accuracy: 0.9225\n","Epoch 47/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.3361 - val_accuracy: 0.9215\n","Epoch 48/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.3415 - val_accuracy: 0.9267\n","Epoch 49/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.3364 - val_accuracy: 0.9256\n","Epoch 50/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.3493 - val_accuracy: 0.9246\n","Epoch 51/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.3379 - val_accuracy: 0.9256\n","Epoch 52/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.3430 - val_accuracy: 0.9256\n","Epoch 53/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.3412 - val_accuracy: 0.9277\n","Epoch 54/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.3355 - val_accuracy: 0.9205\n","Epoch 55/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3336 - val_accuracy: 0.9267\n","Epoch 56/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3357 - val_accuracy: 0.9267\n","Epoch 57/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3449 - val_accuracy: 0.9256\n","Epoch 58/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3661 - val_accuracy: 0.9163\n","Epoch 59/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4189 - accuracy: 0.8809 - val_loss: 0.4547 - val_accuracy: 0.7955\n","Epoch 60/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.1894 - accuracy: 0.9269 - val_loss: 0.3043 - val_accuracy: 0.8905\n","Epoch 61/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0946 - accuracy: 0.9682 - val_loss: 0.4093 - val_accuracy: 0.8750\n","Epoch 62/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0816 - accuracy: 0.9698 - val_loss: 0.3294 - val_accuracy: 0.8946\n","Epoch 63/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0442 - accuracy: 0.9881 - val_loss: 0.5251 - val_accuracy: 0.8564\n","Epoch 64/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0686 - accuracy: 0.9775 - val_loss: 0.3543 - val_accuracy: 0.8957\n","Epoch 65/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0455 - accuracy: 0.9881 - val_loss: 0.3712 - val_accuracy: 0.9101\n","Epoch 66/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0437 - accuracy: 0.9881 - val_loss: 0.3492 - val_accuracy: 0.9101\n","Epoch 67/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0396 - accuracy: 0.9889 - val_loss: 0.3839 - val_accuracy: 0.9019\n","Epoch 68/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0204 - accuracy: 0.9969 - val_loss: 0.3778 - val_accuracy: 0.9060\n","Epoch 69/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0178 - accuracy: 0.9972 - val_loss: 0.4824 - val_accuracy: 0.8926\n","Epoch 70/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0256 - accuracy: 0.9933 - val_loss: 0.3933 - val_accuracy: 0.9060\n","Epoch 71/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0207 - accuracy: 0.9966 - val_loss: 0.4986 - val_accuracy: 0.8791\n","Epoch 72/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0211 - accuracy: 0.9961 - val_loss: 0.4386 - val_accuracy: 0.8998\n","Epoch 73/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0393 - accuracy: 0.9866 - val_loss: 0.4540 - val_accuracy: 0.8905\n","Epoch 74/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0301 - accuracy: 0.9915 - val_loss: 0.4560 - val_accuracy: 0.8895\n","Epoch 75/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0294 - accuracy: 0.9930 - val_loss: 0.4211 - val_accuracy: 0.8977\n","Epoch 76/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0298 - accuracy: 0.9933 - val_loss: 0.4364 - val_accuracy: 0.8998\n","Epoch 77/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0212 - accuracy: 0.9969 - val_loss: 0.4814 - val_accuracy: 0.8905\n","Epoch 78/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0165 - accuracy: 0.9977 - val_loss: 0.5901 - val_accuracy: 0.8729\n","Epoch 79/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0146 - accuracy: 0.9990 - val_loss: 0.4146 - val_accuracy: 0.9174\n","Epoch 80/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0179 - accuracy: 0.9969 - val_loss: 0.4322 - val_accuracy: 0.9050\n","Epoch 81/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0179 - accuracy: 0.9966 - val_loss: 0.4906 - val_accuracy: 0.8957\n","Epoch 82/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0187 - accuracy: 0.9964 - val_loss: 0.5488 - val_accuracy: 0.8853\n","Epoch 83/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0753 - accuracy: 0.9814 - val_loss: 0.5548 - val_accuracy: 0.8688\n","Epoch 84/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0622 - accuracy: 0.9791 - val_loss: 0.4100 - val_accuracy: 0.8729\n","Epoch 85/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0216 - accuracy: 0.9966 - val_loss: 0.4162 - val_accuracy: 0.8977\n","Epoch 86/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0158 - accuracy: 0.9982 - val_loss: 0.4245 - val_accuracy: 0.8895\n","Epoch 87/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0144 - accuracy: 0.9984 - val_loss: 0.4009 - val_accuracy: 0.9112\n","Epoch 88/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0113 - accuracy: 0.9997 - val_loss: 0.4851 - val_accuracy: 0.9029\n","Epoch 89/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.3974 - val_accuracy: 0.9153\n","Epoch 90/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.4010 - val_accuracy: 0.9163\n","Epoch 91/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3857 - val_accuracy: 0.9246\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3922 - val_accuracy: 0.9205\n","Epoch 93/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3831 - val_accuracy: 0.9215\n","Epoch 94/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3854 - val_accuracy: 0.9205\n","Epoch 95/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3783 - val_accuracy: 0.9205\n","Epoch 96/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3809 - val_accuracy: 0.9215\n","Epoch 97/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3753 - val_accuracy: 0.9225\n","Epoch 98/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 0.9225\n","Epoch 99/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3777 - val_accuracy: 0.9205\n","Epoch 100/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.3773 - val_accuracy: 0.9205\n","{'loss': [0.06566409021615982, 0.0376274436712265, 0.01859821192920208, 0.024224083870649338, 0.04394643381237984, 0.03050222247838974, 0.03125908970832825, 0.16740870475769043, 0.04732590168714523, 0.018409907817840576, 0.014062546193599701, 0.012630761601030827, 0.012417052872478962, 0.012198477052152157, 0.012112518772482872, 0.01214247290045023, 0.012172791175544262, 0.012111066840589046, 0.011826279573142529, 0.011775480583310127, 0.011660189367830753, 0.011578377336263657, 0.011521635577082634, 0.011463423259556293, 0.011451217345893383, 0.01135657262057066, 0.011308453045785427, 0.01125216856598854, 0.011217459104955196, 0.011203590780496597, 0.011065463535487652, 0.01104674395173788, 0.010992899537086487, 0.010889503173530102, 0.010872847400605679, 0.0108043747022748, 0.010775373317301273, 0.010686683468520641, 0.010631144046783447, 0.010608736425638199, 0.010531623847782612, 0.010483923368155956, 0.010439228266477585, 0.010391582734882832, 0.010354479774832726, 0.01030643004924059, 0.010246672667562962, 0.010174877010285854, 0.010119377635419369, 0.010130651295185089, 0.010065548121929169, 0.009960033930838108, 0.009932117536664009, 0.009872027672827244, 0.00983065739274025, 0.009761323221027851, 0.009736399166285992, 0.01052240189164877, 0.4189241826534271, 0.18942654132843018, 0.09457951039075851, 0.08161301165819168, 0.04424918070435524, 0.06858766078948975, 0.0454721637070179, 0.04369734600186348, 0.03957493603229523, 0.02036772482097149, 0.017789576202630997, 0.025559591129422188, 0.020735424011945724, 0.02110680192708969, 0.03927217051386833, 0.03005962260067463, 0.02944939397275448, 0.02980656363070011, 0.021160896867513657, 0.01648111268877983, 0.014625197276473045, 0.01788531243801117, 0.01790599524974823, 0.018699070438742638, 0.07527811080217361, 0.06223031133413315, 0.02159370854496956, 0.01576489582657814, 0.014365782961249352, 0.011294683441519737, 0.010525915771722794, 0.00983861368149519, 0.009748494252562523, 0.009670578874647617, 0.009610006585717201, 0.009548689238727093, 0.009500161744654179, 0.009480808861553669, 0.009418100118637085, 0.009382887743413448, 0.009336627088487148, 0.009310076013207436], 'accuracy': [0.9801033735275269, 0.9909560680389404, 0.99896639585495, 0.9963824152946472, 0.987596869468689, 0.9937984347343445, 0.9932816624641418, 0.9457364082336426, 0.9899224638938904, 0.9987080097198486, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8808785676956177, 0.9268733859062195, 0.9682170748710632, 0.9697674512863159, 0.9881137013435364, 0.9775193929672241, 0.9881137013435364, 0.9881137013435364, 0.9888888597488403, 0.9968992471694946, 0.997157633304596, 0.9932816624641418, 0.9966408014297485, 0.9961240291595459, 0.9865633249282837, 0.9914728403091431, 0.9930232763290405, 0.9932816624641418, 0.9968992471694946, 0.9976744055747986, 0.99896639585495, 0.9968992471694946, 0.9966408014297485, 0.9963824152946472, 0.9813953638076782, 0.9790697693824768, 0.9966408014297485, 0.998191237449646, 0.9984496235847473, 0.9997416138648987, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.6772890686988831, 0.6651235818862915, 0.6404329538345337, 0.6187400221824646, 0.6018123626708984, 0.568303108215332, 0.5363889336585999, 0.6150501370429993, 0.5180864930152893, 0.44902724027633667, 0.3741857707500458, 0.32731637358665466, 0.2880726754665375, 0.2603910267353058, 0.24107910692691803, 0.2420268952846527, 0.2262933999300003, 0.230234757065773, 0.23804479837417603, 0.2477179914712906, 0.260585218667984, 0.2744453549385071, 0.28778085112571716, 0.29440489411354065, 0.31308841705322266, 0.3109753131866455, 0.3157740831375122, 0.31876471638679504, 0.33076363801956177, 0.32665586471557617, 0.3303820490837097, 0.3280569612979889, 0.3274551033973694, 0.3288935720920563, 0.32784247398376465, 0.3261394202709198, 0.3268008828163147, 0.32998278737068176, 0.3286019563674927, 0.3361736536026001, 0.32782450318336487, 0.33867147564888, 0.3331218361854553, 0.3344956040382385, 0.33937782049179077, 0.33525723218917847, 0.3361465632915497, 0.3414573669433594, 0.3364427089691162, 0.34930694103240967, 0.3378998339176178, 0.343007892370224, 0.3411768674850464, 0.3354659378528595, 0.3335520625114441, 0.3357323408126831, 0.34485429525375366, 0.36605027318000793, 0.45466524362564087, 0.30432841181755066, 0.40929466485977173, 0.329387366771698, 0.5251196622848511, 0.354309618473053, 0.3712122142314911, 0.34916117787361145, 0.3839101195335388, 0.3778058886528015, 0.4823620617389679, 0.39331379532814026, 0.49856871366500854, 0.4386218786239624, 0.4539563059806824, 0.45597052574157715, 0.4210699498653412, 0.43637800216674805, 0.4813663363456726, 0.5900700688362122, 0.4145890474319458, 0.43215498328208923, 0.49063199758529663, 0.5488404631614685, 0.5547503232955933, 0.4100175201892853, 0.41623812913894653, 0.4245133101940155, 0.40093618631362915, 0.48509275913238525, 0.3974338471889496, 0.40097174048423767, 0.38565167784690857, 0.39219042658805847, 0.3830947279930115, 0.38539019227027893, 0.37830740213394165, 0.38087955117225647, 0.37526455521583557, 0.3778466582298279, 0.3776604235172272, 0.3773340880870819], 'val_accuracy': [0.7675619721412659, 0.8481404781341553, 0.85537189245224, 0.8006198406219482, 0.8450413346290588, 0.8615702390670776, 0.8181818127632141, 0.8181818127632141, 0.8739669322967529, 0.8873966932296753, 0.89462810754776, 0.9049586653709412, 0.9039255976676941, 0.91425621509552, 0.9121900796890259, 0.9070248007774353, 0.922520637512207, 0.9276859760284424, 0.9276859760284424, 0.9297520518302917, 0.9276859760284424, 0.9266529083251953, 0.9245867729187012, 0.9276859760284424, 0.9266529083251953, 0.9245867729187012, 0.9235537052154541, 0.9307851195335388, 0.9307851195335388, 0.9276859760284424, 0.9287189841270447, 0.9287189841270447, 0.9287189841270447, 0.9266529083251953, 0.9245867729187012, 0.9245867729187012, 0.9276859760284424, 0.9276859760284424, 0.9266529083251953, 0.9297520518302917, 0.9245867729187012, 0.9287189841270447, 0.9245867729187012, 0.9276859760284424, 0.922520637512207, 0.922520637512207, 0.9214876294136047, 0.9266529083251953, 0.9256198406219482, 0.9245867729187012, 0.9256198406219482, 0.9256198406219482, 0.9276859760284424, 0.9204545617103577, 0.9266529083251953, 0.9266529083251953, 0.9256198406219482, 0.9163222908973694, 0.7954545617103577, 0.8904958963394165, 0.875, 0.89462810754776, 0.8564049601554871, 0.8956611752510071, 0.9101239442825317, 0.9101239442825317, 0.9018595218658447, 0.9059917330741882, 0.8925619721412659, 0.9059917330741882, 0.8791322112083435, 0.8997933864593506, 0.8904958963394165, 0.8894628286361694, 0.8977272510528564, 0.8997933864593506, 0.8904958963394165, 0.8729338645935059, 0.9173553586006165, 0.9049586653709412, 0.8956611752510071, 0.8853305578231812, 0.8688016533851624, 0.8729338645935059, 0.8977272510528564, 0.8894628286361694, 0.9111570119857788, 0.9028925895690918, 0.9152892827987671, 0.9163222908973694, 0.9245867729187012, 0.9204545617103577, 0.9214876294136047, 0.9204545617103577, 0.9204545617103577, 0.9214876294136047, 0.922520637512207, 0.922520637512207, 0.9204545617103577, 0.9204545617103577]}\n","32/32 [==============================] - 1s 5ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9836"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 7s 58ms/step - loss: 0.0647 - accuracy: 0.9836 - val_loss: 0.6713 - val_accuracy: 0.7931\n","Epoch 2/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0205 - accuracy: 0.9973 - val_loss: 0.6589 - val_accuracy: 0.8416\n","Epoch 3/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0122 - accuracy: 0.9989 - val_loss: 0.6438 - val_accuracy: 0.8362\n","Epoch 4/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0129 - accuracy: 0.9984 - val_loss: 0.6205 - val_accuracy: 0.8319\n","Epoch 5/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0129 - accuracy: 0.9987 - val_loss: 0.5870 - val_accuracy: 0.8265\n","Epoch 6/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0150 - accuracy: 0.9976 - val_loss: 0.5463 - val_accuracy: 0.8222\n","Epoch 7/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0329 - accuracy: 0.9914 - val_loss: 0.5349 - val_accuracy: 0.8341\n","Epoch 8/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.0459 - accuracy: 0.9868 - val_loss: 0.5269 - val_accuracy: 0.8922\n","Epoch 9/100\n","29/29 [==============================] - 1s 33ms/step - loss: 0.0231 - accuracy: 0.9946 - val_loss: 0.4834 - val_accuracy: 0.9106\n","Epoch 10/100\n","29/29 [==============================] - 1s 29ms/step - loss: 0.0135 - accuracy: 0.9984 - val_loss: 0.4031 - val_accuracy: 0.9267\n","Epoch 11/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0168 - accuracy: 0.9973 - val_loss: 0.4038 - val_accuracy: 0.8869\n","Epoch 12/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0350 - accuracy: 0.9914 - val_loss: 0.3647 - val_accuracy: 0.9138\n","Epoch 13/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0209 - accuracy: 0.9960 - val_loss: 0.3005 - val_accuracy: 0.9267\n","Epoch 14/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.1243 - accuracy: 0.9604 - val_loss: 0.3966 - val_accuracy: 0.8739\n","Epoch 15/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0337 - accuracy: 0.9938 - val_loss: 0.2757 - val_accuracy: 0.9030\n","Epoch 16/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.0121 - accuracy: 0.9992 - val_loss: 0.1928 - val_accuracy: 0.9332\n","Epoch 17/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.1618 - val_accuracy: 0.9494\n","Epoch 18/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.1454 - val_accuracy: 0.9515\n","Epoch 19/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.1386 - val_accuracy: 0.9580\n","Epoch 20/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.1361 - val_accuracy: 0.9623\n","Epoch 21/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.1335 - val_accuracy: 0.9612\n","Epoch 22/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.1366 - val_accuracy: 0.9634\n","Epoch 23/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.1372 - val_accuracy: 0.9655\n","Epoch 24/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9666\n","Epoch 25/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.9688\n","Epoch 26/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.1451 - val_accuracy: 0.9688\n","Epoch 27/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.9731\n","Epoch 28/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1527 - val_accuracy: 0.9677\n","Epoch 29/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9698\n","Epoch 30/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9677\n","Epoch 31/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1586 - val_accuracy: 0.9677\n","Epoch 32/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9677\n","Epoch 33/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9666\n","Epoch 34/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1608 - val_accuracy: 0.9666\n","Epoch 35/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1611 - val_accuracy: 0.9666\n","Epoch 36/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1639 - val_accuracy: 0.9666\n","Epoch 37/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1619 - val_accuracy: 0.9666\n","Epoch 38/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1611 - val_accuracy: 0.9666\n","Epoch 39/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1620 - val_accuracy: 0.9666\n","Epoch 40/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1636 - val_accuracy: 0.9655\n","Epoch 41/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1637 - val_accuracy: 0.9677\n","Epoch 42/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1638 - val_accuracy: 0.9677\n","Epoch 43/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1623 - val_accuracy: 0.9677\n","Epoch 44/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1628 - val_accuracy: 0.9666\n","Epoch 45/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1638 - val_accuracy: 0.9655\n","Epoch 46/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1631 - val_accuracy: 0.9655\n","Epoch 47/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1641 - val_accuracy: 0.9655\n","Epoch 48/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1650 - val_accuracy: 0.9644\n","Epoch 49/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1651 - val_accuracy: 0.9666\n","Epoch 50/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9677\n","Epoch 51/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1656 - val_accuracy: 0.9655\n","Epoch 52/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1636 - val_accuracy: 0.9677\n","Epoch 53/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9655\n","Epoch 54/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1651 - val_accuracy: 0.9666\n","Epoch 55/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1655 - val_accuracy: 0.9677\n","Epoch 56/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1661 - val_accuracy: 0.9677\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1697 - val_accuracy: 0.9634\n","Epoch 58/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1677 - val_accuracy: 0.9677\n","Epoch 59/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1687 - val_accuracy: 0.9666\n","Epoch 60/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1692 - val_accuracy: 0.9644\n","Epoch 61/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1682 - val_accuracy: 0.9655\n","Epoch 62/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1686 - val_accuracy: 0.9644\n","Epoch 63/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.1671 - val_accuracy: 0.9688\n","Epoch 64/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.1683 - val_accuracy: 0.9655\n","Epoch 65/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.1694 - val_accuracy: 0.9655\n","Epoch 66/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.1674 - val_accuracy: 0.9666\n","Epoch 67/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.1669 - val_accuracy: 0.9677\n","Epoch 68/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.1676 - val_accuracy: 0.9655\n","Epoch 69/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.1680 - val_accuracy: 0.9655\n","Epoch 70/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1683 - val_accuracy: 0.9655\n","Epoch 71/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9655\n","Epoch 72/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9655\n","Epoch 73/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1706 - val_accuracy: 0.9655\n","Epoch 74/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.1690 - val_accuracy: 0.9688\n","Epoch 75/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1705 - val_accuracy: 0.9666\n","Epoch 76/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9655\n","Epoch 77/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1693 - val_accuracy: 0.9655\n","Epoch 78/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9655\n","Epoch 79/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1697 - val_accuracy: 0.9655\n","Epoch 80/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9666\n","Epoch 81/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1708 - val_accuracy: 0.9666\n","Epoch 82/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1708 - val_accuracy: 0.9666\n","Epoch 83/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9666\n","Epoch 84/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1711 - val_accuracy: 0.9666\n","Epoch 85/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1715 - val_accuracy: 0.9666\n","Epoch 86/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9666\n","Epoch 87/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1710 - val_accuracy: 0.9655\n","Epoch 88/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1724 - val_accuracy: 0.9655\n","Epoch 89/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1717 - val_accuracy: 0.9666\n","Epoch 90/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9655\n","Epoch 91/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9666\n","Epoch 92/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9655\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9655\n","Epoch 94/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.1718 - val_accuracy: 0.9666\n","Epoch 95/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.1720 - val_accuracy: 0.9666\n","Epoch 96/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.1715 - val_accuracy: 0.9666\n","Epoch 97/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.1729 - val_accuracy: 0.9655\n","Epoch 98/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.1729 - val_accuracy: 0.9655\n","Epoch 99/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.1724 - val_accuracy: 0.9655\n","Epoch 100/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.1721 - val_accuracy: 0.9655\n","{'loss': [0.0647125393152237, 0.020544903352856636, 0.012244153767824173, 0.0128896813839674, 0.012902109883725643, 0.01496018748730421, 0.03290809318423271, 0.045919857919216156, 0.023118482902646065, 0.013450788334012032, 0.016826026141643524, 0.0349748358130455, 0.020935481414198875, 0.12432790547609329, 0.033677902072668076, 0.012108433060348034, 0.009503744542598724, 0.00904115941375494, 0.00885728094726801, 0.00882656965404749, 0.00875938218086958, 0.008710181340575218, 0.00869528017938137, 0.008640178479254246, 0.008616498671472073, 0.008593408390879631, 0.008588663302361965, 0.008542371913790703, 0.008509471081197262, 0.00848986767232418, 0.008465098217129707, 0.00844295509159565, 0.008423578925430775, 0.008395655080676079, 0.008392909541726112, 0.00837040226906538, 0.008358762599527836, 0.008326761424541473, 0.008302091620862484, 0.00828476995229721, 0.008268313482403755, 0.008258553221821785, 0.008240960538387299, 0.008220765739679337, 0.00818427000194788, 0.008170371875166893, 0.008143722079694271, 0.008132967166602612, 0.00810759887099266, 0.008098390884697437, 0.008082008920609951, 0.008063063025474548, 0.008045550435781479, 0.008023735135793686, 0.008009255863726139, 0.007989928126335144, 0.007978314533829689, 0.00796585064381361, 0.007937146350741386, 0.007920455187559128, 0.00789702869951725, 0.007878885604441166, 0.007860589772462845, 0.007847968488931656, 0.00782945565879345, 0.007816181518137455, 0.00779158528894186, 0.007771048694849014, 0.007754047401249409, 0.007739908527582884, 0.007725105155259371, 0.007698887027800083, 0.007678489666432142, 0.00766312750056386, 0.00764941843226552, 0.007630984298884869, 0.00762452557682991, 0.0075982664711773396, 0.007576459553092718, 0.0075628370977938175, 0.007543108891695738, 0.007525092922151089, 0.007513111922889948, 0.0074930815026164055, 0.0074749356135725975, 0.007460140623152256, 0.0074385046027600765, 0.007424228824675083, 0.007405416574329138, 0.007394542917609215, 0.007372315041720867, 0.007365833036601543, 0.007340702228248119, 0.0073243845254182816, 0.0073064169846475124, 0.00728834280744195, 0.0072711799293756485, 0.0072524831630289555, 0.007239743135869503, 0.007224452216178179], 'accuracy': [0.9835668206214905, 0.9973060488700867, 0.9989224076271057, 0.998383641242981, 0.998652994632721, 0.9975754022598267, 0.9913793206214905, 0.9867995977401733, 0.9946120977401733, 0.998383641242981, 0.9973060488700867, 0.9913793206214905, 0.9959590435028076, 0.9603987336158752, 0.993803858757019, 0.9991918206214905, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.6712783575057983, 0.6589266061782837, 0.6437714099884033, 0.6204537153244019, 0.586952269077301, 0.5462544560432434, 0.5349259376525879, 0.5269021987915039, 0.4834328293800354, 0.40309274196624756, 0.4038427472114563, 0.3646948039531708, 0.30046260356903076, 0.39659497141838074, 0.27570638060569763, 0.19279445707798004, 0.1618136465549469, 0.14544019103050232, 0.13862064480781555, 0.13613395392894745, 0.13349603116512299, 0.13662835955619812, 0.13722826540470123, 0.14016513526439667, 0.14161787927150726, 0.145067036151886, 0.14859813451766968, 0.1527128517627716, 0.15402796864509583, 0.15624073147773743, 0.15857867896556854, 0.1597151905298233, 0.16071557998657227, 0.1608394980430603, 0.16108308732509613, 0.1638859212398529, 0.16190746426582336, 0.16112470626831055, 0.16199874877929688, 0.1635696142911911, 0.16369380056858063, 0.16380539536476135, 0.16231827437877655, 0.1628231406211853, 0.16378340125083923, 0.16306176781654358, 0.16406658291816711, 0.1650409996509552, 0.16511577367782593, 0.1646709144115448, 0.16561827063560486, 0.16362351179122925, 0.16467046737670898, 0.16510602831840515, 0.16548572480678558, 0.1660798341035843, 0.16973857581615448, 0.16769465804100037, 0.1686551719903946, 0.16922299563884735, 0.1682300716638565, 0.16860580444335938, 0.16714031994342804, 0.1683371514081955, 0.16935716569423676, 0.16735225915908813, 0.16690020263195038, 0.1675785332918167, 0.1680426150560379, 0.168348491191864, 0.16945865750312805, 0.16985832154750824, 0.17060409486293793, 0.16895169019699097, 0.17046435177326202, 0.17140881717205048, 0.16931451857089996, 0.1695070117712021, 0.16973909735679626, 0.1706995815038681, 0.17078840732574463, 0.1707550585269928, 0.1706998497247696, 0.17111431062221527, 0.17146779596805573, 0.16990448534488678, 0.17098669707775116, 0.17236317694187164, 0.17174509167671204, 0.1721939593553543, 0.17219994962215424, 0.1706869751214981, 0.17137961089611053, 0.1717778742313385, 0.17203274369239807, 0.17148858308792114, 0.1728961318731308, 0.1728941649198532, 0.17242322862148285, 0.17212772369384766], 'val_accuracy': [0.7931034564971924, 0.8415948152542114, 0.8362069129943848, 0.8318965435028076, 0.826508641242981, 0.8221982717514038, 0.8340517282485962, 0.892241358757019, 0.9105603694915771, 0.9267241358757019, 0.8868534564971924, 0.9137930870056152, 0.9267241358757019, 0.8739224076271057, 0.9030172228813171, 0.9331896305084229, 0.9493534564971924, 0.951508641242981, 0.9579741358757019, 0.962284505367279, 0.9612069129943848, 0.9633620977401733, 0.9655172228813171, 0.9665948152542114, 0.96875, 0.96875, 0.9730603694915771, 0.9676724076271057, 0.9698275923728943, 0.9676724076271057, 0.9676724076271057, 0.9676724076271057, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9655172228813171, 0.9676724076271057, 0.9676724076271057, 0.9676724076271057, 0.9665948152542114, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171, 0.9644396305084229, 0.9665948152542114, 0.9676724076271057, 0.9655172228813171, 0.9676724076271057, 0.9655172228813171, 0.9665948152542114, 0.9676724076271057, 0.9676724076271057, 0.9633620977401733, 0.9676724076271057, 0.9665948152542114, 0.9644396305084229, 0.9655172228813171, 0.9644396305084229, 0.96875, 0.9655172228813171, 0.9655172228813171, 0.9665948152542114, 0.9676724076271057, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171, 0.96875, 0.9665948152542114, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9655172228813171, 0.9655172228813171, 0.9665948152542114, 0.9655172228813171, 0.9665948152542114, 0.9655172228813171, 0.9655172228813171, 0.9665948152542114, 0.9665948152542114, 0.9665948152542114, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171, 0.9655172228813171]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","25/28 [=========================>....] - ETA: 0s - loss: 0.0881 - accuracy: 0.9706"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 8s 58ms/step - loss: 0.0827 - accuracy: 0.9726 - val_loss: 0.6727 - val_accuracy: 0.8111\n","Epoch 2/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0183 - accuracy: 0.9966 - val_loss: 0.6648 - val_accuracy: 0.8043\n","Epoch 3/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.6492 - val_accuracy: 0.8235\n","Epoch 4/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 0.8213\n","Epoch 5/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.6060 - val_accuracy: 0.8303\n","Epoch 6/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 0.5751 - val_accuracy: 0.8326\n","Epoch 7/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.5385 - val_accuracy: 0.8462\n","Epoch 8/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.4972 - val_accuracy: 0.8507\n","Epoch 9/100\n","28/28 [==============================] - 2s 66ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.4524 - val_accuracy: 0.8631\n","Epoch 10/100\n","28/28 [==============================] - 2s 55ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.4056 - val_accuracy: 0.8654\n","Epoch 11/100\n","28/28 [==============================] - 1s 53ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3576 - val_accuracy: 0.8790\n","Epoch 12/100\n","28/28 [==============================] - 1s 47ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3121 - val_accuracy: 0.8959\n","Epoch 13/100\n","28/28 [==============================] - 1s 38ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 0.9129\n","Epoch 14/100\n","28/28 [==============================] - 1s 37ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2354 - val_accuracy: 0.9186\n","Epoch 15/100\n","28/28 [==============================] - 1s 42ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2064 - val_accuracy: 0.9242\n","Epoch 16/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1847 - val_accuracy: 0.9287\n","Epoch 17/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9389\n","Epoch 18/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9491\n","Epoch 19/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9491\n","Epoch 20/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9525\n","Epoch 21/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9525\n","Epoch 22/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9581\n","Epoch 23/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1674 - val_accuracy: 0.9581\n","Epoch 24/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1735 - val_accuracy: 0.9581\n","Epoch 25/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1806 - val_accuracy: 0.9593\n","Epoch 26/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1811 - val_accuracy: 0.9593\n","Epoch 27/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1837 - val_accuracy: 0.9615\n","Epoch 28/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1900 - val_accuracy: 0.9615\n","Epoch 29/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1920 - val_accuracy: 0.9615\n","Epoch 30/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1916 - val_accuracy: 0.9593\n","Epoch 31/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1926 - val_accuracy: 0.9604\n","Epoch 32/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.1948 - val_accuracy: 0.9604\n","Epoch 33/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.9593\n","Epoch 34/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.1998 - val_accuracy: 0.9593\n","Epoch 35/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2013 - val_accuracy: 0.9593\n","Epoch 36/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2013 - val_accuracy: 0.9604\n","Epoch 37/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2018 - val_accuracy: 0.9593\n","Epoch 38/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2030 - val_accuracy: 0.9593\n","Epoch 39/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2032 - val_accuracy: 0.9593\n","Epoch 40/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9593\n","Epoch 41/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9593\n","Epoch 42/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2047 - val_accuracy: 0.9581\n","Epoch 43/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2031 - val_accuracy: 0.9581\n","Epoch 44/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2067 - val_accuracy: 0.9593\n","Epoch 45/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2080 - val_accuracy: 0.9593\n","Epoch 46/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 0.9604\n","Epoch 47/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2044 - val_accuracy: 0.9581\n","Epoch 48/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2058 - val_accuracy: 0.9581\n","Epoch 49/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9581\n","Epoch 50/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2046 - val_accuracy: 0.9627\n","Epoch 51/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2074 - val_accuracy: 0.9593\n","Epoch 52/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2088 - val_accuracy: 0.9581\n","Epoch 53/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2092 - val_accuracy: 0.9593\n","Epoch 54/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2110 - val_accuracy: 0.9581\n","Epoch 55/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2094 - val_accuracy: 0.9570\n","Epoch 56/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2123 - val_accuracy: 0.9593\n","Epoch 57/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2101 - val_accuracy: 0.9604\n","Epoch 58/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2114 - val_accuracy: 0.9615\n","Epoch 59/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2112 - val_accuracy: 0.9581\n","Epoch 60/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2119 - val_accuracy: 0.9604\n","Epoch 61/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2117 - val_accuracy: 0.9615\n","Epoch 62/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2096 - val_accuracy: 0.9581\n","Epoch 63/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.9604\n","Epoch 64/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2115 - val_accuracy: 0.9604\n","Epoch 65/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2146 - val_accuracy: 0.9581\n","Epoch 66/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2139 - val_accuracy: 0.9593\n","Epoch 67/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 0.9593\n","Epoch 68/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2157 - val_accuracy: 0.9604\n","Epoch 69/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2175 - val_accuracy: 0.9604\n","Epoch 70/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2175 - val_accuracy: 0.9570\n","Epoch 71/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2185 - val_accuracy: 0.9570\n","Epoch 72/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2163 - val_accuracy: 0.9581\n","Epoch 73/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2232 - val_accuracy: 0.9581\n","Epoch 74/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2212 - val_accuracy: 0.9593\n","Epoch 75/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2225 - val_accuracy: 0.9581\n","Epoch 76/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2215 - val_accuracy: 0.9593\n","Epoch 77/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2312 - val_accuracy: 0.9559\n","Epoch 78/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2243 - val_accuracy: 0.9593\n","Epoch 79/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2231 - val_accuracy: 0.9593\n","Epoch 80/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2224 - val_accuracy: 0.9615\n","Epoch 81/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2235 - val_accuracy: 0.9593\n","Epoch 82/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2252 - val_accuracy: 0.9581\n","Epoch 83/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2233 - val_accuracy: 0.9604\n","Epoch 84/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2234 - val_accuracy: 0.9593\n","Epoch 85/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2222 - val_accuracy: 0.9604\n","Epoch 86/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2236 - val_accuracy: 0.9604\n","Epoch 87/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9604\n","Epoch 88/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2261 - val_accuracy: 0.9604\n","Epoch 89/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2261 - val_accuracy: 0.9604\n","Epoch 90/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2273 - val_accuracy: 0.9604\n","Epoch 91/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2275 - val_accuracy: 0.9615\n","Epoch 92/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2284 - val_accuracy: 0.9604\n","Epoch 93/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2289 - val_accuracy: 0.9581\n","Epoch 94/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2286 - val_accuracy: 0.9593\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2299 - val_accuracy: 0.9604\n","Epoch 96/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2328 - val_accuracy: 0.9581\n","Epoch 97/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9593\n","Epoch 98/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2297 - val_accuracy: 0.9627\n","Epoch 99/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2309 - val_accuracy: 0.9604\n","Epoch 100/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2327 - val_accuracy: 0.9593\n","{'loss': [0.08267174661159515, 0.01826946996152401, 0.01054295338690281, 0.009666791185736656, 0.009058061987161636, 0.00894903764128685, 0.008828237652778625, 0.008747047744691372, 0.00871885847300291, 0.008665638975799084, 0.008652165532112122, 0.008617282845079899, 0.008600686676800251, 0.008578285574913025, 0.00855339877307415, 0.008524708449840546, 0.008505969308316708, 0.008480293676257133, 0.008466210216283798, 0.008445942774415016, 0.00842040404677391, 0.008395204320549965, 0.008368405513465405, 0.008346020244061947, 0.008333126083016396, 0.008308094926178455, 0.008287560194730759, 0.008258352987468243, 0.008235253393650055, 0.008211622014641762, 0.008196249604225159, 0.008170058019459248, 0.008143666200339794, 0.00811509694904089, 0.008090617135167122, 0.00806934293359518, 0.008048128336668015, 0.008022408932447433, 0.008001810871064663, 0.007976540364325047, 0.007957201451063156, 0.007935427129268646, 0.00791868194937706, 0.007895097136497498, 0.007871960289776325, 0.007848436012864113, 0.007827086374163628, 0.007800687104463577, 0.007788558956235647, 0.007767901290208101, 0.00774884270504117, 0.007725989446043968, 0.007699268404394388, 0.007679508067667484, 0.007654554210603237, 0.007638405542820692, 0.0076174139976501465, 0.0076019954867661, 0.007580270525068045, 0.007561849430203438, 0.007541326340287924, 0.0075178686529397964, 0.007499748840928078, 0.007477140054106712, 0.007456635590642691, 0.007435773499310017, 0.00742166955024004, 0.007397620938718319, 0.00738050602376461, 0.007366534322500229, 0.0073477416299283504, 0.007332010194659233, 0.007311485707759857, 0.007288913242518902, 0.007270209491252899, 0.007251235190778971, 0.00723754707723856, 0.007217146921902895, 0.0071967896074056625, 0.007178981322795153, 0.007161020301282406, 0.007145365234464407, 0.007126334123313427, 0.0071095675230026245, 0.007091055158525705, 0.007073129527270794, 0.007056372705847025, 0.0070402841083705425, 0.007022360805422068, 0.007007656618952751, 0.006989384535700083, 0.006972065195441246, 0.006955214310437441, 0.006939226761460304, 0.006923891603946686, 0.006906696129590273, 0.006890674587339163, 0.0068762949667871, 0.006859112996608019, 0.006842437665909529], 'accuracy': [0.9725523591041565, 0.9966044425964355, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.6726990938186646, 0.6648079752922058, 0.6492004990577698, 0.6289288997650146, 0.605998158454895, 0.5750506520271301, 0.5384536385536194, 0.4972456395626068, 0.45240849256515503, 0.4055890738964081, 0.3575848639011383, 0.31207311153411865, 0.27089792490005493, 0.2353508174419403, 0.20642884075641632, 0.184656023979187, 0.16991108655929565, 0.15969115495681763, 0.1558620035648346, 0.1553739607334137, 0.15727077424526215, 0.16037727892398834, 0.16735437512397766, 0.17345896363258362, 0.18060573935508728, 0.18114778399467468, 0.18373753130435944, 0.1900002658367157, 0.19202600419521332, 0.1916133463382721, 0.19262348115444183, 0.194828063249588, 0.20047004520893097, 0.19976966083049774, 0.20125339925289154, 0.20133037865161896, 0.2017620950937271, 0.2029772698879242, 0.203158438205719, 0.1993071287870407, 0.20186340808868408, 0.20466461777687073, 0.20308567583560944, 0.2067243456840515, 0.2080037146806717, 0.20845802128314972, 0.2044125497341156, 0.20578789710998535, 0.20551693439483643, 0.2046164572238922, 0.2074008584022522, 0.20875926315784454, 0.2092433124780655, 0.21095266938209534, 0.20939841866493225, 0.2122713178396225, 0.21008999645709991, 0.21136781573295593, 0.21122334897518158, 0.21187900006771088, 0.21167899668216705, 0.20964673161506653, 0.21125590801239014, 0.21149186789989471, 0.21463723480701447, 0.2138826847076416, 0.21660014986991882, 0.2157297432422638, 0.21750414371490479, 0.21753792464733124, 0.21850967407226562, 0.2162969410419464, 0.22324661910533905, 0.22123143076896667, 0.2225489765405655, 0.22147683799266815, 0.23116236925125122, 0.2242884635925293, 0.22310706973075867, 0.22235040366649628, 0.22352983057498932, 0.2251814901828766, 0.22326312959194183, 0.22341573238372803, 0.22221405804157257, 0.22360201179981232, 0.22462408244609833, 0.2261144369840622, 0.22607216238975525, 0.22726692259311676, 0.22749729454517365, 0.22839756309986115, 0.22887365520000458, 0.2285819947719574, 0.2298865169286728, 0.23282942175865173, 0.23261292278766632, 0.22971075773239136, 0.23087356984615326, 0.23267517983913422], 'val_accuracy': [0.8110859990119934, 0.8042986392974854, 0.8235294222831726, 0.8212669491767883, 0.8303167223930359, 0.8325791954994202, 0.8461538553237915, 0.8506787419319153, 0.8631221652030945, 0.8653846383094788, 0.8789592981338501, 0.8959276080131531, 0.912895917892456, 0.918552041053772, 0.9242081642150879, 0.9287330508232117, 0.9389140009880066, 0.9490950107574463, 0.9490950107574463, 0.9524886608123779, 0.9524886608123779, 0.9581447839736938, 0.9581447839736938, 0.9581447839736938, 0.959276020526886, 0.959276020526886, 0.9615384340286255, 0.9615384340286255, 0.9615384340286255, 0.959276020526886, 0.9604072570800781, 0.9604072570800781, 0.959276020526886, 0.959276020526886, 0.959276020526886, 0.9604072570800781, 0.959276020526886, 0.959276020526886, 0.959276020526886, 0.959276020526886, 0.959276020526886, 0.9581447839736938, 0.9581447839736938, 0.959276020526886, 0.959276020526886, 0.9604072570800781, 0.9581447839736938, 0.9581447839736938, 0.9581447839736938, 0.9626696705818176, 0.959276020526886, 0.9581447839736938, 0.959276020526886, 0.9581447839736938, 0.9570135474205017, 0.959276020526886, 0.9604072570800781, 0.9615384340286255, 0.9581447839736938, 0.9604072570800781, 0.9615384340286255, 0.9581447839736938, 0.9604072570800781, 0.9604072570800781, 0.9581447839736938, 0.959276020526886, 0.959276020526886, 0.9604072570800781, 0.9604072570800781, 0.9570135474205017, 0.9570135474205017, 0.9581447839736938, 0.9581447839736938, 0.959276020526886, 0.9581447839736938, 0.959276020526886, 0.9558823704719543, 0.959276020526886, 0.959276020526886, 0.9615384340286255, 0.959276020526886, 0.9581447839736938, 0.9604072570800781, 0.959276020526886, 0.9604072570800781, 0.9604072570800781, 0.9604072570800781, 0.9604072570800781, 0.9604072570800781, 0.9604072570800781, 0.9615384340286255, 0.9604072570800781, 0.9581447839736938, 0.959276020526886, 0.9604072570800781, 0.9581447839736938, 0.959276020526886, 0.9626696705818176, 0.9604072570800781, 0.959276020526886]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 256)            1311744   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1, 256)            525312    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 4296257 (16.39 MB)\n","Trainable params: 4295745 (16.39 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/31 [===========================>..] - ETA: 0s - loss: 0.0809 - accuracy: 0.9755"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 8s 80ms/step - loss: 0.0784 - accuracy: 0.9762 - val_loss: 0.6713 - val_accuracy: 0.7634\n","Epoch 2/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.0173 - accuracy: 0.9984 - val_loss: 0.6583 - val_accuracy: 0.8244\n","Epoch 3/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.6399 - val_accuracy: 0.8202\n","Epoch 4/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 0.8419\n","Epoch 5/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.5858 - val_accuracy: 0.8182\n","Epoch 6/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.5463 - val_accuracy: 0.8564\n","Epoch 7/100\n","31/31 [==============================] - 3s 116ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.5026 - val_accuracy: 0.8688\n","Epoch 8/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.4549 - val_accuracy: 0.8791\n","Epoch 9/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.4039 - val_accuracy: 0.8905\n","Epoch 10/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3530 - val_accuracy: 0.8977\n","Epoch 11/100\n","31/31 [==============================] - 1s 48ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3052 - val_accuracy: 0.9143\n","Epoch 12/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2623 - val_accuracy: 0.9246\n","Epoch 13/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.2291 - val_accuracy: 0.9256\n","Epoch 14/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.2047 - val_accuracy: 0.9329\n","Epoch 15/100\n","31/31 [==============================] - 1s 36ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1890 - val_accuracy: 0.9360\n","Epoch 16/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1796 - val_accuracy: 0.9401\n","Epoch 17/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1788 - val_accuracy: 0.9432\n","Epoch 18/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1802 - val_accuracy: 0.9432\n","Epoch 19/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1831 - val_accuracy: 0.9452\n","Epoch 20/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.1866 - val_accuracy: 0.9411\n","Epoch 21/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1923 - val_accuracy: 0.9442\n","Epoch 22/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.1956 - val_accuracy: 0.9432\n","Epoch 23/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9473\n","Epoch 24/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9432\n","Epoch 25/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2097 - val_accuracy: 0.9463\n","Epoch 26/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2141 - val_accuracy: 0.9483\n","Epoch 27/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2202 - val_accuracy: 0.9473\n","Epoch 28/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2229 - val_accuracy: 0.9483\n","Epoch 29/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2299 - val_accuracy: 0.9483\n","Epoch 30/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2271 - val_accuracy: 0.9514\n","Epoch 31/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9483\n","Epoch 32/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.2324 - val_accuracy: 0.9494\n","Epoch 33/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2306 - val_accuracy: 0.9483\n","Epoch 34/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2315 - val_accuracy: 0.9483\n","Epoch 35/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2357 - val_accuracy: 0.9463\n","Epoch 36/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.2331 - val_accuracy: 0.9473\n","Epoch 37/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2345 - val_accuracy: 0.9473\n","Epoch 38/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2365 - val_accuracy: 0.9494\n","Epoch 39/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2392 - val_accuracy: 0.9504\n","Epoch 40/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 0.9463\n","Epoch 41/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2356 - val_accuracy: 0.9452\n","Epoch 42/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9504\n","Epoch 43/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2384 - val_accuracy: 0.9504\n","Epoch 44/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.2416 - val_accuracy: 0.9483\n","Epoch 45/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2404 - val_accuracy: 0.9463\n","Epoch 46/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2403 - val_accuracy: 0.9473\n","Epoch 47/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2474 - val_accuracy: 0.9442\n","Epoch 48/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.2416 - val_accuracy: 0.9483\n","Epoch 49/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2458 - val_accuracy: 0.9463\n","Epoch 50/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2369 - val_accuracy: 0.9483\n","Epoch 51/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2468 - val_accuracy: 0.9473\n","Epoch 52/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2454 - val_accuracy: 0.9494\n","Epoch 53/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.2497 - val_accuracy: 0.9463\n","Epoch 54/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2516 - val_accuracy: 0.9421\n","Epoch 55/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2512 - val_accuracy: 0.9463\n","Epoch 56/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2483 - val_accuracy: 0.9463\n","Epoch 57/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9473\n","Epoch 58/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2548 - val_accuracy: 0.9442\n","Epoch 59/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2514 - val_accuracy: 0.9463\n","Epoch 60/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2564 - val_accuracy: 0.9463\n","Epoch 61/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2547 - val_accuracy: 0.9432\n","Epoch 62/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.2544 - val_accuracy: 0.9452\n","Epoch 63/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2555 - val_accuracy: 0.9463\n","Epoch 64/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2623 - val_accuracy: 0.9432\n","Epoch 65/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2558 - val_accuracy: 0.9463\n","Epoch 66/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.2587 - val_accuracy: 0.9432\n","Epoch 67/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2583 - val_accuracy: 0.9432\n","Epoch 68/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2583 - val_accuracy: 0.9473\n","Epoch 69/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.9432\n","Epoch 70/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2678 - val_accuracy: 0.9432\n","Epoch 71/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 0.9432\n","Epoch 72/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2622 - val_accuracy: 0.9463\n","Epoch 73/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2679 - val_accuracy: 0.9452\n","Epoch 74/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2708 - val_accuracy: 0.9452\n","Epoch 75/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2688 - val_accuracy: 0.9452\n","Epoch 76/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.2675 - val_accuracy: 0.9463\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2665 - val_accuracy: 0.9463\n","Epoch 78/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2666 - val_accuracy: 0.9442\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2741 - val_accuracy: 0.9432\n","Epoch 80/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2816 - val_accuracy: 0.9432\n","Epoch 81/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.2741 - val_accuracy: 0.9421\n","Epoch 82/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2742 - val_accuracy: 0.9442\n","Epoch 83/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2803 - val_accuracy: 0.9432\n","Epoch 84/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2737 - val_accuracy: 0.9432\n","Epoch 85/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2774 - val_accuracy: 0.9421\n","Epoch 86/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2839 - val_accuracy: 0.9452\n","Epoch 87/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.2828 - val_accuracy: 0.9463\n","Epoch 88/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2751 - val_accuracy: 0.9473\n","Epoch 89/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2750 - val_accuracy: 0.9452\n","Epoch 90/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2782 - val_accuracy: 0.9452\n","Epoch 91/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2792 - val_accuracy: 0.9452\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2868 - val_accuracy: 0.9432\n","Epoch 93/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2850 - val_accuracy: 0.9442\n","Epoch 94/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.2891 - val_accuracy: 0.9452\n","Epoch 95/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.2788 - val_accuracy: 0.9432\n","Epoch 96/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.3134 - val_accuracy: 0.9421\n","Epoch 97/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.2898 - val_accuracy: 0.9421\n","Epoch 98/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.2869 - val_accuracy: 0.9411\n","Epoch 99/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.2998 - val_accuracy: 0.9421\n","Epoch 100/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.2982 - val_accuracy: 0.9421\n","{'loss': [0.07838515937328339, 0.017348874360322952, 0.010485400445759296, 0.009847406297922134, 0.009189475327730179, 0.009051472879946232, 0.008764779195189476, 0.008695699274539948, 0.008672444149851799, 0.008634165860712528, 0.008607747964560986, 0.008585053496062756, 0.008577834814786911, 0.008527803234755993, 0.008492141962051392, 0.008483964018523693, 0.008455660194158554, 0.008426866494119167, 0.008391158655285835, 0.00836806371808052, 0.00833875872194767, 0.008322124369442463, 0.008295165374875069, 0.008264065720140934, 0.008240490220487118, 0.008212369866669178, 0.008184223435819149, 0.008159469813108444, 0.008140912279486656, 0.008115708827972412, 0.008082255721092224, 0.00805612187832594, 0.008037442341446877, 0.008008196018636227, 0.007976497523486614, 0.00796298123896122, 0.00792799424380064, 0.007915519177913666, 0.00788295641541481, 0.007858983241021633, 0.00784175656735897, 0.007809594739228487, 0.007786733563989401, 0.007757387589663267, 0.007734901271760464, 0.007718827109783888, 0.007689427584409714, 0.007667842321097851, 0.007643396966159344, 0.007627986837178469, 0.007595744915306568, 0.007568694651126862, 0.007562011480331421, 0.007529743015766144, 0.007503349334001541, 0.007486568298190832, 0.0074610658921301365, 0.007436156272888184, 0.0074173398315906525, 0.0073959664441645145, 0.007370464038103819, 0.007354373577982187, 0.007333618588745594, 0.007305216509848833, 0.007286162115633488, 0.007264773361384869, 0.00724723981693387, 0.007223164662718773, 0.007204543333500624, 0.007181215099990368, 0.00715941097587347, 0.007145650684833527, 0.00712248682975769, 0.007100709248334169, 0.007080584764480591, 0.007065511774271727, 0.007044076453894377, 0.00702319759875536, 0.007002958096563816, 0.006985833868384361, 0.006966454442590475, 0.00694901030510664, 0.006929812487214804, 0.006911635864526033, 0.006892540957778692, 0.006893805228173733, 0.006863597314804792, 0.006847508251667023, 0.006823813077062368, 0.006808343809098005, 0.006787927821278572, 0.006769469473510981, 0.006753094960004091, 0.006735615432262421, 0.006741329561918974, 0.0067223673686385155, 0.006694897077977657, 0.006675977259874344, 0.006656275596469641, 0.006640281993895769], 'accuracy': [0.9762274026870728, 0.9984496235847473, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.6713119149208069, 0.6583213210105896, 0.6398698091506958, 0.6161008477210999, 0.5857641696929932, 0.5462637543678284, 0.5025657415390015, 0.45490410923957825, 0.4038681089878082, 0.3529871106147766, 0.3052434027194977, 0.2623351514339447, 0.22912520170211792, 0.20472650229930878, 0.18902204930782318, 0.17961329221725464, 0.17880211770534515, 0.18018820881843567, 0.1831037700176239, 0.1865798681974411, 0.19227910041809082, 0.19564150273799896, 0.2019123136997223, 0.20585614442825317, 0.20967547595500946, 0.21410760283470154, 0.2201603651046753, 0.2228534072637558, 0.2299477905035019, 0.22707268595695496, 0.22740721702575684, 0.23240958154201508, 0.23062604665756226, 0.23154416680335999, 0.23566003143787384, 0.23310093581676483, 0.23451749980449677, 0.23648007214069366, 0.23919138312339783, 0.2373441755771637, 0.23562656342983246, 0.24005341529846191, 0.2384003847837448, 0.24163049459457397, 0.24036915600299835, 0.2403017282485962, 0.24738673865795135, 0.24160155653953552, 0.24577397108078003, 0.2369011491537094, 0.24679461121559143, 0.24537305533885956, 0.24974501132965088, 0.25159183144569397, 0.25124919414520264, 0.24831412732601166, 0.25025683641433716, 0.2548442482948303, 0.2513746917247772, 0.25635915994644165, 0.2546883225440979, 0.2543891668319702, 0.25553587079048157, 0.26231294870376587, 0.2557690739631653, 0.2586674392223358, 0.25830480456352234, 0.2582930028438568, 0.2611831724643707, 0.2677697539329529, 0.26554885506629944, 0.2622329294681549, 0.26791101694107056, 0.27080050110816956, 0.2688447833061218, 0.2674567997455597, 0.2664937973022461, 0.26664096117019653, 0.274122029542923, 0.2815949618816376, 0.2740626633167267, 0.27418026328086853, 0.2803443372249603, 0.27374333143234253, 0.277370810508728, 0.2839052081108093, 0.2828061580657959, 0.2750665247440338, 0.27495715022087097, 0.278226375579834, 0.2791892886161804, 0.28684166073799133, 0.28502893447875977, 0.28908079862594604, 0.27877965569496155, 0.3134212791919708, 0.2898063063621521, 0.2868529260158539, 0.2998461425304413, 0.29817861318588257], 'val_accuracy': [0.7634297609329224, 0.8243801593780518, 0.8202479481697083, 0.8419421315193176, 0.8181818127632141, 0.8564049601554871, 0.8688016533851624, 0.8791322112083435, 0.8904958963394165, 0.8977272510528564, 0.91425621509552, 0.9245867729187012, 0.9256198406219482, 0.932851254940033, 0.9359503984451294, 0.9400826692581177, 0.9431818127632141, 0.9431818127632141, 0.9452479481697083, 0.94111567735672, 0.9442148804664612, 0.9431818127632141, 0.9473140239715576, 0.9431818127632141, 0.9462810158729553, 0.9483470916748047, 0.9473140239715576, 0.9483470916748047, 0.9483470916748047, 0.9514462947845459, 0.9483470916748047, 0.9493801593780518, 0.9483470916748047, 0.9483470916748047, 0.9462810158729553, 0.9473140239715576, 0.9473140239715576, 0.9493801593780518, 0.9504132270812988, 0.9462810158729553, 0.9452479481697083, 0.9504132270812988, 0.9504132270812988, 0.9483470916748047, 0.9462810158729553, 0.9473140239715576, 0.9442148804664612, 0.9483470916748047, 0.9462810158729553, 0.9483470916748047, 0.9473140239715576, 0.9493801593780518, 0.9462810158729553, 0.942148745059967, 0.9462810158729553, 0.9462810158729553, 0.9473140239715576, 0.9442148804664612, 0.9462810158729553, 0.9462810158729553, 0.9431818127632141, 0.9452479481697083, 0.9462810158729553, 0.9431818127632141, 0.9462810158729553, 0.9431818127632141, 0.9431818127632141, 0.9473140239715576, 0.9431818127632141, 0.9431818127632141, 0.9431818127632141, 0.9462810158729553, 0.9452479481697083, 0.9452479481697083, 0.9452479481697083, 0.9462810158729553, 0.9462810158729553, 0.9442148804664612, 0.9431818127632141, 0.9431818127632141, 0.942148745059967, 0.9442148804664612, 0.9431818127632141, 0.9431818127632141, 0.942148745059967, 0.9452479481697083, 0.9462810158729553, 0.9473140239715576, 0.9452479481697083, 0.9452479481697083, 0.9452479481697083, 0.9431818127632141, 0.9442148804664612, 0.9452479481697083, 0.9431818127632141, 0.942148745059967, 0.942148745059967, 0.94111567735672, 0.942148745059967, 0.942148745059967]}\n","32/32 [==============================] - 1s 5ms/step\n"]}],"source":["global_model, metrics_df = federated_learning(Theta_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ik5JoVP2NPvY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717401990262,"user_tz":-360,"elapsed":33,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"}},"outputId":"fb3868c3-b6c5-4c84-946d-fcb1e1da2206"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Client  Accuracy  Precision  Recall     F1  Sensitivity  Specificity  \\\n","0        0     0.760      0.739   0.806  0.771        0.806        0.715   \n","1        1     0.802      0.812   0.787  0.799        0.787        0.818   \n","2        2     0.728      0.707   0.779  0.741        0.779        0.677   \n","3        0     0.822      0.820   0.824  0.822        0.824        0.819   \n","4        1     0.841      0.827   0.863  0.845        0.863        0.819   \n","5        2     0.786      0.758   0.841  0.797        0.841        0.731   \n","6        0     0.868      0.862   0.876  0.869        0.876        0.859   \n","7        1     0.882      0.862   0.910  0.885        0.910        0.855   \n","8        2     0.861      0.827   0.914  0.868        0.914        0.809   \n","9        0     0.905      0.886   0.928  0.907        0.928        0.881   \n","10       1     0.914      0.904   0.927  0.915        0.927        0.901   \n","11       2     0.878      0.833   0.944  0.885        0.944        0.811   \n","12       0     0.912      0.887   0.945  0.915        0.945        0.879   \n","13       1     0.932      0.919   0.948  0.933        0.948        0.917   \n","14       2     0.897      0.855   0.956  0.902        0.956        0.837   \n","\n","    Kappa  \n","0   0.521  \n","1   0.605  \n","2   0.456  \n","3   0.643  \n","4   0.682  \n","5   0.572  \n","6   0.735  \n","7   0.764  \n","8   0.723  \n","9   0.809  \n","10  0.828  \n","11  0.755  \n","12  0.824  \n","13  0.864  \n","14  0.793  "],"text/html":["\n","  <div id=\"df-702d23b3-b3db-450f-ad95-420a63706fe1\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Client</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Sensitivity</th>\n","      <th>Specificity</th>\n","      <th>Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.760</td>\n","      <td>0.739</td>\n","      <td>0.806</td>\n","      <td>0.771</td>\n","      <td>0.806</td>\n","      <td>0.715</td>\n","      <td>0.521</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.802</td>\n","      <td>0.812</td>\n","      <td>0.787</td>\n","      <td>0.799</td>\n","      <td>0.787</td>\n","      <td>0.818</td>\n","      <td>0.605</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.728</td>\n","      <td>0.707</td>\n","      <td>0.779</td>\n","      <td>0.741</td>\n","      <td>0.779</td>\n","      <td>0.677</td>\n","      <td>0.456</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.822</td>\n","      <td>0.820</td>\n","      <td>0.824</td>\n","      <td>0.822</td>\n","      <td>0.824</td>\n","      <td>0.819</td>\n","      <td>0.643</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.841</td>\n","      <td>0.827</td>\n","      <td>0.863</td>\n","      <td>0.845</td>\n","      <td>0.863</td>\n","      <td>0.819</td>\n","      <td>0.682</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2</td>\n","      <td>0.786</td>\n","      <td>0.758</td>\n","      <td>0.841</td>\n","      <td>0.797</td>\n","      <td>0.841</td>\n","      <td>0.731</td>\n","      <td>0.572</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0.868</td>\n","      <td>0.862</td>\n","      <td>0.876</td>\n","      <td>0.869</td>\n","      <td>0.876</td>\n","      <td>0.859</td>\n","      <td>0.735</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0.882</td>\n","      <td>0.862</td>\n","      <td>0.910</td>\n","      <td>0.885</td>\n","      <td>0.910</td>\n","      <td>0.855</td>\n","      <td>0.764</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>0.861</td>\n","      <td>0.827</td>\n","      <td>0.914</td>\n","      <td>0.868</td>\n","      <td>0.914</td>\n","      <td>0.809</td>\n","      <td>0.723</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>0.905</td>\n","      <td>0.886</td>\n","      <td>0.928</td>\n","      <td>0.907</td>\n","      <td>0.928</td>\n","      <td>0.881</td>\n","      <td>0.809</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0.914</td>\n","      <td>0.904</td>\n","      <td>0.927</td>\n","      <td>0.915</td>\n","      <td>0.927</td>\n","      <td>0.901</td>\n","      <td>0.828</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2</td>\n","      <td>0.878</td>\n","      <td>0.833</td>\n","      <td>0.944</td>\n","      <td>0.885</td>\n","      <td>0.944</td>\n","      <td>0.811</td>\n","      <td>0.755</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>0.912</td>\n","      <td>0.887</td>\n","      <td>0.945</td>\n","      <td>0.915</td>\n","      <td>0.945</td>\n","      <td>0.879</td>\n","      <td>0.824</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0.932</td>\n","      <td>0.919</td>\n","      <td>0.948</td>\n","      <td>0.933</td>\n","      <td>0.948</td>\n","      <td>0.917</td>\n","      <td>0.864</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2</td>\n","      <td>0.897</td>\n","      <td>0.855</td>\n","      <td>0.956</td>\n","      <td>0.902</td>\n","      <td>0.956</td>\n","      <td>0.837</td>\n","      <td>0.793</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-702d23b3-b3db-450f-ad95-420a63706fe1')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-702d23b3-b3db-450f-ad95-420a63706fe1 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-702d23b3-b3db-450f-ad95-420a63706fe1');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c19d2cab-b319-4579-abae-3daacb52116e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c19d2cab-b319-4579-abae-3daacb52116e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c19d2cab-b319-4579-abae-3daacb52116e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"metrics_df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Client\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06112267134526234,\n        \"min\": 0.728,\n        \"max\": 0.932,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.905,\n          0.878,\n          0.76\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.060560713337938825,\n        \"min\": 0.707,\n        \"max\": 0.919,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.919,\n          0.833,\n          0.739\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0623494758827782,\n        \"min\": 0.779,\n        \"max\": 0.956,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.928,\n          0.944,\n          0.806\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.058582623784905793,\n        \"min\": 0.741,\n        \"max\": 0.933,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          0.933,\n          0.907,\n          0.771\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sensitivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0623494758827782,\n        \"min\": 0.779,\n        \"max\": 0.956,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.928,\n          0.944,\n          0.806\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Specificity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06853452379918425,\n        \"min\": 0.677,\n        \"max\": 0.917,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.901,\n          0.879,\n          0.715\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kappa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12196455378901029,\n        \"min\": 0.456,\n        \"max\": 0.864,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.809,\n          0.755,\n          0.521\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":27}],"source":["\n","metrics_df.round(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ncf_cMAQF6g4"},"outputs":[],"source":["metrics_df.round(4).to_csv('/content/drive/MyDrive/EEG Signal /results/FLresults/Time Domain/CNN_LSTM/Time_domain_CNN_LSTM.csv', index = False)"]},{"cell_type":"markdown","source":["# Graph and save data\n"],"metadata":{"id":"zmVe4Ef2tIdE"}},{"cell_type":"code","source":["import wandb\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import seaborn as sns\n","\n","# Initialize the API\n","api = wandb.Api()\n","\n","# Specify the entity and project\n","entity = \"raihanrabby\"\n","project = \"Beta_time_domain_CNN_Lstm\"\n","\n","# Fetch all runs from the project\n","runs = api.runs(f\"{entity}/{project}\")\n","\n","# List to store the dataframes\n","dataframes = []\n","\n","# Iterate over each run and fetch the history\n","for run in runs:\n","    # Fetch the history for each run\n","    history = run.history()\n","\n","    # Add columns to identify the run, model name, epoch, and client\n","    history['run_id'] = run.id\n","    history['model_name'] = run.name\n","\n","    # Extract epoch and client from model name\n","    match = re.match(r'epoch_(\\d+)_client_(\\d+)', run.name)\n","    if match:\n","        history['epoch_number'] = int(match.group(1))\n","        history['client_number'] = int(match.group(2))\n","    else:\n","        history['epoch_number'] = None\n","        history['client_number'] = None\n","\n","    # Append to the list of dataframes\n","    dataframes.append(history)\n","\n","# Concatenate all dataframes into a single dataframe\n","all_metrics_df = pd.concat(dataframes)\n","\n","# Filter out rows with None epoch_number\n","all_metrics_df = all_metrics_df.dropna(subset=['epoch_number'])\n","\n","# Get the unique epochs\n","unique_epochs = sorted(all_metrics_df['epoch_number'].unique())\n","\n","# Set the Seaborn style\n","sns.set(style=\"whitegrid\")\n","\n","# Create subplots for each epoch\n","fig, axes = plt.subplots(2, len(unique_epochs), figsize=(len(unique_epochs) * 3, 4.5), sharex=True)\n","\n","# Set the color palette\n","palette = sns.color_palette(\"Set1\", n_colors=all_metrics_df['client_number'].nunique())\n","\n","# Store the lines and labels for the legend\n","lines = []\n","labels = []\n","\n","# Iterate through each epoch and plot the training and validation accuracy\n","for i, epoch in enumerate(unique_epochs):\n","    epoch_df = all_metrics_df[all_metrics_df['epoch_number'] == epoch]\n","    for j, client in enumerate(epoch_df['client_number'].unique()):\n","        client_df = epoch_df[epoch_df['client_number'] == client]\n","        line, = axes[0, i].plot(client_df['accuracy'], label=f'Client {client}', alpha=0.6, color=palette[j], linewidth=2.0)\n","        axes[1, i].plot(client_df['val_accuracy'], label=f'Client {client}', alpha=0.6, color=palette[j], linewidth=2.0)\n","\n","        if i == 0:\n","            lines.append(line)\n","            labels.append(f'Client {client}')\n","\n","    axes[0, i].set_title(f'Epoch {epoch}', fontsize=12, fontweight='bold')\n","    axes[0, i].set_xlim(0, 100)  # Ensure x-axis covers exactly 100 steps\n","    axes[1, i].set_xlim(0, 100)  # Ensure x-axis covers exactly 100 steps\n","    # axes[0, i].set_ylim(0.5)  # Ensure y-axis covers full range of accuracy\n","    axes[1, i].set_ylim(0.5, 1.0)  # Ensure y-axis covers full range of accuracy\n","    axes[1, i].set_xlabel('Steps', fontsize=10, fontweight='bold')\n","    axes[0, i].grid(True)\n","    axes[1, i].grid(True)\n","    axes[0, i].tick_params(axis='both', which='major', labelsize=10)\n","    axes[1, i].tick_params(axis='both', which='major', labelsize=10)\n","\n","# Add a single legend for the entire figure\n","fig.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, 1.0), ncol=len(labels), fontsize=10, frameon=True)\n","\n","# Add row labels\n","fig.text(0.04, 0.75, 'Training Accuracy', ha='center', va='center', rotation='vertical', fontsize=12, fontweight='bold')\n","fig.text(0.04, 0.25, 'Validation Accuracy', ha='center', va='center', rotation='vertical', fontsize=12, fontweight='bold')\n","\n","plt.tight_layout(rect=[0.05, 0, 1, 0.95])\n","plt.subplots_adjust(hspace=0.2, top=0.88)  # Add some space between the rows and reduce space between the legend and plot\n","plt.show()\n"],"metadata":{"id":"JDgHE01jtMzw","executionInfo":{"status":"ok","timestamp":1717394802749,"user_tz":-360,"elapsed":11366,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"}},"outputId":"04953dbd-e6b9-4fd7-f131-c1690b314e4e","colab":{"base_uri":"https://localhost:8080/","height":462}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1500x450 with 10 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"QUuWzfMHSNmi"},"source":["# CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8xrx_fxBSWGd"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, LSTM, Dropout\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","import tensorflow_addons as tfa\n","\n","class FederatedData:\n","    def __init__(self, data_path, num_clients, scaler_type='MinMax', test_size=0.2):\n","        self.data_path = data_path\n","        self.num_clients = num_clients\n","        self.scaler_type = scaler_type\n","        self.test_size = test_size\n","        self.partitions = []  # Initialize partitions attribute\n","        self.load_data()\n","        self.scale_data()\n","        self.create_partitions()\n","\n","    def load_data(self):\n","        try:\n","            data = np.load(self.data_path, allow_pickle=True)\n","            self.X = np.moveaxis(data['X'], 1, 2)  # Move axis here\n","            self.Y = data['Y']\n","            self.groups = data['Group']  # Assuming 'Group' contains group identifiers\n","        except KeyError as e:\n","            raise ValueError(f\"Missing expected data field: {e}\")\n","        except FileNotFoundError as e:\n","            raise ValueError(f\"Data file not found: {e}\")\n","\n","    def scale_data(self):\n","        # Reshape data to 2D array for scaling\n","        X_reshaped = self.X.reshape(-1, self.X.shape[-1])\n","\n","        # Select scaler based on input\n","        if self.scaler_type == 'Standard':\n","            scaler = StandardScaler()\n","        elif self.scaler_type == 'MinMax':\n","            scaler = MinMaxScaler(feature_range=(0, 1))\n","        else:\n","            raise ValueError(\"Unsupported scaler type. Choose either 'Standard' or 'MinMax'.\")\n","\n","        # Fit and transform the data\n","        scaled_data_reshaped = scaler.fit_transform(X_reshaped)\n","\n","        # Reshape back to original shape\n","        self.X = scaled_data_reshaped.reshape(self.X.shape)\n","\n","    def create_partitions(self):\n","        gss = GroupShuffleSplit(n_splits=self.num_clients, test_size=self.test_size, random_state=42)\n","        for train_index, test_index in gss.split(self.X, self.Y, self.groups):\n","            X_train, X_test = self.X[train_index], self.X[test_index]\n","            Y_train, Y_test = self.Y[train_index], self.Y[test_index]\n","            self.partitions.append((X_train, Y_train, X_test, Y_test))\n","            print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n","\n","    def get_training_and_validation_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        partition_X_train, partition_Y_train, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        X_train, X_val, Y_train, Y_val = train_test_split(partition_X_train, partition_Y_train, test_size=0.2, random_state=42)\n","        return X_train, X_val, Y_train, Y_val, partition_X_test, partition_Y_test\n","\n","    def get_testing_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        _, _, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        return partition_X_test, partition_Y_test\n","\n","def build_sequential_model(input_shape):\n","    clear_session()\n","    model = Sequential()\n","\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\", input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(512, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(filters=1024, kernel_size=3, strides=1, activation='relu'))\n","    # model.add(Dropout(0.5))\n","    # model.add(LSTM(256, return_sequences=True))\n","    # model.add(LSTM(256, return_sequences=True))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(256, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n","\n","    # opt = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","    opt = tfa.optimizers.RectifiedAdam(learning_rate=0.00009)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = recall\n","\n","    return accuracy, precision, recall, f1, sensitivity, specificity, kappa\n","\n","def federated_learning(data_path):\n","    federated_data = FederatedData(data_path, num_clients=3, scaler_type='MinMax')\n","    federated_data.create_partitions()\n","\n","    # Get the input shape from the data\n","    input_shape = federated_data.X.shape[1:]\n","    global_model = build_sequential_model(input_shape)\n","\n","    num_clients = 3\n","    local_epochs = 5\n","    global_optimizer = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","\n","    # Initialize m and v for Adam optimizer\n","    m = [np.zeros_like(w) for w in global_model.get_weights()]\n","    v = [np.zeros_like(w) for w in global_model.get_weights()]\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 1e-7\n","    t = 0\n","\n","    client_data = []\n","    for client_idx in range(num_clients):\n","        x_train, x_val, y_train, y_val, x_test, y_test = federated_data.get_training_and_validation_data(client_idx)\n","        client_data.append((x_train, x_val, y_train, y_val, x_test, y_test))\n","\n","    metrics_list = []\n","\n","    for epoch in range(local_epochs):\n","        client_models = []\n","\n","        for client in range(num_clients):\n","            x_train, x_val, y_train, y_val, x_test, y_test = client_data[client]\n","            client_model = build_sequential_model(input_shape)\n","            client_model.set_weights(global_model.get_weights())  # Initialize with global weights\n","            opt = Adam(learning_rate=0.000009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","            client_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","            # Set up callbacks\n","            run_name = f\"epoch_{epoch}_client_{client}\"\n","            checkpoint_filepath = f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time_domain/Total/CNN/best_model_{run_name}.h5'\n","            checkpoint_callback = ModelCheckpoint(\n","                checkpoint_filepath,\n","                monitor='val_accuracy',\n","                save_best_only=True,\n","                mode='max'\n","            )\n","            csv_logger = CSVLogger(f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time_domain/Total/CNN/training_log_{run_name}.csv', append=True, separator=';')\n","\n","            history = client_model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_val, y_val), callbacks=[checkpoint_callback, csv_logger])\n","            client_models.append(client_model)\n","\n","            print(history.history)\n","\n","            # Load the best model before evaluation\n","            best_model = load_model(checkpoint_filepath)\n","\n","            # Evaluate client model\n","            y_pred = (best_model.predict(x_test) > 0.5).astype(\"int32\")\n","            accuracy, precision, recall, f1, sensitivity, specificity, kappa = compute_metrics(y_test, y_pred)\n","            metrics_list.append({\n","                'Client': client,\n","                'Accuracy': accuracy,\n","                'Precision': precision,\n","                'Recall': recall,\n","                'F1': f1,\n","                'Sensitivity': sensitivity,\n","                'Specificity': specificity,\n","                'Kappa': kappa\n","            })\n","\n","        global_weights = global_model.get_weights()\n","        layer_start_idx = 0\n","        for layer in global_model.layers:\n","            layer_weights = []\n","            layer_biases = []\n","            num_params = len(layer.get_weights())\n","            for i in range(num_clients):\n","                layer_params = client_models[i].get_weights()[layer_start_idx:layer_start_idx + num_params]\n","                if num_params > 0:\n","                    layer_weights.append(layer_params[0])\n","                if num_params > 1:\n","                    layer_biases.append(layer_params[1])\n","\n","            if len(layer_weights) > 0:\n","                averaged_layer_weights = np.mean(layer_weights, axis=0)\n","                if averaged_layer_weights.shape == global_weights[layer_start_idx].shape:\n","                    global_weights[layer_start_idx] = averaged_layer_weights\n","                else:\n","                    print(f\"Warning: Shape mismatch for weights at layer {layer.name}, expected {global_weights[layer_start_idx].shape} but got {averaged_layer_weights.shape}\")\n","            if len(layer_biases) > 0:\n","                averaged_layer_biases = np.mean(layer_biases, axis=0)\n","                if averaged_layer_biases.shape == global_weights[layer_start_idx + 1].shape:\n","                    global_weights[layer_start_idx + 1] = averaged_layer_biases\n","                else:\n","                    print(f\"Warning: Shape mismatch for biases at layer {layer.name}, expected {global_weights[layer_start_idx + 1].shape} but got {averaged_layer_biases.shape}\")\n","\n","            layer_start_idx += num_params\n","\n","        # Apply FedOpt (Adam) update to global weights\n","        t += 1\n","        for i in range(len(global_weights)):\n","            g_t = global_weights[i] - global_model.get_weights()[i]  # Gradient\n","            m[i] = beta1 * m[i] + (1 - beta1) * g_t\n","            v[i] = beta2 * v[i] + (1 - beta2) * (g_t ** 2)\n","            m_hat = m[i] / (1 - beta1 ** t)\n","            v_hat = v[i] / (1 - beta2 ** t)\n","            global_weights[i] -= global_optimizer.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n","\n","        global_model.set_weights(global_weights)\n","\n","    metrics_df = pd.DataFrame(metrics_list)\n","    return global_model, metrics_df\n","\n","# Example usage\n","# data_path = 'path/to/your/data.npz'\n","# global_model, metrics_df = federated_learning(data_path)\n","# print(metrics_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":725818,"status":"ok","timestamp":1717398957545,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"H1TaNdIbSfkq","outputId":"7e13f12b-a8d6-4f7f-890f-1a987dcb2cbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (4640, 18, 29), Test shape: (1194, 18, 29)\n","Train shape: (4418, 18, 29), Test shape: (1416, 18, 29)\n","Train shape: (4838, 18, 29), Test shape: (996, 18, 29)\n","Train shape: (4640, 18, 29), Test shape: (1194, 18, 29)\n","Train shape: (4418, 18, 29), Test shape: (1416, 18, 29)\n","Train shape: (4838, 18, 29), Test shape: (996, 18, 29)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 4s 26ms/step - loss: 1.7371 - accuracy: 0.7031 - val_loss: 1.7638 - val_accuracy: 0.4849\n","Epoch 2/100\n"," 1/29 [>.............................] - ETA: 0s - loss: 1.7229 - accuracy: 0.6797"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 1s 18ms/step - loss: 1.6787 - accuracy: 0.7320 - val_loss: 1.7510 - val_accuracy: 0.6422\n","Epoch 3/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.6308 - accuracy: 0.7430 - val_loss: 1.7372 - val_accuracy: 0.6121\n","Epoch 4/100\n","29/29 [==============================] - 1s 28ms/step - loss: 1.5973 - accuracy: 0.7505 - val_loss: 1.7217 - val_accuracy: 0.7252\n","Epoch 5/100\n","29/29 [==============================] - 1s 36ms/step - loss: 1.5704 - accuracy: 0.7554 - val_loss: 1.7061 - val_accuracy: 0.7511\n","Epoch 6/100\n","29/29 [==============================] - 1s 38ms/step - loss: 1.5493 - accuracy: 0.7640 - val_loss: 1.6906 - val_accuracy: 0.7629\n","Epoch 7/100\n","29/29 [==============================] - 1s 40ms/step - loss: 1.5275 - accuracy: 0.7753 - val_loss: 1.6733 - val_accuracy: 0.7834\n","Epoch 8/100\n","29/29 [==============================] - 1s 27ms/step - loss: 1.5085 - accuracy: 0.7799 - val_loss: 1.6573 - val_accuracy: 0.7985\n","Epoch 9/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.4934 - accuracy: 0.7848 - val_loss: 1.6379 - val_accuracy: 0.7662\n","Epoch 10/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.4849 - accuracy: 0.7802 - val_loss: 1.6175 - val_accuracy: 0.7974\n","Epoch 11/100\n","29/29 [==============================] - 1s 26ms/step - loss: 1.4634 - accuracy: 0.7858 - val_loss: 1.5986 - val_accuracy: 0.8050\n","Epoch 12/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.4490 - accuracy: 0.7923 - val_loss: 1.5757 - val_accuracy: 0.8006\n","Epoch 13/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.4340 - accuracy: 0.7996 - val_loss: 1.5532 - val_accuracy: 0.7996\n","Epoch 14/100\n","29/29 [==============================] - 1s 33ms/step - loss: 1.4215 - accuracy: 0.7988 - val_loss: 1.5285 - val_accuracy: 0.8147\n","Epoch 15/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.4101 - accuracy: 0.7996 - val_loss: 1.5036 - val_accuracy: 0.8017\n","Epoch 16/100\n","29/29 [==============================] - 1s 17ms/step - loss: 1.3974 - accuracy: 0.8001 - val_loss: 1.4781 - val_accuracy: 0.8071\n","Epoch 17/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.3878 - accuracy: 0.7977 - val_loss: 1.4542 - val_accuracy: 0.8060\n","Epoch 18/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.3753 - accuracy: 0.8020 - val_loss: 1.4347 - val_accuracy: 0.8006\n","Epoch 19/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.3807 - accuracy: 0.7982 - val_loss: 1.4193 - val_accuracy: 0.7942\n","Epoch 20/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.3566 - accuracy: 0.8052 - val_loss: 1.3856 - val_accuracy: 0.8125\n","Epoch 21/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.3488 - accuracy: 0.8039 - val_loss: 1.3710 - val_accuracy: 0.8050\n","Epoch 22/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3342 - accuracy: 0.8087 - val_loss: 1.3514 - val_accuracy: 0.8125\n","Epoch 23/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.3236 - accuracy: 0.8128 - val_loss: 1.3356 - val_accuracy: 0.8125\n","Epoch 24/100\n","29/29 [==============================] - 1s 27ms/step - loss: 1.3178 - accuracy: 0.8136 - val_loss: 1.3211 - val_accuracy: 0.8276\n","Epoch 25/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.3080 - accuracy: 0.8106 - val_loss: 1.3089 - val_accuracy: 0.8211\n","Epoch 26/100\n","29/29 [==============================] - 1s 25ms/step - loss: 1.2970 - accuracy: 0.8109 - val_loss: 1.2975 - val_accuracy: 0.8211\n","Epoch 27/100\n","29/29 [==============================] - 1s 30ms/step - loss: 1.2866 - accuracy: 0.8195 - val_loss: 1.2897 - val_accuracy: 0.8319\n","Epoch 28/100\n","29/29 [==============================] - 1s 26ms/step - loss: 1.2805 - accuracy: 0.8176 - val_loss: 1.2748 - val_accuracy: 0.8276\n","Epoch 29/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.2708 - accuracy: 0.8200 - val_loss: 1.2657 - val_accuracy: 0.8254\n","Epoch 30/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.2615 - accuracy: 0.8219 - val_loss: 1.2568 - val_accuracy: 0.8222\n","Epoch 31/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.2510 - accuracy: 0.8227 - val_loss: 1.2602 - val_accuracy: 0.8190\n","Epoch 32/100\n","29/29 [==============================] - 1s 26ms/step - loss: 1.2486 - accuracy: 0.8190 - val_loss: 1.2424 - val_accuracy: 0.8351\n","Epoch 33/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.2343 - accuracy: 0.8268 - val_loss: 1.2323 - val_accuracy: 0.8265\n","Epoch 34/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.2325 - accuracy: 0.8225 - val_loss: 1.2485 - val_accuracy: 0.8147\n","Epoch 35/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.2250 - accuracy: 0.8273 - val_loss: 1.2191 - val_accuracy: 0.8265\n","Epoch 36/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.2102 - accuracy: 0.8303 - val_loss: 1.2096 - val_accuracy: 0.8287\n","Epoch 37/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.2042 - accuracy: 0.8327 - val_loss: 1.2036 - val_accuracy: 0.8297\n","Epoch 38/100\n","29/29 [==============================] - 1s 17ms/step - loss: 1.2019 - accuracy: 0.8289 - val_loss: 1.2195 - val_accuracy: 0.8222\n","Epoch 39/100\n","29/29 [==============================] - 1s 29ms/step - loss: 1.1922 - accuracy: 0.8327 - val_loss: 1.2005 - val_accuracy: 0.8362\n","Epoch 40/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1803 - accuracy: 0.8354 - val_loss: 1.1826 - val_accuracy: 0.8319\n","Epoch 41/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1725 - accuracy: 0.8324 - val_loss: 1.1756 - val_accuracy: 0.8308\n","Epoch 42/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.1659 - accuracy: 0.8376 - val_loss: 1.1705 - val_accuracy: 0.8330\n","Epoch 43/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1605 - accuracy: 0.8319 - val_loss: 1.1657 - val_accuracy: 0.8276\n","Epoch 44/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1579 - accuracy: 0.8281 - val_loss: 1.1634 - val_accuracy: 0.8362\n","Epoch 45/100\n","29/29 [==============================] - 1s 29ms/step - loss: 1.1466 - accuracy: 0.8330 - val_loss: 1.1530 - val_accuracy: 0.8405\n","Epoch 46/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.1424 - accuracy: 0.8359 - val_loss: 1.1549 - val_accuracy: 0.8351\n","Epoch 47/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.1269 - accuracy: 0.8408 - val_loss: 1.1426 - val_accuracy: 0.8394\n","Epoch 48/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1216 - accuracy: 0.8432 - val_loss: 1.1328 - val_accuracy: 0.8351\n","Epoch 49/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.1149 - accuracy: 0.8402 - val_loss: 1.1277 - val_accuracy: 0.8394\n","Epoch 50/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.1070 - accuracy: 0.8413 - val_loss: 1.1204 - val_accuracy: 0.8351\n","Epoch 51/100\n","29/29 [==============================] - 1s 37ms/step - loss: 1.1024 - accuracy: 0.8475 - val_loss: 1.1165 - val_accuracy: 0.8438\n","Epoch 52/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.1016 - accuracy: 0.8381 - val_loss: 1.1089 - val_accuracy: 0.8351\n","Epoch 53/100\n","29/29 [==============================] - 1s 25ms/step - loss: 1.0874 - accuracy: 0.8462 - val_loss: 1.1239 - val_accuracy: 0.8297\n","Epoch 54/100\n","29/29 [==============================] - 1s 25ms/step - loss: 1.0816 - accuracy: 0.8470 - val_loss: 1.0953 - val_accuracy: 0.8405\n","Epoch 55/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.0788 - accuracy: 0.8424 - val_loss: 1.0908 - val_accuracy: 0.8394\n","Epoch 56/100\n","29/29 [==============================] - 1s 26ms/step - loss: 1.0713 - accuracy: 0.8475 - val_loss: 1.1140 - val_accuracy: 0.8190\n","Epoch 57/100\n","29/29 [==============================] - 1s 29ms/step - loss: 1.0699 - accuracy: 0.8435 - val_loss: 1.0802 - val_accuracy: 0.8394\n","Epoch 58/100\n","29/29 [==============================] - 1s 18ms/step - loss: 1.0558 - accuracy: 0.8464 - val_loss: 1.0775 - val_accuracy: 0.8362\n","Epoch 59/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0530 - accuracy: 0.8470 - val_loss: 1.0816 - val_accuracy: 0.8341\n","Epoch 60/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0482 - accuracy: 0.8456 - val_loss: 1.0630 - val_accuracy: 0.8427\n","Epoch 61/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.0345 - accuracy: 0.8559 - val_loss: 1.0577 - val_accuracy: 0.8427\n","Epoch 62/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.0322 - accuracy: 0.8516 - val_loss: 1.0523 - val_accuracy: 0.8416\n","Epoch 63/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.0247 - accuracy: 0.8548 - val_loss: 1.0486 - val_accuracy: 0.8448\n","Epoch 64/100\n","29/29 [==============================] - 0s 16ms/step - loss: 1.0165 - accuracy: 0.8567 - val_loss: 1.0427 - val_accuracy: 0.8405\n","Epoch 65/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.0092 - accuracy: 0.8588 - val_loss: 1.0386 - val_accuracy: 0.8405\n","Epoch 66/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.0094 - accuracy: 0.8559 - val_loss: 1.0470 - val_accuracy: 0.8287\n","Epoch 67/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.0024 - accuracy: 0.8564 - val_loss: 1.0464 - val_accuracy: 0.8330\n","Epoch 68/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.9930 - accuracy: 0.8640 - val_loss: 1.0249 - val_accuracy: 0.8405\n","Epoch 69/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.9922 - accuracy: 0.8572 - val_loss: 1.0205 - val_accuracy: 0.8438\n","Epoch 70/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.9829 - accuracy: 0.8591 - val_loss: 1.0153 - val_accuracy: 0.8405\n","Epoch 71/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.9748 - accuracy: 0.8623 - val_loss: 1.0080 - val_accuracy: 0.8448\n","Epoch 72/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.9675 - accuracy: 0.8602 - val_loss: 1.0037 - val_accuracy: 0.8448\n","Epoch 73/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.9610 - accuracy: 0.8677 - val_loss: 1.0043 - val_accuracy: 0.8416\n","Epoch 74/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9572 - accuracy: 0.8650 - val_loss: 0.9963 - val_accuracy: 0.8459\n","Epoch 75/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.9520 - accuracy: 0.8677 - val_loss: 0.9964 - val_accuracy: 0.8362\n","Epoch 76/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.9454 - accuracy: 0.8683 - val_loss: 0.9844 - val_accuracy: 0.8438\n","Epoch 77/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.9401 - accuracy: 0.8685 - val_loss: 0.9789 - val_accuracy: 0.8438\n","Epoch 78/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.9374 - accuracy: 0.8693 - val_loss: 0.9773 - val_accuracy: 0.8470\n","Epoch 79/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.9291 - accuracy: 0.8731 - val_loss: 0.9700 - val_accuracy: 0.8438\n","Epoch 80/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.9265 - accuracy: 0.8656 - val_loss: 0.9712 - val_accuracy: 0.8405\n","Epoch 81/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.9246 - accuracy: 0.8677 - val_loss: 0.9620 - val_accuracy: 0.8438\n","Epoch 82/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.9131 - accuracy: 0.8731 - val_loss: 0.9778 - val_accuracy: 0.8362\n","Epoch 83/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.9059 - accuracy: 0.8734 - val_loss: 0.9550 - val_accuracy: 0.8438\n","Epoch 84/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.9001 - accuracy: 0.8745 - val_loss: 0.9521 - val_accuracy: 0.8416\n","Epoch 85/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8976 - accuracy: 0.8737 - val_loss: 0.9618 - val_accuracy: 0.8351\n","Epoch 86/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8918 - accuracy: 0.8720 - val_loss: 0.9563 - val_accuracy: 0.8373\n","Epoch 87/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8870 - accuracy: 0.8715 - val_loss: 0.9520 - val_accuracy: 0.8351\n","Epoch 88/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8777 - accuracy: 0.8809 - val_loss: 0.9404 - val_accuracy: 0.8384\n","Epoch 89/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8726 - accuracy: 0.8847 - val_loss: 0.9290 - val_accuracy: 0.8416\n","Epoch 90/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8673 - accuracy: 0.8812 - val_loss: 0.9294 - val_accuracy: 0.8470\n","Epoch 91/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.8598 - accuracy: 0.8825 - val_loss: 0.9205 - val_accuracy: 0.8481\n","Epoch 92/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8544 - accuracy: 0.8844 - val_loss: 0.9165 - val_accuracy: 0.8481\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8516 - accuracy: 0.8852 - val_loss: 0.9164 - val_accuracy: 0.8470\n","Epoch 94/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8473 - accuracy: 0.8882 - val_loss: 0.9122 - val_accuracy: 0.8470\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8425 - accuracy: 0.8869 - val_loss: 0.9090 - val_accuracy: 0.8448\n","Epoch 96/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.8377 - accuracy: 0.8855 - val_loss: 0.9126 - val_accuracy: 0.8416\n","Epoch 97/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.8386 - accuracy: 0.8834 - val_loss: 0.8993 - val_accuracy: 0.8481\n","Epoch 98/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.8243 - accuracy: 0.8901 - val_loss: 0.8961 - val_accuracy: 0.8427\n","Epoch 99/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.8233 - accuracy: 0.8836 - val_loss: 0.9042 - val_accuracy: 0.8384\n","Epoch 100/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.8184 - accuracy: 0.8939 - val_loss: 0.8966 - val_accuracy: 0.8416\n","{'loss': [1.7371084690093994, 1.678687334060669, 1.6307547092437744, 1.5972527265548706, 1.5704461336135864, 1.5493113994598389, 1.5275051593780518, 1.5084930658340454, 1.4934452772140503, 1.4848625659942627, 1.4634339809417725, 1.4489514827728271, 1.4340167045593262, 1.4215291738510132, 1.4100921154022217, 1.3973612785339355, 1.3877532482147217, 1.375293254852295, 1.3806521892547607, 1.3566068410873413, 1.3487757444381714, 1.3342169523239136, 1.323578953742981, 1.3178253173828125, 1.308020830154419, 1.2970367670059204, 1.286597728729248, 1.2805063724517822, 1.270817518234253, 1.2614504098892212, 1.2509630918502808, 1.24855375289917, 1.2343056201934814, 1.2325328588485718, 1.2249891757965088, 1.2101657390594482, 1.2042065858840942, 1.2019370794296265, 1.1922451257705688, 1.1802973747253418, 1.1724942922592163, 1.1659104824066162, 1.1605002880096436, 1.157888650894165, 1.146643042564392, 1.1423840522766113, 1.1269097328186035, 1.1216237545013428, 1.1148622035980225, 1.1070421934127808, 1.102357029914856, 1.1015774011611938, 1.0874415636062622, 1.0816278457641602, 1.0788252353668213, 1.0712701082229614, 1.0698989629745483, 1.055799961090088, 1.0529991388320923, 1.048160195350647, 1.0344784259796143, 1.0322239398956299, 1.0246787071228027, 1.016526460647583, 1.0092253684997559, 1.0093762874603271, 1.0023900270462036, 0.9929942488670349, 0.9921882748603821, 0.9828972816467285, 0.9747822880744934, 0.9675122499465942, 0.9609982967376709, 0.9572482109069824, 0.9519747495651245, 0.9454295635223389, 0.9401484727859497, 0.937393069267273, 0.9290584921836853, 0.9265332221984863, 0.9246494770050049, 0.9131457805633545, 0.9058713316917419, 0.9001097679138184, 0.8976278901100159, 0.8918490409851074, 0.8869925141334534, 0.8776921033859253, 0.8726253509521484, 0.8672502040863037, 0.8597649931907654, 0.8544031977653503, 0.8515819907188416, 0.8473325371742249, 0.8425057530403137, 0.8377369046211243, 0.838608980178833, 0.8242881298065186, 0.8232744932174683, 0.8183735609054565], 'accuracy': [0.703125, 0.7319504022598267, 0.7429956793785095, 0.7505387663841248, 0.7553879022598267, 0.764008641242981, 0.7753232717514038, 0.779902994632721, 0.7847521305084229, 0.7801724076271057, 0.7858297228813171, 0.7922952771186829, 0.7995689511299133, 0.7987607717514038, 0.7995689511299133, 0.8001077771186829, 0.7976831793785095, 0.8019935488700867, 0.798222005367279, 0.8052262663841248, 0.8038793206214905, 0.8087284564971924, 0.8127694129943848, 0.8135775923728943, 0.8106142282485962, 0.810883641242981, 0.8195043206214905, 0.8176185488700867, 0.8200430870056152, 0.821928858757019, 0.8227370977401733, 0.818965494632721, 0.826777994632721, 0.8224676847457886, 0.8273168206214905, 0.8302801847457886, 0.8327047228813171, 0.8289331793785095, 0.8327047228813171, 0.8353987336158752, 0.8324353694915771, 0.837553858757019, 0.8318965435028076, 0.828125, 0.8329741358757019, 0.8359375, 0.8407866358757019, 0.8432112336158752, 0.8402478694915771, 0.8413254022598267, 0.8475215435028076, 0.8380926847457886, 0.8461745977401733, 0.8469827771186829, 0.842402994632721, 0.8475215435028076, 0.8434805870056152, 0.8464439511299133, 0.8469827771186829, 0.8456357717514038, 0.8558728694915771, 0.8515625, 0.8547952771186829, 0.8566810488700867, 0.8588362336158752, 0.8558728694915771, 0.8564116358757019, 0.8639547228813171, 0.8572198152542114, 0.8591055870056152, 0.8623383641242981, 0.8601831793785095, 0.8677262663841248, 0.8650323152542114, 0.8677262663841248, 0.8682650923728943, 0.868534505367279, 0.8693426847457886, 0.8731142282485962, 0.865571141242981, 0.8677262663841248, 0.8731142282485962, 0.873383641242981, 0.8744612336158752, 0.873652994632721, 0.8720366358757019, 0.8714978694915771, 0.8809267282485962, 0.8846982717514038, 0.881196141242981, 0.8825430870056152, 0.884428858757019, 0.8852370977401733, 0.8882004022598267, 0.8868534564971924, 0.8855064511299133, 0.8833512663841248, 0.8900862336158752, 0.8836206793785095, 0.8938577771186829], 'val_loss': [1.7637821435928345, 1.750962257385254, 1.7372143268585205, 1.7217299938201904, 1.7061185836791992, 1.690595030784607, 1.6733245849609375, 1.6573420763015747, 1.6379255056381226, 1.6174757480621338, 1.5985989570617676, 1.5756577253341675, 1.553165078163147, 1.5284610986709595, 1.5036312341690063, 1.4780982732772827, 1.4541711807250977, 1.4347189664840698, 1.4192607402801514, 1.385572075843811, 1.3709694147109985, 1.3513638973236084, 1.3355567455291748, 1.321085810661316, 1.3088555335998535, 1.2975070476531982, 1.289721965789795, 1.2748442888259888, 1.2656832933425903, 1.2567646503448486, 1.2602323293685913, 1.2424460649490356, 1.2322933673858643, 1.2485288381576538, 1.219053864479065, 1.2095779180526733, 1.2036149501800537, 1.2194879055023193, 1.200525164604187, 1.1826320886611938, 1.1756374835968018, 1.1704556941986084, 1.165716290473938, 1.1634358167648315, 1.1529698371887207, 1.1548842191696167, 1.1425836086273193, 1.1327818632125854, 1.1276780366897583, 1.1204423904418945, 1.1165399551391602, 1.108883261680603, 1.1239348649978638, 1.0953083038330078, 1.0907572507858276, 1.113952875137329, 1.080214500427246, 1.0775470733642578, 1.081630825996399, 1.0629534721374512, 1.0577439069747925, 1.052315592765808, 1.0486379861831665, 1.0427134037017822, 1.0386477708816528, 1.0469945669174194, 1.0463840961456299, 1.0249221324920654, 1.0205270051956177, 1.0152705907821655, 1.0079588890075684, 1.0037153959274292, 1.004257082939148, 0.9963436126708984, 0.9964326024055481, 0.9843811988830566, 0.9789448976516724, 0.9773353934288025, 0.9700145125389099, 0.9712386727333069, 0.9619532227516174, 0.9777528047561646, 0.9550496935844421, 0.9521024823188782, 0.9618202447891235, 0.9562506079673767, 0.952018141746521, 0.9403637647628784, 0.9289513230323792, 0.9294160008430481, 0.9205036759376526, 0.9164900183677673, 0.9164361357688904, 0.9121526479721069, 0.9090327620506287, 0.9125751256942749, 0.8993134498596191, 0.896081805229187, 0.9042057394981384, 0.8966236114501953], 'val_accuracy': [0.48491379618644714, 0.642241358757019, 0.6120689511299133, 0.725215494632721, 0.7510775923728943, 0.7629310488700867, 0.7834051847457886, 0.798491358757019, 0.7661637663841248, 0.7974137663841248, 0.8049569129943848, 0.8006465435028076, 0.7995689511299133, 0.8146551847457886, 0.8017241358757019, 0.8071120977401733, 0.806034505367279, 0.8006465435028076, 0.7941810488700867, 0.8125, 0.8049569129943848, 0.8125, 0.8125, 0.8275862336158752, 0.8211206793785095, 0.8211206793785095, 0.8318965435028076, 0.8275862336158752, 0.8254310488700867, 0.8221982717514038, 0.818965494632721, 0.8351293206214905, 0.826508641242981, 0.8146551847457886, 0.826508641242981, 0.8286637663841248, 0.829741358757019, 0.8221982717514038, 0.8362069129943848, 0.8318965435028076, 0.8308189511299133, 0.8329741358757019, 0.8275862336158752, 0.8362069129943848, 0.8405172228813171, 0.8351293206214905, 0.8394396305084229, 0.8351293206214905, 0.8394396305084229, 0.8351293206214905, 0.84375, 0.8351293206214905, 0.829741358757019, 0.8405172228813171, 0.8394396305084229, 0.818965494632721, 0.8394396305084229, 0.8362069129943848, 0.8340517282485962, 0.8426724076271057, 0.8426724076271057, 0.8415948152542114, 0.8448275923728943, 0.8405172228813171, 0.8405172228813171, 0.8286637663841248, 0.8329741358757019, 0.8405172228813171, 0.84375, 0.8405172228813171, 0.8448275923728943, 0.8448275923728943, 0.8415948152542114, 0.8459051847457886, 0.8362069129943848, 0.84375, 0.84375, 0.8469827771186829, 0.84375, 0.8405172228813171, 0.84375, 0.8362069129943848, 0.84375, 0.8415948152542114, 0.8351293206214905, 0.837284505367279, 0.8351293206214905, 0.8383620977401733, 0.8415948152542114, 0.8469827771186829, 0.8480603694915771, 0.8480603694915771, 0.8469827771186829, 0.8469827771186829, 0.8448275923728943, 0.8415948152542114, 0.8480603694915771, 0.8426724076271057, 0.8383620977401733, 0.8415948152542114]}\n","38/38 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 4s 31ms/step - loss: 1.7491 - accuracy: 0.6783 - val_loss: 1.7646 - val_accuracy: 0.4966\n","Epoch 2/100\n"," 1/28 [>.............................] - ETA: 0s - loss: 1.7234 - accuracy: 0.6641"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["28/28 [==============================] - 0s 17ms/step - loss: 1.7043 - accuracy: 0.7049 - val_loss: 1.7536 - val_accuracy: 0.5158\n","Epoch 3/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.6662 - accuracy: 0.7159 - val_loss: 1.7415 - val_accuracy: 0.6425\n","Epoch 4/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.6345 - accuracy: 0.7221 - val_loss: 1.7292 - val_accuracy: 0.6652\n","Epoch 5/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.6099 - accuracy: 0.7306 - val_loss: 1.7158 - val_accuracy: 0.7376\n","Epoch 6/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.5863 - accuracy: 0.7442 - val_loss: 1.7026 - val_accuracy: 0.7443\n","Epoch 7/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.5666 - accuracy: 0.7561 - val_loss: 1.6884 - val_accuracy: 0.7579\n","Epoch 8/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.5468 - accuracy: 0.7586 - val_loss: 1.6739 - val_accuracy: 0.7557\n","Epoch 9/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.5290 - accuracy: 0.7651 - val_loss: 1.6575 - val_accuracy: 0.7590\n","Epoch 10/100\n","28/28 [==============================] - 1s 19ms/step - loss: 1.5110 - accuracy: 0.7705 - val_loss: 1.6413 - val_accuracy: 0.7670\n","Epoch 11/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.4934 - accuracy: 0.7779 - val_loss: 1.6234 - val_accuracy: 0.7624\n","Epoch 12/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.4802 - accuracy: 0.7793 - val_loss: 1.6040 - val_accuracy: 0.7771\n","Epoch 13/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.4628 - accuracy: 0.7847 - val_loss: 1.5874 - val_accuracy: 0.7489\n","Epoch 14/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.4481 - accuracy: 0.7858 - val_loss: 1.5645 - val_accuracy: 0.7658\n","Epoch 15/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.4345 - accuracy: 0.7892 - val_loss: 1.5439 - val_accuracy: 0.7636\n","Epoch 16/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.4225 - accuracy: 0.7917 - val_loss: 1.5175 - val_accuracy: 0.7692\n","Epoch 17/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.4131 - accuracy: 0.7900 - val_loss: 1.5049 - val_accuracy: 0.7579\n","Epoch 18/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.3985 - accuracy: 0.7960 - val_loss: 1.4761 - val_accuracy: 0.7726\n","Epoch 19/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.3901 - accuracy: 0.7943 - val_loss: 1.4508 - val_accuracy: 0.7783\n","Epoch 20/100\n","28/28 [==============================] - 0s 11ms/step - loss: 1.3783 - accuracy: 0.7999 - val_loss: 1.4302 - val_accuracy: 0.7726\n","Epoch 21/100\n","28/28 [==============================] - 0s 12ms/step - loss: 1.3683 - accuracy: 0.7980 - val_loss: 1.4114 - val_accuracy: 0.7771\n","Epoch 22/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.3551 - accuracy: 0.8053 - val_loss: 1.4042 - val_accuracy: 0.7726\n","Epoch 23/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3480 - accuracy: 0.8045 - val_loss: 1.3856 - val_accuracy: 0.7862\n","Epoch 24/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.3381 - accuracy: 0.8073 - val_loss: 1.3783 - val_accuracy: 0.7760\n","Epoch 25/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.3291 - accuracy: 0.8053 - val_loss: 1.3546 - val_accuracy: 0.7919\n","Epoch 26/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.3194 - accuracy: 0.8138 - val_loss: 1.3437 - val_accuracy: 0.7952\n","Epoch 27/100\n","28/28 [==============================] - 0s 11ms/step - loss: 1.3085 - accuracy: 0.8118 - val_loss: 1.3512 - val_accuracy: 0.7805\n","Epoch 28/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.3008 - accuracy: 0.8138 - val_loss: 1.3253 - val_accuracy: 0.7941\n","Epoch 29/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.2907 - accuracy: 0.8164 - val_loss: 1.3276 - val_accuracy: 0.7885\n","Epoch 30/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.2877 - accuracy: 0.8079 - val_loss: 1.3193 - val_accuracy: 0.7930\n","Epoch 31/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.2744 - accuracy: 0.8132 - val_loss: 1.3078 - val_accuracy: 0.7907\n","Epoch 32/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.2690 - accuracy: 0.8096 - val_loss: 1.2963 - val_accuracy: 0.7873\n","Epoch 33/100\n","28/28 [==============================] - 0s 12ms/step - loss: 1.2623 - accuracy: 0.8158 - val_loss: 1.3011 - val_accuracy: 0.7828\n","Epoch 34/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.2546 - accuracy: 0.8144 - val_loss: 1.2828 - val_accuracy: 0.7896\n","Epoch 35/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2452 - accuracy: 0.8172 - val_loss: 1.2795 - val_accuracy: 0.7986\n","Epoch 36/100\n","28/28 [==============================] - 0s 11ms/step - loss: 1.2340 - accuracy: 0.8229 - val_loss: 1.2752 - val_accuracy: 0.7873\n","Epoch 37/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2293 - accuracy: 0.8223 - val_loss: 1.2606 - val_accuracy: 0.8032\n","Epoch 38/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.2232 - accuracy: 0.8175 - val_loss: 1.2628 - val_accuracy: 0.7998\n","Epoch 39/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.2113 - accuracy: 0.8240 - val_loss: 1.2471 - val_accuracy: 0.7998\n","Epoch 40/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.2057 - accuracy: 0.8294 - val_loss: 1.2482 - val_accuracy: 0.7998\n","Epoch 41/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.1969 - accuracy: 0.8248 - val_loss: 1.2369 - val_accuracy: 0.7930\n","Epoch 42/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.1942 - accuracy: 0.8226 - val_loss: 1.2370 - val_accuracy: 0.7975\n","Epoch 43/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.1811 - accuracy: 0.8271 - val_loss: 1.2221 - val_accuracy: 0.8066\n","Epoch 44/100\n","28/28 [==============================] - 0s 11ms/step - loss: 1.1760 - accuracy: 0.8277 - val_loss: 1.2155 - val_accuracy: 0.8032\n","Epoch 45/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1674 - accuracy: 0.8308 - val_loss: 1.2101 - val_accuracy: 0.8100\n","Epoch 46/100\n","28/28 [==============================] - 0s 12ms/step - loss: 1.1610 - accuracy: 0.8322 - val_loss: 1.2193 - val_accuracy: 0.7885\n","Epoch 47/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.1531 - accuracy: 0.8331 - val_loss: 1.1984 - val_accuracy: 0.8100\n","Epoch 48/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.1461 - accuracy: 0.8311 - val_loss: 1.1945 - val_accuracy: 0.8043\n","Epoch 49/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.1400 - accuracy: 0.8331 - val_loss: 1.1868 - val_accuracy: 0.7998\n","Epoch 50/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.1387 - accuracy: 0.8331 - val_loss: 1.1866 - val_accuracy: 0.7964\n","Epoch 51/100\n","28/28 [==============================] - 0s 12ms/step - loss: 1.1298 - accuracy: 0.8347 - val_loss: 1.1773 - val_accuracy: 0.8043\n","Epoch 52/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.1220 - accuracy: 0.8359 - val_loss: 1.1677 - val_accuracy: 0.8088\n","Epoch 53/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1157 - accuracy: 0.8316 - val_loss: 1.1643 - val_accuracy: 0.8100\n","Epoch 54/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.1079 - accuracy: 0.8427 - val_loss: 1.1591 - val_accuracy: 0.7975\n","Epoch 55/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1019 - accuracy: 0.8362 - val_loss: 1.1532 - val_accuracy: 0.8054\n","Epoch 56/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.0958 - accuracy: 0.8373 - val_loss: 1.1447 - val_accuracy: 0.8122\n","Epoch 57/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0950 - accuracy: 0.8336 - val_loss: 1.1545 - val_accuracy: 0.8009\n","Epoch 58/100\n","28/28 [==============================] - 0s 11ms/step - loss: 1.0863 - accuracy: 0.8373 - val_loss: 1.1359 - val_accuracy: 0.8122\n","Epoch 59/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.0801 - accuracy: 0.8390 - val_loss: 1.1389 - val_accuracy: 0.8032\n","Epoch 60/100\n","28/28 [==============================] - 0s 11ms/step - loss: 1.0726 - accuracy: 0.8381 - val_loss: 1.1329 - val_accuracy: 0.8066\n","Epoch 61/100\n","28/28 [==============================] - 0s 11ms/step - loss: 1.0663 - accuracy: 0.8407 - val_loss: 1.1197 - val_accuracy: 0.8077\n","Epoch 62/100\n","28/28 [==============================] - 0s 11ms/step - loss: 1.0615 - accuracy: 0.8387 - val_loss: 1.1128 - val_accuracy: 0.8122\n","Epoch 63/100\n","28/28 [==============================] - 0s 12ms/step - loss: 1.0531 - accuracy: 0.8413 - val_loss: 1.1109 - val_accuracy: 0.8088\n","Epoch 64/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.0488 - accuracy: 0.8410 - val_loss: 1.1102 - val_accuracy: 0.8100\n","Epoch 65/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.0414 - accuracy: 0.8458 - val_loss: 1.1008 - val_accuracy: 0.8088\n","Epoch 66/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.0418 - accuracy: 0.8463 - val_loss: 1.1089 - val_accuracy: 0.7975\n","Epoch 67/100\n","28/28 [==============================] - 0s 18ms/step - loss: 1.0317 - accuracy: 0.8430 - val_loss: 1.0893 - val_accuracy: 0.8145\n","Epoch 68/100\n","28/28 [==============================] - 0s 12ms/step - loss: 1.0248 - accuracy: 0.8461 - val_loss: 1.0873 - val_accuracy: 0.8100\n","Epoch 69/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.0172 - accuracy: 0.8486 - val_loss: 1.0792 - val_accuracy: 0.8088\n","Epoch 70/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.0107 - accuracy: 0.8495 - val_loss: 1.0775 - val_accuracy: 0.8088\n","Epoch 71/100\n","28/28 [==============================] - 0s 10ms/step - loss: 1.0023 - accuracy: 0.8497 - val_loss: 1.0717 - val_accuracy: 0.8088\n","Epoch 72/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.9979 - accuracy: 0.8509 - val_loss: 1.0638 - val_accuracy: 0.8122\n","Epoch 73/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.9922 - accuracy: 0.8529 - val_loss: 1.0720 - val_accuracy: 0.8145\n","Epoch 74/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.9968 - accuracy: 0.8387 - val_loss: 1.0579 - val_accuracy: 0.8088\n","Epoch 75/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.9844 - accuracy: 0.8506 - val_loss: 1.0614 - val_accuracy: 0.8145\n","Epoch 76/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9816 - accuracy: 0.8483 - val_loss: 1.0455 - val_accuracy: 0.8167\n","Epoch 77/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.9703 - accuracy: 0.8560 - val_loss: 1.0415 - val_accuracy: 0.8167\n","Epoch 78/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.9686 - accuracy: 0.8534 - val_loss: 1.0472 - val_accuracy: 0.8133\n","Epoch 79/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9595 - accuracy: 0.8588 - val_loss: 1.0324 - val_accuracy: 0.8179\n","Epoch 80/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9566 - accuracy: 0.8557 - val_loss: 1.0261 - val_accuracy: 0.8201\n","Epoch 81/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.9502 - accuracy: 0.8585 - val_loss: 1.0249 - val_accuracy: 0.8179\n","Epoch 82/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9439 - accuracy: 0.8546 - val_loss: 1.0220 - val_accuracy: 0.8213\n","Epoch 83/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.9381 - accuracy: 0.8577 - val_loss: 1.0287 - val_accuracy: 0.8122\n","Epoch 84/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.9328 - accuracy: 0.8560 - val_loss: 1.0180 - val_accuracy: 0.8088\n","Epoch 85/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.9279 - accuracy: 0.8605 - val_loss: 1.0134 - val_accuracy: 0.8156\n","Epoch 86/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9258 - accuracy: 0.8563 - val_loss: 1.0022 - val_accuracy: 0.8235\n","Epoch 87/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.9189 - accuracy: 0.8591 - val_loss: 0.9978 - val_accuracy: 0.8258\n","Epoch 88/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.9128 - accuracy: 0.8647 - val_loss: 0.9981 - val_accuracy: 0.8201\n","Epoch 89/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.9227 - accuracy: 0.8540 - val_loss: 0.9978 - val_accuracy: 0.8179\n","Epoch 90/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9041 - accuracy: 0.8633 - val_loss: 0.9947 - val_accuracy: 0.8179\n","Epoch 91/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.8998 - accuracy: 0.8622 - val_loss: 0.9790 - val_accuracy: 0.8247\n","Epoch 92/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8926 - accuracy: 0.8664 - val_loss: 0.9840 - val_accuracy: 0.8213\n","Epoch 93/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.8857 - accuracy: 0.8687 - val_loss: 0.9722 - val_accuracy: 0.8292\n","Epoch 94/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8835 - accuracy: 0.8670 - val_loss: 0.9716 - val_accuracy: 0.8247\n","Epoch 95/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8749 - accuracy: 0.8710 - val_loss: 0.9631 - val_accuracy: 0.8292\n","Epoch 96/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8704 - accuracy: 0.8710 - val_loss: 0.9706 - val_accuracy: 0.8179\n","Epoch 97/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8678 - accuracy: 0.8727 - val_loss: 0.9563 - val_accuracy: 0.8292\n","Epoch 98/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.8602 - accuracy: 0.8746 - val_loss: 0.9544 - val_accuracy: 0.8247\n","Epoch 99/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8565 - accuracy: 0.8761 - val_loss: 0.9459 - val_accuracy: 0.8348\n","Epoch 100/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.8530 - accuracy: 0.8718 - val_loss: 0.9613 - val_accuracy: 0.8054\n","{'loss': [1.749118685722351, 1.7042720317840576, 1.6662381887435913, 1.6345099210739136, 1.6099088191986084, 1.586320400238037, 1.5666249990463257, 1.5468254089355469, 1.5290379524230957, 1.5109734535217285, 1.4933884143829346, 1.4801549911499023, 1.4628046751022339, 1.4480947256088257, 1.4345470666885376, 1.4224729537963867, 1.413062572479248, 1.3984591960906982, 1.3901445865631104, 1.3782612085342407, 1.3683065176010132, 1.3551263809204102, 1.3479996919631958, 1.3381181955337524, 1.329067349433899, 1.3194329738616943, 1.308514952659607, 1.3007808923721313, 1.2906649112701416, 1.2876981496810913, 1.2744346857070923, 1.2689523696899414, 1.2622767686843872, 1.2545911073684692, 1.24520742893219, 1.2339962720870972, 1.2293238639831543, 1.2232012748718262, 1.2113161087036133, 1.2057240009307861, 1.1969391107559204, 1.1941529512405396, 1.1811294555664062, 1.1759949922561646, 1.1674048900604248, 1.1609649658203125, 1.1530556678771973, 1.1460870504379272, 1.1399528980255127, 1.1387046575546265, 1.129786729812622, 1.1219875812530518, 1.1157397031784058, 1.1078989505767822, 1.101853847503662, 1.0957659482955933, 1.0949757099151611, 1.0862780809402466, 1.080061435699463, 1.072596788406372, 1.0663087368011475, 1.061467170715332, 1.0531269311904907, 1.0487579107284546, 1.0414360761642456, 1.0418076515197754, 1.0316873788833618, 1.024758219718933, 1.017176628112793, 1.0106855630874634, 1.0023314952850342, 0.9978931546211243, 0.9922435283660889, 0.9968219995498657, 0.9843667149543762, 0.9816451668739319, 0.9702907800674438, 0.9686264991760254, 0.9595372676849365, 0.9566483497619629, 0.9501978754997253, 0.9438722729682922, 0.9380616545677185, 0.9328390955924988, 0.9279424548149109, 0.9258190393447876, 0.9188505411148071, 0.9127874374389648, 0.9226943254470825, 0.9040679335594177, 0.8997824192047119, 0.8925751447677612, 0.8857470154762268, 0.8835265040397644, 0.874871551990509, 0.8703864216804504, 0.8677546977996826, 0.8601943254470825, 0.8564643263816833, 0.853046178817749], 'accuracy': [0.6782682538032532, 0.7048670053482056, 0.7159026861190796, 0.7221279144287109, 0.7306168675422668, 0.7441992163658142, 0.7560837864875793, 0.7586304545402527, 0.7651386260986328, 0.7705150246620178, 0.7778720855712891, 0.7792869210243225, 0.7846632599830627, 0.7857951521873474, 0.7891907095909119, 0.79173743724823, 0.790039598941803, 0.7959818840026855, 0.7942841053009033, 0.7999433875083923, 0.7979626655578613, 0.8053197264671326, 0.8044708371162415, 0.8073005080223083, 0.8053197264671326, 0.8138087391853333, 0.8118279576301575, 0.8138087391853333, 0.8163554072380066, 0.8078664541244507, 0.8132427930831909, 0.8095642328262329, 0.8157894611358643, 0.8143746256828308, 0.8172042965888977, 0.8228636384010315, 0.8222976922988892, 0.8174872398376465, 0.8239954710006714, 0.8293718099594116, 0.8248443603515625, 0.8225806355476379, 0.8271080851554871, 0.8276740312576294, 0.8307866454124451, 0.8322014808654785, 0.8330503702163696, 0.8310695886611938, 0.8330503702163696, 0.8330503702163696, 0.8347481489181519, 0.8358800411224365, 0.8316355347633362, 0.8426712155342102, 0.8361629843711853, 0.83729487657547, 0.833616316318512, 0.83729487657547, 0.8389926552772522, 0.8381437659263611, 0.8406904339790344, 0.8387096524238586, 0.8412563800811768, 0.8409733772277832, 0.8457838296890259, 0.8463497161865234, 0.842954158782959, 0.8460667729377747, 0.848613440990448, 0.8494623899459839, 0.8497453331947327, 0.8508771657943726, 0.8528579473495483, 0.8387096524238586, 0.8505942225456238, 0.8483304977416992, 0.855970561504364, 0.8534238934516907, 0.8588002324104309, 0.8556876182556152, 0.8585172891616821, 0.8545557260513306, 0.8576683402061462, 0.855970561504364, 0.8604980111122131, 0.8562535643577576, 0.8590831756591797, 0.8647425174713135, 0.853989839553833, 0.86332768201828, 0.8621957898139954, 0.8664402961730957, 0.8687040209770203, 0.867006242275238, 0.8709677457809448, 0.8709677457809448, 0.872665524482727, 0.8746463060379028, 0.8760611414909363, 0.8718166351318359], 'val_loss': [1.7645820379257202, 1.7536143064498901, 1.7415440082550049, 1.729170799255371, 1.7158228158950806, 1.7026156187057495, 1.6883833408355713, 1.6739119291305542, 1.6574937105178833, 1.641287922859192, 1.6234147548675537, 1.603982925415039, 1.5874170064926147, 1.564527988433838, 1.5438982248306274, 1.5175235271453857, 1.504938006401062, 1.4760963916778564, 1.4507683515548706, 1.4301658868789673, 1.4114055633544922, 1.4041986465454102, 1.385574221611023, 1.378340244293213, 1.354557752609253, 1.343706727027893, 1.3511933088302612, 1.3252902030944824, 1.3276176452636719, 1.3193132877349854, 1.3078012466430664, 1.2962865829467773, 1.301130771636963, 1.2828102111816406, 1.279466152191162, 1.2752243280410767, 1.2606207132339478, 1.2628203630447388, 1.247061848640442, 1.248203158378601, 1.2369135618209839, 1.237002968788147, 1.2221258878707886, 1.2154908180236816, 1.2100939750671387, 1.2192957401275635, 1.198428750038147, 1.1945176124572754, 1.1867852210998535, 1.1865737438201904, 1.1772907972335815, 1.1676812171936035, 1.1643449068069458, 1.1590907573699951, 1.1532360315322876, 1.1447052955627441, 1.1544909477233887, 1.1359268426895142, 1.1388602256774902, 1.132927656173706, 1.1196906566619873, 1.1127724647521973, 1.1109261512756348, 1.110164761543274, 1.1008431911468506, 1.1088840961456299, 1.089307188987732, 1.0873370170593262, 1.0791558027267456, 1.0774692296981812, 1.0716527700424194, 1.0638405084609985, 1.072022795677185, 1.0579122304916382, 1.0613876581192017, 1.0454533100128174, 1.0414665937423706, 1.047176718711853, 1.0324383974075317, 1.0261051654815674, 1.0249316692352295, 1.022010087966919, 1.0287141799926758, 1.0180209875106812, 1.0133620500564575, 1.0022039413452148, 0.997794508934021, 0.9981313347816467, 0.9978039860725403, 0.9946810603141785, 0.9790292382240295, 0.9839967489242554, 0.9721855521202087, 0.9715638756752014, 0.9630760550498962, 0.9706323146820068, 0.9563323855400085, 0.9544192552566528, 0.945858359336853, 0.9613364934921265], 'val_accuracy': [0.49660632014274597, 0.5158371329307556, 0.6425339579582214, 0.6651583909988403, 0.7375565767288208, 0.7443438768386841, 0.7579185366630554, 0.7556561231613159, 0.7590497732162476, 0.766968309879303, 0.7624434232711792, 0.7771493196487427, 0.7488687634468079, 0.7658371329307556, 0.7635746598243713, 0.7692307829856873, 0.7579185366630554, 0.7726244330406189, 0.7782805562019348, 0.7726244330406189, 0.7771493196487427, 0.7726244330406189, 0.7861990928649902, 0.7760180830955505, 0.7918552160263062, 0.7952488660812378, 0.7805429697036743, 0.7941176295280457, 0.7884615659713745, 0.7929864525794983, 0.790723979473114, 0.7873303294181824, 0.7828054428100586, 0.7895927429199219, 0.7986425161361694, 0.7873303294181824, 0.8031674027442932, 0.7997737526893616, 0.7997737526893616, 0.7997737526893616, 0.7929864525794983, 0.7975113391876221, 0.8065611124038696, 0.8031674027442932, 0.8099547624588013, 0.7884615659713745, 0.8099547624588013, 0.8042986392974854, 0.7997737526893616, 0.7963801026344299, 0.8042986392974854, 0.8088235259056091, 0.8099547624588013, 0.7975113391876221, 0.8054298758506775, 0.8122171759605408, 0.8009049892425537, 0.8122171759605408, 0.8031674027442932, 0.8065611124038696, 0.807692289352417, 0.8122171759605408, 0.8088235259056091, 0.8099547624588013, 0.8088235259056091, 0.7975113391876221, 0.814479649066925, 0.8099547624588013, 0.8088235259056091, 0.8088235259056091, 0.8088235259056091, 0.8122171759605408, 0.814479649066925, 0.8088235259056091, 0.814479649066925, 0.8167420625686646, 0.8167420625686646, 0.8133484125137329, 0.8178732991218567, 0.820135772228241, 0.8178732991218567, 0.8212669491767883, 0.8122171759605408, 0.8088235259056091, 0.8156108856201172, 0.8235294222831726, 0.8257918357849121, 0.820135772228241, 0.8178732991218567, 0.8178732991218567, 0.8246606588363647, 0.8212669491767883, 0.8291855454444885, 0.8246606588363647, 0.8291855454444885, 0.8178732991218567, 0.8291855454444885, 0.8246606588363647, 0.8348416090011597, 0.8054298758506775]}\n","45/45 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 3s 27ms/step - loss: 1.7442 - accuracy: 0.6589 - val_loss: 1.7636 - val_accuracy: 0.4866\n","Epoch 2/100\n"," 1/31 [..............................] - ETA: 0s - loss: 1.7123 - accuracy: 0.6719"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 0s 12ms/step - loss: 1.6896 - accuracy: 0.7065 - val_loss: 1.7520 - val_accuracy: 0.4855\n","Epoch 3/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.6510 - accuracy: 0.7072 - val_loss: 1.7384 - val_accuracy: 0.6488\n","Epoch 4/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.6178 - accuracy: 0.7233 - val_loss: 1.7247 - val_accuracy: 0.6684\n","Epoch 5/100\n","31/31 [==============================] - 1s 23ms/step - loss: 1.5890 - accuracy: 0.7362 - val_loss: 1.7102 - val_accuracy: 0.6921\n","Epoch 6/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.5619 - accuracy: 0.7550 - val_loss: 1.6937 - val_accuracy: 0.7469\n","Epoch 7/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.5351 - accuracy: 0.7680 - val_loss: 1.6771 - val_accuracy: 0.7386\n","Epoch 8/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.5114 - accuracy: 0.7744 - val_loss: 1.6581 - val_accuracy: 0.7614\n","Epoch 9/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.4905 - accuracy: 0.7788 - val_loss: 1.6371 - val_accuracy: 0.7727\n","Epoch 10/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.4706 - accuracy: 0.7827 - val_loss: 1.6165 - val_accuracy: 0.7727\n","Epoch 11/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.4508 - accuracy: 0.7902 - val_loss: 1.5922 - val_accuracy: 0.7779\n","Epoch 12/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.4401 - accuracy: 0.7855 - val_loss: 1.5674 - val_accuracy: 0.7810\n","Epoch 13/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.4223 - accuracy: 0.7961 - val_loss: 1.5437 - val_accuracy: 0.7851\n","Epoch 14/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.4068 - accuracy: 0.7984 - val_loss: 1.5284 - val_accuracy: 0.7593\n","Epoch 15/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.3936 - accuracy: 0.8034 - val_loss: 1.4942 - val_accuracy: 0.7882\n","Epoch 16/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.3839 - accuracy: 0.8054 - val_loss: 1.4640 - val_accuracy: 0.7955\n","Epoch 17/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.3714 - accuracy: 0.8044 - val_loss: 1.4416 - val_accuracy: 0.8017\n","Epoch 18/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.3613 - accuracy: 0.8103 - val_loss: 1.4319 - val_accuracy: 0.7882\n","Epoch 19/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.3447 - accuracy: 0.8163 - val_loss: 1.4105 - val_accuracy: 0.7903\n","Epoch 20/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.3405 - accuracy: 0.8106 - val_loss: 1.4022 - val_accuracy: 0.7862\n","Epoch 21/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.3260 - accuracy: 0.8109 - val_loss: 1.3725 - val_accuracy: 0.7996\n","Epoch 22/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.3118 - accuracy: 0.8214 - val_loss: 1.3485 - val_accuracy: 0.8099\n","Epoch 23/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.3096 - accuracy: 0.8155 - val_loss: 1.3359 - val_accuracy: 0.8110\n","Epoch 24/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.2946 - accuracy: 0.8212 - val_loss: 1.3335 - val_accuracy: 0.8130\n","Epoch 25/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2827 - accuracy: 0.8258 - val_loss: 1.3169 - val_accuracy: 0.8151\n","Epoch 26/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.2721 - accuracy: 0.8261 - val_loss: 1.3038 - val_accuracy: 0.8130\n","Epoch 27/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.2661 - accuracy: 0.8225 - val_loss: 1.3028 - val_accuracy: 0.7975\n","Epoch 28/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.2549 - accuracy: 0.8256 - val_loss: 1.2868 - val_accuracy: 0.8089\n","Epoch 29/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.2484 - accuracy: 0.8287 - val_loss: 1.2795 - val_accuracy: 0.8161\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.2369 - accuracy: 0.8315 - val_loss: 1.2721 - val_accuracy: 0.8202\n","Epoch 31/100\n","31/31 [==============================] - 0s 12ms/step - loss: 1.2289 - accuracy: 0.8292 - val_loss: 1.2606 - val_accuracy: 0.8202\n","Epoch 32/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.2207 - accuracy: 0.8284 - val_loss: 1.2584 - val_accuracy: 0.8027\n","Epoch 33/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.2090 - accuracy: 0.8346 - val_loss: 1.2461 - val_accuracy: 0.8161\n","Epoch 34/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.2007 - accuracy: 0.8377 - val_loss: 1.2455 - val_accuracy: 0.8027\n","Epoch 35/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.1932 - accuracy: 0.8372 - val_loss: 1.2350 - val_accuracy: 0.8202\n","Epoch 36/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.1829 - accuracy: 0.8403 - val_loss: 1.2231 - val_accuracy: 0.8233\n","Epoch 37/100\n","31/31 [==============================] - 0s 12ms/step - loss: 1.1777 - accuracy: 0.8385 - val_loss: 1.2350 - val_accuracy: 0.8027\n","Epoch 38/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.1696 - accuracy: 0.8377 - val_loss: 1.2080 - val_accuracy: 0.8264\n","Epoch 39/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1585 - accuracy: 0.8382 - val_loss: 1.2012 - val_accuracy: 0.8244\n","Epoch 40/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.1518 - accuracy: 0.8408 - val_loss: 1.1961 - val_accuracy: 0.8140\n","Epoch 41/100\n","31/31 [==============================] - 0s 12ms/step - loss: 1.1455 - accuracy: 0.8426 - val_loss: 1.1934 - val_accuracy: 0.8151\n","Epoch 42/100\n","31/31 [==============================] - 0s 12ms/step - loss: 1.1383 - accuracy: 0.8411 - val_loss: 1.1963 - val_accuracy: 0.8048\n","Epoch 43/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.1296 - accuracy: 0.8444 - val_loss: 1.1787 - val_accuracy: 0.8151\n","Epoch 44/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.1194 - accuracy: 0.8483 - val_loss: 1.1701 - val_accuracy: 0.8223\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1214 - accuracy: 0.8393 - val_loss: 1.1619 - val_accuracy: 0.8202\n","Epoch 46/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1069 - accuracy: 0.8468 - val_loss: 1.1553 - val_accuracy: 0.8223\n","Epoch 47/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.0998 - accuracy: 0.8444 - val_loss: 1.1643 - val_accuracy: 0.8079\n","Epoch 48/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.0958 - accuracy: 0.8473 - val_loss: 1.1448 - val_accuracy: 0.8171\n","Epoch 49/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.0848 - accuracy: 0.8488 - val_loss: 1.1455 - val_accuracy: 0.8171\n","Epoch 50/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.0819 - accuracy: 0.8499 - val_loss: 1.1430 - val_accuracy: 0.8151\n","Epoch 51/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.0766 - accuracy: 0.8450 - val_loss: 1.1268 - val_accuracy: 0.8223\n","Epoch 52/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.0636 - accuracy: 0.8532 - val_loss: 1.1224 - val_accuracy: 0.8213\n","Epoch 53/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.0564 - accuracy: 0.8540 - val_loss: 1.1271 - val_accuracy: 0.8161\n","Epoch 54/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.0495 - accuracy: 0.8556 - val_loss: 1.1119 - val_accuracy: 0.8202\n","Epoch 55/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0423 - accuracy: 0.8540 - val_loss: 1.1035 - val_accuracy: 0.8275\n","Epoch 56/100\n","31/31 [==============================] - 0s 10ms/step - loss: 1.0330 - accuracy: 0.8550 - val_loss: 1.1026 - val_accuracy: 0.8233\n","Epoch 57/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0364 - accuracy: 0.8473 - val_loss: 1.0914 - val_accuracy: 0.8306\n","Epoch 58/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.0208 - accuracy: 0.8576 - val_loss: 1.0860 - val_accuracy: 0.8306\n","Epoch 59/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.0166 - accuracy: 0.8605 - val_loss: 1.0809 - val_accuracy: 0.8306\n","Epoch 60/100\n","31/31 [==============================] - 0s 16ms/step - loss: 1.0112 - accuracy: 0.8592 - val_loss: 1.0752 - val_accuracy: 0.8337\n","Epoch 61/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.0035 - accuracy: 0.8605 - val_loss: 1.0727 - val_accuracy: 0.8244\n","Epoch 62/100\n","31/31 [==============================] - 0s 11ms/step - loss: 1.0029 - accuracy: 0.8550 - val_loss: 1.0650 - val_accuracy: 0.8285\n","Epoch 63/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.9977 - accuracy: 0.8576 - val_loss: 1.0652 - val_accuracy: 0.8182\n","Epoch 64/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.9903 - accuracy: 0.8579 - val_loss: 1.0759 - val_accuracy: 0.8120\n","Epoch 65/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.9829 - accuracy: 0.8605 - val_loss: 1.0514 - val_accuracy: 0.8244\n","Epoch 66/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.9724 - accuracy: 0.8659 - val_loss: 1.0480 - val_accuracy: 0.8244\n","Epoch 67/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.9681 - accuracy: 0.8623 - val_loss: 1.0477 - val_accuracy: 0.8182\n","Epoch 68/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.9576 - accuracy: 0.8703 - val_loss: 1.0339 - val_accuracy: 0.8316\n","Epoch 69/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.9568 - accuracy: 0.8664 - val_loss: 1.0291 - val_accuracy: 0.8295\n","Epoch 70/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9466 - accuracy: 0.8638 - val_loss: 1.0247 - val_accuracy: 0.8285\n","Epoch 71/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.9444 - accuracy: 0.8654 - val_loss: 1.0201 - val_accuracy: 0.8326\n","Epoch 72/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9367 - accuracy: 0.8633 - val_loss: 1.0152 - val_accuracy: 0.8326\n","Epoch 73/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.9297 - accuracy: 0.8711 - val_loss: 1.0102 - val_accuracy: 0.8316\n","Epoch 74/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9240 - accuracy: 0.8646 - val_loss: 1.0112 - val_accuracy: 0.8275\n","Epoch 75/100\n","31/31 [==============================] - 1s 32ms/step - loss: 0.9170 - accuracy: 0.8716 - val_loss: 1.0004 - val_accuracy: 0.8316\n","Epoch 76/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9100 - accuracy: 0.8703 - val_loss: 1.0016 - val_accuracy: 0.8326\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9039 - accuracy: 0.8718 - val_loss: 0.9916 - val_accuracy: 0.8316\n","Epoch 78/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9036 - accuracy: 0.8708 - val_loss: 0.9969 - val_accuracy: 0.8223\n","Epoch 79/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8965 - accuracy: 0.8713 - val_loss: 1.0152 - val_accuracy: 0.8151\n","Epoch 80/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8948 - accuracy: 0.8742 - val_loss: 0.9977 - val_accuracy: 0.8244\n","Epoch 81/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8886 - accuracy: 0.8695 - val_loss: 0.9758 - val_accuracy: 0.8275\n","Epoch 82/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.8757 - accuracy: 0.8762 - val_loss: 0.9753 - val_accuracy: 0.8264\n","Epoch 83/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8716 - accuracy: 0.8747 - val_loss: 0.9679 - val_accuracy: 0.8306\n","Epoch 84/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8681 - accuracy: 0.8755 - val_loss: 0.9659 - val_accuracy: 0.8347\n","Epoch 85/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.8723 - accuracy: 0.8721 - val_loss: 0.9617 - val_accuracy: 0.8337\n","Epoch 86/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.8680 - accuracy: 0.8726 - val_loss: 0.9908 - val_accuracy: 0.8171\n","Epoch 87/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8558 - accuracy: 0.8742 - val_loss: 0.9504 - val_accuracy: 0.8285\n","Epoch 88/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8472 - accuracy: 0.8798 - val_loss: 0.9465 - val_accuracy: 0.8295\n","Epoch 89/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.8379 - accuracy: 0.8845 - val_loss: 0.9536 - val_accuracy: 0.8275\n","Epoch 90/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8339 - accuracy: 0.8824 - val_loss: 0.9376 - val_accuracy: 0.8264\n","Epoch 91/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8284 - accuracy: 0.8855 - val_loss: 0.9322 - val_accuracy: 0.8295\n","Epoch 92/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8237 - accuracy: 0.8819 - val_loss: 0.9337 - val_accuracy: 0.8316\n","Epoch 93/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8181 - accuracy: 0.8835 - val_loss: 0.9386 - val_accuracy: 0.8316\n","Epoch 94/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8230 - accuracy: 0.8801 - val_loss: 0.9254 - val_accuracy: 0.8326\n","Epoch 95/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.8058 - accuracy: 0.8876 - val_loss: 0.9227 - val_accuracy: 0.8357\n","Epoch 96/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8144 - accuracy: 0.8824 - val_loss: 0.9242 - val_accuracy: 0.8347\n","Epoch 97/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.8007 - accuracy: 0.8879 - val_loss: 0.9187 - val_accuracy: 0.8306\n","Epoch 98/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7908 - accuracy: 0.8920 - val_loss: 0.9063 - val_accuracy: 0.8285\n","Epoch 99/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7878 - accuracy: 0.8886 - val_loss: 0.9359 - val_accuracy: 0.8233\n","Epoch 100/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.7931 - accuracy: 0.8886 - val_loss: 0.9050 - val_accuracy: 0.8368\n","{'loss': [1.7441680431365967, 1.6895856857299805, 1.6510382890701294, 1.6177860498428345, 1.5890190601348877, 1.5619215965270996, 1.5350561141967773, 1.511413812637329, 1.4904745817184448, 1.4705997705459595, 1.4508150815963745, 1.440081238746643, 1.4222753047943115, 1.4068471193313599, 1.393643856048584, 1.3838835954666138, 1.37139892578125, 1.3612803220748901, 1.3446961641311646, 1.340531587600708, 1.3259512186050415, 1.3118294477462769, 1.3096351623535156, 1.2946194410324097, 1.2827359437942505, 1.2721118927001953, 1.2660735845565796, 1.2548701763153076, 1.2483693361282349, 1.2369036674499512, 1.2288531064987183, 1.2206510305404663, 1.2089557647705078, 1.2007259130477905, 1.193170189857483, 1.1829179525375366, 1.1777474880218506, 1.169606328010559, 1.1585475206375122, 1.1517993211746216, 1.145466685295105, 1.138339638710022, 1.1295653581619263, 1.1193701028823853, 1.121376395225525, 1.106925368309021, 1.0998448133468628, 1.0957865715026855, 1.0847619771957397, 1.0818606615066528, 1.0765892267227173, 1.0635688304901123, 1.056400179862976, 1.049530029296875, 1.0422873497009277, 1.0330288410186768, 1.0364384651184082, 1.0208293199539185, 1.0166206359863281, 1.0111547708511353, 1.0034948587417603, 1.002935767173767, 0.9976741075515747, 0.990277111530304, 0.9828601479530334, 0.9724041223526001, 0.9681100845336914, 0.9575576782226562, 0.9568040370941162, 0.9466179013252258, 0.9443822503089905, 0.9366565346717834, 0.9296607971191406, 0.9240017533302307, 0.917022168636322, 0.9099756479263306, 0.9039108753204346, 0.9035843014717102, 0.8965252041816711, 0.894795298576355, 0.8885883688926697, 0.8756586313247681, 0.8715537190437317, 0.8681414127349854, 0.8723233342170715, 0.8679667711257935, 0.8557724356651306, 0.8471895456314087, 0.8378723859786987, 0.8339088559150696, 0.828363835811615, 0.8236595988273621, 0.8180978894233704, 0.8229804039001465, 0.8058494925498962, 0.8144474029541016, 0.8006533980369568, 0.7908095717430115, 0.7878350019454956, 0.7930752038955688], 'accuracy': [0.6589147448539734, 0.7064599394798279, 0.7072351574897766, 0.7232558131217957, 0.7361757159233093, 0.7550387382507324, 0.7679586410522461, 0.7744185924530029, 0.7788113951683044, 0.7826873660087585, 0.7901808619499207, 0.7855297327041626, 0.7961240410804749, 0.7984496355056763, 0.8033591508865356, 0.8054263591766357, 0.8043927550315857, 0.8103359341621399, 0.8162790536880493, 0.8105943202972412, 0.8108527064323425, 0.8214470148086548, 0.8155038952827454, 0.8211886286735535, 0.8258398175239563, 0.8260982036590576, 0.8224806189537048, 0.8255813717842102, 0.8286821842193604, 0.8315245509147644, 0.829198956489563, 0.828423798084259, 0.8346253037452698, 0.8377261161804199, 0.8372092843055725, 0.8403100967407227, 0.8385012745857239, 0.8377261161804199, 0.8382428884506226, 0.8408268690109253, 0.8426356315612793, 0.8410852551460266, 0.8444444537162781, 0.8483204245567322, 0.8392764925956726, 0.8467700481414795, 0.8444444537162781, 0.8472868204116821, 0.8488371968269348, 0.8498708009719849, 0.8449612259864807, 0.8532299995422363, 0.8540051579475403, 0.855555534362793, 0.8540051579475403, 0.8550387620925903, 0.8472868204116821, 0.8576227426528931, 0.8604651093482971, 0.8591731190681458, 0.8604651093482971, 0.8550387620925903, 0.8576227426528931, 0.8578811287879944, 0.8604651093482971, 0.8658914566040039, 0.8622739315032959, 0.8702842593193054, 0.8664082884788513, 0.8638243079185486, 0.8653746843338013, 0.8633074760437012, 0.8710594177246094, 0.8645994663238525, 0.8715762495994568, 0.8702842593193054, 0.8718346357345581, 0.8708010315895081, 0.8713178038597107, 0.8741602301597595, 0.8695090413093567, 0.8762273788452148, 0.8746770024299622, 0.8754522204399109, 0.8720930218696594, 0.8726097941398621, 0.8741602301597595, 0.8798449635505676, 0.8844961524009705, 0.8824289441108704, 0.8855296969413757, 0.8819121718406677, 0.8834625482559204, 0.880103349685669, 0.8875969052314758, 0.8824289441108704, 0.8878552913665771, 0.8919896483421326, 0.8886305093765259, 0.8886305093765259], 'val_loss': [1.7636014223098755, 1.7519906759262085, 1.738417387008667, 1.7246779203414917, 1.71018385887146, 1.6937206983566284, 1.6771212816238403, 1.6580976247787476, 1.6370563507080078, 1.6165482997894287, 1.5921550989151, 1.5673719644546509, 1.5436760187149048, 1.5283862352371216, 1.4942377805709839, 1.463989019393921, 1.441617727279663, 1.43190598487854, 1.4104632139205933, 1.4022228717803955, 1.372546911239624, 1.3485398292541504, 1.3358652591705322, 1.3334946632385254, 1.3168894052505493, 1.3038220405578613, 1.3028373718261719, 1.2868295907974243, 1.2795071601867676, 1.2721129655838013, 1.2606459856033325, 1.2583832740783691, 1.246057152748108, 1.2455295324325562, 1.2350318431854248, 1.2231214046478271, 1.2350159883499146, 1.2080293893814087, 1.2012438774108887, 1.1960636377334595, 1.193359375, 1.1963324546813965, 1.178673267364502, 1.1700520515441895, 1.161867618560791, 1.155325174331665, 1.1642975807189941, 1.1448426246643066, 1.1455053091049194, 1.1430288553237915, 1.1267669200897217, 1.122361183166504, 1.1270699501037598, 1.1119147539138794, 1.10345458984375, 1.1026464700698853, 1.0914299488067627, 1.0859503746032715, 1.0809024572372437, 1.075223445892334, 1.0727035999298096, 1.065047025680542, 1.0651986598968506, 1.075947642326355, 1.0513899326324463, 1.0480135679244995, 1.0476983785629272, 1.0338940620422363, 1.0291141271591187, 1.0247421264648438, 1.0200754404067993, 1.015157699584961, 1.0102248191833496, 1.0112178325653076, 1.0003728866577148, 1.0015525817871094, 0.9916342496871948, 0.9969378113746643, 1.0152487754821777, 0.9977439641952515, 0.9757747054100037, 0.9753091931343079, 0.9679363369941711, 0.9659493565559387, 0.9617382884025574, 0.9908022284507751, 0.9503976702690125, 0.9465001821517944, 0.9535971283912659, 0.9376477599143982, 0.9321880340576172, 0.9337416291236877, 0.9386017918586731, 0.9253903031349182, 0.9227085709571838, 0.924237847328186, 0.9186583757400513, 0.906298041343689, 0.9358929991722107, 0.9050114750862122], 'val_accuracy': [0.48657023906707764, 0.48553720116615295, 0.6487603187561035, 0.6683884263038635, 0.692148745059967, 0.7469007968902588, 0.7386363744735718, 0.7613636255264282, 0.7727272510528564, 0.7727272510528564, 0.7778925895690918, 0.7809917330741882, 0.7851239442825317, 0.7592975497245789, 0.788223147392273, 0.7954545617103577, 0.8016529083251953, 0.788223147392273, 0.7902892827987671, 0.7861570119857788, 0.7995867729187012, 0.8099173307418823, 0.8109503984451294, 0.8130165338516235, 0.8150826692581177, 0.8130165338516235, 0.797520637512207, 0.80888432264328, 0.81611567735672, 0.8202479481697083, 0.8202479481697083, 0.8026859760284424, 0.81611567735672, 0.8026859760284424, 0.8202479481697083, 0.8233470916748047, 0.8026859760284424, 0.8264462947845459, 0.8243801593780518, 0.8140496015548706, 0.8150826692581177, 0.8047520518302917, 0.8150826692581177, 0.8223140239715576, 0.8202479481697083, 0.8223140239715576, 0.807851254940033, 0.817148745059967, 0.817148745059967, 0.8150826692581177, 0.8223140239715576, 0.8212810158729553, 0.81611567735672, 0.8202479481697083, 0.827479362487793, 0.8233470916748047, 0.8305785059928894, 0.8305785059928894, 0.8305785059928894, 0.8336777091026306, 0.8243801593780518, 0.8285123705863953, 0.8181818127632141, 0.8119834661483765, 0.8243801593780518, 0.8243801593780518, 0.8181818127632141, 0.8316115736961365, 0.8295454382896423, 0.8285123705863953, 0.8326446413993835, 0.8326446413993835, 0.8316115736961365, 0.827479362487793, 0.8316115736961365, 0.8326446413993835, 0.8316115736961365, 0.8223140239715576, 0.8150826692581177, 0.8243801593780518, 0.827479362487793, 0.8264462947845459, 0.8305785059928894, 0.8347107172012329, 0.8336777091026306, 0.817148745059967, 0.8285123705863953, 0.8295454382896423, 0.827479362487793, 0.8264462947845459, 0.8295454382896423, 0.8316115736961365, 0.8316115736961365, 0.8326446413993835, 0.83574378490448, 0.8347107172012329, 0.8305785059928894, 0.8285123705863953, 0.8233470916748047, 0.836776852607727]}\n","32/32 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 4s 35ms/step - loss: 0.8834 - accuracy: 0.8508 - val_loss: 1.2037 - val_accuracy: 0.4849\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 0s 14ms/step - loss: 0.8705 - accuracy: 0.8526 - val_loss: 1.1903 - val_accuracy: 0.4849\n","Epoch 3/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8613 - accuracy: 0.8502 - val_loss: 1.1673 - val_accuracy: 0.5172\n","Epoch 4/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8526 - accuracy: 0.8588 - val_loss: 1.1559 - val_accuracy: 0.5280\n","Epoch 5/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.8405 - accuracy: 0.8588 - val_loss: 1.1358 - val_accuracy: 0.6929\n","Epoch 6/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.8348 - accuracy: 0.8666 - val_loss: 1.1177 - val_accuracy: 0.7845\n","Epoch 7/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.8267 - accuracy: 0.8661 - val_loss: 1.0948 - val_accuracy: 0.8330\n","Epoch 8/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.8220 - accuracy: 0.8696 - val_loss: 1.0810 - val_accuracy: 0.8244\n","Epoch 9/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.8159 - accuracy: 0.8712 - val_loss: 1.0576 - val_accuracy: 0.8330\n","Epoch 10/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8165 - accuracy: 0.8615 - val_loss: 1.0373 - val_accuracy: 0.8394\n","Epoch 11/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.8056 - accuracy: 0.8739 - val_loss: 1.0177 - val_accuracy: 0.8308\n","Epoch 12/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8002 - accuracy: 0.8718 - val_loss: 0.9959 - val_accuracy: 0.8448\n","Epoch 13/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.7973 - accuracy: 0.8742 - val_loss: 0.9745 - val_accuracy: 0.8287\n","Epoch 14/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.7932 - accuracy: 0.8704 - val_loss: 0.9477 - val_accuracy: 0.8308\n","Epoch 15/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.7889 - accuracy: 0.8734 - val_loss: 0.9270 - val_accuracy: 0.8438\n","Epoch 16/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7794 - accuracy: 0.8804 - val_loss: 0.9355 - val_accuracy: 0.7877\n","Epoch 17/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7760 - accuracy: 0.8790 - val_loss: 0.8902 - val_accuracy: 0.8319\n","Epoch 18/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.7683 - accuracy: 0.8772 - val_loss: 0.8672 - val_accuracy: 0.8373\n","Epoch 19/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7652 - accuracy: 0.8842 - val_loss: 0.8475 - val_accuracy: 0.8534\n","Epoch 20/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7748 - accuracy: 0.8734 - val_loss: 0.8423 - val_accuracy: 0.8405\n","Epoch 21/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.7589 - accuracy: 0.8780 - val_loss: 0.8453 - val_accuracy: 0.8265\n","Epoch 22/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7527 - accuracy: 0.8823 - val_loss: 0.8148 - val_accuracy: 0.8491\n","Epoch 23/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7464 - accuracy: 0.8850 - val_loss: 0.8238 - val_accuracy: 0.8373\n","Epoch 24/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.7422 - accuracy: 0.8852 - val_loss: 0.7927 - val_accuracy: 0.8556\n","Epoch 25/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.7419 - accuracy: 0.8807 - val_loss: 0.7902 - val_accuracy: 0.8534\n","Epoch 26/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.7358 - accuracy: 0.8869 - val_loss: 0.8003 - val_accuracy: 0.8448\n","Epoch 27/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.7289 - accuracy: 0.8882 - val_loss: 0.7772 - val_accuracy: 0.8556\n","Epoch 28/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7224 - accuracy: 0.8885 - val_loss: 0.7801 - val_accuracy: 0.8491\n","Epoch 29/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7180 - accuracy: 0.8898 - val_loss: 0.7689 - val_accuracy: 0.8610\n","Epoch 30/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.7170 - accuracy: 0.8901 - val_loss: 0.7628 - val_accuracy: 0.8578\n","Epoch 31/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7301 - accuracy: 0.8817 - val_loss: 0.7669 - val_accuracy: 0.8513\n","Epoch 32/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7055 - accuracy: 0.8974 - val_loss: 0.7618 - val_accuracy: 0.8556\n","Epoch 33/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7032 - accuracy: 0.8914 - val_loss: 0.7626 - val_accuracy: 0.8556\n","Epoch 34/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6958 - accuracy: 0.8960 - val_loss: 0.7544 - val_accuracy: 0.8588\n","Epoch 35/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6966 - accuracy: 0.8987 - val_loss: 0.7509 - val_accuracy: 0.8567\n","Epoch 36/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6910 - accuracy: 0.8966 - val_loss: 0.7568 - val_accuracy: 0.8545\n","Epoch 37/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6877 - accuracy: 0.8984 - val_loss: 0.7481 - val_accuracy: 0.8545\n","Epoch 38/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6818 - accuracy: 0.9025 - val_loss: 0.7610 - val_accuracy: 0.8534\n","Epoch 39/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6786 - accuracy: 0.9003 - val_loss: 0.7422 - val_accuracy: 0.8556\n","Epoch 40/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6815 - accuracy: 0.9006 - val_loss: 0.7718 - val_accuracy: 0.8491\n","Epoch 41/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6696 - accuracy: 0.9025 - val_loss: 0.7348 - val_accuracy: 0.8578\n","Epoch 42/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6657 - accuracy: 0.9033 - val_loss: 0.7365 - val_accuracy: 0.8578\n","Epoch 43/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6626 - accuracy: 0.9054 - val_loss: 0.7329 - val_accuracy: 0.8556\n","Epoch 44/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.6557 - accuracy: 0.9087 - val_loss: 0.7284 - val_accuracy: 0.8621\n","Epoch 45/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6539 - accuracy: 0.9076 - val_loss: 0.7265 - val_accuracy: 0.8567\n","Epoch 46/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6591 - accuracy: 0.9033 - val_loss: 0.7219 - val_accuracy: 0.8610\n","Epoch 47/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6516 - accuracy: 0.9060 - val_loss: 0.7347 - val_accuracy: 0.8578\n","Epoch 48/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6570 - accuracy: 0.9003 - val_loss: 0.7516 - val_accuracy: 0.8534\n","Epoch 49/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6510 - accuracy: 0.9038 - val_loss: 0.7369 - val_accuracy: 0.8524\n","Epoch 50/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6425 - accuracy: 0.9089 - val_loss: 0.7286 - val_accuracy: 0.8556\n","Epoch 51/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6319 - accuracy: 0.9146 - val_loss: 0.7163 - val_accuracy: 0.8524\n","Epoch 52/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.6306 - accuracy: 0.9130 - val_loss: 0.7215 - val_accuracy: 0.8567\n","Epoch 53/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6234 - accuracy: 0.9176 - val_loss: 0.7122 - val_accuracy: 0.8545\n","Epoch 54/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6240 - accuracy: 0.9108 - val_loss: 0.7309 - val_accuracy: 0.8578\n","Epoch 55/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6216 - accuracy: 0.9149 - val_loss: 0.7076 - val_accuracy: 0.8481\n","Epoch 56/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6246 - accuracy: 0.9111 - val_loss: 0.7059 - val_accuracy: 0.8545\n","Epoch 57/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6174 - accuracy: 0.9168 - val_loss: 0.7110 - val_accuracy: 0.8545\n","Epoch 58/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6132 - accuracy: 0.9165 - val_loss: 0.7040 - val_accuracy: 0.8502\n","Epoch 59/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6071 - accuracy: 0.9200 - val_loss: 0.7335 - val_accuracy: 0.8534\n","Epoch 60/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6086 - accuracy: 0.9149 - val_loss: 0.7131 - val_accuracy: 0.8534\n","Epoch 61/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6021 - accuracy: 0.9219 - val_loss: 0.6976 - val_accuracy: 0.8556\n","Epoch 62/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6034 - accuracy: 0.9213 - val_loss: 0.6961 - val_accuracy: 0.8599\n","Epoch 63/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5950 - accuracy: 0.9227 - val_loss: 0.7013 - val_accuracy: 0.8631\n","Epoch 64/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.6034 - accuracy: 0.9141 - val_loss: 0.7008 - val_accuracy: 0.8588\n","Epoch 65/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5885 - accuracy: 0.9254 - val_loss: 0.6979 - val_accuracy: 0.8610\n","Epoch 66/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5815 - accuracy: 0.9270 - val_loss: 0.6973 - val_accuracy: 0.8642\n","Epoch 67/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5868 - accuracy: 0.9203 - val_loss: 0.6910 - val_accuracy: 0.8534\n","Epoch 68/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5754 - accuracy: 0.9329 - val_loss: 0.6953 - val_accuracy: 0.8642\n","Epoch 69/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5846 - accuracy: 0.9224 - val_loss: 0.6912 - val_accuracy: 0.8599\n","Epoch 70/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.5736 - accuracy: 0.9267 - val_loss: 0.7413 - val_accuracy: 0.8502\n","Epoch 71/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5740 - accuracy: 0.9267 - val_loss: 0.6939 - val_accuracy: 0.8534\n","Epoch 72/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.5732 - accuracy: 0.9281 - val_loss: 0.6850 - val_accuracy: 0.8534\n","Epoch 73/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5650 - accuracy: 0.9308 - val_loss: 0.6828 - val_accuracy: 0.8567\n","Epoch 74/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.5602 - accuracy: 0.9335 - val_loss: 0.7137 - val_accuracy: 0.8534\n","Epoch 75/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.5579 - accuracy: 0.9283 - val_loss: 0.6959 - val_accuracy: 0.8524\n","Epoch 76/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5550 - accuracy: 0.9335 - val_loss: 0.6994 - val_accuracy: 0.8534\n","Epoch 77/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5517 - accuracy: 0.9364 - val_loss: 0.6894 - val_accuracy: 0.8556\n","Epoch 78/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5471 - accuracy: 0.9383 - val_loss: 0.6768 - val_accuracy: 0.8599\n","Epoch 79/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5504 - accuracy: 0.9332 - val_loss: 0.6755 - val_accuracy: 0.8588\n","Epoch 80/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5480 - accuracy: 0.9329 - val_loss: 0.6920 - val_accuracy: 0.8621\n","Epoch 81/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5496 - accuracy: 0.9329 - val_loss: 0.6792 - val_accuracy: 0.8567\n","Epoch 82/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5397 - accuracy: 0.9388 - val_loss: 0.6771 - val_accuracy: 0.8545\n","Epoch 83/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5369 - accuracy: 0.9402 - val_loss: 0.6799 - val_accuracy: 0.8599\n","Epoch 84/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5337 - accuracy: 0.9402 - val_loss: 0.7147 - val_accuracy: 0.8513\n","Epoch 85/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5334 - accuracy: 0.9383 - val_loss: 0.6891 - val_accuracy: 0.8599\n","Epoch 86/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5339 - accuracy: 0.9337 - val_loss: 0.6697 - val_accuracy: 0.8631\n","Epoch 87/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5238 - accuracy: 0.9453 - val_loss: 0.6910 - val_accuracy: 0.8502\n","Epoch 88/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.5343 - accuracy: 0.9348 - val_loss: 0.6742 - val_accuracy: 0.8599\n","Epoch 89/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.5172 - accuracy: 0.9475 - val_loss: 0.6681 - val_accuracy: 0.8621\n","Epoch 90/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5270 - accuracy: 0.9386 - val_loss: 0.6688 - val_accuracy: 0.8664\n","Epoch 91/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.5233 - accuracy: 0.9353 - val_loss: 0.6718 - val_accuracy: 0.8545\n","Epoch 92/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5106 - accuracy: 0.9475 - val_loss: 0.6667 - val_accuracy: 0.8588\n","Epoch 93/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5111 - accuracy: 0.9437 - val_loss: 0.6897 - val_accuracy: 0.8599\n","Epoch 94/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5077 - accuracy: 0.9469 - val_loss: 0.6643 - val_accuracy: 0.8631\n","Epoch 95/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5024 - accuracy: 0.9477 - val_loss: 0.6725 - val_accuracy: 0.8642\n","Epoch 96/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.5022 - accuracy: 0.9461 - val_loss: 0.6685 - val_accuracy: 0.8610\n","Epoch 97/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5019 - accuracy: 0.9477 - val_loss: 0.6743 - val_accuracy: 0.8599\n","Epoch 98/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4950 - accuracy: 0.9488 - val_loss: 0.6654 - val_accuracy: 0.8610\n","Epoch 99/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4948 - accuracy: 0.9499 - val_loss: 0.6612 - val_accuracy: 0.8599\n","Epoch 100/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.4955 - accuracy: 0.9459 - val_loss: 0.6682 - val_accuracy: 0.8653\n","{'loss': [0.8833898901939392, 0.8705266118049622, 0.8612880110740662, 0.8525827527046204, 0.8405401706695557, 0.8347825407981873, 0.8266682028770447, 0.8220375776290894, 0.8159275054931641, 0.8164512515068054, 0.8056187033653259, 0.8001795411109924, 0.7972826361656189, 0.7931610345840454, 0.7889324426651001, 0.7794468998908997, 0.7759737372398376, 0.7683203220367432, 0.7651814222335815, 0.7747521996498108, 0.7589229345321655, 0.7527151703834534, 0.7463756203651428, 0.7422466278076172, 0.741888701915741, 0.7358014583587646, 0.7289334535598755, 0.7223776578903198, 0.7180068492889404, 0.7170066237449646, 0.7300834655761719, 0.7054886221885681, 0.703166127204895, 0.6958022713661194, 0.6966010332107544, 0.6910461783409119, 0.6877180337905884, 0.6818056702613831, 0.6785679459571838, 0.6814736723899841, 0.6696046590805054, 0.6656937003135681, 0.6625849008560181, 0.6556745767593384, 0.6539219617843628, 0.6590911746025085, 0.6516357660293579, 0.6569729447364807, 0.6509658098220825, 0.6424673199653625, 0.6318844556808472, 0.6305992007255554, 0.6234318017959595, 0.6239694952964783, 0.6215888857841492, 0.6246423125267029, 0.6174368262290955, 0.6131922602653503, 0.6070688366889954, 0.6085885763168335, 0.6020556092262268, 0.6033953428268433, 0.5949585437774658, 0.6034352779388428, 0.5885319709777832, 0.5815333127975464, 0.5868403911590576, 0.5754355192184448, 0.5845757722854614, 0.5735738277435303, 0.573991060256958, 0.5732058882713318, 0.5650388598442078, 0.5602291822433472, 0.5579150915145874, 0.5550145506858826, 0.5517115592956543, 0.5470593571662903, 0.5504232048988342, 0.5479918718338013, 0.5495669841766357, 0.5396686792373657, 0.5368667840957642, 0.5336802005767822, 0.5334348678588867, 0.5338524580001831, 0.5238038897514343, 0.5342581272125244, 0.5171628594398499, 0.5270248651504517, 0.5232826471328735, 0.5106078386306763, 0.5110573768615723, 0.5076894760131836, 0.5024033188819885, 0.5022391676902771, 0.5018736720085144, 0.4950063228607178, 0.4947751760482788, 0.4954559803009033], 'accuracy': [0.8507543206214905, 0.8526400923728943, 0.850215494632721, 0.8588362336158752, 0.8588362336158752, 0.8666487336158752, 0.8661099076271057, 0.8696120977401733, 0.8712284564971924, 0.8615301847457886, 0.8739224076271057, 0.8717672228813171, 0.8741918206214905, 0.8704202771186829, 0.873383641242981, 0.8803879022598267, 0.8790409564971924, 0.8771551847457886, 0.884159505367279, 0.873383641242981, 0.8779633641242981, 0.8822737336158752, 0.8849676847457886, 0.8852370977401733, 0.8806573152542114, 0.8868534564971924, 0.8882004022598267, 0.8884698152542114, 0.8898168206214905, 0.8900862336158752, 0.8817349076271057, 0.8973599076271057, 0.8914331793785095, 0.8960129022598267, 0.8987069129943848, 0.8965517282485962, 0.8984375, 0.9024784564971924, 0.9003232717514038, 0.9005926847457886, 0.9024784564971924, 0.9032866358757019, 0.9054418206214905, 0.9086745977401733, 0.907597005367279, 0.9032866358757019, 0.9059805870056152, 0.9003232717514038, 0.9038254022598267, 0.9089439511299133, 0.9146012663841248, 0.9129849076271057, 0.9175646305084229, 0.9108297228813171, 0.9148706793785095, 0.9110991358757019, 0.9167564511299133, 0.9164870977401733, 0.9199892282485962, 0.9148706793785095, 0.921875, 0.9213362336158752, 0.9226831793785095, 0.9140625, 0.9253771305084229, 0.9269935488700867, 0.920258641242981, 0.9329202771186829, 0.9224137663841248, 0.9267241358757019, 0.9267241358757019, 0.928071141242981, 0.9307650923728943, 0.9334590435028076, 0.928340494632721, 0.9334590435028076, 0.9364224076271057, 0.9383081793785095, 0.9331896305084229, 0.9329202771186829, 0.9329202771186829, 0.938847005367279, 0.9401939511299133, 0.9401939511299133, 0.9383081793785095, 0.9337284564971924, 0.9453125, 0.9348060488700867, 0.9474676847457886, 0.9385775923728943, 0.9353448152542114, 0.9474676847457886, 0.943696141242981, 0.946928858757019, 0.9477370977401733, 0.9461206793785095, 0.9477370977401733, 0.9488146305084229, 0.9498922228813171, 0.9458512663841248], 'val_loss': [1.2037005424499512, 1.1902673244476318, 1.167280673980713, 1.1558948755264282, 1.1358320713043213, 1.1176658868789673, 1.094775915145874, 1.0809924602508545, 1.057644009590149, 1.0373095273971558, 1.0177333354949951, 0.9958592057228088, 0.9744710326194763, 0.9476718306541443, 0.9270129799842834, 0.9355348348617554, 0.8902087211608887, 0.8671786785125732, 0.8475016951560974, 0.8422614932060242, 0.8452892303466797, 0.814765214920044, 0.8237630724906921, 0.7926671504974365, 0.790190577507019, 0.8002698421478271, 0.7771756649017334, 0.780068576335907, 0.768875241279602, 0.7628172039985657, 0.7669053077697754, 0.761784017086029, 0.7626372575759888, 0.7543594241142273, 0.7509257197380066, 0.7568252682685852, 0.7480579614639282, 0.7609605193138123, 0.7422381043434143, 0.7717881202697754, 0.734807550907135, 0.7364620566368103, 0.7329325079917908, 0.7283573746681213, 0.7264853715896606, 0.7218871116638184, 0.7346749901771545, 0.7515778541564941, 0.736868679523468, 0.7285556197166443, 0.7163352370262146, 0.7215175032615662, 0.7121660113334656, 0.7309495806694031, 0.7075623273849487, 0.7058799862861633, 0.7109879851341248, 0.7039608955383301, 0.7335056066513062, 0.7130506038665771, 0.6976064443588257, 0.6960853338241577, 0.7012817859649658, 0.7007735967636108, 0.6979436278343201, 0.6972588300704956, 0.6909794807434082, 0.6953451633453369, 0.6912340521812439, 0.7413179874420166, 0.6939337253570557, 0.6849640011787415, 0.6828429698944092, 0.713699221611023, 0.6958884000778198, 0.6994132399559021, 0.6894257068634033, 0.676842451095581, 0.6754610538482666, 0.6919502019882202, 0.679198145866394, 0.6770768165588379, 0.6799076199531555, 0.7146583795547485, 0.6891137361526489, 0.6697494387626648, 0.6910302639007568, 0.6741876006126404, 0.6681376695632935, 0.6688103675842285, 0.6718035340309143, 0.6667184233665466, 0.6896548271179199, 0.6643381714820862, 0.6725271344184875, 0.6684995293617249, 0.6742777824401855, 0.6654197573661804, 0.6612372994422913, 0.6682254672050476], 'val_accuracy': [0.48491379618644714, 0.48491379618644714, 0.517241358757019, 0.5280172228813171, 0.6928879022598267, 0.7844827771186829, 0.8329741358757019, 0.8243534564971924, 0.8329741358757019, 0.8394396305084229, 0.8308189511299133, 0.8448275923728943, 0.8286637663841248, 0.8308189511299133, 0.84375, 0.787715494632721, 0.8318965435028076, 0.837284505367279, 0.8534482717514038, 0.8405172228813171, 0.826508641242981, 0.8491379022598267, 0.837284505367279, 0.8556034564971924, 0.8534482717514038, 0.8448275923728943, 0.8556034564971924, 0.8491379022598267, 0.860991358757019, 0.857758641242981, 0.8512930870056152, 0.8556034564971924, 0.8556034564971924, 0.8588362336158752, 0.8566810488700867, 0.8545258641242981, 0.8545258641242981, 0.8534482717514038, 0.8556034564971924, 0.8491379022598267, 0.857758641242981, 0.857758641242981, 0.8556034564971924, 0.8620689511299133, 0.8566810488700867, 0.860991358757019, 0.857758641242981, 0.8534482717514038, 0.8523706793785095, 0.8556034564971924, 0.8523706793785095, 0.8566810488700867, 0.8545258641242981, 0.857758641242981, 0.8480603694915771, 0.8545258641242981, 0.8545258641242981, 0.850215494632721, 0.8534482717514038, 0.8534482717514038, 0.8556034564971924, 0.8599137663841248, 0.8631465435028076, 0.8588362336158752, 0.860991358757019, 0.8642241358757019, 0.8534482717514038, 0.8642241358757019, 0.8599137663841248, 0.850215494632721, 0.8534482717514038, 0.8534482717514038, 0.8566810488700867, 0.8534482717514038, 0.8523706793785095, 0.8534482717514038, 0.8556034564971924, 0.8599137663841248, 0.8588362336158752, 0.8620689511299133, 0.8566810488700867, 0.8545258641242981, 0.8599137663841248, 0.8512930870056152, 0.8599137663841248, 0.8631465435028076, 0.850215494632721, 0.8599137663841248, 0.8620689511299133, 0.8663793206214905, 0.8545258641242981, 0.8588362336158752, 0.8599137663841248, 0.8631465435028076, 0.8642241358757019, 0.860991358757019, 0.8599137663841248, 0.860991358757019, 0.8599137663841248, 0.8653017282485962]}\n","38/38 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 5s 30ms/step - loss: 0.8979 - accuracy: 0.8432 - val_loss: 1.2053 - val_accuracy: 0.4955\n","Epoch 2/100\n"," 1/28 [>.............................] - ETA: 0s - loss: 0.8384 - accuracy: 0.8906"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["28/28 [==============================] - 0s 11ms/step - loss: 0.8831 - accuracy: 0.8461 - val_loss: 1.1884 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8759 - accuracy: 0.8497 - val_loss: 1.1726 - val_accuracy: 0.5294\n","Epoch 4/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8629 - accuracy: 0.8551 - val_loss: 1.1585 - val_accuracy: 0.5724\n","Epoch 5/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8557 - accuracy: 0.8531 - val_loss: 1.1446 - val_accuracy: 0.6742\n","Epoch 6/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8494 - accuracy: 0.8594 - val_loss: 1.1262 - val_accuracy: 0.7952\n","Epoch 7/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.8414 - accuracy: 0.8605 - val_loss: 1.1145 - val_accuracy: 0.8020\n","Epoch 8/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8373 - accuracy: 0.8616 - val_loss: 1.0947 - val_accuracy: 0.8235\n","Epoch 9/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.8305 - accuracy: 0.8616 - val_loss: 1.0792 - val_accuracy: 0.8167\n","Epoch 10/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.8258 - accuracy: 0.8659 - val_loss: 1.0622 - val_accuracy: 0.8213\n","Epoch 11/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.8291 - accuracy: 0.8596 - val_loss: 1.0394 - val_accuracy: 0.8100\n","Epoch 12/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.8143 - accuracy: 0.8611 - val_loss: 1.0235 - val_accuracy: 0.8190\n","Epoch 13/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.8147 - accuracy: 0.8622 - val_loss: 1.0054 - val_accuracy: 0.8100\n","Epoch 14/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.8029 - accuracy: 0.8667 - val_loss: 0.9870 - val_accuracy: 0.8088\n","Epoch 15/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7994 - accuracy: 0.8690 - val_loss: 0.9704 - val_accuracy: 0.8020\n","Epoch 16/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7968 - accuracy: 0.8667 - val_loss: 0.9622 - val_accuracy: 0.7986\n","Epoch 17/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7954 - accuracy: 0.8650 - val_loss: 0.9259 - val_accuracy: 0.8247\n","Epoch 18/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.7912 - accuracy: 0.8718 - val_loss: 0.9122 - val_accuracy: 0.8054\n","Epoch 19/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7788 - accuracy: 0.8732 - val_loss: 0.9039 - val_accuracy: 0.8043\n","Epoch 20/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7741 - accuracy: 0.8769 - val_loss: 0.8944 - val_accuracy: 0.8054\n","Epoch 21/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7679 - accuracy: 0.8812 - val_loss: 0.8658 - val_accuracy: 0.8224\n","Epoch 22/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7673 - accuracy: 0.8763 - val_loss: 0.9015 - val_accuracy: 0.7964\n","Epoch 23/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7632 - accuracy: 0.8783 - val_loss: 0.8634 - val_accuracy: 0.8100\n","Epoch 24/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7540 - accuracy: 0.8800 - val_loss: 0.8418 - val_accuracy: 0.8167\n","Epoch 25/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.7487 - accuracy: 0.8843 - val_loss: 0.8462 - val_accuracy: 0.8201\n","Epoch 26/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.7492 - accuracy: 0.8797 - val_loss: 0.8434 - val_accuracy: 0.8213\n","Epoch 27/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7433 - accuracy: 0.8829 - val_loss: 0.8187 - val_accuracy: 0.8416\n","Epoch 28/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7441 - accuracy: 0.8772 - val_loss: 0.8304 - val_accuracy: 0.8269\n","Epoch 29/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7451 - accuracy: 0.8732 - val_loss: 0.8108 - val_accuracy: 0.8462\n","Epoch 30/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7327 - accuracy: 0.8834 - val_loss: 0.8069 - val_accuracy: 0.8450\n","Epoch 31/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7252 - accuracy: 0.8854 - val_loss: 0.8098 - val_accuracy: 0.8416\n","Epoch 32/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7202 - accuracy: 0.8874 - val_loss: 0.8058 - val_accuracy: 0.8428\n","Epoch 33/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.7153 - accuracy: 0.8902 - val_loss: 0.8155 - val_accuracy: 0.8337\n","Epoch 34/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7132 - accuracy: 0.8891 - val_loss: 0.7992 - val_accuracy: 0.8428\n","Epoch 35/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7063 - accuracy: 0.8922 - val_loss: 0.7946 - val_accuracy: 0.8495\n","Epoch 36/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7048 - accuracy: 0.8928 - val_loss: 0.7972 - val_accuracy: 0.8382\n","Epoch 37/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7005 - accuracy: 0.8913 - val_loss: 0.7869 - val_accuracy: 0.8450\n","Epoch 38/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6966 - accuracy: 0.8922 - val_loss: 0.7850 - val_accuracy: 0.8473\n","Epoch 39/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6906 - accuracy: 0.8964 - val_loss: 0.7810 - val_accuracy: 0.8462\n","Epoch 40/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6884 - accuracy: 0.8970 - val_loss: 0.7791 - val_accuracy: 0.8439\n","Epoch 41/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.7002 - accuracy: 0.8826 - val_loss: 0.7931 - val_accuracy: 0.8337\n","Epoch 42/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6865 - accuracy: 0.8894 - val_loss: 0.7971 - val_accuracy: 0.8314\n","Epoch 43/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6874 - accuracy: 0.8913 - val_loss: 0.7994 - val_accuracy: 0.8292\n","Epoch 44/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6769 - accuracy: 0.8967 - val_loss: 0.7688 - val_accuracy: 0.8495\n","Epoch 45/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6706 - accuracy: 0.8953 - val_loss: 0.7735 - val_accuracy: 0.8484\n","Epoch 46/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6625 - accuracy: 0.9044 - val_loss: 0.7773 - val_accuracy: 0.8495\n","Epoch 47/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6602 - accuracy: 0.9063 - val_loss: 0.7618 - val_accuracy: 0.8484\n","Epoch 48/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6553 - accuracy: 0.9078 - val_loss: 0.7686 - val_accuracy: 0.8473\n","Epoch 49/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6689 - accuracy: 0.8936 - val_loss: 0.7616 - val_accuracy: 0.8518\n","Epoch 50/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.6729 - accuracy: 0.8894 - val_loss: 0.7678 - val_accuracy: 0.8416\n","Epoch 51/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6611 - accuracy: 0.8964 - val_loss: 0.7668 - val_accuracy: 0.8428\n","Epoch 52/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6448 - accuracy: 0.9066 - val_loss: 0.7524 - val_accuracy: 0.8462\n","Epoch 53/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6384 - accuracy: 0.9072 - val_loss: 0.7530 - val_accuracy: 0.8462\n","Epoch 54/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.6368 - accuracy: 0.9126 - val_loss: 0.7535 - val_accuracy: 0.8473\n","Epoch 55/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.6462 - accuracy: 0.8942 - val_loss: 0.7495 - val_accuracy: 0.8462\n","Epoch 56/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.6314 - accuracy: 0.9103 - val_loss: 0.7528 - val_accuracy: 0.8473\n","Epoch 57/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6267 - accuracy: 0.9114 - val_loss: 0.7444 - val_accuracy: 0.8473\n","Epoch 58/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6289 - accuracy: 0.9055 - val_loss: 0.7590 - val_accuracy: 0.8405\n","Epoch 59/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6228 - accuracy: 0.9120 - val_loss: 0.7596 - val_accuracy: 0.8439\n","Epoch 60/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.6248 - accuracy: 0.9061 - val_loss: 0.7627 - val_accuracy: 0.8416\n","Epoch 61/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6321 - accuracy: 0.9007 - val_loss: 0.7492 - val_accuracy: 0.8450\n","Epoch 62/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.6120 - accuracy: 0.9143 - val_loss: 0.7341 - val_accuracy: 0.8518\n","Epoch 63/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6076 - accuracy: 0.9185 - val_loss: 0.7338 - val_accuracy: 0.8495\n","Epoch 64/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6096 - accuracy: 0.9162 - val_loss: 0.7315 - val_accuracy: 0.8552\n","Epoch 65/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6012 - accuracy: 0.9151 - val_loss: 0.7365 - val_accuracy: 0.8518\n","Epoch 66/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.6001 - accuracy: 0.9165 - val_loss: 0.7345 - val_accuracy: 0.8473\n","Epoch 67/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.6001 - accuracy: 0.9157 - val_loss: 0.7267 - val_accuracy: 0.8518\n","Epoch 68/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5996 - accuracy: 0.9109 - val_loss: 0.7248 - val_accuracy: 0.8507\n","Epoch 69/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5897 - accuracy: 0.9213 - val_loss: 0.7284 - val_accuracy: 0.8495\n","Epoch 70/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5904 - accuracy: 0.9194 - val_loss: 0.7357 - val_accuracy: 0.8495\n","Epoch 71/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5837 - accuracy: 0.9253 - val_loss: 0.7202 - val_accuracy: 0.8495\n","Epoch 72/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5804 - accuracy: 0.9250 - val_loss: 0.7183 - val_accuracy: 0.8518\n","Epoch 73/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5779 - accuracy: 0.9236 - val_loss: 0.7368 - val_accuracy: 0.8484\n","Epoch 74/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.5792 - accuracy: 0.9194 - val_loss: 0.7200 - val_accuracy: 0.8541\n","Epoch 75/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5765 - accuracy: 0.9225 - val_loss: 0.7129 - val_accuracy: 0.8529\n","Epoch 76/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5743 - accuracy: 0.9219 - val_loss: 0.7137 - val_accuracy: 0.8541\n","Epoch 77/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5669 - accuracy: 0.9267 - val_loss: 0.7209 - val_accuracy: 0.8575\n","Epoch 78/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5657 - accuracy: 0.9242 - val_loss: 0.7209 - val_accuracy: 0.8507\n","Epoch 79/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5660 - accuracy: 0.9225 - val_loss: 0.7116 - val_accuracy: 0.8518\n","Epoch 80/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.5623 - accuracy: 0.9278 - val_loss: 0.7132 - val_accuracy: 0.8541\n","Epoch 81/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5615 - accuracy: 0.9244 - val_loss: 0.7449 - val_accuracy: 0.8326\n","Epoch 82/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5577 - accuracy: 0.9281 - val_loss: 0.7316 - val_accuracy: 0.8405\n","Epoch 83/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.5533 - accuracy: 0.9295 - val_loss: 0.7105 - val_accuracy: 0.8563\n","Epoch 84/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5492 - accuracy: 0.9295 - val_loss: 0.7194 - val_accuracy: 0.8484\n","Epoch 85/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5457 - accuracy: 0.9293 - val_loss: 0.7035 - val_accuracy: 0.8541\n","Epoch 86/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5488 - accuracy: 0.9304 - val_loss: 0.7053 - val_accuracy: 0.8563\n","Epoch 87/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.5406 - accuracy: 0.9386 - val_loss: 0.7018 - val_accuracy: 0.8529\n","Epoch 88/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5351 - accuracy: 0.9363 - val_loss: 0.7137 - val_accuracy: 0.8495\n","Epoch 89/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.5359 - accuracy: 0.9369 - val_loss: 0.7134 - val_accuracy: 0.8439\n","Epoch 90/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5541 - accuracy: 0.9202 - val_loss: 0.6981 - val_accuracy: 0.8552\n","Epoch 91/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5297 - accuracy: 0.9394 - val_loss: 0.7053 - val_accuracy: 0.8507\n","Epoch 92/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5328 - accuracy: 0.9304 - val_loss: 0.7127 - val_accuracy: 0.8484\n","Epoch 93/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5266 - accuracy: 0.9369 - val_loss: 0.6975 - val_accuracy: 0.8552\n","Epoch 94/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5207 - accuracy: 0.9417 - val_loss: 0.6964 - val_accuracy: 0.8552\n","Epoch 95/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5191 - accuracy: 0.9392 - val_loss: 0.6996 - val_accuracy: 0.8631\n","Epoch 96/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.5184 - accuracy: 0.9380 - val_loss: 0.7198 - val_accuracy: 0.8371\n","Epoch 97/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5370 - accuracy: 0.9259 - val_loss: 0.6985 - val_accuracy: 0.8552\n","Epoch 98/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.5158 - accuracy: 0.9386 - val_loss: 0.6943 - val_accuracy: 0.8507\n","Epoch 99/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.5206 - accuracy: 0.9329 - val_loss: 0.7245 - val_accuracy: 0.8326\n","Epoch 100/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.5136 - accuracy: 0.9403 - val_loss: 0.7098 - val_accuracy: 0.8495\n","{'loss': [0.8978636860847473, 0.883056104183197, 0.875908613204956, 0.8628777265548706, 0.8557097315788269, 0.8494331240653992, 0.8414270877838135, 0.8372725248336792, 0.8304720520973206, 0.8257988691329956, 0.8290618658065796, 0.8142555356025696, 0.8147377967834473, 0.8029158115386963, 0.7994260191917419, 0.7967646718025208, 0.7954450845718384, 0.79117751121521, 0.778759241104126, 0.7740744948387146, 0.7678683400154114, 0.7672902345657349, 0.7631611824035645, 0.7540431022644043, 0.748702347278595, 0.7491549253463745, 0.743257462978363, 0.7440993785858154, 0.7451179027557373, 0.7327027916908264, 0.7251760363578796, 0.7201656103134155, 0.715341329574585, 0.7132493853569031, 0.7062550187110901, 0.7047854661941528, 0.7005172967910767, 0.6966440677642822, 0.6905656456947327, 0.6883634328842163, 0.7001624703407288, 0.6865220665931702, 0.6873962879180908, 0.676933228969574, 0.6706374883651733, 0.6624807715415955, 0.6602224111557007, 0.6552518606185913, 0.6689311265945435, 0.6728589534759521, 0.661119818687439, 0.644761860370636, 0.6383878588676453, 0.636806070804596, 0.6462341547012329, 0.6313751339912415, 0.626698911190033, 0.6289197206497192, 0.6227502226829529, 0.6248267889022827, 0.6320960521697998, 0.6120104193687439, 0.607581377029419, 0.6095720529556274, 0.6011950969696045, 0.6001457571983337, 0.6001089811325073, 0.5995587110519409, 0.5896772146224976, 0.5904437303543091, 0.5836604833602905, 0.5803988575935364, 0.5779035091400146, 0.5791575908660889, 0.5765410661697388, 0.574260950088501, 0.5669408440589905, 0.5657362341880798, 0.5660285353660583, 0.562328577041626, 0.5615062117576599, 0.5577294230461121, 0.553345799446106, 0.5491849184036255, 0.5456517338752747, 0.548816442489624, 0.5406407117843628, 0.5351467132568359, 0.5358818769454956, 0.5541001558303833, 0.5297349691390991, 0.532755434513092, 0.5266311764717102, 0.5206875801086426, 0.5191130638122559, 0.518365204334259, 0.536950409412384, 0.5157619714736938, 0.5206082463264465, 0.513562798500061], 'accuracy': [0.8432371020317078, 0.8460667729377747, 0.8497453331947327, 0.8551216721534729, 0.8531408905982971, 0.8593661785125732, 0.8604980111122131, 0.8616299033164978, 0.8616299033164978, 0.8658743500709534, 0.859649121761322, 0.8610639572143555, 0.8621957898139954, 0.8667232394218445, 0.868986964225769, 0.8667232394218445, 0.8650254607200623, 0.8718166351318359, 0.8732314705848694, 0.8769100308418274, 0.881154477596283, 0.8763440847396851, 0.8783248662948608, 0.8800226449966431, 0.8842670917510986, 0.8797396421432495, 0.88285231590271, 0.8771929740905762, 0.8732314705848694, 0.8834182024002075, 0.8853989839553833, 0.8873797655105591, 0.8902093768119812, 0.8890775442123413, 0.892190158367157, 0.8927561044692993, 0.8913412690162659, 0.892190158367157, 0.8964346647262573, 0.8970005512237549, 0.8825693130493164, 0.8893604874610901, 0.8913412690162659, 0.8967176079750061, 0.8953027725219727, 0.9043576717376709, 0.9063384532928467, 0.9077532291412354, 0.8936049938201904, 0.8893604874610901, 0.8964346647262573, 0.9066213965415955, 0.9071873426437378, 0.912563681602478, 0.8941709399223328, 0.9102999567985535, 0.9114317893981934, 0.9054895043373108, 0.9119977355003357, 0.9060554504394531, 0.9006791114807129, 0.9142614603042603, 0.9185059666633606, 0.916242241859436, 0.9151103496551514, 0.9165251851081848, 0.9156762957572937, 0.9108659029006958, 0.9213355779647827, 0.9193548560142517, 0.9252971410751343, 0.9250141382217407, 0.9235993027687073, 0.9193548560142517, 0.9224674701690674, 0.921901524066925, 0.926711916923523, 0.9241652488708496, 0.9224674701690674, 0.9278438091278076, 0.9244481921195984, 0.9281267523765564, 0.9295415878295898, 0.9295415878295898, 0.9292586445808411, 0.930390477180481, 0.9385964870452881, 0.9363327622413635, 0.9368987083435059, 0.9202037453651428, 0.9394453763961792, 0.930390477180481, 0.9368987083435059, 0.9417091012001038, 0.9391624331474304, 0.9380305409431458, 0.9258630275726318, 0.9385964870452881, 0.9329372048377991, 0.9402942657470703], 'val_loss': [1.2053216695785522, 1.1884251832962036, 1.1725761890411377, 1.1585123538970947, 1.144558310508728, 1.1262052059173584, 1.1144931316375732, 1.094651699066162, 1.0791960954666138, 1.062221646308899, 1.039358377456665, 1.0235373973846436, 1.0053737163543701, 0.9870399236679077, 0.9703683853149414, 0.9621861577033997, 0.9258605241775513, 0.9122461080551147, 0.9038861989974976, 0.894355058670044, 0.8657854795455933, 0.9015349745750427, 0.8633663058280945, 0.8417629599571228, 0.8461734056472778, 0.843401312828064, 0.8187080025672913, 0.8304432034492493, 0.8107581734657288, 0.8069310784339905, 0.8097766041755676, 0.8057919144630432, 0.8155155181884766, 0.7992132306098938, 0.7945988774299622, 0.7971798181533813, 0.78689044713974, 0.784957766532898, 0.7810184359550476, 0.7791444063186646, 0.7930966019630432, 0.797099232673645, 0.7994089722633362, 0.76882004737854, 0.7734568119049072, 0.7772530317306519, 0.7617544531822205, 0.7685961127281189, 0.761640191078186, 0.7677781581878662, 0.7668265700340271, 0.752376139163971, 0.7529862523078918, 0.7535464763641357, 0.7494537234306335, 0.752765417098999, 0.7444139719009399, 0.7590010762214661, 0.7595707178115845, 0.7626891136169434, 0.7492218017578125, 0.7341078519821167, 0.7338391542434692, 0.7314813733100891, 0.7364782094955444, 0.7344747185707092, 0.7266717553138733, 0.7247642278671265, 0.7284306883811951, 0.7357387542724609, 0.7201569676399231, 0.7183184027671814, 0.7368450164794922, 0.7199576497077942, 0.7129225730895996, 0.7137246131896973, 0.7209420800209045, 0.7208766341209412, 0.711555540561676, 0.7132171988487244, 0.7448858618736267, 0.73161780834198, 0.7105280160903931, 0.7194284200668335, 0.703477680683136, 0.7052620053291321, 0.7018484473228455, 0.7137316465377808, 0.7134130001068115, 0.6980863809585571, 0.7052590250968933, 0.7126688957214355, 0.6975401639938354, 0.6964343190193176, 0.6996230483055115, 0.7198275327682495, 0.6984883546829224, 0.6943051218986511, 0.7245023846626282, 0.709798276424408], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.529411792755127, 0.5723981857299805, 0.6742081642150879, 0.7952488660812378, 0.8020362257957458, 0.8235294222831726, 0.8167420625686646, 0.8212669491767883, 0.8099547624588013, 0.8190045356750488, 0.8099547624588013, 0.8088235259056091, 0.8020362257957458, 0.7986425161361694, 0.8246606588363647, 0.8054298758506775, 0.8042986392974854, 0.8054298758506775, 0.8223981857299805, 0.7963801026344299, 0.8099547624588013, 0.8167420625686646, 0.820135772228241, 0.8212669491767883, 0.8416289687156677, 0.8269230723381042, 0.8461538553237915, 0.8450226187705994, 0.8416289687156677, 0.8427602052688599, 0.8337104320526123, 0.8427602052688599, 0.8495475053787231, 0.8382353186607361, 0.8450226187705994, 0.8472850918769836, 0.8461538553237915, 0.8438913822174072, 0.8337104320526123, 0.831447958946228, 0.8291855454444885, 0.8495475053787231, 0.848416268825531, 0.8495475053787231, 0.848416268825531, 0.8472850918769836, 0.8518099784851074, 0.8416289687156677, 0.8427602052688599, 0.8461538553237915, 0.8461538553237915, 0.8472850918769836, 0.8461538553237915, 0.8472850918769836, 0.8472850918769836, 0.8404977321624756, 0.8438913822174072, 0.8416289687156677, 0.8450226187705994, 0.8518099784851074, 0.8495475053787231, 0.8552036285400391, 0.8518099784851074, 0.8472850918769836, 0.8518099784851074, 0.8506787419319153, 0.8495475053787231, 0.8495475053787231, 0.8495475053787231, 0.8518099784851074, 0.848416268825531, 0.8540723919868469, 0.8529411554336548, 0.8540723919868469, 0.8574660420417786, 0.8506787419319153, 0.8518099784851074, 0.8540723919868469, 0.8325791954994202, 0.8404977321624756, 0.8563348650932312, 0.848416268825531, 0.8540723919868469, 0.8563348650932312, 0.8529411554336548, 0.8495475053787231, 0.8438913822174072, 0.8552036285400391, 0.8506787419319153, 0.848416268825531, 0.8552036285400391, 0.8552036285400391, 0.8631221652030945, 0.837104082107544, 0.8552036285400391, 0.8506787419319153, 0.8325791954994202, 0.8495475053787231]}\n","45/45 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 4s 28ms/step - loss: 0.8809 - accuracy: 0.8527 - val_loss: 1.2083 - val_accuracy: 0.4855\n","Epoch 2/100\n"," 1/31 [..............................] - ETA: 0s - loss: 0.8632 - accuracy: 0.8359"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 0s 16ms/step - loss: 0.8586 - accuracy: 0.8581 - val_loss: 1.1877 - val_accuracy: 0.4886\n","Epoch 3/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.8488 - accuracy: 0.8610 - val_loss: 1.1714 - val_accuracy: 0.5114\n","Epoch 4/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8436 - accuracy: 0.8623 - val_loss: 1.1536 - val_accuracy: 0.5940\n","Epoch 5/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8308 - accuracy: 0.8659 - val_loss: 1.1351 - val_accuracy: 0.7397\n","Epoch 6/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8245 - accuracy: 0.8677 - val_loss: 1.1167 - val_accuracy: 0.7831\n","Epoch 7/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.8204 - accuracy: 0.8682 - val_loss: 1.0964 - val_accuracy: 0.8357\n","Epoch 8/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.8108 - accuracy: 0.8724 - val_loss: 1.0769 - val_accuracy: 0.8337\n","Epoch 9/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.8068 - accuracy: 0.8729 - val_loss: 1.0575 - val_accuracy: 0.8244\n","Epoch 10/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7967 - accuracy: 0.8755 - val_loss: 1.0378 - val_accuracy: 0.8140\n","Epoch 11/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7971 - accuracy: 0.8752 - val_loss: 1.0243 - val_accuracy: 0.7893\n","Epoch 12/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.7990 - accuracy: 0.8724 - val_loss: 0.9932 - val_accuracy: 0.8223\n","Epoch 13/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7846 - accuracy: 0.8736 - val_loss: 0.9853 - val_accuracy: 0.7820\n","Epoch 14/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7760 - accuracy: 0.8783 - val_loss: 0.9504 - val_accuracy: 0.8140\n","Epoch 15/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7697 - accuracy: 0.8786 - val_loss: 0.9348 - val_accuracy: 0.8048\n","Epoch 16/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7680 - accuracy: 0.8749 - val_loss: 0.9211 - val_accuracy: 0.8058\n","Epoch 17/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.7617 - accuracy: 0.8801 - val_loss: 0.8928 - val_accuracy: 0.8233\n","Epoch 18/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.7549 - accuracy: 0.8842 - val_loss: 0.8777 - val_accuracy: 0.8295\n","Epoch 19/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7527 - accuracy: 0.8819 - val_loss: 0.8732 - val_accuracy: 0.8202\n","Epoch 20/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.7437 - accuracy: 0.8884 - val_loss: 0.8590 - val_accuracy: 0.8285\n","Epoch 21/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7448 - accuracy: 0.8835 - val_loss: 0.8862 - val_accuracy: 0.8037\n","Epoch 22/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.7370 - accuracy: 0.8871 - val_loss: 0.8690 - val_accuracy: 0.8161\n","Epoch 23/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.7313 - accuracy: 0.8871 - val_loss: 0.8219 - val_accuracy: 0.8388\n","Epoch 24/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7236 - accuracy: 0.8886 - val_loss: 0.8112 - val_accuracy: 0.8440\n","Epoch 25/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7204 - accuracy: 0.8948 - val_loss: 0.8053 - val_accuracy: 0.8492\n","Epoch 26/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7260 - accuracy: 0.8871 - val_loss: 0.8001 - val_accuracy: 0.8492\n","Epoch 27/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.7230 - accuracy: 0.8817 - val_loss: 0.7964 - val_accuracy: 0.8492\n","Epoch 28/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.7118 - accuracy: 0.8871 - val_loss: 0.7939 - val_accuracy: 0.8461\n","Epoch 29/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.7012 - accuracy: 0.8933 - val_loss: 0.7978 - val_accuracy: 0.8481\n","Epoch 30/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.6972 - accuracy: 0.8920 - val_loss: 0.7892 - val_accuracy: 0.8481\n","Epoch 31/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6931 - accuracy: 0.8992 - val_loss: 0.7870 - val_accuracy: 0.8471\n","Epoch 32/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6948 - accuracy: 0.8946 - val_loss: 0.7875 - val_accuracy: 0.8481\n","Epoch 33/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6910 - accuracy: 0.8925 - val_loss: 0.8405 - val_accuracy: 0.8233\n","Epoch 34/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.7006 - accuracy: 0.8904 - val_loss: 0.7819 - val_accuracy: 0.8512\n","Epoch 35/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.6878 - accuracy: 0.8920 - val_loss: 0.7916 - val_accuracy: 0.8399\n","Epoch 36/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6841 - accuracy: 0.8951 - val_loss: 0.7845 - val_accuracy: 0.8430\n","Epoch 37/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.6687 - accuracy: 0.9072 - val_loss: 0.7715 - val_accuracy: 0.8512\n","Epoch 38/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.6627 - accuracy: 0.9059 - val_loss: 0.7778 - val_accuracy: 0.8471\n","Epoch 39/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6717 - accuracy: 0.8938 - val_loss: 0.7734 - val_accuracy: 0.8461\n","Epoch 40/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6590 - accuracy: 0.9021 - val_loss: 0.7653 - val_accuracy: 0.8450\n","Epoch 41/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6513 - accuracy: 0.9098 - val_loss: 0.7678 - val_accuracy: 0.8461\n","Epoch 42/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.6534 - accuracy: 0.9034 - val_loss: 0.7660 - val_accuracy: 0.8471\n","Epoch 43/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.6434 - accuracy: 0.9090 - val_loss: 0.7592 - val_accuracy: 0.8502\n","Epoch 44/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6458 - accuracy: 0.9052 - val_loss: 0.7740 - val_accuracy: 0.8450\n","Epoch 45/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6416 - accuracy: 0.9070 - val_loss: 0.7652 - val_accuracy: 0.8492\n","Epoch 46/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.6421 - accuracy: 0.9021 - val_loss: 0.7870 - val_accuracy: 0.8368\n","Epoch 47/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.6340 - accuracy: 0.9106 - val_loss: 0.7499 - val_accuracy: 0.8502\n","Epoch 48/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.6284 - accuracy: 0.9106 - val_loss: 0.7579 - val_accuracy: 0.8523\n","Epoch 49/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.6272 - accuracy: 0.9096 - val_loss: 0.7479 - val_accuracy: 0.8481\n","Epoch 50/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.6299 - accuracy: 0.9096 - val_loss: 0.7440 - val_accuracy: 0.8492\n","Epoch 51/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.6196 - accuracy: 0.9114 - val_loss: 0.7435 - val_accuracy: 0.8450\n","Epoch 52/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.6145 - accuracy: 0.9147 - val_loss: 0.7540 - val_accuracy: 0.8492\n","Epoch 53/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6130 - accuracy: 0.9150 - val_loss: 0.7441 - val_accuracy: 0.8502\n","Epoch 54/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.6293 - accuracy: 0.9034 - val_loss: 0.7551 - val_accuracy: 0.8502\n","Epoch 55/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6047 - accuracy: 0.9165 - val_loss: 0.7345 - val_accuracy: 0.8492\n","Epoch 56/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6019 - accuracy: 0.9186 - val_loss: 0.7370 - val_accuracy: 0.8481\n","Epoch 57/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5962 - accuracy: 0.9217 - val_loss: 0.7354 - val_accuracy: 0.8523\n","Epoch 58/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5922 - accuracy: 0.9214 - val_loss: 0.7327 - val_accuracy: 0.8492\n","Epoch 59/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5962 - accuracy: 0.9145 - val_loss: 0.7304 - val_accuracy: 0.8543\n","Epoch 60/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5874 - accuracy: 0.9220 - val_loss: 0.7337 - val_accuracy: 0.8492\n","Epoch 61/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5903 - accuracy: 0.9207 - val_loss: 0.7321 - val_accuracy: 0.8512\n","Epoch 62/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5883 - accuracy: 0.9199 - val_loss: 0.7295 - val_accuracy: 0.8533\n","Epoch 63/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5808 - accuracy: 0.9243 - val_loss: 0.7326 - val_accuracy: 0.8595\n","Epoch 64/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5786 - accuracy: 0.9227 - val_loss: 0.7256 - val_accuracy: 0.8554\n","Epoch 65/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5730 - accuracy: 0.9238 - val_loss: 0.7233 - val_accuracy: 0.8533\n","Epoch 66/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5716 - accuracy: 0.9235 - val_loss: 0.7398 - val_accuracy: 0.8492\n","Epoch 67/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.5751 - accuracy: 0.9243 - val_loss: 0.7211 - val_accuracy: 0.8543\n","Epoch 68/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.5696 - accuracy: 0.9243 - val_loss: 0.7521 - val_accuracy: 0.8409\n","Epoch 69/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5671 - accuracy: 0.9256 - val_loss: 0.7172 - val_accuracy: 0.8554\n","Epoch 70/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.5561 - accuracy: 0.9292 - val_loss: 0.7324 - val_accuracy: 0.8492\n","Epoch 71/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5549 - accuracy: 0.9320 - val_loss: 0.7404 - val_accuracy: 0.8471\n","Epoch 72/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5624 - accuracy: 0.9240 - val_loss: 0.7149 - val_accuracy: 0.8585\n","Epoch 73/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.5492 - accuracy: 0.9326 - val_loss: 0.7192 - val_accuracy: 0.8492\n","Epoch 74/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.5500 - accuracy: 0.9320 - val_loss: 0.7103 - val_accuracy: 0.8533\n","Epoch 75/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5427 - accuracy: 0.9341 - val_loss: 0.7125 - val_accuracy: 0.8564\n","Epoch 76/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5400 - accuracy: 0.9346 - val_loss: 0.7110 - val_accuracy: 0.8574\n","Epoch 77/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5361 - accuracy: 0.9344 - val_loss: 0.7284 - val_accuracy: 0.8481\n","Epoch 78/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5387 - accuracy: 0.9307 - val_loss: 0.7155 - val_accuracy: 0.8554\n","Epoch 79/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5334 - accuracy: 0.9372 - val_loss: 0.7231 - val_accuracy: 0.8492\n","Epoch 80/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.5299 - accuracy: 0.9346 - val_loss: 0.7062 - val_accuracy: 0.8533\n","Epoch 81/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5304 - accuracy: 0.9364 - val_loss: 0.7625 - val_accuracy: 0.8213\n","Epoch 82/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.5692 - accuracy: 0.9088 - val_loss: 0.7127 - val_accuracy: 0.8554\n","Epoch 83/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.5272 - accuracy: 0.9346 - val_loss: 0.7058 - val_accuracy: 0.8564\n","Epoch 84/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5188 - accuracy: 0.9375 - val_loss: 0.7187 - val_accuracy: 0.8502\n","Epoch 85/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5180 - accuracy: 0.9385 - val_loss: 0.7028 - val_accuracy: 0.8626\n","Epoch 86/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5177 - accuracy: 0.9382 - val_loss: 0.7057 - val_accuracy: 0.8523\n","Epoch 87/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5141 - accuracy: 0.9413 - val_loss: 0.7097 - val_accuracy: 0.8564\n","Epoch 88/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5105 - accuracy: 0.9434 - val_loss: 0.7187 - val_accuracy: 0.8471\n","Epoch 89/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.5118 - accuracy: 0.9385 - val_loss: 0.7090 - val_accuracy: 0.8523\n","Epoch 90/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5042 - accuracy: 0.9426 - val_loss: 0.7019 - val_accuracy: 0.8533\n","Epoch 91/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5019 - accuracy: 0.9429 - val_loss: 0.7570 - val_accuracy: 0.8244\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5114 - accuracy: 0.9357 - val_loss: 0.7174 - val_accuracy: 0.8502\n","Epoch 93/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5047 - accuracy: 0.9385 - val_loss: 0.7191 - val_accuracy: 0.8523\n","Epoch 94/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5073 - accuracy: 0.9362 - val_loss: 0.7138 - val_accuracy: 0.8502\n","Epoch 95/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5072 - accuracy: 0.9382 - val_loss: 0.6983 - val_accuracy: 0.8512\n","Epoch 96/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5036 - accuracy: 0.9403 - val_loss: 0.7097 - val_accuracy: 0.8543\n","Epoch 97/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5011 - accuracy: 0.9377 - val_loss: 0.7001 - val_accuracy: 0.8574\n","Epoch 98/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4949 - accuracy: 0.9411 - val_loss: 0.7176 - val_accuracy: 0.8533\n","Epoch 99/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4942 - accuracy: 0.9406 - val_loss: 0.6965 - val_accuracy: 0.8564\n","Epoch 100/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4807 - accuracy: 0.9535 - val_loss: 0.6991 - val_accuracy: 0.8564\n","{'loss': [0.8809210062026978, 0.8585598468780518, 0.848821222782135, 0.8436192870140076, 0.830848217010498, 0.8245028257369995, 0.8204060792922974, 0.8108434677124023, 0.806801974773407, 0.7967448830604553, 0.7971454858779907, 0.7990266680717468, 0.7846142053604126, 0.776018500328064, 0.7697489857673645, 0.768018901348114, 0.7616533041000366, 0.754856526851654, 0.7526645660400391, 0.743705153465271, 0.7448373436927795, 0.7369763851165771, 0.7313283085823059, 0.7236464619636536, 0.7203587889671326, 0.7259775996208191, 0.7229983806610107, 0.7117522358894348, 0.7012151479721069, 0.6972432136535645, 0.6931174397468567, 0.6948243975639343, 0.6909947991371155, 0.7006301879882812, 0.6877850294113159, 0.684095025062561, 0.6686719059944153, 0.6626912355422974, 0.6717439889907837, 0.65897536277771, 0.6513492465019226, 0.65341717004776, 0.6433858871459961, 0.6458263993263245, 0.6416456699371338, 0.6420552730560303, 0.6339608430862427, 0.6283659338951111, 0.627220869064331, 0.6299225687980652, 0.6196170449256897, 0.6144652366638184, 0.6129983067512512, 0.6293312311172485, 0.6047332286834717, 0.6019001603126526, 0.5961586236953735, 0.5922061800956726, 0.596170961856842, 0.5874000787734985, 0.5903025269508362, 0.5883246660232544, 0.5808479189872742, 0.5785833597183228, 0.5730063915252686, 0.5715722441673279, 0.5751002430915833, 0.5696297287940979, 0.567054033279419, 0.5561186671257019, 0.5548535585403442, 0.5624105334281921, 0.5492285490036011, 0.5499874353408813, 0.5427266955375671, 0.5399531126022339, 0.5361358523368835, 0.5387201309204102, 0.5334146022796631, 0.5298742055892944, 0.5304403901100159, 0.5691806077957153, 0.5272384285926819, 0.5187947750091553, 0.5180294513702393, 0.517749011516571, 0.5140715837478638, 0.5104661583900452, 0.5117940306663513, 0.5042160749435425, 0.5019364356994629, 0.5113537907600403, 0.5047266483306885, 0.5073003172874451, 0.5072290897369385, 0.5035662055015564, 0.5011276006698608, 0.4949186146259308, 0.4941709041595459, 0.480650395154953], 'accuracy': [0.8527131676673889, 0.8581395149230957, 0.8609819412231445, 0.8622739315032959, 0.8658914566040039, 0.8677002787590027, 0.8682170510292053, 0.8723514080047607, 0.8728682398796082, 0.8754522204399109, 0.8751937747001648, 0.8723514080047607, 0.8736433982849121, 0.8782945871353149, 0.8785529732704163, 0.8749353885650635, 0.880103349685669, 0.8842377066612244, 0.8819121718406677, 0.8883720636367798, 0.8834625482559204, 0.8870801329612732, 0.8870801329612732, 0.8886305093765259, 0.8948320150375366, 0.8870801329612732, 0.8816537261009216, 0.8870801329612732, 0.8932816386222839, 0.8919896483421326, 0.8992248177528381, 0.8945736289024353, 0.89250648021698, 0.8904392719268799, 0.8919896483421326, 0.8950904607772827, 0.9072351455688477, 0.9059431552886963, 0.8937984704971313, 0.9020671844482422, 0.9098191261291504, 0.9033591747283936, 0.9090439081192017, 0.9051679372787476, 0.9069767594337463, 0.9020671844482422, 0.9105943441390991, 0.9105943441390991, 0.9095607399940491, 0.9095607399940491, 0.9113695025444031, 0.9147287011146545, 0.9149870872497559, 0.9033591747283936, 0.9165374636650085, 0.9186046719551086, 0.921705424785614, 0.9214470386505127, 0.9144702553749084, 0.9219638109207153, 0.920671820640564, 0.91989666223526, 0.9242894053459167, 0.9227390289306641, 0.9237726330757141, 0.923514187335968, 0.9242894053459167, 0.9242894053459167, 0.9255813956260681, 0.9291989803314209, 0.932041347026825, 0.9240310192108154, 0.9325581192970276, 0.932041347026825, 0.934108555316925, 0.9346253275871277, 0.9343669414520264, 0.9307493567466736, 0.9372093081474304, 0.9346253275871277, 0.9364340901374817, 0.9087855219841003, 0.9346253275871277, 0.9374676942825317, 0.9385012984275818, 0.9382429122924805, 0.9413436651229858, 0.9434108734130859, 0.9385012984275818, 0.9426356554031372, 0.9428940415382385, 0.9356589317321777, 0.9385012984275818, 0.9361757040023804, 0.9382429122924805, 0.9403100609779358, 0.9377260804176331, 0.9410852789878845, 0.9405684471130371, 0.9534883499145508], 'val_loss': [1.2082555294036865, 1.1876962184906006, 1.1713602542877197, 1.1535998582839966, 1.1351426839828491, 1.1167123317718506, 1.0963876247406006, 1.076869249343872, 1.0575052499771118, 1.0378150939941406, 1.024344563484192, 0.9931575059890747, 0.9852762222290039, 0.9504351615905762, 0.9347919225692749, 0.9210695624351501, 0.8927876949310303, 0.8777301907539368, 0.8731632828712463, 0.859042227268219, 0.8861909508705139, 0.8690406680107117, 0.8219460248947144, 0.8112261295318604, 0.8052772879600525, 0.8001028895378113, 0.7964334487915039, 0.793856680393219, 0.7977727055549622, 0.7891588807106018, 0.7869929671287537, 0.7874720692634583, 0.8405459523200989, 0.7818838357925415, 0.7915974259376526, 0.7844776511192322, 0.7714961171150208, 0.7777995467185974, 0.7734269499778748, 0.7653143405914307, 0.7678328156471252, 0.7659667730331421, 0.7591837644577026, 0.7740200161933899, 0.7652313113212585, 0.7870422005653381, 0.7499229311943054, 0.7579347491264343, 0.7479348182678223, 0.744036853313446, 0.7435035109519958, 0.7539622187614441, 0.7440643906593323, 0.755050778388977, 0.7344590425491333, 0.7370407581329346, 0.735447108745575, 0.7327001094818115, 0.730371356010437, 0.7337484359741211, 0.7321015000343323, 0.7294551134109497, 0.7325924038887024, 0.725612223148346, 0.7232505679130554, 0.7398448586463928, 0.721105694770813, 0.7521371245384216, 0.7171878218650818, 0.7324025630950928, 0.7403980493545532, 0.7149161696434021, 0.7192426919937134, 0.7102746963500977, 0.712485671043396, 0.7110034823417664, 0.7283754348754883, 0.7154629230499268, 0.7230692505836487, 0.7062169909477234, 0.7624920010566711, 0.7127089500427246, 0.7057501673698425, 0.7186700701713562, 0.7028151750564575, 0.7057031393051147, 0.7097217440605164, 0.7186967730522156, 0.7089570760726929, 0.7019458413124084, 0.7570025324821472, 0.7174088954925537, 0.7191249132156372, 0.713767945766449, 0.6983156204223633, 0.7096869945526123, 0.700093686580658, 0.717643678188324, 0.6965245604515076, 0.6991156935691833], 'val_accuracy': [0.48553720116615295, 0.4886363744735718, 0.5113636255264282, 0.5940082669258118, 0.7396694421768188, 0.7830578684806824, 0.83574378490448, 0.8336777091026306, 0.8243801593780518, 0.8140496015548706, 0.78925621509552, 0.8223140239715576, 0.7820248007774353, 0.8140496015548706, 0.8047520518302917, 0.8057851195335388, 0.8233470916748047, 0.8295454382896423, 0.8202479481697083, 0.8285123705863953, 0.8037189841270447, 0.81611567735672, 0.8388429880142212, 0.8440082669258118, 0.8491735458374023, 0.8491735458374023, 0.8491735458374023, 0.8460744023323059, 0.8481404781341553, 0.8481404781341553, 0.8471074104309082, 0.8481404781341553, 0.8233470916748047, 0.8512396812438965, 0.8398760557174683, 0.8429751992225647, 0.8512396812438965, 0.8471074104309082, 0.8460744023323059, 0.8450413346290588, 0.8460744023323059, 0.8471074104309082, 0.8502066135406494, 0.8450413346290588, 0.8491735458374023, 0.836776852607727, 0.8502066135406494, 0.8522727489471436, 0.8481404781341553, 0.8491735458374023, 0.8450413346290588, 0.8491735458374023, 0.8502066135406494, 0.8502066135406494, 0.8491735458374023, 0.8481404781341553, 0.8522727489471436, 0.8491735458374023, 0.8543388247489929, 0.8491735458374023, 0.8512396812438965, 0.8533057570457458, 0.8595041036605835, 0.85537189245224, 0.8533057570457458, 0.8491735458374023, 0.8543388247489929, 0.8409090638160706, 0.85537189245224, 0.8491735458374023, 0.8471074104309082, 0.8584710955619812, 0.8491735458374023, 0.8533057570457458, 0.8564049601554871, 0.8574380278587341, 0.8481404781341553, 0.85537189245224, 0.8491735458374023, 0.8533057570457458, 0.8212810158729553, 0.85537189245224, 0.8564049601554871, 0.8502066135406494, 0.8626033067703247, 0.8522727489471436, 0.8564049601554871, 0.8471074104309082, 0.8522727489471436, 0.8533057570457458, 0.8243801593780518, 0.8502066135406494, 0.8522727489471436, 0.8502066135406494, 0.8512396812438965, 0.8543388247489929, 0.8574380278587341, 0.8533057570457458, 0.8564049601554871, 0.8564049601554871]}\n","32/32 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 4s 35ms/step - loss: 0.5569 - accuracy: 0.9146 - val_loss: 1.0412 - val_accuracy: 0.4849\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 1s 20ms/step - loss: 0.5374 - accuracy: 0.9230 - val_loss: 1.0034 - val_accuracy: 0.4871\n","Epoch 3/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5320 - accuracy: 0.9291 - val_loss: 0.9768 - val_accuracy: 0.4957\n","Epoch 4/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.5397 - accuracy: 0.9189 - val_loss: 0.9430 - val_accuracy: 0.5334\n","Epoch 5/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5222 - accuracy: 0.9324 - val_loss: 0.9223 - val_accuracy: 0.5905\n","Epoch 6/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5191 - accuracy: 0.9337 - val_loss: 0.8935 - val_accuracy: 0.7144\n","Epoch 7/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.5151 - accuracy: 0.9353 - val_loss: 0.8826 - val_accuracy: 0.6983\n","Epoch 8/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5089 - accuracy: 0.9399 - val_loss: 0.8688 - val_accuracy: 0.7080\n","Epoch 9/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5109 - accuracy: 0.9402 - val_loss: 0.8457 - val_accuracy: 0.7651\n","Epoch 10/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.5084 - accuracy: 0.9362 - val_loss: 0.8008 - val_accuracy: 0.8502\n","Epoch 11/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5262 - accuracy: 0.9216 - val_loss: 0.8194 - val_accuracy: 0.7619\n","Epoch 12/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5021 - accuracy: 0.9432 - val_loss: 0.7771 - val_accuracy: 0.8448\n","Epoch 13/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5003 - accuracy: 0.9423 - val_loss: 0.8012 - val_accuracy: 0.7565\n","Epoch 14/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5016 - accuracy: 0.9378 - val_loss: 0.7382 - val_accuracy: 0.8545\n","Epoch 15/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4936 - accuracy: 0.9432 - val_loss: 0.7236 - val_accuracy: 0.8578\n","Epoch 16/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4858 - accuracy: 0.9483 - val_loss: 0.6933 - val_accuracy: 0.8675\n","Epoch 17/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4923 - accuracy: 0.9407 - val_loss: 0.6635 - val_accuracy: 0.8610\n","Epoch 18/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4931 - accuracy: 0.9402 - val_loss: 0.6512 - val_accuracy: 0.8631\n","Epoch 19/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4924 - accuracy: 0.9375 - val_loss: 0.6363 - val_accuracy: 0.8653\n","Epoch 20/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4784 - accuracy: 0.9477 - val_loss: 0.6314 - val_accuracy: 0.8610\n","Epoch 21/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4796 - accuracy: 0.9467 - val_loss: 0.6134 - val_accuracy: 0.8696\n","Epoch 22/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4776 - accuracy: 0.9502 - val_loss: 0.6095 - val_accuracy: 0.8707\n","Epoch 23/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4770 - accuracy: 0.9461 - val_loss: 0.6089 - val_accuracy: 0.8685\n","Epoch 24/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4680 - accuracy: 0.9555 - val_loss: 0.5920 - val_accuracy: 0.8761\n","Epoch 25/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4716 - accuracy: 0.9499 - val_loss: 0.6150 - val_accuracy: 0.8685\n","Epoch 26/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4677 - accuracy: 0.9529 - val_loss: 0.5821 - val_accuracy: 0.8922\n","Epoch 27/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4710 - accuracy: 0.9477 - val_loss: 0.5938 - val_accuracy: 0.8782\n","Epoch 28/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.4731 - accuracy: 0.9388 - val_loss: 0.5770 - val_accuracy: 0.8955\n","Epoch 29/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.4569 - accuracy: 0.9564 - val_loss: 0.5766 - val_accuracy: 0.8912\n","Epoch 30/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4547 - accuracy: 0.9553 - val_loss: 0.5754 - val_accuracy: 0.8944\n","Epoch 31/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.4738 - accuracy: 0.9407 - val_loss: 0.6334 - val_accuracy: 0.8707\n","Epoch 32/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4746 - accuracy: 0.9429 - val_loss: 0.5937 - val_accuracy: 0.8804\n","Epoch 33/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4587 - accuracy: 0.9523 - val_loss: 0.5829 - val_accuracy: 0.8890\n","Epoch 34/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4539 - accuracy: 0.9539 - val_loss: 0.5735 - val_accuracy: 0.8933\n","Epoch 35/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4477 - accuracy: 0.9564 - val_loss: 0.5857 - val_accuracy: 0.8847\n","Epoch 36/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4505 - accuracy: 0.9555 - val_loss: 0.5756 - val_accuracy: 0.8869\n","Epoch 37/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4511 - accuracy: 0.9566 - val_loss: 0.5800 - val_accuracy: 0.8825\n","Epoch 38/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4438 - accuracy: 0.9599 - val_loss: 0.5702 - val_accuracy: 0.8912\n","Epoch 39/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4442 - accuracy: 0.9582 - val_loss: 0.5840 - val_accuracy: 0.8847\n","Epoch 40/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4436 - accuracy: 0.9580 - val_loss: 0.5743 - val_accuracy: 0.8901\n","Epoch 41/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4457 - accuracy: 0.9553 - val_loss: 0.5729 - val_accuracy: 0.8912\n","Epoch 42/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4345 - accuracy: 0.9612 - val_loss: 0.5702 - val_accuracy: 0.8901\n","Epoch 43/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4329 - accuracy: 0.9639 - val_loss: 0.5717 - val_accuracy: 0.8922\n","Epoch 44/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4292 - accuracy: 0.9652 - val_loss: 0.5685 - val_accuracy: 0.8966\n","Epoch 45/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4321 - accuracy: 0.9644 - val_loss: 0.5973 - val_accuracy: 0.8836\n","Epoch 46/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4435 - accuracy: 0.9547 - val_loss: 0.5709 - val_accuracy: 0.8955\n","Epoch 47/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.4255 - accuracy: 0.9661 - val_loss: 0.5712 - val_accuracy: 0.8912\n","Epoch 48/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4364 - accuracy: 0.9553 - val_loss: 0.6068 - val_accuracy: 0.8685\n","Epoch 49/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4249 - accuracy: 0.9631 - val_loss: 0.5682 - val_accuracy: 0.8966\n","Epoch 50/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4178 - accuracy: 0.9696 - val_loss: 0.5713 - val_accuracy: 0.8933\n","Epoch 51/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4223 - accuracy: 0.9658 - val_loss: 0.5860 - val_accuracy: 0.8793\n","Epoch 52/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4176 - accuracy: 0.9669 - val_loss: 0.5696 - val_accuracy: 0.8879\n","Epoch 53/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4153 - accuracy: 0.9682 - val_loss: 0.5708 - val_accuracy: 0.8912\n","Epoch 54/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4152 - accuracy: 0.9669 - val_loss: 0.6249 - val_accuracy: 0.8653\n","Epoch 55/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4211 - accuracy: 0.9620 - val_loss: 0.5753 - val_accuracy: 0.8869\n","Epoch 56/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4153 - accuracy: 0.9663 - val_loss: 0.5918 - val_accuracy: 0.8825\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4190 - accuracy: 0.9626 - val_loss: 0.5872 - val_accuracy: 0.8804\n","Epoch 58/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4120 - accuracy: 0.9671 - val_loss: 0.5727 - val_accuracy: 0.8922\n","Epoch 59/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4056 - accuracy: 0.9709 - val_loss: 0.5721 - val_accuracy: 0.8933\n","Epoch 60/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4027 - accuracy: 0.9728 - val_loss: 0.5694 - val_accuracy: 0.8869\n","Epoch 61/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4023 - accuracy: 0.9723 - val_loss: 0.5719 - val_accuracy: 0.8858\n","Epoch 62/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4103 - accuracy: 0.9642 - val_loss: 0.5689 - val_accuracy: 0.8890\n","Epoch 63/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4015 - accuracy: 0.9701 - val_loss: 0.5750 - val_accuracy: 0.8922\n","Epoch 64/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.4022 - accuracy: 0.9674 - val_loss: 0.5706 - val_accuracy: 0.8869\n","Epoch 65/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4040 - accuracy: 0.9690 - val_loss: 0.5750 - val_accuracy: 0.8912\n","Epoch 66/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3992 - accuracy: 0.9698 - val_loss: 0.5767 - val_accuracy: 0.8955\n","Epoch 67/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3970 - accuracy: 0.9717 - val_loss: 0.5864 - val_accuracy: 0.8858\n","Epoch 68/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3930 - accuracy: 0.9733 - val_loss: 0.5835 - val_accuracy: 0.8804\n","Epoch 69/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3940 - accuracy: 0.9723 - val_loss: 0.5732 - val_accuracy: 0.8955\n","Epoch 70/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3943 - accuracy: 0.9698 - val_loss: 0.5731 - val_accuracy: 0.8879\n","Epoch 71/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3963 - accuracy: 0.9698 - val_loss: 0.5991 - val_accuracy: 0.8782\n","Epoch 72/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.4018 - accuracy: 0.9658 - val_loss: 0.5873 - val_accuracy: 0.8793\n","Epoch 73/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3968 - accuracy: 0.9663 - val_loss: 0.5693 - val_accuracy: 0.8879\n","Epoch 74/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3947 - accuracy: 0.9685 - val_loss: 0.5686 - val_accuracy: 0.8890\n","Epoch 75/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3916 - accuracy: 0.9688 - val_loss: 0.5933 - val_accuracy: 0.8804\n","Epoch 76/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3869 - accuracy: 0.9741 - val_loss: 0.5771 - val_accuracy: 0.8836\n","Epoch 77/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3884 - accuracy: 0.9725 - val_loss: 0.6025 - val_accuracy: 0.8750\n","Epoch 78/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3910 - accuracy: 0.9706 - val_loss: 0.5867 - val_accuracy: 0.8815\n","Epoch 79/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3805 - accuracy: 0.9749 - val_loss: 0.5800 - val_accuracy: 0.8912\n","Epoch 80/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3763 - accuracy: 0.9782 - val_loss: 0.5724 - val_accuracy: 0.8944\n","Epoch 81/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3842 - accuracy: 0.9723 - val_loss: 0.5801 - val_accuracy: 0.8955\n","Epoch 82/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3764 - accuracy: 0.9782 - val_loss: 0.5791 - val_accuracy: 0.8869\n","Epoch 83/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3728 - accuracy: 0.9779 - val_loss: 0.5861 - val_accuracy: 0.8912\n","Epoch 84/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3727 - accuracy: 0.9784 - val_loss: 0.5758 - val_accuracy: 0.8901\n","Epoch 85/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3722 - accuracy: 0.9774 - val_loss: 0.5924 - val_accuracy: 0.8869\n","Epoch 86/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3760 - accuracy: 0.9747 - val_loss: 0.5760 - val_accuracy: 0.8836\n","Epoch 87/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3668 - accuracy: 0.9809 - val_loss: 0.5723 - val_accuracy: 0.8901\n","Epoch 88/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3641 - accuracy: 0.9820 - val_loss: 0.5820 - val_accuracy: 0.8966\n","Epoch 89/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3714 - accuracy: 0.9776 - val_loss: 0.6001 - val_accuracy: 0.8782\n","Epoch 90/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3644 - accuracy: 0.9814 - val_loss: 0.5751 - val_accuracy: 0.8966\n","Epoch 91/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3615 - accuracy: 0.9814 - val_loss: 0.5823 - val_accuracy: 0.8944\n","Epoch 92/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3662 - accuracy: 0.9782 - val_loss: 0.5710 - val_accuracy: 0.8912\n","Epoch 93/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3619 - accuracy: 0.9825 - val_loss: 0.5836 - val_accuracy: 0.8944\n","Epoch 94/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3596 - accuracy: 0.9814 - val_loss: 0.5945 - val_accuracy: 0.8836\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3577 - accuracy: 0.9836 - val_loss: 0.5755 - val_accuracy: 0.8955\n","Epoch 96/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3564 - accuracy: 0.9825 - val_loss: 0.5883 - val_accuracy: 0.8815\n","Epoch 97/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3639 - accuracy: 0.9793 - val_loss: 0.5840 - val_accuracy: 0.8858\n","Epoch 98/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3602 - accuracy: 0.9801 - val_loss: 0.5806 - val_accuracy: 0.8825\n","Epoch 99/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3521 - accuracy: 0.9855 - val_loss: 0.5849 - val_accuracy: 0.8858\n","Epoch 100/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3508 - accuracy: 0.9857 - val_loss: 0.5813 - val_accuracy: 0.8858\n","{'loss': [0.5568697452545166, 0.5373860597610474, 0.5320085883140564, 0.5396698117256165, 0.5221697092056274, 0.5190795063972473, 0.5150655508041382, 0.5089433789253235, 0.5108968615531921, 0.5083974599838257, 0.5261587500572205, 0.502122700214386, 0.5002868175506592, 0.5016376972198486, 0.4935958683490753, 0.4857823848724365, 0.4922630488872528, 0.493062287569046, 0.49243220686912537, 0.4784093499183655, 0.4796462953090668, 0.47763320803642273, 0.4770183861255646, 0.4679586887359619, 0.4716012179851532, 0.46765413880348206, 0.47101274132728577, 0.4730971157550812, 0.45689886808395386, 0.45472803711891174, 0.4738086462020874, 0.474621057510376, 0.45869290828704834, 0.45391950011253357, 0.44768327474594116, 0.45054328441619873, 0.4510968029499054, 0.44381415843963623, 0.44419312477111816, 0.4435942769050598, 0.44566458463668823, 0.4345203638076782, 0.4329158663749695, 0.42915403842926025, 0.43210047483444214, 0.4434891641139984, 0.42554834485054016, 0.43642356991767883, 0.42485272884368896, 0.4178415834903717, 0.42233946919441223, 0.41764160990715027, 0.41533660888671875, 0.4151850938796997, 0.4210945665836334, 0.41527020931243896, 0.41898006200790405, 0.41196873784065247, 0.40559566020965576, 0.4026743471622467, 0.4023338854312897, 0.41028445959091187, 0.40145328640937805, 0.40221497416496277, 0.4039812982082367, 0.39924994111061096, 0.3970299959182739, 0.3930261433124542, 0.3940073847770691, 0.3942515552043915, 0.3963470458984375, 0.40180930495262146, 0.3968396782875061, 0.39473626017570496, 0.3916100263595581, 0.3868575394153595, 0.3883925676345825, 0.391008198261261, 0.38047143816947937, 0.3763297498226166, 0.3842059075832367, 0.3764018416404724, 0.372829407453537, 0.37265798449516296, 0.37223467230796814, 0.37598758935928345, 0.36679041385650635, 0.3641338646411896, 0.37136805057525635, 0.36441570520401, 0.36154770851135254, 0.36615145206451416, 0.36193838715553284, 0.3595825731754303, 0.35768067836761475, 0.3564055263996124, 0.36392155289649963, 0.36018499732017517, 0.35206183791160583, 0.3507680892944336], 'accuracy': [0.9146012663841248, 0.9229525923728943, 0.9291487336158752, 0.9189116358757019, 0.9323814511299133, 0.9337284564971924, 0.9353448152542114, 0.9399245977401733, 0.9401939511299133, 0.936152994632721, 0.9216055870056152, 0.9431573152542114, 0.9423491358757019, 0.9377694129943848, 0.9431573152542114, 0.9482758641242981, 0.9407327771186829, 0.9401939511299133, 0.9375, 0.9477370977401733, 0.946659505367279, 0.9501616358757019, 0.9461206793785095, 0.9555495977401733, 0.9498922228813171, 0.9528555870056152, 0.9477370977401733, 0.938847005367279, 0.9563577771186829, 0.9552801847457886, 0.9407327771186829, 0.9428879022598267, 0.9523168206214905, 0.9539331793785095, 0.9563577771186829, 0.9555495977401733, 0.9566271305084229, 0.9598599076271057, 0.9582435488700867, 0.9579741358757019, 0.9552801847457886, 0.9612069129943848, 0.9639008641242981, 0.9652478694915771, 0.9644396305084229, 0.954741358757019, 0.9660560488700867, 0.9552801847457886, 0.9630926847457886, 0.9695581793785095, 0.9657866358757019, 0.9668642282485962, 0.9682112336158752, 0.9668642282485962, 0.9620150923728943, 0.9663254022598267, 0.962553858757019, 0.967133641242981, 0.9709051847457886, 0.9727909564971924, 0.9722521305084229, 0.9641702771186829, 0.970097005367279, 0.967402994632721, 0.9690194129943848, 0.9698275923728943, 0.9717133641242981, 0.9733297228813171, 0.9722521305084229, 0.9698275923728943, 0.9698275923728943, 0.9657866358757019, 0.9663254022598267, 0.9684805870056152, 0.96875, 0.9741379022598267, 0.9725215435028076, 0.9706357717514038, 0.974946141242981, 0.978178858757019, 0.9722521305084229, 0.978178858757019, 0.977909505367279, 0.9784482717514038, 0.9773706793785095, 0.9746767282485962, 0.9808728694915771, 0.9819504022598267, 0.9776400923728943, 0.9814116358757019, 0.9814116358757019, 0.978178858757019, 0.9824892282485962, 0.9814116358757019, 0.9835668206214905, 0.9824892282485962, 0.9792564511299133, 0.9800646305084229, 0.9854525923728943, 0.985722005367279], 'val_loss': [1.0411806106567383, 1.0033924579620361, 0.9767506718635559, 0.9430496096611023, 0.9223243594169617, 0.8935344815254211, 0.882607638835907, 0.8687992691993713, 0.8457267880439758, 0.8007558584213257, 0.8194104433059692, 0.7771434783935547, 0.8012264966964722, 0.7381978034973145, 0.7235792279243469, 0.6932747960090637, 0.6635093688964844, 0.6512483358383179, 0.636309802532196, 0.6314162015914917, 0.6133651733398438, 0.6094920039176941, 0.608855664730072, 0.5919747352600098, 0.6150259375572205, 0.5820954442024231, 0.5938204526901245, 0.5769673585891724, 0.5765597820281982, 0.5753880739212036, 0.6333722472190857, 0.5937365889549255, 0.5828677415847778, 0.5735105872154236, 0.5856832265853882, 0.5756466388702393, 0.580040693283081, 0.5702098608016968, 0.5839767456054688, 0.574323832988739, 0.5729463696479797, 0.5702073574066162, 0.5717151165008545, 0.568522036075592, 0.5972959995269775, 0.5708752274513245, 0.5711821913719177, 0.6067618131637573, 0.5681650042533875, 0.5713310241699219, 0.5859591364860535, 0.569634199142456, 0.5708279013633728, 0.624891996383667, 0.5753234624862671, 0.5918440222740173, 0.5872077345848083, 0.5726935267448425, 0.5720844268798828, 0.5693566799163818, 0.5718508362770081, 0.5688992142677307, 0.5750333666801453, 0.5705569982528687, 0.575028657913208, 0.5766517519950867, 0.5863750576972961, 0.5835198163986206, 0.5731887817382812, 0.5731234550476074, 0.5990670919418335, 0.587311327457428, 0.5693269968032837, 0.5686028599739075, 0.5932985544204712, 0.5771059989929199, 0.6025385856628418, 0.5867172479629517, 0.5799635052680969, 0.57243812084198, 0.5801398754119873, 0.5790812969207764, 0.5861021876335144, 0.5758153200149536, 0.5923654437065125, 0.5760005712509155, 0.5722846388816833, 0.5819583535194397, 0.6000771522521973, 0.5751223564147949, 0.5822688937187195, 0.5709717869758606, 0.5836200714111328, 0.5945049524307251, 0.5754711031913757, 0.5883103609085083, 0.5840386748313904, 0.5806339383125305, 0.5849270224571228, 0.5813268423080444], 'val_accuracy': [0.48491379618644714, 0.48706895112991333, 0.49568966031074524, 0.5334051847457886, 0.5905172228813171, 0.7144396305084229, 0.6982758641242981, 0.7079741358757019, 0.7650862336158752, 0.850215494632721, 0.7618534564971924, 0.8448275923728943, 0.756465494632721, 0.8545258641242981, 0.857758641242981, 0.8674569129943848, 0.860991358757019, 0.8631465435028076, 0.8653017282485962, 0.860991358757019, 0.8696120977401733, 0.8706896305084229, 0.868534505367279, 0.8760775923728943, 0.868534505367279, 0.892241358757019, 0.8782327771186829, 0.8954741358757019, 0.8911637663841248, 0.8943965435028076, 0.8706896305084229, 0.8803879022598267, 0.889008641242981, 0.8933189511299133, 0.8846982717514038, 0.8868534564971924, 0.8825430870056152, 0.8911637663841248, 0.8846982717514038, 0.8900862336158752, 0.8911637663841248, 0.8900862336158752, 0.892241358757019, 0.8965517282485962, 0.8836206793785095, 0.8954741358757019, 0.8911637663841248, 0.868534505367279, 0.8965517282485962, 0.8933189511299133, 0.8793103694915771, 0.8879310488700867, 0.8911637663841248, 0.8653017282485962, 0.8868534564971924, 0.8825430870056152, 0.8803879022598267, 0.892241358757019, 0.8933189511299133, 0.8868534564971924, 0.8857758641242981, 0.889008641242981, 0.892241358757019, 0.8868534564971924, 0.8911637663841248, 0.8954741358757019, 0.8857758641242981, 0.8803879022598267, 0.8954741358757019, 0.8879310488700867, 0.8782327771186829, 0.8793103694915771, 0.8879310488700867, 0.889008641242981, 0.8803879022598267, 0.8836206793785095, 0.875, 0.881465494632721, 0.8911637663841248, 0.8943965435028076, 0.8954741358757019, 0.8868534564971924, 0.8911637663841248, 0.8900862336158752, 0.8868534564971924, 0.8836206793785095, 0.8900862336158752, 0.8965517282485962, 0.8782327771186829, 0.8965517282485962, 0.8943965435028076, 0.8911637663841248, 0.8943965435028076, 0.8836206793785095, 0.8954741358757019, 0.881465494632721, 0.8857758641242981, 0.8825430870056152, 0.8857758641242981, 0.8857758641242981]}\n","38/38 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 3s 31ms/step - loss: 0.5595 - accuracy: 0.9143 - val_loss: 1.0331 - val_accuracy: 0.4955\n","Epoch 2/100\n"," 1/28 [>.............................] - ETA: 0s - loss: 0.5088 - accuracy: 0.9297"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["28/28 [==============================] - 0s 17ms/step - loss: 0.5494 - accuracy: 0.9177 - val_loss: 1.0046 - val_accuracy: 0.4966\n","Epoch 3/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5546 - accuracy: 0.9103 - val_loss: 0.9750 - val_accuracy: 0.5124\n","Epoch 4/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5417 - accuracy: 0.9177 - val_loss: 0.9544 - val_accuracy: 0.5339\n","Epoch 5/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5306 - accuracy: 0.9298 - val_loss: 0.9272 - val_accuracy: 0.6143\n","Epoch 6/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5256 - accuracy: 0.9281 - val_loss: 0.9097 - val_accuracy: 0.6833\n","Epoch 7/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5218 - accuracy: 0.9307 - val_loss: 0.8916 - val_accuracy: 0.7059\n","Epoch 8/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5238 - accuracy: 0.9307 - val_loss: 0.8768 - val_accuracy: 0.7545\n","Epoch 9/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5215 - accuracy: 0.9293 - val_loss: 0.8481 - val_accuracy: 0.8394\n","Epoch 10/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5242 - accuracy: 0.9256 - val_loss: 0.8281 - val_accuracy: 0.8416\n","Epoch 11/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.5237 - accuracy: 0.9256 - val_loss: 0.8153 - val_accuracy: 0.8360\n","Epoch 12/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.5201 - accuracy: 0.9244 - val_loss: 0.8067 - val_accuracy: 0.8100\n","Epoch 13/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.5153 - accuracy: 0.9278 - val_loss: 0.7896 - val_accuracy: 0.8247\n","Epoch 14/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.5086 - accuracy: 0.9344 - val_loss: 0.7538 - val_accuracy: 0.8484\n","Epoch 15/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5045 - accuracy: 0.9324 - val_loss: 0.7283 - val_accuracy: 0.8484\n","Epoch 16/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4988 - accuracy: 0.9377 - val_loss: 0.7238 - val_accuracy: 0.8575\n","Epoch 17/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.4981 - accuracy: 0.9400 - val_loss: 0.7045 - val_accuracy: 0.8643\n","Epoch 18/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5006 - accuracy: 0.9355 - val_loss: 0.6970 - val_accuracy: 0.8529\n","Epoch 19/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4942 - accuracy: 0.9377 - val_loss: 0.6692 - val_accuracy: 0.8597\n","Epoch 20/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4982 - accuracy: 0.9321 - val_loss: 0.6567 - val_accuracy: 0.8620\n","Epoch 21/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4959 - accuracy: 0.9366 - val_loss: 0.6503 - val_accuracy: 0.8507\n","Epoch 22/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4833 - accuracy: 0.9451 - val_loss: 0.6376 - val_accuracy: 0.8676\n","Epoch 23/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.4924 - accuracy: 0.9358 - val_loss: 0.6378 - val_accuracy: 0.8586\n","Epoch 24/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.4796 - accuracy: 0.9474 - val_loss: 0.6231 - val_accuracy: 0.8699\n","Epoch 25/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4854 - accuracy: 0.9383 - val_loss: 0.6151 - val_accuracy: 0.8756\n","Epoch 26/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4796 - accuracy: 0.9417 - val_loss: 0.6265 - val_accuracy: 0.8676\n","Epoch 27/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4837 - accuracy: 0.9400 - val_loss: 0.6088 - val_accuracy: 0.8790\n","Epoch 28/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4740 - accuracy: 0.9496 - val_loss: 0.6199 - val_accuracy: 0.8699\n","Epoch 29/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4718 - accuracy: 0.9454 - val_loss: 0.6069 - val_accuracy: 0.8756\n","Epoch 30/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.4700 - accuracy: 0.9513 - val_loss: 0.6057 - val_accuracy: 0.8756\n","Epoch 31/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4741 - accuracy: 0.9454 - val_loss: 0.6102 - val_accuracy: 0.8699\n","Epoch 32/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.4626 - accuracy: 0.9530 - val_loss: 0.6086 - val_accuracy: 0.8733\n","Epoch 33/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4645 - accuracy: 0.9437 - val_loss: 0.6724 - val_accuracy: 0.8586\n","Epoch 34/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4724 - accuracy: 0.9420 - val_loss: 0.6118 - val_accuracy: 0.8744\n","Epoch 35/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4595 - accuracy: 0.9519 - val_loss: 0.6292 - val_accuracy: 0.8710\n","Epoch 36/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4642 - accuracy: 0.9445 - val_loss: 0.6074 - val_accuracy: 0.8733\n","Epoch 37/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4625 - accuracy: 0.9491 - val_loss: 0.6225 - val_accuracy: 0.8733\n","Epoch 38/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4657 - accuracy: 0.9414 - val_loss: 0.6383 - val_accuracy: 0.8744\n","Epoch 39/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4551 - accuracy: 0.9485 - val_loss: 0.6166 - val_accuracy: 0.8812\n","Epoch 40/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4500 - accuracy: 0.9550 - val_loss: 0.6206 - val_accuracy: 0.8778\n","Epoch 41/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4627 - accuracy: 0.9440 - val_loss: 0.6248 - val_accuracy: 0.8733\n","Epoch 42/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4581 - accuracy: 0.9460 - val_loss: 0.6121 - val_accuracy: 0.8778\n","Epoch 43/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4504 - accuracy: 0.9508 - val_loss: 0.6059 - val_accuracy: 0.8733\n","Epoch 44/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4418 - accuracy: 0.9573 - val_loss: 0.6202 - val_accuracy: 0.8733\n","Epoch 45/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4492 - accuracy: 0.9516 - val_loss: 0.6126 - val_accuracy: 0.8733\n","Epoch 46/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4411 - accuracy: 0.9567 - val_loss: 0.6095 - val_accuracy: 0.8733\n","Epoch 47/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4333 - accuracy: 0.9610 - val_loss: 0.6091 - val_accuracy: 0.8710\n","Epoch 48/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4364 - accuracy: 0.9584 - val_loss: 0.6227 - val_accuracy: 0.8722\n","Epoch 49/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4466 - accuracy: 0.9468 - val_loss: 0.6128 - val_accuracy: 0.8767\n","Epoch 50/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4427 - accuracy: 0.9474 - val_loss: 0.6104 - val_accuracy: 0.8756\n","Epoch 51/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4292 - accuracy: 0.9635 - val_loss: 0.6066 - val_accuracy: 0.8722\n","Epoch 52/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4299 - accuracy: 0.9629 - val_loss: 0.6081 - val_accuracy: 0.8778\n","Epoch 53/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4283 - accuracy: 0.9601 - val_loss: 0.6124 - val_accuracy: 0.8733\n","Epoch 54/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4289 - accuracy: 0.9604 - val_loss: 0.6086 - val_accuracy: 0.8722\n","Epoch 55/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4252 - accuracy: 0.9576 - val_loss: 0.6155 - val_accuracy: 0.8744\n","Epoch 56/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4325 - accuracy: 0.9550 - val_loss: 0.6085 - val_accuracy: 0.8756\n","Epoch 57/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4204 - accuracy: 0.9635 - val_loss: 0.6272 - val_accuracy: 0.8676\n","Epoch 58/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4274 - accuracy: 0.9570 - val_loss: 0.6136 - val_accuracy: 0.8756\n","Epoch 59/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4207 - accuracy: 0.9621 - val_loss: 0.6205 - val_accuracy: 0.8722\n","Epoch 60/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4212 - accuracy: 0.9618 - val_loss: 0.6070 - val_accuracy: 0.8722\n","Epoch 61/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4110 - accuracy: 0.9669 - val_loss: 0.6064 - val_accuracy: 0.8722\n","Epoch 62/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4131 - accuracy: 0.9660 - val_loss: 0.6054 - val_accuracy: 0.8699\n","Epoch 63/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4114 - accuracy: 0.9655 - val_loss: 0.6183 - val_accuracy: 0.8790\n","Epoch 64/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4265 - accuracy: 0.9559 - val_loss: 0.6443 - val_accuracy: 0.8744\n","Epoch 65/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4168 - accuracy: 0.9612 - val_loss: 0.6329 - val_accuracy: 0.8756\n","Epoch 66/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4192 - accuracy: 0.9593 - val_loss: 0.6085 - val_accuracy: 0.8699\n","Epoch 67/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.4037 - accuracy: 0.9700 - val_loss: 0.6097 - val_accuracy: 0.8722\n","Epoch 68/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4068 - accuracy: 0.9658 - val_loss: 0.6074 - val_accuracy: 0.8710\n","Epoch 69/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.4071 - accuracy: 0.9692 - val_loss: 0.6407 - val_accuracy: 0.8744\n","Epoch 70/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.4002 - accuracy: 0.9692 - val_loss: 0.6089 - val_accuracy: 0.8676\n","Epoch 71/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4013 - accuracy: 0.9711 - val_loss: 0.6131 - val_accuracy: 0.8744\n","Epoch 72/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3956 - accuracy: 0.9740 - val_loss: 0.6058 - val_accuracy: 0.8744\n","Epoch 73/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3941 - accuracy: 0.9717 - val_loss: 0.6213 - val_accuracy: 0.8744\n","Epoch 74/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3956 - accuracy: 0.9709 - val_loss: 0.6232 - val_accuracy: 0.8710\n","Epoch 75/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3933 - accuracy: 0.9711 - val_loss: 0.6137 - val_accuracy: 0.8688\n","Epoch 76/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3905 - accuracy: 0.9711 - val_loss: 0.6320 - val_accuracy: 0.8620\n","Epoch 77/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3965 - accuracy: 0.9692 - val_loss: 0.6269 - val_accuracy: 0.8733\n","Epoch 78/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3881 - accuracy: 0.9720 - val_loss: 0.6245 - val_accuracy: 0.8722\n","Epoch 79/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3894 - accuracy: 0.9731 - val_loss: 0.6376 - val_accuracy: 0.8801\n","Epoch 80/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3935 - accuracy: 0.9692 - val_loss: 0.6146 - val_accuracy: 0.8676\n","Epoch 81/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3862 - accuracy: 0.9726 - val_loss: 0.6414 - val_accuracy: 0.8733\n","Epoch 82/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3946 - accuracy: 0.9675 - val_loss: 0.6118 - val_accuracy: 0.8767\n","Epoch 83/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3905 - accuracy: 0.9672 - val_loss: 0.6171 - val_accuracy: 0.8722\n","Epoch 84/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3828 - accuracy: 0.9728 - val_loss: 0.6743 - val_accuracy: 0.8699\n","Epoch 85/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3933 - accuracy: 0.9646 - val_loss: 0.6134 - val_accuracy: 0.8778\n","Epoch 86/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3757 - accuracy: 0.9779 - val_loss: 0.6128 - val_accuracy: 0.8722\n","Epoch 87/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3838 - accuracy: 0.9723 - val_loss: 0.6186 - val_accuracy: 0.8699\n","Epoch 88/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3764 - accuracy: 0.9751 - val_loss: 0.6129 - val_accuracy: 0.8744\n","Epoch 89/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3753 - accuracy: 0.9788 - val_loss: 0.6146 - val_accuracy: 0.8676\n","Epoch 90/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3730 - accuracy: 0.9776 - val_loss: 0.6169 - val_accuracy: 0.8733\n","Epoch 91/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3721 - accuracy: 0.9779 - val_loss: 0.6791 - val_accuracy: 0.8541\n","Epoch 92/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3743 - accuracy: 0.9726 - val_loss: 0.6185 - val_accuracy: 0.8756\n","Epoch 93/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3663 - accuracy: 0.9813 - val_loss: 0.6196 - val_accuracy: 0.8631\n","Epoch 94/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3684 - accuracy: 0.9802 - val_loss: 0.6257 - val_accuracy: 0.8710\n","Epoch 95/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3735 - accuracy: 0.9751 - val_loss: 0.6379 - val_accuracy: 0.8744\n","Epoch 96/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3647 - accuracy: 0.9825 - val_loss: 0.6160 - val_accuracy: 0.8722\n","Epoch 97/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3669 - accuracy: 0.9802 - val_loss: 0.6514 - val_accuracy: 0.8665\n","Epoch 98/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3745 - accuracy: 0.9709 - val_loss: 0.6352 - val_accuracy: 0.8733\n","Epoch 99/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3639 - accuracy: 0.9796 - val_loss: 0.6488 - val_accuracy: 0.8665\n","Epoch 100/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3647 - accuracy: 0.9810 - val_loss: 0.6270 - val_accuracy: 0.8778\n","{'loss': [0.5594685673713684, 0.5493713617324829, 0.5546488165855408, 0.541729748249054, 0.5306112170219421, 0.5255743265151978, 0.5218201279640198, 0.5237698554992676, 0.5214533805847168, 0.5241563320159912, 0.5236639380455017, 0.5201429724693298, 0.5153468251228333, 0.508609414100647, 0.5044525861740112, 0.49876365065574646, 0.49805453419685364, 0.5005897283554077, 0.4941602051258087, 0.49817413091659546, 0.4958679974079132, 0.4832828640937805, 0.49244987964630127, 0.479580819606781, 0.48536309599876404, 0.479625403881073, 0.4837014377117157, 0.47397804260253906, 0.47177737951278687, 0.4699806272983551, 0.4741251766681671, 0.4625747799873352, 0.46453148126602173, 0.47239747643470764, 0.4595221281051636, 0.46415627002716064, 0.4625474810600281, 0.46573078632354736, 0.455063134431839, 0.4500313401222229, 0.46273142099380493, 0.458137184381485, 0.45038965344429016, 0.44184640049934387, 0.4492185711860657, 0.44106024503707886, 0.43333062529563904, 0.43635258078575134, 0.4466399848461151, 0.4427175521850586, 0.4291757047176361, 0.429859459400177, 0.42831534147262573, 0.4288593530654907, 0.4251546263694763, 0.4325420558452606, 0.42039602994918823, 0.42737627029418945, 0.4206821918487549, 0.4212196469306946, 0.41100895404815674, 0.4130973815917969, 0.4113658666610718, 0.4265310764312744, 0.41684427857398987, 0.4192293584346771, 0.40368157625198364, 0.4067757725715637, 0.4071091413497925, 0.4001912474632263, 0.40126708149909973, 0.3956355154514313, 0.39408037066459656, 0.3956278860569, 0.3933180868625641, 0.39046138525009155, 0.3964911699295044, 0.3881331980228424, 0.389443963766098, 0.3934711217880249, 0.38619938492774963, 0.39455604553222656, 0.39054957032203674, 0.3828037977218628, 0.39330756664276123, 0.3756682872772217, 0.3837521970272064, 0.37638309597969055, 0.3752703368663788, 0.3730453550815582, 0.3721481263637543, 0.3742782175540924, 0.36630359292030334, 0.36840322613716125, 0.3734789788722992, 0.36467620730400085, 0.3669098913669586, 0.37448081374168396, 0.363865464925766, 0.36469539999961853], 'accuracy': [0.9142614603042603, 0.9176570177078247, 0.9102999567985535, 0.9176570177078247, 0.9298245906829834, 0.9281267523765564, 0.9306734800338745, 0.9306734800338745, 0.9292586445808411, 0.9255800843238831, 0.9255800843238831, 0.9244481921195984, 0.9278438091278076, 0.9343519806861877, 0.9323712587356567, 0.937747597694397, 0.9400113224983215, 0.9354838728904724, 0.937747597694397, 0.9320882558822632, 0.9366157054901123, 0.945104718208313, 0.9357668161392212, 0.9473684430122375, 0.9383135437965393, 0.9417091012001038, 0.9400113224983215, 0.9496321678161621, 0.9453876614570618, 0.9513299465179443, 0.9453876614570618, 0.9530277252197266, 0.9436898827552795, 0.9419921040534973, 0.9518958926200867, 0.9445387721061707, 0.9490662217140198, 0.941426157951355, 0.9485002756118774, 0.9550085067749023, 0.9439728260040283, 0.9459536075592041, 0.950764000415802, 0.9572722315788269, 0.9516128897666931, 0.9567062854766846, 0.9609507918357849, 0.9584040641784668, 0.9468024969100952, 0.9473684430122375, 0.9634974598884583, 0.9629315137863159, 0.960101842880249, 0.9603848457336426, 0.9575551748275757, 0.9550085067749023, 0.9634974598884583, 0.9569892287254333, 0.9620826244354248, 0.961799681186676, 0.9668930172920227, 0.9660441279411316, 0.965478241443634, 0.9558573961257935, 0.9612337350845337, 0.9592529535293579, 0.9700056314468384, 0.9657611846923828, 0.9691567420959473, 0.9691567420959473, 0.971137523651123, 0.9739671945571899, 0.9717034697532654, 0.9708545804023743, 0.971137523651123, 0.971137523651123, 0.9691567420959473, 0.9719864130020142, 0.9731183052062988, 0.9691567420959473, 0.9725523591041565, 0.967458963394165, 0.9671760201454163, 0.9728353023529053, 0.9646292924880981, 0.9779286980628967, 0.9722693562507629, 0.9750990271568298, 0.9787775874137878, 0.977645754814148, 0.9779286980628967, 0.9725523591041565, 0.9813242554664612, 0.9801924228668213, 0.9750990271568298, 0.9824561476707458, 0.9801924228668213, 0.9708545804023743, 0.979626476764679, 0.9810413122177124], 'val_loss': [1.0330731868743896, 1.0045950412750244, 0.9749563932418823, 0.9544191956520081, 0.9272305369377136, 0.9096981287002563, 0.8915859460830688, 0.8767592906951904, 0.8480995893478394, 0.8281283378601074, 0.8153120875358582, 0.8067468404769897, 0.7896375060081482, 0.7537901401519775, 0.7283046245574951, 0.723790168762207, 0.704540491104126, 0.6970258355140686, 0.6692383289337158, 0.6567395925521851, 0.6502907872200012, 0.6376149654388428, 0.637780487537384, 0.6231418251991272, 0.6150625348091125, 0.6264757513999939, 0.6088414192199707, 0.6199383735656738, 0.6068883538246155, 0.6057006120681763, 0.6102392673492432, 0.6086245179176331, 0.6723568439483643, 0.6118204593658447, 0.6292449831962585, 0.6074351072311401, 0.6224636435508728, 0.6382837295532227, 0.616604745388031, 0.6205731630325317, 0.6247634291648865, 0.6120896935462952, 0.6059203743934631, 0.6201635003089905, 0.6126207709312439, 0.6094630360603333, 0.6091251969337463, 0.6226730942726135, 0.6128439903259277, 0.6103931665420532, 0.6065731048583984, 0.6081006526947021, 0.6123894453048706, 0.6085966229438782, 0.6155282258987427, 0.6084690093994141, 0.6272020936012268, 0.6135505437850952, 0.6204709410667419, 0.6069924235343933, 0.6064278483390808, 0.6054434180259705, 0.618344247341156, 0.6442597508430481, 0.6329201459884644, 0.6085079908370972, 0.6097137331962585, 0.6073868274688721, 0.6407052278518677, 0.6089045405387878, 0.613121509552002, 0.6058366894721985, 0.6212796568870544, 0.6231529712677002, 0.6137373447418213, 0.6320251822471619, 0.6268825531005859, 0.6245104670524597, 0.6375564932823181, 0.6146416068077087, 0.6414455771446228, 0.6118022799491882, 0.6171144247055054, 0.6743081212043762, 0.61342853307724, 0.6127965450286865, 0.6186127066612244, 0.6128883361816406, 0.6145668029785156, 0.6168567538261414, 0.679123044013977, 0.6184931993484497, 0.6196407675743103, 0.6257205605506897, 0.6378666162490845, 0.6159551739692688, 0.6513755321502686, 0.6352311372756958, 0.6488297581672668, 0.6269827485084534], 'val_accuracy': [0.4954751133918762, 0.49660632014274597, 0.5124434232711792, 0.5339366793632507, 0.6142534017562866, 0.6832579374313354, 0.7058823704719543, 0.7545248866081238, 0.8393664956092834, 0.8416289687156677, 0.8359728455543518, 0.8099547624588013, 0.8246606588363647, 0.848416268825531, 0.848416268825531, 0.8574660420417786, 0.8642534017562866, 0.8529411554336548, 0.8597285151481628, 0.8619909286499023, 0.8506787419319153, 0.8676470518112183, 0.8585972785949707, 0.8699095249176025, 0.8755655884742737, 0.8676470518112183, 0.8789592981338501, 0.8699095249176025, 0.8755655884742737, 0.8755655884742737, 0.8699095249176025, 0.8733031749725342, 0.8585972785949707, 0.8744344115257263, 0.8710407018661499, 0.8733031749725342, 0.8733031749725342, 0.8744344115257263, 0.8812217116355896, 0.877828061580658, 0.8733031749725342, 0.877828061580658, 0.8733031749725342, 0.8733031749725342, 0.8733031749725342, 0.8733031749725342, 0.8710407018661499, 0.872171938419342, 0.8766968250274658, 0.8755655884742737, 0.872171938419342, 0.877828061580658, 0.8733031749725342, 0.872171938419342, 0.8744344115257263, 0.8755655884742737, 0.8676470518112183, 0.8755655884742737, 0.872171938419342, 0.872171938419342, 0.872171938419342, 0.8699095249176025, 0.8789592981338501, 0.8744344115257263, 0.8755655884742737, 0.8699095249176025, 0.872171938419342, 0.8710407018661499, 0.8744344115257263, 0.8676470518112183, 0.8744344115257263, 0.8744344115257263, 0.8744344115257263, 0.8710407018661499, 0.8687782883644104, 0.8619909286499023, 0.8733031749725342, 0.872171938419342, 0.8800904750823975, 0.8676470518112183, 0.8733031749725342, 0.8766968250274658, 0.872171938419342, 0.8699095249176025, 0.877828061580658, 0.872171938419342, 0.8699095249176025, 0.8744344115257263, 0.8676470518112183, 0.8733031749725342, 0.8540723919868469, 0.8755655884742737, 0.8631221652030945, 0.8710407018661499, 0.8744344115257263, 0.872171938419342, 0.8665158152580261, 0.8733031749725342, 0.8665158152580261, 0.877828061580658]}\n","45/45 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 3s 27ms/step - loss: 0.5446 - accuracy: 0.9196 - val_loss: 1.0373 - val_accuracy: 0.4855\n","Epoch 2/100\n"," 1/31 [..............................] - ETA: 0s - loss: 0.4748 - accuracy: 0.9609"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 0s 16ms/step - loss: 0.5328 - accuracy: 0.9233 - val_loss: 1.0006 - val_accuracy: 0.4917\n","Epoch 3/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5288 - accuracy: 0.9266 - val_loss: 0.9702 - val_accuracy: 0.5052\n","Epoch 4/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5156 - accuracy: 0.9323 - val_loss: 0.9378 - val_accuracy: 0.5806\n","Epoch 5/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.5189 - accuracy: 0.9331 - val_loss: 0.9127 - val_accuracy: 0.6694\n","Epoch 6/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5138 - accuracy: 0.9289 - val_loss: 0.8923 - val_accuracy: 0.7376\n","Epoch 7/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.5078 - accuracy: 0.9344 - val_loss: 0.8664 - val_accuracy: 0.8192\n","Epoch 8/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5062 - accuracy: 0.9364 - val_loss: 0.8520 - val_accuracy: 0.8079\n","Epoch 9/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4996 - accuracy: 0.9370 - val_loss: 0.8370 - val_accuracy: 0.8017\n","Epoch 10/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.5003 - accuracy: 0.9385 - val_loss: 0.8296 - val_accuracy: 0.7510\n","Epoch 11/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4947 - accuracy: 0.9390 - val_loss: 0.8013 - val_accuracy: 0.8058\n","Epoch 12/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4956 - accuracy: 0.9385 - val_loss: 0.7850 - val_accuracy: 0.8068\n","Epoch 13/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4941 - accuracy: 0.9377 - val_loss: 0.7390 - val_accuracy: 0.8564\n","Epoch 14/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4936 - accuracy: 0.9390 - val_loss: 0.7211 - val_accuracy: 0.8543\n","Epoch 15/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4936 - accuracy: 0.9395 - val_loss: 0.7090 - val_accuracy: 0.8306\n","Epoch 16/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5013 - accuracy: 0.9320 - val_loss: 0.6800 - val_accuracy: 0.8585\n","Epoch 17/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4799 - accuracy: 0.9444 - val_loss: 0.6865 - val_accuracy: 0.8378\n","Epoch 18/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4939 - accuracy: 0.9315 - val_loss: 0.6569 - val_accuracy: 0.8647\n","Epoch 19/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4769 - accuracy: 0.9442 - val_loss: 0.6600 - val_accuracy: 0.8492\n","Epoch 20/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4763 - accuracy: 0.9442 - val_loss: 0.6436 - val_accuracy: 0.8709\n","Epoch 21/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4702 - accuracy: 0.9460 - val_loss: 0.6671 - val_accuracy: 0.8554\n","Epoch 22/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4693 - accuracy: 0.9501 - val_loss: 0.6610 - val_accuracy: 0.8574\n","Epoch 23/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4693 - accuracy: 0.9468 - val_loss: 0.6315 - val_accuracy: 0.8791\n","Epoch 24/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4678 - accuracy: 0.9465 - val_loss: 0.6204 - val_accuracy: 0.8760\n","Epoch 25/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4608 - accuracy: 0.9509 - val_loss: 0.6221 - val_accuracy: 0.8729\n","Epoch 26/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4631 - accuracy: 0.9488 - val_loss: 0.6169 - val_accuracy: 0.8812\n","Epoch 27/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4681 - accuracy: 0.9470 - val_loss: 0.6289 - val_accuracy: 0.8812\n","Epoch 28/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4595 - accuracy: 0.9504 - val_loss: 0.6295 - val_accuracy: 0.8698\n","Epoch 29/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4535 - accuracy: 0.9525 - val_loss: 0.6207 - val_accuracy: 0.8781\n","Epoch 30/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4540 - accuracy: 0.9509 - val_loss: 0.6519 - val_accuracy: 0.8657\n","Epoch 31/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4645 - accuracy: 0.9421 - val_loss: 0.6249 - val_accuracy: 0.8791\n","Epoch 32/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4448 - accuracy: 0.9550 - val_loss: 0.6235 - val_accuracy: 0.8719\n","Epoch 33/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4462 - accuracy: 0.9532 - val_loss: 0.6319 - val_accuracy: 0.8667\n","Epoch 34/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4456 - accuracy: 0.9556 - val_loss: 0.6303 - val_accuracy: 0.8781\n","Epoch 35/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4396 - accuracy: 0.9615 - val_loss: 0.6237 - val_accuracy: 0.8771\n","Epoch 36/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4418 - accuracy: 0.9530 - val_loss: 0.6247 - val_accuracy: 0.8781\n","Epoch 37/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4377 - accuracy: 0.9566 - val_loss: 0.6362 - val_accuracy: 0.8802\n","Epoch 38/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4373 - accuracy: 0.9574 - val_loss: 0.6328 - val_accuracy: 0.8688\n","Epoch 39/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4329 - accuracy: 0.9594 - val_loss: 0.6258 - val_accuracy: 0.8771\n","Epoch 40/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4302 - accuracy: 0.9625 - val_loss: 0.6302 - val_accuracy: 0.8760\n","Epoch 41/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4368 - accuracy: 0.9574 - val_loss: 0.6278 - val_accuracy: 0.8719\n","Epoch 42/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4277 - accuracy: 0.9602 - val_loss: 0.6264 - val_accuracy: 0.8719\n","Epoch 43/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4284 - accuracy: 0.9594 - val_loss: 0.6574 - val_accuracy: 0.8574\n","Epoch 44/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4410 - accuracy: 0.9496 - val_loss: 0.6290 - val_accuracy: 0.8698\n","Epoch 45/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4255 - accuracy: 0.9615 - val_loss: 0.6734 - val_accuracy: 0.8626\n","Epoch 46/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4293 - accuracy: 0.9556 - val_loss: 0.6250 - val_accuracy: 0.8698\n","Epoch 47/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4246 - accuracy: 0.9568 - val_loss: 0.6383 - val_accuracy: 0.8698\n","Epoch 48/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4184 - accuracy: 0.9636 - val_loss: 0.6380 - val_accuracy: 0.8667\n","Epoch 49/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4171 - accuracy: 0.9625 - val_loss: 0.6247 - val_accuracy: 0.8781\n","Epoch 50/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4154 - accuracy: 0.9633 - val_loss: 0.6281 - val_accuracy: 0.8771\n","Epoch 51/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4136 - accuracy: 0.9651 - val_loss: 0.6250 - val_accuracy: 0.8822\n","Epoch 52/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4105 - accuracy: 0.9656 - val_loss: 0.6252 - val_accuracy: 0.8657\n","Epoch 53/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4095 - accuracy: 0.9649 - val_loss: 0.6788 - val_accuracy: 0.8512\n","Epoch 54/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4202 - accuracy: 0.9576 - val_loss: 0.6308 - val_accuracy: 0.8781\n","Epoch 55/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4019 - accuracy: 0.9705 - val_loss: 0.6567 - val_accuracy: 0.8657\n","Epoch 56/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4122 - accuracy: 0.9594 - val_loss: 0.6366 - val_accuracy: 0.8853\n","Epoch 57/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4012 - accuracy: 0.9680 - val_loss: 0.6442 - val_accuracy: 0.8657\n","Epoch 58/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.4116 - accuracy: 0.9599 - val_loss: 0.6328 - val_accuracy: 0.8709\n","Epoch 59/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3994 - accuracy: 0.9687 - val_loss: 0.6254 - val_accuracy: 0.8740\n","Epoch 60/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4020 - accuracy: 0.9643 - val_loss: 0.6367 - val_accuracy: 0.8678\n","Epoch 61/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4019 - accuracy: 0.9646 - val_loss: 0.6702 - val_accuracy: 0.8605\n","Epoch 62/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3993 - accuracy: 0.9672 - val_loss: 0.6397 - val_accuracy: 0.8688\n","Epoch 63/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3991 - accuracy: 0.9669 - val_loss: 0.7139 - val_accuracy: 0.8512\n","Epoch 64/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.4173 - accuracy: 0.9519 - val_loss: 0.6503 - val_accuracy: 0.8667\n","Epoch 65/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.4053 - accuracy: 0.9615 - val_loss: 0.6360 - val_accuracy: 0.8709\n","Epoch 66/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3887 - accuracy: 0.9721 - val_loss: 0.6401 - val_accuracy: 0.8688\n","Epoch 67/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3886 - accuracy: 0.9716 - val_loss: 0.6325 - val_accuracy: 0.8719\n","Epoch 68/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3926 - accuracy: 0.9659 - val_loss: 0.6441 - val_accuracy: 0.8771\n","Epoch 69/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3957 - accuracy: 0.9661 - val_loss: 0.6427 - val_accuracy: 0.8729\n","Epoch 70/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3866 - accuracy: 0.9677 - val_loss: 0.6356 - val_accuracy: 0.8698\n","Epoch 71/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3823 - accuracy: 0.9747 - val_loss: 0.6440 - val_accuracy: 0.8678\n","Epoch 72/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3810 - accuracy: 0.9718 - val_loss: 0.6368 - val_accuracy: 0.8626\n","Epoch 73/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3802 - accuracy: 0.9736 - val_loss: 0.6363 - val_accuracy: 0.8802\n","Epoch 74/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3810 - accuracy: 0.9685 - val_loss: 0.6413 - val_accuracy: 0.8750\n","Epoch 75/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.3801 - accuracy: 0.9742 - val_loss: 0.6657 - val_accuracy: 0.8636\n","Epoch 76/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.3840 - accuracy: 0.9718 - val_loss: 0.7345 - val_accuracy: 0.8378\n","Epoch 77/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.4023 - accuracy: 0.9563 - val_loss: 0.6450 - val_accuracy: 0.8698\n","Epoch 78/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.3812 - accuracy: 0.9708 - val_loss: 0.6507 - val_accuracy: 0.8771\n","Epoch 79/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.3711 - accuracy: 0.9770 - val_loss: 0.6345 - val_accuracy: 0.8688\n","Epoch 80/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.3682 - accuracy: 0.9788 - val_loss: 0.6644 - val_accuracy: 0.8719\n","Epoch 81/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3716 - accuracy: 0.9736 - val_loss: 0.6426 - val_accuracy: 0.8719\n","Epoch 82/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.3703 - accuracy: 0.9762 - val_loss: 0.6399 - val_accuracy: 0.8709\n","Epoch 83/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3719 - accuracy: 0.9744 - val_loss: 0.6441 - val_accuracy: 0.8740\n","Epoch 84/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3664 - accuracy: 0.9780 - val_loss: 0.6517 - val_accuracy: 0.8729\n","Epoch 85/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3634 - accuracy: 0.9798 - val_loss: 0.6440 - val_accuracy: 0.8719\n","Epoch 86/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3676 - accuracy: 0.9744 - val_loss: 0.6483 - val_accuracy: 0.8647\n","Epoch 87/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3630 - accuracy: 0.9791 - val_loss: 0.6614 - val_accuracy: 0.8667\n","Epoch 88/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3642 - accuracy: 0.9793 - val_loss: 0.6475 - val_accuracy: 0.8698\n","Epoch 89/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3577 - accuracy: 0.9804 - val_loss: 0.6494 - val_accuracy: 0.8750\n","Epoch 90/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3595 - accuracy: 0.9806 - val_loss: 0.6513 - val_accuracy: 0.8626\n","Epoch 91/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3598 - accuracy: 0.9798 - val_loss: 0.6552 - val_accuracy: 0.8709\n","Epoch 92/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3594 - accuracy: 0.9773 - val_loss: 0.6726 - val_accuracy: 0.8709\n","Epoch 93/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3541 - accuracy: 0.9827 - val_loss: 0.6556 - val_accuracy: 0.8709\n","Epoch 94/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3551 - accuracy: 0.9793 - val_loss: 0.6449 - val_accuracy: 0.8729\n","Epoch 95/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3547 - accuracy: 0.9824 - val_loss: 0.6704 - val_accuracy: 0.8688\n","Epoch 96/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3538 - accuracy: 0.9809 - val_loss: 0.6548 - val_accuracy: 0.8678\n","Epoch 97/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3501 - accuracy: 0.9832 - val_loss: 0.6520 - val_accuracy: 0.8719\n","Epoch 98/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3507 - accuracy: 0.9842 - val_loss: 0.6592 - val_accuracy: 0.8698\n","Epoch 99/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3483 - accuracy: 0.9850 - val_loss: 0.6567 - val_accuracy: 0.8698\n","Epoch 100/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3494 - accuracy: 0.9822 - val_loss: 0.6865 - val_accuracy: 0.8688\n","{'loss': [0.5445634722709656, 0.5328160524368286, 0.5288230776786804, 0.5155740976333618, 0.5189173817634583, 0.5137698650360107, 0.5078437328338623, 0.5061579346656799, 0.4996159076690674, 0.5002555847167969, 0.4946620464324951, 0.4955962002277374, 0.4940635859966278, 0.49364495277404785, 0.49359309673309326, 0.5012789964675903, 0.47988128662109375, 0.4939369261264801, 0.47690752148628235, 0.47634461522102356, 0.4702039957046509, 0.46931540966033936, 0.4693240821361542, 0.4677804708480835, 0.4607921242713928, 0.46314120292663574, 0.46811914443969727, 0.45953673124313354, 0.4535112977027893, 0.45403924584388733, 0.4644799530506134, 0.4447872042655945, 0.4462134540081024, 0.4455938935279846, 0.4395964443683624, 0.4418415427207947, 0.43774622678756714, 0.43734821677207947, 0.4329432249069214, 0.4302130937576294, 0.43682152032852173, 0.4276752471923828, 0.4284124970436096, 0.4410495460033417, 0.42550504207611084, 0.4292609393596649, 0.42455360293388367, 0.4184163510799408, 0.4170922636985779, 0.41537025570869446, 0.4135860204696655, 0.4105190336704254, 0.40945935249328613, 0.4202049672603607, 0.4019230008125305, 0.41223806142807007, 0.4012093245983124, 0.41157954931259155, 0.3993821144104004, 0.40203139185905457, 0.40190672874450684, 0.39932921528816223, 0.39913296699523926, 0.4173199236392975, 0.4053114652633667, 0.3886857330799103, 0.38858476281166077, 0.3926030397415161, 0.3957168757915497, 0.38660740852355957, 0.3822847008705139, 0.38097211718559265, 0.3801976442337036, 0.38095158338546753, 0.3801479637622833, 0.3839617669582367, 0.40228766202926636, 0.3811621367931366, 0.3711152672767639, 0.3681984841823578, 0.37156417965888977, 0.37032854557037354, 0.37192609906196594, 0.366418719291687, 0.3634257912635803, 0.3676280975341797, 0.3629577159881592, 0.3641999065876007, 0.3577271103858948, 0.3594539165496826, 0.3598421514034271, 0.3593626022338867, 0.3541238009929657, 0.3550737798213959, 0.3546653389930725, 0.3538287281990051, 0.3500900864601135, 0.3507336378097534, 0.3483045697212219, 0.34940096735954285], 'accuracy': [0.9196382164955139, 0.9232558012008667, 0.9266149997711182, 0.9322997331619263, 0.933074951171875, 0.9289405941963196, 0.9343669414520264, 0.9364340901374817, 0.9369509220123291, 0.9385012984275818, 0.9390180706977844, 0.9385012984275818, 0.9377260804176331, 0.9390180706977844, 0.9395349025726318, 0.932041347026825, 0.9444444179534912, 0.9315245747566223, 0.9441860318183899, 0.9441860318183899, 0.9459948539733887, 0.9501292109489441, 0.9467700123786926, 0.9465116262435913, 0.950904369354248, 0.9488372206687927, 0.947028398513794, 0.9503875970840454, 0.9524548053741455, 0.950904369354248, 0.9421188831329346, 0.9550387859344482, 0.9532299637794495, 0.9555555582046509, 0.9614987373352051, 0.9529715776443481, 0.9565891623497009, 0.9573643207550049, 0.959431529045105, 0.9625322818756104, 0.9573643207550049, 0.9602067470550537, 0.959431529045105, 0.9496123790740967, 0.9614987373352051, 0.9555555582046509, 0.9568475484848022, 0.9635658860206604, 0.9625322818756104, 0.9633074998855591, 0.9651162624359131, 0.9656330943107605, 0.9648578763008118, 0.957622766494751, 0.9705426096916199, 0.959431529045105, 0.9679586291313171, 0.9599483013153076, 0.9687338471412659, 0.9643411040306091, 0.9645994901657104, 0.9671834707260132, 0.9669250845909119, 0.9519379734992981, 0.9614987373352051, 0.9720930457115173, 0.9715762138366699, 0.9658914804458618, 0.9661498665809631, 0.9677002429962158, 0.9746770262718201, 0.9718345999717712, 0.97364342212677, 0.9684754610061646, 0.9741601943969727, 0.9718345999717712, 0.9563307762145996, 0.970801055431366, 0.9770025610923767, 0.9788113832473755, 0.97364342212677, 0.9762274026870728, 0.974418580532074, 0.9780361652374268, 0.9798449873924255, 0.974418580532074, 0.9790697693824768, 0.9793281555175781, 0.9803617596626282, 0.9806201457977295, 0.9798449873924255, 0.9772610068321228, 0.9826873540878296, 0.9793281555175781, 0.9824289679527283, 0.9808785319328308, 0.9832041263580322, 0.9842377305030823, 0.985012948513031, 0.9821705222129822], 'val_loss': [1.0373345613479614, 1.0005712509155273, 0.9702015519142151, 0.9377812743186951, 0.9127253890037537, 0.8922714591026306, 0.8664409518241882, 0.8519830703735352, 0.8370116949081421, 0.8296484351158142, 0.8012933731079102, 0.7849915027618408, 0.738982081413269, 0.7210873961448669, 0.7089967131614685, 0.6800202131271362, 0.6865379810333252, 0.6569435596466064, 0.6600425243377686, 0.6436169147491455, 0.6671038269996643, 0.6610053777694702, 0.6315030455589294, 0.6203753352165222, 0.6220961213111877, 0.6168556809425354, 0.6288938522338867, 0.6295148134231567, 0.6207213401794434, 0.6518939733505249, 0.6249393224716187, 0.623453676700592, 0.6318710446357727, 0.6302908062934875, 0.6236501932144165, 0.6247301697731018, 0.6361825466156006, 0.6327564120292664, 0.6257869005203247, 0.6302472352981567, 0.6277518272399902, 0.6264460682868958, 0.6573635935783386, 0.6289659738540649, 0.6734122037887573, 0.624989926815033, 0.6383466720581055, 0.6380175948143005, 0.624723494052887, 0.6281129121780396, 0.6250498294830322, 0.6252346634864807, 0.6788322925567627, 0.6307555437088013, 0.656671941280365, 0.6365565061569214, 0.6441502571105957, 0.6327743530273438, 0.6253760457038879, 0.6367433667182922, 0.6701990365982056, 0.6397095322608948, 0.7138983607292175, 0.6503209471702576, 0.636006236076355, 0.6401432156562805, 0.6324687004089355, 0.6441413760185242, 0.6426771283149719, 0.6355515718460083, 0.6439724564552307, 0.6367899775505066, 0.6362796425819397, 0.6412537097930908, 0.6657167673110962, 0.7345394492149353, 0.6449607610702515, 0.6507356762886047, 0.6344626545906067, 0.6643731594085693, 0.6426417231559753, 0.6399136185646057, 0.6441388130187988, 0.6517166495323181, 0.6439608931541443, 0.6482793092727661, 0.6614232063293457, 0.6474829912185669, 0.6494001150131226, 0.6513047218322754, 0.6551626920700073, 0.6725805401802063, 0.6556413769721985, 0.6448841094970703, 0.6704248189926147, 0.6547989249229431, 0.6519627571105957, 0.6592468023300171, 0.6566856503486633, 0.686522364616394], 'val_accuracy': [0.48553720116615295, 0.4917355477809906, 0.5051652789115906, 0.5805785059928894, 0.6694214940071106, 0.7376033067703247, 0.8192148804664612, 0.807851254940033, 0.8016529083251953, 0.7510330677032471, 0.8057851195335388, 0.8068181872367859, 0.8564049601554871, 0.8543388247489929, 0.8305785059928894, 0.8584710955619812, 0.8378099203109741, 0.8646694421768188, 0.8491735458374023, 0.8708677887916565, 0.85537189245224, 0.8574380278587341, 0.8791322112083435, 0.8760330677032471, 0.8729338645935059, 0.8811983466148376, 0.8811983466148376, 0.8698347210884094, 0.8780992031097412, 0.8657024502754211, 0.8791322112083435, 0.8719007968902588, 0.8667355179786682, 0.8780992031097412, 0.8770661354064941, 0.8780992031097412, 0.8801652789115906, 0.8688016533851624, 0.8770661354064941, 0.8760330677032471, 0.8719007968902588, 0.8719007968902588, 0.8574380278587341, 0.8698347210884094, 0.8626033067703247, 0.8698347210884094, 0.8698347210884094, 0.8667355179786682, 0.8780992031097412, 0.8770661354064941, 0.8822314143180847, 0.8657024502754211, 0.8512396812438965, 0.8780992031097412, 0.8657024502754211, 0.8853305578231812, 0.8657024502754211, 0.8708677887916565, 0.8739669322967529, 0.8677685856819153, 0.8605371713638306, 0.8688016533851624, 0.8512396812438965, 0.8667355179786682, 0.8708677887916565, 0.8688016533851624, 0.8719007968902588, 0.8770661354064941, 0.8729338645935059, 0.8698347210884094, 0.8677685856819153, 0.8626033067703247, 0.8801652789115906, 0.875, 0.8636363744735718, 0.8378099203109741, 0.8698347210884094, 0.8770661354064941, 0.8688016533851624, 0.8719007968902588, 0.8719007968902588, 0.8708677887916565, 0.8739669322967529, 0.8729338645935059, 0.8719007968902588, 0.8646694421768188, 0.8667355179786682, 0.8698347210884094, 0.875, 0.8626033067703247, 0.8708677887916565, 0.8708677887916565, 0.8708677887916565, 0.8729338645935059, 0.8688016533851624, 0.8677685856819153, 0.8719007968902588, 0.8698347210884094, 0.8698347210884094, 0.8688016533851624]}\n","32/32 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 5s 34ms/step - loss: 0.4528 - accuracy: 0.9356 - val_loss: 1.0744 - val_accuracy: 0.4849\n","Epoch 2/100\n"," 1/29 [>.............................] - ETA: 0s - loss: 0.4988 - accuracy: 0.9141"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 0s 10ms/step - loss: 0.4009 - accuracy: 0.9612 - val_loss: 1.0435 - val_accuracy: 0.4849\n","Epoch 3/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3923 - accuracy: 0.9669 - val_loss: 0.9868 - val_accuracy: 0.4935\n","Epoch 4/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3897 - accuracy: 0.9647 - val_loss: 0.9323 - val_accuracy: 0.5054\n","Epoch 5/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.3883 - accuracy: 0.9688 - val_loss: 0.9231 - val_accuracy: 0.5129\n","Epoch 6/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3877 - accuracy: 0.9698 - val_loss: 0.9041 - val_accuracy: 0.5248\n","Epoch 7/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.3870 - accuracy: 0.9688 - val_loss: 0.8260 - val_accuracy: 0.6541\n","Epoch 8/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3840 - accuracy: 0.9669 - val_loss: 0.8517 - val_accuracy: 0.5819\n","Epoch 9/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3782 - accuracy: 0.9731 - val_loss: 0.8631 - val_accuracy: 0.5744\n","Epoch 10/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3749 - accuracy: 0.9739 - val_loss: 0.8483 - val_accuracy: 0.5991\n","Epoch 11/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3794 - accuracy: 0.9693 - val_loss: 0.7890 - val_accuracy: 0.6875\n","Epoch 12/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3705 - accuracy: 0.9755 - val_loss: 0.8024 - val_accuracy: 0.6756\n","Epoch 13/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3673 - accuracy: 0.9787 - val_loss: 0.8043 - val_accuracy: 0.6800\n","Epoch 14/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.3654 - accuracy: 0.9795 - val_loss: 0.7560 - val_accuracy: 0.7306\n","Epoch 15/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3648 - accuracy: 0.9798 - val_loss: 0.7739 - val_accuracy: 0.7209\n","Epoch 16/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3657 - accuracy: 0.9776 - val_loss: 0.6553 - val_accuracy: 0.8341\n","Epoch 17/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3694 - accuracy: 0.9717 - val_loss: 0.7355 - val_accuracy: 0.7640\n","Epoch 18/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.3686 - accuracy: 0.9736 - val_loss: 0.6156 - val_accuracy: 0.8675\n","Epoch 19/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3574 - accuracy: 0.9817 - val_loss: 0.5701 - val_accuracy: 0.8836\n","Epoch 20/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3636 - accuracy: 0.9766 - val_loss: 0.5642 - val_accuracy: 0.8718\n","Epoch 21/100\n","29/29 [==============================] - 1s 30ms/step - loss: 0.3581 - accuracy: 0.9803 - val_loss: 0.5430 - val_accuracy: 0.8858\n","Epoch 22/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3641 - accuracy: 0.9758 - val_loss: 0.5509 - val_accuracy: 0.8836\n","Epoch 23/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3525 - accuracy: 0.9833 - val_loss: 0.5364 - val_accuracy: 0.9019\n","Epoch 24/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3520 - accuracy: 0.9828 - val_loss: 0.5113 - val_accuracy: 0.9138\n","Epoch 25/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3526 - accuracy: 0.9811 - val_loss: 0.5626 - val_accuracy: 0.8804\n","Epoch 26/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3549 - accuracy: 0.9803 - val_loss: 0.5274 - val_accuracy: 0.8890\n","Epoch 27/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3483 - accuracy: 0.9836 - val_loss: 0.5093 - val_accuracy: 0.9073\n","Epoch 28/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3478 - accuracy: 0.9852 - val_loss: 0.5021 - val_accuracy: 0.9095\n","Epoch 29/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.3440 - accuracy: 0.9857 - val_loss: 0.4978 - val_accuracy: 0.9181\n","Epoch 30/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3435 - accuracy: 0.9852 - val_loss: 0.4934 - val_accuracy: 0.9181\n","Epoch 31/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3432 - accuracy: 0.9863 - val_loss: 0.5037 - val_accuracy: 0.9127\n","Epoch 32/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3417 - accuracy: 0.9838 - val_loss: 0.5032 - val_accuracy: 0.9127\n","Epoch 33/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3454 - accuracy: 0.9809 - val_loss: 0.5175 - val_accuracy: 0.9052\n","Epoch 34/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3406 - accuracy: 0.9830 - val_loss: 0.4983 - val_accuracy: 0.9138\n","Epoch 35/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3401 - accuracy: 0.9849 - val_loss: 0.5079 - val_accuracy: 0.9009\n","Epoch 36/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3418 - accuracy: 0.9833 - val_loss: 0.5037 - val_accuracy: 0.9127\n","Epoch 37/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3408 - accuracy: 0.9836 - val_loss: 0.4948 - val_accuracy: 0.9138\n","Epoch 38/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3382 - accuracy: 0.9860 - val_loss: 0.5302 - val_accuracy: 0.8922\n","Epoch 39/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3343 - accuracy: 0.9892 - val_loss: 0.5008 - val_accuracy: 0.9149\n","Epoch 40/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3308 - accuracy: 0.9876 - val_loss: 0.5005 - val_accuracy: 0.9127\n","Epoch 41/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3367 - accuracy: 0.9855 - val_loss: 0.5183 - val_accuracy: 0.9106\n","Epoch 42/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3334 - accuracy: 0.9876 - val_loss: 0.5018 - val_accuracy: 0.9062\n","Epoch 43/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3395 - accuracy: 0.9828 - val_loss: 0.4993 - val_accuracy: 0.9170\n","Epoch 44/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3308 - accuracy: 0.9879 - val_loss: 0.4994 - val_accuracy: 0.9159\n","Epoch 45/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3344 - accuracy: 0.9852 - val_loss: 0.5071 - val_accuracy: 0.9127\n","Epoch 46/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3336 - accuracy: 0.9855 - val_loss: 0.5107 - val_accuracy: 0.9095\n","Epoch 47/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3290 - accuracy: 0.9881 - val_loss: 0.5118 - val_accuracy: 0.9041\n","Epoch 48/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3334 - accuracy: 0.9844 - val_loss: 0.5815 - val_accuracy: 0.8815\n","Epoch 49/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3439 - accuracy: 0.9779 - val_loss: 0.5214 - val_accuracy: 0.9073\n","Epoch 50/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3281 - accuracy: 0.9879 - val_loss: 0.5171 - val_accuracy: 0.9106\n","Epoch 51/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3258 - accuracy: 0.9871 - val_loss: 0.5021 - val_accuracy: 0.9159\n","Epoch 52/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3326 - accuracy: 0.9855 - val_loss: 0.5217 - val_accuracy: 0.9073\n","Epoch 53/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3425 - accuracy: 0.9758 - val_loss: 0.5104 - val_accuracy: 0.9149\n","Epoch 54/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3316 - accuracy: 0.9814 - val_loss: 0.5011 - val_accuracy: 0.9127\n","Epoch 55/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3221 - accuracy: 0.9892 - val_loss: 0.5635 - val_accuracy: 0.8869\n","Epoch 56/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3294 - accuracy: 0.9849 - val_loss: 0.5179 - val_accuracy: 0.9106\n","Epoch 57/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3201 - accuracy: 0.9898 - val_loss: 0.5242 - val_accuracy: 0.9106\n","Epoch 58/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3218 - accuracy: 0.9881 - val_loss: 0.5335 - val_accuracy: 0.8966\n","Epoch 59/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3203 - accuracy: 0.9900 - val_loss: 0.5218 - val_accuracy: 0.9009\n","Epoch 60/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3152 - accuracy: 0.9922 - val_loss: 0.5108 - val_accuracy: 0.9127\n","Epoch 61/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3139 - accuracy: 0.9916 - val_loss: 0.5087 - val_accuracy: 0.9127\n","Epoch 62/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3128 - accuracy: 0.9933 - val_loss: 0.5111 - val_accuracy: 0.9073\n","Epoch 63/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3099 - accuracy: 0.9933 - val_loss: 0.5187 - val_accuracy: 0.9127\n","Epoch 64/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3108 - accuracy: 0.9925 - val_loss: 0.5195 - val_accuracy: 0.9095\n","Epoch 65/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3105 - accuracy: 0.9938 - val_loss: 0.5116 - val_accuracy: 0.9127\n","Epoch 66/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3083 - accuracy: 0.9938 - val_loss: 0.5463 - val_accuracy: 0.8933\n","Epoch 67/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3126 - accuracy: 0.9906 - val_loss: 0.5231 - val_accuracy: 0.9095\n","Epoch 68/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3103 - accuracy: 0.9916 - val_loss: 0.5383 - val_accuracy: 0.8955\n","Epoch 69/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3115 - accuracy: 0.9916 - val_loss: 0.5259 - val_accuracy: 0.9052\n","Epoch 70/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3103 - accuracy: 0.9922 - val_loss: 0.5372 - val_accuracy: 0.9052\n","Epoch 71/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3085 - accuracy: 0.9922 - val_loss: 0.5293 - val_accuracy: 0.8998\n","Epoch 72/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3174 - accuracy: 0.9879 - val_loss: 0.5321 - val_accuracy: 0.9127\n","Epoch 73/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3121 - accuracy: 0.9898 - val_loss: 0.5224 - val_accuracy: 0.9095\n","Epoch 74/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3032 - accuracy: 0.9952 - val_loss: 0.5217 - val_accuracy: 0.9138\n","Epoch 75/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3019 - accuracy: 0.9943 - val_loss: 0.5220 - val_accuracy: 0.9159\n","Epoch 76/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3008 - accuracy: 0.9949 - val_loss: 0.5210 - val_accuracy: 0.9116\n","Epoch 77/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3040 - accuracy: 0.9938 - val_loss: 0.5219 - val_accuracy: 0.9106\n","Epoch 78/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3337 - accuracy: 0.9739 - val_loss: 0.5894 - val_accuracy: 0.8912\n","Epoch 79/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3075 - accuracy: 0.9914 - val_loss: 0.5180 - val_accuracy: 0.9116\n","Epoch 80/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3029 - accuracy: 0.9930 - val_loss: 0.5480 - val_accuracy: 0.9052\n","Epoch 81/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3059 - accuracy: 0.9906 - val_loss: 0.5466 - val_accuracy: 0.9030\n","Epoch 82/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.3088 - accuracy: 0.9908 - val_loss: 0.5195 - val_accuracy: 0.9062\n","Epoch 83/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3004 - accuracy: 0.9946 - val_loss: 0.5320 - val_accuracy: 0.9041\n","Epoch 84/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2995 - accuracy: 0.9933 - val_loss: 0.5212 - val_accuracy: 0.9084\n","Epoch 85/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3056 - accuracy: 0.9914 - val_loss: 0.5234 - val_accuracy: 0.9106\n","Epoch 86/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2962 - accuracy: 0.9946 - val_loss: 0.5241 - val_accuracy: 0.9084\n","Epoch 87/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2957 - accuracy: 0.9970 - val_loss: 0.5255 - val_accuracy: 0.9095\n","Epoch 88/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.3080 - accuracy: 0.9881 - val_loss: 0.5589 - val_accuracy: 0.9009\n","Epoch 89/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2974 - accuracy: 0.9949 - val_loss: 0.5254 - val_accuracy: 0.9062\n","Epoch 90/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2973 - accuracy: 0.9946 - val_loss: 0.5267 - val_accuracy: 0.9073\n","Epoch 91/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2934 - accuracy: 0.9960 - val_loss: 0.5422 - val_accuracy: 0.9116\n","Epoch 92/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2933 - accuracy: 0.9957 - val_loss: 0.5288 - val_accuracy: 0.9052\n","Epoch 93/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2910 - accuracy: 0.9973 - val_loss: 0.5521 - val_accuracy: 0.9009\n","Epoch 94/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2928 - accuracy: 0.9962 - val_loss: 0.5370 - val_accuracy: 0.9019\n","Epoch 95/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2906 - accuracy: 0.9976 - val_loss: 0.5359 - val_accuracy: 0.9116\n","Epoch 96/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2914 - accuracy: 0.9960 - val_loss: 0.5250 - val_accuracy: 0.9095\n","Epoch 97/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2880 - accuracy: 0.9965 - val_loss: 0.5461 - val_accuracy: 0.8987\n","Epoch 98/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2876 - accuracy: 0.9978 - val_loss: 0.5349 - val_accuracy: 0.9138\n","Epoch 99/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2909 - accuracy: 0.9965 - val_loss: 0.5414 - val_accuracy: 0.9041\n","Epoch 100/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2876 - accuracy: 0.9970 - val_loss: 0.5290 - val_accuracy: 0.9019\n","{'loss': [0.4528428912162781, 0.40093034505844116, 0.39226996898651123, 0.38968777656555176, 0.38827836513519287, 0.3877142071723938, 0.38702231645584106, 0.3840440809726715, 0.3782130479812622, 0.37492379546165466, 0.37935397028923035, 0.370531290769577, 0.36733806133270264, 0.3653983473777771, 0.36475804448127747, 0.36569836735725403, 0.36940956115722656, 0.3685853183269501, 0.3574393689632416, 0.36362704634666443, 0.358088880777359, 0.36406731605529785, 0.35250023007392883, 0.35196006298065186, 0.35264644026756287, 0.35494551062583923, 0.3482520580291748, 0.34779930114746094, 0.34395793080329895, 0.3435000479221344, 0.3431585133075714, 0.3417188823223114, 0.3453541100025177, 0.3405870199203491, 0.3401406705379486, 0.3418113589286804, 0.3407847583293915, 0.3382461369037628, 0.3343447744846344, 0.3308318555355072, 0.33669108152389526, 0.3334033489227295, 0.3394968509674072, 0.3308485150337219, 0.3344486355781555, 0.33363911509513855, 0.32903316617012024, 0.33339744806289673, 0.3439379036426544, 0.3281276226043701, 0.32582417130470276, 0.33258670568466187, 0.3424549996852875, 0.33155661821365356, 0.32207638025283813, 0.3294358551502228, 0.3201454281806946, 0.32180550694465637, 0.320322722196579, 0.31517189741134644, 0.3138633668422699, 0.3128255009651184, 0.30988937616348267, 0.31080684065818787, 0.31048113107681274, 0.30826783180236816, 0.31261083483695984, 0.31031468510627747, 0.3114922046661377, 0.31031882762908936, 0.30850860476493835, 0.31738027930259705, 0.3121185600757599, 0.3032360374927521, 0.30191975831985474, 0.30083414912223816, 0.30396682024002075, 0.33366939425468445, 0.30747342109680176, 0.30292773246765137, 0.3058754801750183, 0.30879929661750793, 0.30040764808654785, 0.299545019865036, 0.3055986166000366, 0.29616209864616394, 0.2956556975841522, 0.30798089504241943, 0.2974017560482025, 0.2972618341445923, 0.29344555735588074, 0.2932606041431427, 0.2910299301147461, 0.2927841544151306, 0.29061344265937805, 0.2913820445537567, 0.28800341486930847, 0.2876277565956116, 0.29088693857192993, 0.2875533699989319], 'accuracy': [0.9356142282485962, 0.9612069129943848, 0.9668642282485962, 0.9647090435028076, 0.96875, 0.9698275923728943, 0.96875, 0.9668642282485962, 0.9730603694915771, 0.9738685488700867, 0.9692887663841248, 0.9754849076271057, 0.9787176847457886, 0.9795258641242981, 0.9797952771186829, 0.9776400923728943, 0.9717133641242981, 0.9735991358757019, 0.9816810488700867, 0.9765625, 0.9803340435028076, 0.9757543206214905, 0.9832974076271057, 0.982758641242981, 0.9811422228813171, 0.9803340435028076, 0.9835668206214905, 0.9851831793785095, 0.985722005367279, 0.9851831793785095, 0.9862607717514038, 0.9838362336158752, 0.9808728694915771, 0.983027994632721, 0.9849137663841248, 0.9832974076271057, 0.9835668206214905, 0.985991358757019, 0.9892241358757019, 0.9876077771186829, 0.9854525923728943, 0.9876077771186829, 0.982758641242981, 0.9878771305084229, 0.9851831793785095, 0.9854525923728943, 0.9881465435028076, 0.984375, 0.977909505367279, 0.9878771305084229, 0.9870689511299133, 0.9854525923728943, 0.9757543206214905, 0.9814116358757019, 0.9892241358757019, 0.9849137663841248, 0.9897629022598267, 0.9881465435028076, 0.9900323152542114, 0.9921875, 0.9916487336158752, 0.9932650923728943, 0.9932650923728943, 0.9924569129943848, 0.993803858757019, 0.993803858757019, 0.990571141242981, 0.9916487336158752, 0.9916487336158752, 0.9921875, 0.9921875, 0.9878771305084229, 0.9897629022598267, 0.9951508641242981, 0.9943426847457886, 0.9948814511299133, 0.993803858757019, 0.9738685488700867, 0.9913793206214905, 0.9929956793785095, 0.990571141242981, 0.990840494632721, 0.9946120977401733, 0.9932650923728943, 0.9913793206214905, 0.9946120977401733, 0.9970366358757019, 0.9881465435028076, 0.9948814511299133, 0.9946120977401733, 0.9959590435028076, 0.9956896305084229, 0.9973060488700867, 0.9962284564971924, 0.9975754022598267, 0.9959590435028076, 0.9964978694915771, 0.9978448152542114, 0.9964978694915771, 0.9970366358757019], 'val_loss': [1.0744315385818481, 1.0435497760772705, 0.9867721796035767, 0.9323433041572571, 0.9231016039848328, 0.9041086435317993, 0.8260101675987244, 0.8516852855682373, 0.8631382584571838, 0.8482574224472046, 0.7890297174453735, 0.8023667931556702, 0.8043379187583923, 0.755997896194458, 0.7738534212112427, 0.6553175449371338, 0.7354992032051086, 0.6156099438667297, 0.570098340511322, 0.5641767382621765, 0.5430279970169067, 0.5508695244789124, 0.5364106297492981, 0.5113365054130554, 0.5625945925712585, 0.527413547039032, 0.5093440413475037, 0.5020557045936584, 0.4977843761444092, 0.49338793754577637, 0.5037246346473694, 0.5031741857528687, 0.5174971222877502, 0.4983486533164978, 0.5078925490379333, 0.5036750435829163, 0.4948383867740631, 0.5301834940910339, 0.50075763463974, 0.5005289316177368, 0.5182538628578186, 0.5018362402915955, 0.4992760419845581, 0.4993584454059601, 0.5071401000022888, 0.5106577277183533, 0.511771023273468, 0.5815201997756958, 0.5213982462882996, 0.5171366333961487, 0.5021373629570007, 0.5216609835624695, 0.5103513598442078, 0.5010684728622437, 0.5634546875953674, 0.5178836584091187, 0.5241600275039673, 0.533541202545166, 0.5217981934547424, 0.5108259916305542, 0.5087268352508545, 0.5110712647438049, 0.5186771750450134, 0.5194618105888367, 0.5115888714790344, 0.5463364720344543, 0.5231066942214966, 0.5383296608924866, 0.5259315967559814, 0.5371875166893005, 0.5293440222740173, 0.5321342349052429, 0.5223786234855652, 0.5216814279556274, 0.5220291614532471, 0.5209940671920776, 0.5219045877456665, 0.5894373655319214, 0.5180357694625854, 0.5480417609214783, 0.546600341796875, 0.5195187926292419, 0.5319761037826538, 0.5212303400039673, 0.523388147354126, 0.5241292119026184, 0.5255306959152222, 0.5589348077774048, 0.5254409313201904, 0.5267338156700134, 0.5421651005744934, 0.5287958383560181, 0.5520707368850708, 0.5369588136672974, 0.5358823537826538, 0.5249871611595154, 0.5460500121116638, 0.5348541736602783, 0.5414292216300964, 0.5289718508720398], 'val_accuracy': [0.48491379618644714, 0.48491379618644714, 0.49353447556495667, 0.5053879022598267, 0.5129310488700867, 0.524784505367279, 0.6540948152542114, 0.5818965435028076, 0.5743534564971924, 0.5991379022598267, 0.6875, 0.6756465435028076, 0.6799569129943848, 0.7306034564971924, 0.7209051847457886, 0.8340517282485962, 0.764008641242981, 0.8674569129943848, 0.8836206793785095, 0.8717672228813171, 0.8857758641242981, 0.8836206793785095, 0.9019396305084229, 0.9137930870056152, 0.8803879022598267, 0.889008641242981, 0.9073275923728943, 0.9094827771186829, 0.9181034564971924, 0.9181034564971924, 0.912715494632721, 0.912715494632721, 0.9051724076271057, 0.9137930870056152, 0.9008620977401733, 0.912715494632721, 0.9137930870056152, 0.892241358757019, 0.9148706793785095, 0.912715494632721, 0.9105603694915771, 0.90625, 0.9170258641242981, 0.9159482717514038, 0.912715494632721, 0.9094827771186829, 0.9040948152542114, 0.881465494632721, 0.9073275923728943, 0.9105603694915771, 0.9159482717514038, 0.9073275923728943, 0.9148706793785095, 0.912715494632721, 0.8868534564971924, 0.9105603694915771, 0.9105603694915771, 0.8965517282485962, 0.9008620977401733, 0.912715494632721, 0.912715494632721, 0.9073275923728943, 0.912715494632721, 0.9094827771186829, 0.912715494632721, 0.8933189511299133, 0.9094827771186829, 0.8954741358757019, 0.9051724076271057, 0.9051724076271057, 0.899784505367279, 0.912715494632721, 0.9094827771186829, 0.9137930870056152, 0.9159482717514038, 0.9116379022598267, 0.9105603694915771, 0.8911637663841248, 0.9116379022598267, 0.9051724076271057, 0.9030172228813171, 0.90625, 0.9040948152542114, 0.9084051847457886, 0.9105603694915771, 0.9084051847457886, 0.9094827771186829, 0.9008620977401733, 0.90625, 0.9073275923728943, 0.9116379022598267, 0.9051724076271057, 0.9008620977401733, 0.9019396305084229, 0.9116379022598267, 0.9094827771186829, 0.8987069129943848, 0.9137930870056152, 0.9040948152542114, 0.9019396305084229]}\n","38/38 [==============================] - 0s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 4s 29ms/step - loss: 0.4554 - accuracy: 0.9295 - val_loss: 1.0770 - val_accuracy: 0.4955\n","Epoch 2/100\n"," 1/28 [>.............................] - ETA: 0s - loss: 0.4602 - accuracy: 0.9141"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["28/28 [==============================] - 0s 12ms/step - loss: 0.3972 - accuracy: 0.9584 - val_loss: 1.0382 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3944 - accuracy: 0.9618 - val_loss: 0.9893 - val_accuracy: 0.5034\n","Epoch 4/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.3929 - accuracy: 0.9632 - val_loss: 0.9209 - val_accuracy: 0.5294\n","Epoch 5/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3899 - accuracy: 0.9638 - val_loss: 0.9003 - val_accuracy: 0.5452\n","Epoch 6/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3852 - accuracy: 0.9683 - val_loss: 0.8997 - val_accuracy: 0.5475\n","Epoch 7/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3820 - accuracy: 0.9689 - val_loss: 0.8556 - val_accuracy: 0.5928\n","Epoch 8/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3796 - accuracy: 0.9697 - val_loss: 0.8497 - val_accuracy: 0.5984\n","Epoch 9/100\n","28/28 [==============================] - 1s 31ms/step - loss: 0.3752 - accuracy: 0.9728 - val_loss: 0.8432 - val_accuracy: 0.6063\n","Epoch 10/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3812 - accuracy: 0.9694 - val_loss: 0.8093 - val_accuracy: 0.6561\n","Epoch 11/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3800 - accuracy: 0.9663 - val_loss: 0.8142 - val_accuracy: 0.6482\n","Epoch 12/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3778 - accuracy: 0.9680 - val_loss: 0.8154 - val_accuracy: 0.6437\n","Epoch 13/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3757 - accuracy: 0.9706 - val_loss: 0.7545 - val_accuracy: 0.7308\n","Epoch 14/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3667 - accuracy: 0.9734 - val_loss: 0.7855 - val_accuracy: 0.6900\n","Epoch 15/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.3664 - accuracy: 0.9765 - val_loss: 0.7135 - val_accuracy: 0.7862\n","Epoch 16/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3685 - accuracy: 0.9723 - val_loss: 0.7258 - val_accuracy: 0.7568\n","Epoch 17/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.3685 - accuracy: 0.9743 - val_loss: 0.6626 - val_accuracy: 0.8258\n","Epoch 18/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.3614 - accuracy: 0.9765 - val_loss: 0.6381 - val_accuracy: 0.8428\n","Epoch 19/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3651 - accuracy: 0.9740 - val_loss: 0.6118 - val_accuracy: 0.8620\n","Epoch 20/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3581 - accuracy: 0.9793 - val_loss: 0.5973 - val_accuracy: 0.8744\n","Epoch 21/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3577 - accuracy: 0.9771 - val_loss: 0.5791 - val_accuracy: 0.8733\n","Epoch 22/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3716 - accuracy: 0.9680 - val_loss: 0.6043 - val_accuracy: 0.8620\n","Epoch 23/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3646 - accuracy: 0.9723 - val_loss: 0.5753 - val_accuracy: 0.8812\n","Epoch 24/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3532 - accuracy: 0.9808 - val_loss: 0.5680 - val_accuracy: 0.8880\n","Epoch 25/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3571 - accuracy: 0.9779 - val_loss: 0.5524 - val_accuracy: 0.8914\n","Epoch 26/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3523 - accuracy: 0.9816 - val_loss: 0.5508 - val_accuracy: 0.8869\n","Epoch 27/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3517 - accuracy: 0.9816 - val_loss: 0.5499 - val_accuracy: 0.8903\n","Epoch 28/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3568 - accuracy: 0.9740 - val_loss: 0.5902 - val_accuracy: 0.8846\n","Epoch 29/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3529 - accuracy: 0.9791 - val_loss: 0.5544 - val_accuracy: 0.8914\n","Epoch 30/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3545 - accuracy: 0.9779 - val_loss: 0.5441 - val_accuracy: 0.8959\n","Epoch 31/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3450 - accuracy: 0.9827 - val_loss: 0.5493 - val_accuracy: 0.9016\n","Epoch 32/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3559 - accuracy: 0.9751 - val_loss: 0.6065 - val_accuracy: 0.8801\n","Epoch 33/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3495 - accuracy: 0.9805 - val_loss: 0.5497 - val_accuracy: 0.8982\n","Epoch 34/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3402 - accuracy: 0.9856 - val_loss: 0.5529 - val_accuracy: 0.8982\n","Epoch 35/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3396 - accuracy: 0.9864 - val_loss: 0.5510 - val_accuracy: 0.9005\n","Epoch 36/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3352 - accuracy: 0.9878 - val_loss: 0.5620 - val_accuracy: 0.8948\n","Epoch 37/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3404 - accuracy: 0.9847 - val_loss: 0.5515 - val_accuracy: 0.8914\n","Epoch 38/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3343 - accuracy: 0.9870 - val_loss: 0.5559 - val_accuracy: 0.8948\n","Epoch 39/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3341 - accuracy: 0.9861 - val_loss: 0.5781 - val_accuracy: 0.8812\n","Epoch 40/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3360 - accuracy: 0.9853 - val_loss: 0.5592 - val_accuracy: 0.8903\n","Epoch 41/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3394 - accuracy: 0.9873 - val_loss: 0.6175 - val_accuracy: 0.8790\n","Epoch 42/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3455 - accuracy: 0.9825 - val_loss: 0.5584 - val_accuracy: 0.8971\n","Epoch 43/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3491 - accuracy: 0.9726 - val_loss: 0.5817 - val_accuracy: 0.8778\n","Epoch 44/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3389 - accuracy: 0.9827 - val_loss: 0.5956 - val_accuracy: 0.8778\n","Epoch 45/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3566 - accuracy: 0.9709 - val_loss: 0.5682 - val_accuracy: 0.8880\n","Epoch 46/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3438 - accuracy: 0.9802 - val_loss: 0.5780 - val_accuracy: 0.8925\n","Epoch 47/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3291 - accuracy: 0.9873 - val_loss: 0.5692 - val_accuracy: 0.8824\n","Epoch 48/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3264 - accuracy: 0.9887 - val_loss: 0.5674 - val_accuracy: 0.8869\n","Epoch 49/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3303 - accuracy: 0.9881 - val_loss: 0.6117 - val_accuracy: 0.8801\n","Epoch 50/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3236 - accuracy: 0.9907 - val_loss: 0.5647 - val_accuracy: 0.8982\n","Epoch 51/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3241 - accuracy: 0.9890 - val_loss: 0.5686 - val_accuracy: 0.8835\n","Epoch 52/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3233 - accuracy: 0.9901 - val_loss: 0.5697 - val_accuracy: 0.8937\n","Epoch 53/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3222 - accuracy: 0.9901 - val_loss: 0.5636 - val_accuracy: 0.8948\n","Epoch 54/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3282 - accuracy: 0.9861 - val_loss: 0.5830 - val_accuracy: 0.8857\n","Epoch 55/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.3200 - accuracy: 0.9918 - val_loss: 0.5670 - val_accuracy: 0.8857\n","Epoch 56/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3181 - accuracy: 0.9912 - val_loss: 0.5685 - val_accuracy: 0.8937\n","Epoch 57/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.3235 - accuracy: 0.9890 - val_loss: 0.5972 - val_accuracy: 0.8801\n","Epoch 58/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3306 - accuracy: 0.9859 - val_loss: 0.5761 - val_accuracy: 0.8903\n","Epoch 59/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3366 - accuracy: 0.9819 - val_loss: 0.5754 - val_accuracy: 0.8903\n","Epoch 60/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3284 - accuracy: 0.9861 - val_loss: 0.5741 - val_accuracy: 0.8857\n","Epoch 61/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3248 - accuracy: 0.9853 - val_loss: 0.5760 - val_accuracy: 0.8812\n","Epoch 62/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3210 - accuracy: 0.9901 - val_loss: 0.6623 - val_accuracy: 0.8699\n","Epoch 63/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3216 - accuracy: 0.9878 - val_loss: 0.6047 - val_accuracy: 0.8778\n","Epoch 64/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3190 - accuracy: 0.9890 - val_loss: 0.5939 - val_accuracy: 0.8812\n","Epoch 65/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3199 - accuracy: 0.9884 - val_loss: 0.5777 - val_accuracy: 0.8835\n","Epoch 66/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3142 - accuracy: 0.9912 - val_loss: 0.5871 - val_accuracy: 0.8869\n","Epoch 67/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3111 - accuracy: 0.9946 - val_loss: 0.5740 - val_accuracy: 0.8914\n","Epoch 68/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3150 - accuracy: 0.9909 - val_loss: 0.5874 - val_accuracy: 0.8801\n","Epoch 69/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3086 - accuracy: 0.9949 - val_loss: 0.5793 - val_accuracy: 0.8971\n","Epoch 70/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3094 - accuracy: 0.9938 - val_loss: 0.5891 - val_accuracy: 0.8812\n","Epoch 71/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3105 - accuracy: 0.9929 - val_loss: 0.5862 - val_accuracy: 0.8903\n","Epoch 72/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3162 - accuracy: 0.9907 - val_loss: 0.5843 - val_accuracy: 0.8857\n","Epoch 73/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3146 - accuracy: 0.9875 - val_loss: 0.5792 - val_accuracy: 0.8903\n","Epoch 74/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3117 - accuracy: 0.9921 - val_loss: 0.5952 - val_accuracy: 0.8925\n","Epoch 75/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3122 - accuracy: 0.9909 - val_loss: 0.5756 - val_accuracy: 0.8948\n","Epoch 76/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3069 - accuracy: 0.9926 - val_loss: 0.5858 - val_accuracy: 0.8824\n","Epoch 77/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3045 - accuracy: 0.9955 - val_loss: 0.5822 - val_accuracy: 0.8914\n","Epoch 78/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3072 - accuracy: 0.9935 - val_loss: 0.5884 - val_accuracy: 0.8914\n","Epoch 79/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3096 - accuracy: 0.9907 - val_loss: 0.5889 - val_accuracy: 0.8903\n","Epoch 80/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3045 - accuracy: 0.9946 - val_loss: 0.5963 - val_accuracy: 0.8812\n","Epoch 81/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3050 - accuracy: 0.9941 - val_loss: 0.5846 - val_accuracy: 0.8835\n","Epoch 82/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3035 - accuracy: 0.9949 - val_loss: 0.5912 - val_accuracy: 0.8835\n","Epoch 83/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3031 - accuracy: 0.9932 - val_loss: 0.5841 - val_accuracy: 0.8869\n","Epoch 84/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2998 - accuracy: 0.9972 - val_loss: 0.5939 - val_accuracy: 0.8869\n","Epoch 85/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2977 - accuracy: 0.9977 - val_loss: 0.5891 - val_accuracy: 0.8835\n","Epoch 86/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2993 - accuracy: 0.9966 - val_loss: 0.5882 - val_accuracy: 0.8903\n","Epoch 87/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2986 - accuracy: 0.9949 - val_loss: 0.5886 - val_accuracy: 0.8914\n","Epoch 88/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2968 - accuracy: 0.9969 - val_loss: 0.5911 - val_accuracy: 0.8948\n","Epoch 89/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2974 - accuracy: 0.9955 - val_loss: 0.5952 - val_accuracy: 0.8857\n","Epoch 90/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2968 - accuracy: 0.9960 - val_loss: 0.5859 - val_accuracy: 0.8903\n","Epoch 91/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2940 - accuracy: 0.9975 - val_loss: 0.5934 - val_accuracy: 0.8891\n","Epoch 92/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2961 - accuracy: 0.9955 - val_loss: 0.5975 - val_accuracy: 0.8846\n","Epoch 93/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2995 - accuracy: 0.9941 - val_loss: 0.5954 - val_accuracy: 0.8891\n","Epoch 94/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2972 - accuracy: 0.9960 - val_loss: 0.5986 - val_accuracy: 0.8937\n","Epoch 95/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2960 - accuracy: 0.9958 - val_loss: 0.6075 - val_accuracy: 0.8744\n","Epoch 96/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2937 - accuracy: 0.9966 - val_loss: 0.5976 - val_accuracy: 0.8790\n","Epoch 97/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2904 - accuracy: 0.9977 - val_loss: 0.5961 - val_accuracy: 0.8903\n","Epoch 98/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2918 - accuracy: 0.9977 - val_loss: 0.6121 - val_accuracy: 0.8925\n","Epoch 99/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2951 - accuracy: 0.9958 - val_loss: 0.6641 - val_accuracy: 0.8710\n","Epoch 100/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2973 - accuracy: 0.9952 - val_loss: 0.5983 - val_accuracy: 0.8959\n","{'loss': [0.45535925030708313, 0.397225558757782, 0.394398957490921, 0.3928602933883667, 0.3898795545101166, 0.3851706385612488, 0.38196760416030884, 0.37955549359321594, 0.37523123621940613, 0.381203830242157, 0.3799613118171692, 0.37783315777778625, 0.37568286061286926, 0.36667150259017944, 0.3663681149482727, 0.3685402274131775, 0.3685143291950226, 0.3613871932029724, 0.3650955259799957, 0.3581082820892334, 0.35774391889572144, 0.37157538533210754, 0.3646225035190582, 0.3532394766807556, 0.3570960462093353, 0.3523293137550354, 0.3516533672809601, 0.3567609488964081, 0.35289645195007324, 0.35451260209083557, 0.34504544734954834, 0.3558637201786041, 0.34953922033309937, 0.3401903212070465, 0.339642196893692, 0.33515989780426025, 0.3404199182987213, 0.3342803418636322, 0.3340669870376587, 0.33596763014793396, 0.3393853008747101, 0.34548455476760864, 0.34911632537841797, 0.3389067053794861, 0.35656359791755676, 0.3438405394554138, 0.32908228039741516, 0.3263692557811737, 0.3303169012069702, 0.32360824942588806, 0.3240647614002228, 0.3233303427696228, 0.3221745789051056, 0.3281547725200653, 0.31995636224746704, 0.31810370087623596, 0.323491632938385, 0.3305821120738983, 0.3366365134716034, 0.32835495471954346, 0.3247535526752472, 0.32096657156944275, 0.3216399848461151, 0.3189602494239807, 0.31991657614707947, 0.3141948878765106, 0.31112298369407654, 0.31498849391937256, 0.3085862994194031, 0.3093752861022949, 0.3105440139770508, 0.3161531686782837, 0.3145712614059448, 0.3116915822029114, 0.3122296631336212, 0.3068559467792511, 0.3045295178890228, 0.3071763813495636, 0.30959320068359375, 0.30445918440818787, 0.3050074577331543, 0.30347415804862976, 0.30309629440307617, 0.29979467391967773, 0.29765284061431885, 0.2992730438709259, 0.2985730469226837, 0.296834796667099, 0.2974150478839874, 0.29675784707069397, 0.2940462827682495, 0.29612061381340027, 0.2994655966758728, 0.2971824109554291, 0.29595887660980225, 0.2936613857746124, 0.2903866171836853, 0.2917839288711548, 0.2950584292411804, 0.2973146438598633], 'accuracy': [0.9295415878295898, 0.9584040641784668, 0.961799681186676, 0.9632145166397095, 0.963780403137207, 0.9683078527450562, 0.9688737988471985, 0.9697226881980896, 0.9728353023529053, 0.9694397449493408, 0.9663271307945251, 0.9680249094963074, 0.9705715775489807, 0.9734012484550476, 0.9765138626098633, 0.9722693562507629, 0.9742501378059387, 0.9765138626098633, 0.9739671945571899, 0.9793435335159302, 0.9770798087120056, 0.9680249094963074, 0.9722693562507629, 0.9807583689689636, 0.9779286980628967, 0.9816072583198547, 0.9816072583198547, 0.9739671945571899, 0.9790605306625366, 0.9779286980628967, 0.9827390909194946, 0.9750990271568298, 0.9804753661155701, 0.9855687618255615, 0.9864176511764526, 0.9878324866294861, 0.9847198724746704, 0.986983597278595, 0.9861347079277039, 0.9852858185768127, 0.9872665405273438, 0.9824561476707458, 0.9725523591041565, 0.9827390909194946, 0.9708545804023743, 0.9801924228668213, 0.9872665405273438, 0.9886813759803772, 0.9881154298782349, 0.990662157535553, 0.988964319229126, 0.9900962114334106, 0.9900962114334106, 0.9861347079277039, 0.9917939901351929, 0.9912280440330505, 0.988964319229126, 0.9858517050743103, 0.9818902015686035, 0.9861347079277039, 0.9852858185768127, 0.9900962114334106, 0.9878324866294861, 0.988964319229126, 0.9883984327316284, 0.9912280440330505, 0.9946236610412598, 0.9909451007843018, 0.9949066042900085, 0.9937747716903687, 0.9929258823394775, 0.990662157535553, 0.9875495433807373, 0.9920769929885864, 0.9909451007843018, 0.992642879486084, 0.9954725503921509, 0.9934917688369751, 0.990662157535553, 0.9946236610412598, 0.9940577149391174, 0.9949066042900085, 0.9932088255882263, 0.9971703290939331, 0.9977362751960754, 0.9966044425964355, 0.9949066042900085, 0.9968873858451843, 0.9954725503921509, 0.9960384964942932, 0.9974533319473267, 0.9954725503921509, 0.9940577149391174, 0.9960384964942932, 0.9957554936408997, 0.9966044425964355, 0.9977362751960754, 0.9977362751960754, 0.9957554936408997, 0.9951896071434021], 'val_loss': [1.077028512954712, 1.0381885766983032, 0.98934406042099, 0.9208500385284424, 0.9003117084503174, 0.8996656537055969, 0.8555588126182556, 0.8497260808944702, 0.843176543712616, 0.8092747926712036, 0.8142141103744507, 0.8153785467147827, 0.7544860243797302, 0.7855058908462524, 0.7135066390037537, 0.7257960438728333, 0.6625600457191467, 0.6381097435951233, 0.6117852926254272, 0.5973332524299622, 0.5790907740592957, 0.6043185591697693, 0.575343668460846, 0.567986011505127, 0.5523964166641235, 0.5508170127868652, 0.5499073266983032, 0.5901954174041748, 0.5543757081031799, 0.5440607070922852, 0.5493158102035522, 0.606544554233551, 0.5497313737869263, 0.5528513789176941, 0.5509670376777649, 0.5620273351669312, 0.5515022873878479, 0.5559127330780029, 0.5780503153800964, 0.5592473149299622, 0.6174896955490112, 0.5583742260932922, 0.5816646218299866, 0.5955696105957031, 0.5682215094566345, 0.5780038833618164, 0.5691717267036438, 0.5674238801002502, 0.6116523742675781, 0.5646577477455139, 0.5686405301094055, 0.5697126984596252, 0.5635712742805481, 0.5830029845237732, 0.566961944103241, 0.5684870481491089, 0.5971865653991699, 0.5760723948478699, 0.5753787159919739, 0.5740675926208496, 0.5759904384613037, 0.6623407602310181, 0.6046991348266602, 0.5938767790794373, 0.5776891112327576, 0.5870869755744934, 0.5739536881446838, 0.5874353647232056, 0.5793187022209167, 0.5891382098197937, 0.58616042137146, 0.5842753052711487, 0.5792242288589478, 0.5951721668243408, 0.5755969285964966, 0.5858407020568848, 0.5822107195854187, 0.5884339213371277, 0.588921070098877, 0.5963348746299744, 0.5845733284950256, 0.5911818742752075, 0.5840938091278076, 0.5938560962677002, 0.5891436338424683, 0.5881653428077698, 0.5886356830596924, 0.5911337733268738, 0.5952039361000061, 0.5858860611915588, 0.5934203267097473, 0.5974723100662231, 0.595361053943634, 0.598584771156311, 0.6075384616851807, 0.5976155996322632, 0.596089780330658, 0.6121333837509155, 0.6641021370887756, 0.5983390212059021], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.5033936500549316, 0.529411792755127, 0.5452488660812378, 0.5475113391876221, 0.5927602052688599, 0.598416268825531, 0.6063348650932312, 0.6561086177825928, 0.6481900215148926, 0.6436651349067688, 0.7307692170143127, 0.6900452375411987, 0.7861990928649902, 0.7567873597145081, 0.8257918357849121, 0.8427602052688599, 0.8619909286499023, 0.8744344115257263, 0.8733031749725342, 0.8619909286499023, 0.8812217116355896, 0.8880090713500977, 0.8914027214050293, 0.8868778347969055, 0.8902714848518372, 0.8846153616905212, 0.8914027214050293, 0.8959276080131531, 0.901583731174469, 0.8800904750823975, 0.8981900215148926, 0.8981900215148926, 0.9004524946212769, 0.8947963714599609, 0.8914027214050293, 0.8947963714599609, 0.8812217116355896, 0.8902714848518372, 0.8789592981338501, 0.8970588445663452, 0.877828061580658, 0.877828061580658, 0.8880090713500977, 0.8925339579582214, 0.8823529481887817, 0.8868778347969055, 0.8800904750823975, 0.8981900215148926, 0.8834841847419739, 0.8936651349067688, 0.8947963714599609, 0.8857465982437134, 0.8857465982437134, 0.8936651349067688, 0.8800904750823975, 0.8902714848518372, 0.8902714848518372, 0.8857465982437134, 0.8812217116355896, 0.8699095249176025, 0.877828061580658, 0.8812217116355896, 0.8834841847419739, 0.8868778347969055, 0.8914027214050293, 0.8800904750823975, 0.8970588445663452, 0.8812217116355896, 0.8902714848518372, 0.8857465982437134, 0.8902714848518372, 0.8925339579582214, 0.8947963714599609, 0.8823529481887817, 0.8914027214050293, 0.8914027214050293, 0.8902714848518372, 0.8812217116355896, 0.8834841847419739, 0.8834841847419739, 0.8868778347969055, 0.8868778347969055, 0.8834841847419739, 0.8902714848518372, 0.8914027214050293, 0.8947963714599609, 0.8857465982437134, 0.8902714848518372, 0.889140248298645, 0.8846153616905212, 0.889140248298645, 0.8936651349067688, 0.8744344115257263, 0.8789592981338501, 0.8902714848518372, 0.8925339579582214, 0.8710407018661499, 0.8959276080131531]}\n","45/45 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 3s 27ms/step - loss: 0.4198 - accuracy: 0.9473 - val_loss: 1.0912 - val_accuracy: 0.4855\n","Epoch 2/100\n"," 1/31 [..............................] - ETA: 0s - loss: 0.3706 - accuracy: 0.9609"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 0s 16ms/step - loss: 0.3873 - accuracy: 0.9625 - val_loss: 1.0358 - val_accuracy: 0.4866\n","Epoch 3/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.3842 - accuracy: 0.9682 - val_loss: 0.9743 - val_accuracy: 0.4979\n","Epoch 4/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3800 - accuracy: 0.9674 - val_loss: 0.9172 - val_accuracy: 0.5248\n","Epoch 5/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3829 - accuracy: 0.9687 - val_loss: 0.8533 - val_accuracy: 0.6136\n","Epoch 6/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3899 - accuracy: 0.9636 - val_loss: 0.8715 - val_accuracy: 0.5630\n","Epoch 7/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3777 - accuracy: 0.9685 - val_loss: 0.8472 - val_accuracy: 0.6012\n","Epoch 8/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3789 - accuracy: 0.9695 - val_loss: 0.8499 - val_accuracy: 0.5961\n","Epoch 9/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3687 - accuracy: 0.9747 - val_loss: 0.8206 - val_accuracy: 0.6364\n","Epoch 10/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3751 - accuracy: 0.9718 - val_loss: 0.8115 - val_accuracy: 0.6457\n","Epoch 11/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3675 - accuracy: 0.9762 - val_loss: 0.7949 - val_accuracy: 0.6694\n","Epoch 12/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3627 - accuracy: 0.9791 - val_loss: 0.7723 - val_accuracy: 0.7087\n","Epoch 13/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.3642 - accuracy: 0.9742 - val_loss: 0.7029 - val_accuracy: 0.7975\n","Epoch 14/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3687 - accuracy: 0.9713 - val_loss: 0.7133 - val_accuracy: 0.7851\n","Epoch 15/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.3607 - accuracy: 0.9765 - val_loss: 0.6662 - val_accuracy: 0.8285\n","Epoch 16/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3573 - accuracy: 0.9793 - val_loss: 0.7018 - val_accuracy: 0.7975\n","Epoch 17/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3720 - accuracy: 0.9654 - val_loss: 0.6711 - val_accuracy: 0.8202\n","Epoch 18/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.3691 - accuracy: 0.9667 - val_loss: 0.5906 - val_accuracy: 0.8760\n","Epoch 19/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.3530 - accuracy: 0.9801 - val_loss: 0.5859 - val_accuracy: 0.8812\n","Epoch 20/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3593 - accuracy: 0.9767 - val_loss: 0.6214 - val_accuracy: 0.8647\n","Epoch 21/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3496 - accuracy: 0.9811 - val_loss: 0.5889 - val_accuracy: 0.8791\n","Epoch 22/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3511 - accuracy: 0.9798 - val_loss: 0.5674 - val_accuracy: 0.8915\n","Epoch 23/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3577 - accuracy: 0.9767 - val_loss: 0.5927 - val_accuracy: 0.8833\n","Epoch 24/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3560 - accuracy: 0.9742 - val_loss: 0.5591 - val_accuracy: 0.8915\n","Epoch 25/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3521 - accuracy: 0.9780 - val_loss: 0.5660 - val_accuracy: 0.8957\n","Epoch 26/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3557 - accuracy: 0.9742 - val_loss: 0.5723 - val_accuracy: 0.8833\n","Epoch 27/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3510 - accuracy: 0.9786 - val_loss: 0.5615 - val_accuracy: 0.8884\n","Epoch 28/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3445 - accuracy: 0.9840 - val_loss: 0.5642 - val_accuracy: 0.8967\n","Epoch 29/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3429 - accuracy: 0.9837 - val_loss: 0.5679 - val_accuracy: 0.8926\n","Epoch 30/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3427 - accuracy: 0.9840 - val_loss: 0.5707 - val_accuracy: 0.8957\n","Epoch 31/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3396 - accuracy: 0.9845 - val_loss: 0.5895 - val_accuracy: 0.8833\n","Epoch 32/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3454 - accuracy: 0.9798 - val_loss: 0.5826 - val_accuracy: 0.8905\n","Epoch 33/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3405 - accuracy: 0.9848 - val_loss: 0.5861 - val_accuracy: 0.8905\n","Epoch 34/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3384 - accuracy: 0.9837 - val_loss: 0.6100 - val_accuracy: 0.8771\n","Epoch 35/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3348 - accuracy: 0.9873 - val_loss: 0.5984 - val_accuracy: 0.8905\n","Epoch 36/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3379 - accuracy: 0.9850 - val_loss: 0.5803 - val_accuracy: 0.8884\n","Epoch 37/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3305 - accuracy: 0.9894 - val_loss: 0.5801 - val_accuracy: 0.8884\n","Epoch 38/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3305 - accuracy: 0.9884 - val_loss: 0.5963 - val_accuracy: 0.8905\n","Epoch 39/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3398 - accuracy: 0.9814 - val_loss: 0.5870 - val_accuracy: 0.8895\n","Epoch 40/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3326 - accuracy: 0.9840 - val_loss: 0.5782 - val_accuracy: 0.8864\n","Epoch 41/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3264 - accuracy: 0.9917 - val_loss: 0.5798 - val_accuracy: 0.8864\n","Epoch 42/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3268 - accuracy: 0.9902 - val_loss: 0.5953 - val_accuracy: 0.8895\n","Epoch 43/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3275 - accuracy: 0.9873 - val_loss: 0.5802 - val_accuracy: 0.8957\n","Epoch 44/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3261 - accuracy: 0.9891 - val_loss: 0.5964 - val_accuracy: 0.8884\n","Epoch 45/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3308 - accuracy: 0.9876 - val_loss: 0.6225 - val_accuracy: 0.8781\n","Epoch 46/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3403 - accuracy: 0.9801 - val_loss: 0.5814 - val_accuracy: 0.8895\n","Epoch 47/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3265 - accuracy: 0.9886 - val_loss: 0.6257 - val_accuracy: 0.8729\n","Epoch 48/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3281 - accuracy: 0.9860 - val_loss: 0.5946 - val_accuracy: 0.8864\n","Epoch 49/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3227 - accuracy: 0.9889 - val_loss: 0.5923 - val_accuracy: 0.8895\n","Epoch 50/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3199 - accuracy: 0.9912 - val_loss: 0.5924 - val_accuracy: 0.8967\n","Epoch 51/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3191 - accuracy: 0.9899 - val_loss: 0.5899 - val_accuracy: 0.8967\n","Epoch 52/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.3162 - accuracy: 0.9930 - val_loss: 0.5905 - val_accuracy: 0.8864\n","Epoch 53/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3185 - accuracy: 0.9915 - val_loss: 0.6021 - val_accuracy: 0.8833\n","Epoch 54/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3204 - accuracy: 0.9904 - val_loss: 0.6124 - val_accuracy: 0.8822\n","Epoch 55/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3154 - accuracy: 0.9910 - val_loss: 0.6082 - val_accuracy: 0.8864\n","Epoch 56/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3141 - accuracy: 0.9917 - val_loss: 0.5889 - val_accuracy: 0.8905\n","Epoch 57/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3187 - accuracy: 0.9876 - val_loss: 0.6015 - val_accuracy: 0.8905\n","Epoch 58/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3135 - accuracy: 0.9928 - val_loss: 0.5972 - val_accuracy: 0.8905\n","Epoch 59/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3123 - accuracy: 0.9943 - val_loss: 0.5982 - val_accuracy: 0.8884\n","Epoch 60/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.3112 - accuracy: 0.9943 - val_loss: 0.6001 - val_accuracy: 0.8977\n","Epoch 61/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3121 - accuracy: 0.9928 - val_loss: 0.6324 - val_accuracy: 0.8874\n","Epoch 62/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3154 - accuracy: 0.9897 - val_loss: 0.6037 - val_accuracy: 0.8884\n","Epoch 63/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3087 - accuracy: 0.9935 - val_loss: 0.6173 - val_accuracy: 0.8833\n","Epoch 64/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3128 - accuracy: 0.9912 - val_loss: 0.6171 - val_accuracy: 0.8853\n","Epoch 65/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3074 - accuracy: 0.9933 - val_loss: 0.6506 - val_accuracy: 0.8740\n","Epoch 66/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3193 - accuracy: 0.9845 - val_loss: 0.6482 - val_accuracy: 0.8729\n","Epoch 67/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3139 - accuracy: 0.9902 - val_loss: 0.6007 - val_accuracy: 0.8895\n","Epoch 68/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3053 - accuracy: 0.9938 - val_loss: 0.6343 - val_accuracy: 0.8864\n","Epoch 69/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3041 - accuracy: 0.9943 - val_loss: 0.6283 - val_accuracy: 0.8802\n","Epoch 70/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3140 - accuracy: 0.9891 - val_loss: 0.6306 - val_accuracy: 0.8781\n","Epoch 71/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3100 - accuracy: 0.9886 - val_loss: 0.6030 - val_accuracy: 0.8771\n","Epoch 72/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3047 - accuracy: 0.9938 - val_loss: 0.6094 - val_accuracy: 0.8802\n","Epoch 73/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3023 - accuracy: 0.9948 - val_loss: 0.6233 - val_accuracy: 0.8874\n","Epoch 74/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3042 - accuracy: 0.9943 - val_loss: 0.6289 - val_accuracy: 0.8822\n","Epoch 75/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2997 - accuracy: 0.9951 - val_loss: 0.6125 - val_accuracy: 0.8853\n","Epoch 76/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2997 - accuracy: 0.9956 - val_loss: 0.6314 - val_accuracy: 0.8936\n","Epoch 77/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2997 - accuracy: 0.9953 - val_loss: 0.6167 - val_accuracy: 0.8915\n","Epoch 78/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2965 - accuracy: 0.9961 - val_loss: 0.6195 - val_accuracy: 0.8874\n","Epoch 79/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2952 - accuracy: 0.9974 - val_loss: 0.6182 - val_accuracy: 0.8853\n","Epoch 80/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2954 - accuracy: 0.9959 - val_loss: 0.6075 - val_accuracy: 0.8853\n","Epoch 81/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2971 - accuracy: 0.9961 - val_loss: 0.6403 - val_accuracy: 0.8895\n","Epoch 82/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3040 - accuracy: 0.9938 - val_loss: 0.6168 - val_accuracy: 0.8791\n","Epoch 83/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2974 - accuracy: 0.9948 - val_loss: 0.6323 - val_accuracy: 0.8926\n","Epoch 84/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3003 - accuracy: 0.9922 - val_loss: 0.6162 - val_accuracy: 0.8864\n","Epoch 85/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2946 - accuracy: 0.9961 - val_loss: 0.6333 - val_accuracy: 0.8936\n","Epoch 86/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2941 - accuracy: 0.9961 - val_loss: 0.6200 - val_accuracy: 0.8905\n","Epoch 87/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3011 - accuracy: 0.9915 - val_loss: 0.6439 - val_accuracy: 0.8822\n","Epoch 88/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2993 - accuracy: 0.9922 - val_loss: 0.6135 - val_accuracy: 0.8812\n","Epoch 89/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2900 - accuracy: 0.9972 - val_loss: 0.6275 - val_accuracy: 0.8874\n","Epoch 90/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2932 - accuracy: 0.9953 - val_loss: 0.6266 - val_accuracy: 0.8936\n","Epoch 91/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2907 - accuracy: 0.9969 - val_loss: 0.6528 - val_accuracy: 0.8760\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2997 - accuracy: 0.9907 - val_loss: 0.6284 - val_accuracy: 0.8874\n","Epoch 93/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2911 - accuracy: 0.9964 - val_loss: 0.6199 - val_accuracy: 0.8833\n","Epoch 94/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2901 - accuracy: 0.9969 - val_loss: 0.6233 - val_accuracy: 0.8884\n","Epoch 95/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2868 - accuracy: 0.9972 - val_loss: 0.6234 - val_accuracy: 0.8895\n","Epoch 96/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2880 - accuracy: 0.9972 - val_loss: 0.6615 - val_accuracy: 0.8843\n","Epoch 97/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2911 - accuracy: 0.9948 - val_loss: 0.6528 - val_accuracy: 0.8874\n","Epoch 98/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2861 - accuracy: 0.9966 - val_loss: 0.6231 - val_accuracy: 0.8822\n","Epoch 99/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2898 - accuracy: 0.9953 - val_loss: 0.6428 - val_accuracy: 0.8926\n","Epoch 100/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2850 - accuracy: 0.9969 - val_loss: 0.6270 - val_accuracy: 0.8853\n","{'loss': [0.41983291506767273, 0.3872649371623993, 0.384215772151947, 0.3800202012062073, 0.38291239738464355, 0.38991203904151917, 0.3777232766151428, 0.3788837492465973, 0.36868828535079956, 0.37506163120269775, 0.3674911558628082, 0.3626745939254761, 0.36415982246398926, 0.36868366599082947, 0.3607053756713867, 0.3573227822780609, 0.3719587028026581, 0.36912813782691956, 0.35297706723213196, 0.3592514991760254, 0.3495943546295166, 0.3511488139629364, 0.35765907168388367, 0.3559608459472656, 0.3521091639995575, 0.3557261526584625, 0.35097259283065796, 0.34453284740448, 0.3428683578968048, 0.34265077114105225, 0.3396429717540741, 0.34542861580848694, 0.34052249789237976, 0.3384007513523102, 0.3348376452922821, 0.3379124701023102, 0.3305097818374634, 0.33054521679878235, 0.3397814631462097, 0.332594096660614, 0.32642853260040283, 0.3267667293548584, 0.3274640738964081, 0.32608509063720703, 0.33076637983322144, 0.34029096364974976, 0.3264528512954712, 0.32811877131462097, 0.3227158784866333, 0.3199375867843628, 0.3191431760787964, 0.31616440415382385, 0.3185035288333893, 0.32041600346565247, 0.31538811326026917, 0.31414687633514404, 0.3186506927013397, 0.3134561777114868, 0.3122994005680084, 0.31117111444473267, 0.3120995759963989, 0.31537574529647827, 0.3087233603000641, 0.3127601742744446, 0.3074341416358948, 0.3192700743675232, 0.31390365958213806, 0.3052549958229065, 0.30405277013778687, 0.3139936625957489, 0.30997201800346375, 0.3047164976596832, 0.3022778630256653, 0.30422043800354004, 0.2996644675731659, 0.2996605336666107, 0.29968178272247314, 0.2965022921562195, 0.29521259665489197, 0.29543861746788025, 0.2971331477165222, 0.3039858043193817, 0.2973969280719757, 0.30028578639030457, 0.29456761479377747, 0.29409149289131165, 0.30108124017715454, 0.2992994785308838, 0.2899526059627533, 0.2931525707244873, 0.29072830080986023, 0.2997485101222992, 0.2911071479320526, 0.290065199136734, 0.286776065826416, 0.2879910171031952, 0.2910907566547394, 0.2860826849937439, 0.28975945711135864, 0.2850499153137207], 'accuracy': [0.94728684425354, 0.9625322818756104, 0.9682170748710632, 0.9674418568611145, 0.9687338471412659, 0.9635658860206604, 0.9684754610061646, 0.9695090651512146, 0.9746770262718201, 0.9718345999717712, 0.9762274026870728, 0.9790697693824768, 0.9741601943969727, 0.9713178277015686, 0.9764857888221741, 0.9793281555175781, 0.9653746485710144, 0.9666666388511658, 0.9801033735275269, 0.9767441749572754, 0.9811369776725769, 0.9798449873924255, 0.9767441749572754, 0.9741601943969727, 0.9780361652374268, 0.9741601943969727, 0.9785529971122742, 0.983979344367981, 0.9837209582328796, 0.983979344367981, 0.9844961166381836, 0.9798449873924255, 0.9847545027732849, 0.9837209582328796, 0.9873384833335876, 0.985012948513031, 0.9894056916236877, 0.9883720874786377, 0.9813953638076782, 0.983979344367981, 0.9917312860488892, 0.9901808500289917, 0.9873384833335876, 0.9891473054885864, 0.987596869468689, 0.9801033735275269, 0.988630473613739, 0.9860464930534363, 0.9888888597488403, 0.9912144541740417, 0.9899224638938904, 0.9930232763290405, 0.9914728403091431, 0.9904392957687378, 0.9909560680389404, 0.9917312860488892, 0.987596869468689, 0.9927648305892944, 0.9943152666091919, 0.9943152666091919, 0.9927648305892944, 0.9896640777587891, 0.9935400485992432, 0.9912144541740417, 0.9932816624641418, 0.9844961166381836, 0.9901808500289917, 0.9937984347343445, 0.9943152666091919, 0.9891473054885864, 0.988630473613739, 0.9937984347343445, 0.9948320388793945, 0.9943152666091919, 0.9950904250144958, 0.9956072568893433, 0.9953488111495972, 0.9961240291595459, 0.9974160194396973, 0.9958656430244446, 0.9961240291595459, 0.9937984347343445, 0.9948320388793945, 0.9922480583190918, 0.9961240291595459, 0.9961240291595459, 0.9914728403091431, 0.9922480583190918, 0.997157633304596, 0.9953488111495972, 0.9968992471694946, 0.9906976819038391, 0.9963824152946472, 0.9968992471694946, 0.997157633304596, 0.997157633304596, 0.9948320388793945, 0.9966408014297485, 0.9953488111495972, 0.9968992471694946], 'val_loss': [1.0912495851516724, 1.0357646942138672, 0.9742903113365173, 0.9171585440635681, 0.853337824344635, 0.8715384006500244, 0.8471584320068359, 0.8498691320419312, 0.8205546736717224, 0.8115015029907227, 0.7949299216270447, 0.7722713351249695, 0.7028676271438599, 0.7132648229598999, 0.6661948561668396, 0.7018007636070251, 0.6710661053657532, 0.5906237959861755, 0.5859491229057312, 0.6214171051979065, 0.5888881087303162, 0.5674376487731934, 0.5927174687385559, 0.5590808391571045, 0.5660432577133179, 0.572325587272644, 0.5614879727363586, 0.5641819834709167, 0.5679097175598145, 0.5707359910011292, 0.5895372033119202, 0.5825603008270264, 0.586128830909729, 0.6099777817726135, 0.5983986854553223, 0.5803404450416565, 0.5800632834434509, 0.5963079929351807, 0.5870081782341003, 0.578181803226471, 0.5797854065895081, 0.5953226089477539, 0.5801612138748169, 0.5963833332061768, 0.6225050687789917, 0.581368088722229, 0.6256988048553467, 0.5945824384689331, 0.5922531485557556, 0.5923759341239929, 0.5899477005004883, 0.5904791951179504, 0.6021256446838379, 0.6124434471130371, 0.6082079410552979, 0.58894282579422, 0.601503849029541, 0.5971875190734863, 0.5982229709625244, 0.6000535488128662, 0.632444441318512, 0.6036986112594604, 0.6173047423362732, 0.6170973181724548, 0.6505870223045349, 0.6481852531433105, 0.6006770133972168, 0.6342522501945496, 0.6283024549484253, 0.6306381821632385, 0.603046178817749, 0.6093748807907104, 0.6232818365097046, 0.6289181113243103, 0.6125072240829468, 0.6313982605934143, 0.6167371869087219, 0.6194995045661926, 0.6181529760360718, 0.6074889898300171, 0.6403124332427979, 0.6167886257171631, 0.6322805285453796, 0.6162179708480835, 0.6332685947418213, 0.6200046539306641, 0.6439126133918762, 0.6135159730911255, 0.6274657845497131, 0.6265726685523987, 0.652780294418335, 0.6284487247467041, 0.6198965907096863, 0.6233065128326416, 0.6233794689178467, 0.6614894270896912, 0.6528313159942627, 0.6231476068496704, 0.6428387761116028, 0.6270064115524292], 'val_accuracy': [0.48553720116615295, 0.48657023906707764, 0.49793389439582825, 0.5247933864593506, 0.6136363744735718, 0.5630165338516235, 0.6012396812438965, 0.5960744023323059, 0.6363636255264282, 0.6456611752510071, 0.6694214940071106, 0.7086777091026306, 0.797520637512207, 0.7851239442825317, 0.8285123705863953, 0.797520637512207, 0.8202479481697083, 0.8760330677032471, 0.8811983466148376, 0.8646694421768188, 0.8791322112083435, 0.8915289044380188, 0.8832644820213318, 0.8915289044380188, 0.8956611752510071, 0.8832644820213318, 0.8884297609329224, 0.8966942429542542, 0.8925619721412659, 0.8956611752510071, 0.8832644820213318, 0.8904958963394165, 0.8904958963394165, 0.8770661354064941, 0.8904958963394165, 0.8884297609329224, 0.8884297609329224, 0.8904958963394165, 0.8894628286361694, 0.8863636255264282, 0.8863636255264282, 0.8894628286361694, 0.8956611752510071, 0.8884297609329224, 0.8780992031097412, 0.8894628286361694, 0.8729338645935059, 0.8863636255264282, 0.8894628286361694, 0.8966942429542542, 0.8966942429542542, 0.8863636255264282, 0.8832644820213318, 0.8822314143180847, 0.8863636255264282, 0.8904958963394165, 0.8904958963394165, 0.8904958963394165, 0.8884297609329224, 0.8977272510528564, 0.8873966932296753, 0.8884297609329224, 0.8832644820213318, 0.8853305578231812, 0.8739669322967529, 0.8729338645935059, 0.8894628286361694, 0.8863636255264282, 0.8801652789115906, 0.8780992031097412, 0.8770661354064941, 0.8801652789115906, 0.8873966932296753, 0.8822314143180847, 0.8853305578231812, 0.8935950398445129, 0.8915289044380188, 0.8873966932296753, 0.8853305578231812, 0.8853305578231812, 0.8894628286361694, 0.8791322112083435, 0.8925619721412659, 0.8863636255264282, 0.8935950398445129, 0.8904958963394165, 0.8822314143180847, 0.8811983466148376, 0.8873966932296753, 0.8935950398445129, 0.8760330677032471, 0.8873966932296753, 0.8832644820213318, 0.8884297609329224, 0.8894628286361694, 0.8842975497245789, 0.8873966932296753, 0.8822314143180847, 0.8925619721412659, 0.8853305578231812]}\n","32/32 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 3s 30ms/step - loss: 0.3699 - accuracy: 0.9655 - val_loss: 1.2162 - val_accuracy: 0.4849\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3207 - accuracy: 0.9846 - val_loss: 1.1483 - val_accuracy: 0.4849\n","Epoch 3/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.3170 - accuracy: 0.9863 - val_loss: 1.0885 - val_accuracy: 0.4892\n","Epoch 4/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.3191 - accuracy: 0.9830 - val_loss: 1.0283 - val_accuracy: 0.4968\n","Epoch 5/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.3111 - accuracy: 0.9887 - val_loss: 0.9726 - val_accuracy: 0.5097\n","Epoch 6/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.3109 - accuracy: 0.9884 - val_loss: 0.9523 - val_accuracy: 0.5162\n","Epoch 7/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.3099 - accuracy: 0.9884 - val_loss: 0.9144 - val_accuracy: 0.5334\n","Epoch 8/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.3041 - accuracy: 0.9914 - val_loss: 0.8953 - val_accuracy: 0.5517\n","Epoch 9/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3074 - accuracy: 0.9887 - val_loss: 0.9720 - val_accuracy: 0.5377\n","Epoch 10/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.3048 - accuracy: 0.9892 - val_loss: 0.9111 - val_accuracy: 0.5700\n","Epoch 11/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2992 - accuracy: 0.9930 - val_loss: 0.9831 - val_accuracy: 0.5614\n","Epoch 12/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.3021 - accuracy: 0.9908 - val_loss: 0.8562 - val_accuracy: 0.6412\n","Epoch 13/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3063 - accuracy: 0.9881 - val_loss: 0.9406 - val_accuracy: 0.6153\n","Epoch 14/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2958 - accuracy: 0.9933 - val_loss: 0.9864 - val_accuracy: 0.6067\n","Epoch 15/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.2990 - accuracy: 0.9916 - val_loss: 0.9506 - val_accuracy: 0.6444\n","Epoch 16/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2946 - accuracy: 0.9949 - val_loss: 0.7553 - val_accuracy: 0.7608\n","Epoch 17/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.2939 - accuracy: 0.9952 - val_loss: 0.6418 - val_accuracy: 0.8276\n","Epoch 18/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2991 - accuracy: 0.9919 - val_loss: 0.7005 - val_accuracy: 0.8071\n","Epoch 19/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.2931 - accuracy: 0.9922 - val_loss: 0.6570 - val_accuracy: 0.8341\n","Epoch 20/100\n","29/29 [==============================] - 1s 18ms/step - loss: 0.2993 - accuracy: 0.9922 - val_loss: 0.5495 - val_accuracy: 0.8804\n","Epoch 21/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.2986 - accuracy: 0.9908 - val_loss: 0.5388 - val_accuracy: 0.8901\n","Epoch 22/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.3244 - accuracy: 0.9776 - val_loss: 0.5451 - val_accuracy: 0.8922\n","Epoch 23/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.3023 - accuracy: 0.9900 - val_loss: 0.5153 - val_accuracy: 0.9073\n","Epoch 24/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2880 - accuracy: 0.9941 - val_loss: 0.4899 - val_accuracy: 0.9095\n","Epoch 25/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.2889 - accuracy: 0.9946 - val_loss: 0.4912 - val_accuracy: 0.9181\n","Epoch 26/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.2885 - accuracy: 0.9954 - val_loss: 0.4686 - val_accuracy: 0.9213\n","Epoch 27/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2857 - accuracy: 0.9965 - val_loss: 0.4700 - val_accuracy: 0.9192\n","Epoch 28/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2851 - accuracy: 0.9962 - val_loss: 0.4605 - val_accuracy: 0.9203\n","Epoch 29/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2842 - accuracy: 0.9954 - val_loss: 0.4586 - val_accuracy: 0.9267\n","Epoch 30/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2855 - accuracy: 0.9968 - val_loss: 0.4809 - val_accuracy: 0.9192\n","Epoch 31/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2848 - accuracy: 0.9962 - val_loss: 0.4723 - val_accuracy: 0.9203\n","Epoch 32/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2829 - accuracy: 0.9968 - val_loss: 0.4605 - val_accuracy: 0.9181\n","Epoch 33/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2842 - accuracy: 0.9962 - val_loss: 0.4665 - val_accuracy: 0.9192\n","Epoch 34/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2831 - accuracy: 0.9973 - val_loss: 0.4550 - val_accuracy: 0.9267\n","Epoch 35/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2809 - accuracy: 0.9978 - val_loss: 0.4657 - val_accuracy: 0.9224\n","Epoch 36/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2805 - accuracy: 0.9973 - val_loss: 0.4638 - val_accuracy: 0.9224\n","Epoch 37/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2796 - accuracy: 0.9973 - val_loss: 0.4553 - val_accuracy: 0.9256\n","Epoch 38/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2789 - accuracy: 0.9978 - val_loss: 0.4557 - val_accuracy: 0.9203\n","Epoch 39/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2795 - accuracy: 0.9978 - val_loss: 0.4611 - val_accuracy: 0.9256\n","Epoch 40/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2809 - accuracy: 0.9952 - val_loss: 0.4642 - val_accuracy: 0.9203\n","Epoch 41/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2819 - accuracy: 0.9962 - val_loss: 0.4914 - val_accuracy: 0.9170\n","Epoch 42/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2799 - accuracy: 0.9962 - val_loss: 0.4654 - val_accuracy: 0.9203\n","Epoch 43/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2766 - accuracy: 0.9981 - val_loss: 0.4609 - val_accuracy: 0.9224\n","Epoch 44/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2750 - accuracy: 0.9981 - val_loss: 0.4661 - val_accuracy: 0.9170\n","Epoch 45/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2759 - accuracy: 0.9978 - val_loss: 0.4746 - val_accuracy: 0.9203\n","Epoch 46/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2781 - accuracy: 0.9970 - val_loss: 0.4648 - val_accuracy: 0.9235\n","Epoch 47/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2755 - accuracy: 0.9978 - val_loss: 0.4706 - val_accuracy: 0.9170\n","Epoch 48/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.2759 - accuracy: 0.9968 - val_loss: 0.4748 - val_accuracy: 0.9181\n","Epoch 49/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2765 - accuracy: 0.9976 - val_loss: 0.4629 - val_accuracy: 0.9235\n","Epoch 50/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2733 - accuracy: 0.9984 - val_loss: 0.4666 - val_accuracy: 0.9159\n","Epoch 51/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2714 - accuracy: 0.9989 - val_loss: 0.4775 - val_accuracy: 0.9203\n","Epoch 52/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2727 - accuracy: 0.9981 - val_loss: 0.4644 - val_accuracy: 0.9235\n","Epoch 53/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2739 - accuracy: 0.9981 - val_loss: 0.4752 - val_accuracy: 0.9192\n","Epoch 54/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2708 - accuracy: 0.9989 - val_loss: 0.4699 - val_accuracy: 0.9256\n","Epoch 55/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2733 - accuracy: 0.9984 - val_loss: 0.4670 - val_accuracy: 0.9224\n","Epoch 56/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2696 - accuracy: 0.9987 - val_loss: 0.4702 - val_accuracy: 0.9213\n","Epoch 57/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2745 - accuracy: 0.9968 - val_loss: 0.4815 - val_accuracy: 0.9149\n","Epoch 58/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2704 - accuracy: 0.9987 - val_loss: 0.4666 - val_accuracy: 0.9159\n","Epoch 59/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2675 - accuracy: 0.9987 - val_loss: 0.4857 - val_accuracy: 0.9192\n","Epoch 60/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2686 - accuracy: 0.9992 - val_loss: 0.4676 - val_accuracy: 0.9170\n","Epoch 61/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2664 - accuracy: 0.9989 - val_loss: 0.4646 - val_accuracy: 0.9170\n","Epoch 62/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2680 - accuracy: 0.9987 - val_loss: 0.4681 - val_accuracy: 0.9138\n","Epoch 63/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2685 - accuracy: 0.9989 - val_loss: 0.4793 - val_accuracy: 0.9159\n","Epoch 64/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2682 - accuracy: 0.9989 - val_loss: 0.4782 - val_accuracy: 0.9159\n","Epoch 65/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2678 - accuracy: 0.9984 - val_loss: 0.4978 - val_accuracy: 0.9213\n","Epoch 66/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2698 - accuracy: 0.9976 - val_loss: 0.4857 - val_accuracy: 0.9192\n","Epoch 67/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2676 - accuracy: 0.9984 - val_loss: 0.4812 - val_accuracy: 0.9138\n","Epoch 68/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2648 - accuracy: 0.9984 - val_loss: 0.4683 - val_accuracy: 0.9138\n","Epoch 69/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2636 - accuracy: 0.9992 - val_loss: 0.4815 - val_accuracy: 0.9138\n","Epoch 70/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2628 - accuracy: 0.9995 - val_loss: 0.4783 - val_accuracy: 0.9235\n","Epoch 71/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2630 - accuracy: 0.9992 - val_loss: 0.4863 - val_accuracy: 0.9170\n","Epoch 72/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2847 - accuracy: 0.9879 - val_loss: 0.5557 - val_accuracy: 0.9019\n","Epoch 73/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2803 - accuracy: 0.9914 - val_loss: 0.5232 - val_accuracy: 0.9041\n","Epoch 74/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2676 - accuracy: 0.9973 - val_loss: 0.4788 - val_accuracy: 0.9203\n","Epoch 75/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2617 - accuracy: 0.9995 - val_loss: 0.4723 - val_accuracy: 0.9170\n","Epoch 76/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2611 - accuracy: 0.9989 - val_loss: 0.4805 - val_accuracy: 0.9181\n","Epoch 77/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2627 - accuracy: 0.9984 - val_loss: 0.4906 - val_accuracy: 0.9170\n","Epoch 78/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2609 - accuracy: 0.9995 - val_loss: 0.4811 - val_accuracy: 0.9203\n","Epoch 79/100\n","29/29 [==============================] - 1s 32ms/step - loss: 0.2601 - accuracy: 0.9987 - val_loss: 0.4806 - val_accuracy: 0.9278\n","Epoch 80/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2604 - accuracy: 0.9984 - val_loss: 0.4764 - val_accuracy: 0.9203\n","Epoch 81/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2600 - accuracy: 0.9989 - val_loss: 0.4775 - val_accuracy: 0.9224\n","Epoch 82/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2607 - accuracy: 0.9987 - val_loss: 0.4687 - val_accuracy: 0.9159\n","Epoch 83/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2577 - accuracy: 0.9989 - val_loss: 0.4724 - val_accuracy: 0.9170\n","Epoch 84/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2566 - accuracy: 0.9995 - val_loss: 0.4785 - val_accuracy: 0.9170\n","Epoch 85/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2569 - accuracy: 0.9995 - val_loss: 0.4885 - val_accuracy: 0.9192\n","Epoch 86/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2581 - accuracy: 0.9984 - val_loss: 0.4840 - val_accuracy: 0.9235\n","Epoch 87/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2570 - accuracy: 0.9992 - val_loss: 0.4783 - val_accuracy: 0.9192\n","Epoch 88/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2575 - accuracy: 0.9987 - val_loss: 0.4800 - val_accuracy: 0.9159\n","Epoch 89/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2559 - accuracy: 0.9995 - val_loss: 0.4754 - val_accuracy: 0.9138\n","Epoch 90/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2546 - accuracy: 0.9995 - val_loss: 0.4791 - val_accuracy: 0.9203\n","Epoch 91/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2543 - accuracy: 0.9995 - val_loss: 0.4761 - val_accuracy: 0.9170\n","Epoch 92/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2544 - accuracy: 0.9995 - val_loss: 0.4782 - val_accuracy: 0.9138\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2552 - accuracy: 0.9989 - val_loss: 0.4909 - val_accuracy: 0.9181\n","Epoch 94/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2575 - accuracy: 0.9984 - val_loss: 0.4803 - val_accuracy: 0.9181\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2571 - accuracy: 0.9984 - val_loss: 0.4875 - val_accuracy: 0.9224\n","Epoch 96/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2631 - accuracy: 0.9968 - val_loss: 0.4868 - val_accuracy: 0.9170\n","Epoch 97/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2533 - accuracy: 0.9989 - val_loss: 0.4823 - val_accuracy: 0.9170\n","Epoch 98/100\n","29/29 [==============================] - 0s 10ms/step - loss: 0.2545 - accuracy: 0.9989 - val_loss: 0.4808 - val_accuracy: 0.9170\n","Epoch 99/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2520 - accuracy: 0.9992 - val_loss: 0.4973 - val_accuracy: 0.9127\n","Epoch 100/100\n","29/29 [==============================] - 0s 9ms/step - loss: 0.2508 - accuracy: 0.9995 - val_loss: 0.4889 - val_accuracy: 0.9159\n","{'loss': [0.3698708713054657, 0.32073161005973816, 0.31695592403411865, 0.3191065192222595, 0.31110119819641113, 0.3108859658241272, 0.3098651170730591, 0.3040773272514343, 0.30736926198005676, 0.30483514070510864, 0.2991551160812378, 0.3021317422389984, 0.306304931640625, 0.2957860231399536, 0.2990332841873169, 0.29459628462791443, 0.2939242422580719, 0.2990551292896271, 0.29313063621520996, 0.299251914024353, 0.2985613942146301, 0.3243843615055084, 0.3022559881210327, 0.2879864275455475, 0.28894445300102234, 0.28853362798690796, 0.2856619954109192, 0.28512123227119446, 0.2841532826423645, 0.285473108291626, 0.28484252095222473, 0.28288012742996216, 0.2841798663139343, 0.28306642174720764, 0.2809247076511383, 0.28054964542388916, 0.27964693307876587, 0.2789168953895569, 0.27951979637145996, 0.28091761469841003, 0.28188738226890564, 0.27986276149749756, 0.2765916883945465, 0.27499210834503174, 0.27588504552841187, 0.2780984342098236, 0.2754864990711212, 0.2759173512458801, 0.27647650241851807, 0.2733224928379059, 0.2713816165924072, 0.2726670503616333, 0.2739080488681793, 0.2708306610584259, 0.27331840991973877, 0.2696422338485718, 0.27445265650749207, 0.2704448699951172, 0.26752060651779175, 0.2685512602329254, 0.26642361283302307, 0.2679876685142517, 0.2684772312641144, 0.26821109652519226, 0.2677709758281708, 0.26983940601348877, 0.2676309049129486, 0.26477330923080444, 0.26361119747161865, 0.26280921697616577, 0.2629531919956207, 0.28466424345970154, 0.2803294360637665, 0.26760363578796387, 0.26174524426460266, 0.2610865533351898, 0.26270127296447754, 0.2609264850616455, 0.26006338000297546, 0.2603721022605896, 0.2600252628326416, 0.2606613337993622, 0.2576735019683838, 0.2566382884979248, 0.2569383680820465, 0.25813016295433044, 0.25702518224716187, 0.25748616456985474, 0.25594446063041687, 0.25458988547325134, 0.2542824149131775, 0.2543736398220062, 0.25520333647727966, 0.25751370191574097, 0.25714612007141113, 0.2631363272666931, 0.2532566487789154, 0.25447991490364075, 0.25202274322509766, 0.2508228123188019], 'accuracy': [0.9655172228813171, 0.9846444129943848, 0.9862607717514038, 0.983027994632721, 0.9886853694915771, 0.9884159564971924, 0.9884159564971924, 0.9913793206214905, 0.9886853694915771, 0.9892241358757019, 0.9929956793785095, 0.990840494632721, 0.9881465435028076, 0.9932650923728943, 0.9916487336158752, 0.9948814511299133, 0.9951508641242981, 0.9919180870056152, 0.9921875, 0.9921875, 0.990840494632721, 0.9776400923728943, 0.9900323152542114, 0.9940732717514038, 0.9946120977401733, 0.9954202771186829, 0.9964978694915771, 0.9962284564971924, 0.9954202771186829, 0.9967672228813171, 0.9962284564971924, 0.9967672228813171, 0.9962284564971924, 0.9973060488700867, 0.9978448152542114, 0.9973060488700867, 0.9973060488700867, 0.9978448152542114, 0.9978448152542114, 0.9951508641242981, 0.9962284564971924, 0.9962284564971924, 0.9981142282485962, 0.9981142282485962, 0.9978448152542114, 0.9970366358757019, 0.9978448152542114, 0.9967672228813171, 0.9975754022598267, 0.998383641242981, 0.9989224076271057, 0.9981142282485962, 0.9981142282485962, 0.9989224076271057, 0.998383641242981, 0.998652994632721, 0.9967672228813171, 0.998652994632721, 0.998652994632721, 0.9991918206214905, 0.9989224076271057, 0.998652994632721, 0.9989224076271057, 0.9989224076271057, 0.998383641242981, 0.9975754022598267, 0.998383641242981, 0.998383641242981, 0.9991918206214905, 0.9994612336158752, 0.9991918206214905, 0.9878771305084229, 0.9913793206214905, 0.9973060488700867, 0.9994612336158752, 0.9989224076271057, 0.998383641242981, 0.9994612336158752, 0.998652994632721, 0.998383641242981, 0.9989224076271057, 0.998652994632721, 0.9989224076271057, 0.9994612336158752, 0.9994612336158752, 0.998383641242981, 0.9991918206214905, 0.998652994632721, 0.9994612336158752, 0.9994612336158752, 0.9994612336158752, 0.9994612336158752, 0.9989224076271057, 0.998383641242981, 0.998383641242981, 0.9967672228813171, 0.9989224076271057, 0.9989224076271057, 0.9991918206214905, 0.9994612336158752], 'val_loss': [1.2162256240844727, 1.1483089923858643, 1.0885260105133057, 1.0283383131027222, 0.972602128982544, 0.9523336291313171, 0.9144046902656555, 0.8952763080596924, 0.9719938635826111, 0.9111065864562988, 0.9831087589263916, 0.8562467098236084, 0.9406495094299316, 0.9863882660865784, 0.9505934119224548, 0.7552943229675293, 0.6418076157569885, 0.7005229592323303, 0.6570357084274292, 0.5494687557220459, 0.5388448238372803, 0.5450963377952576, 0.5152687430381775, 0.48988524079322815, 0.49115702509880066, 0.4685535728931427, 0.4700469374656677, 0.4605400264263153, 0.45859578251838684, 0.480933278799057, 0.4722592532634735, 0.4605374038219452, 0.46646690368652344, 0.4549907445907593, 0.46573346853256226, 0.4637984037399292, 0.4552678167819977, 0.4557412266731262, 0.4610694348812103, 0.4641757309436798, 0.4914492666721344, 0.46540889143943787, 0.4608566462993622, 0.46611011028289795, 0.47459256649017334, 0.46479663252830505, 0.4705783724784851, 0.47477519512176514, 0.462891161441803, 0.4665912091732025, 0.4775318503379822, 0.4643544554710388, 0.4751830995082855, 0.469871461391449, 0.46702840924263, 0.4702119827270508, 0.48152124881744385, 0.46660158038139343, 0.48571860790252686, 0.4676307141780853, 0.46458786725997925, 0.4680854082107544, 0.4792737364768982, 0.4781882166862488, 0.49781596660614014, 0.48572883009910583, 0.481153666973114, 0.46829503774642944, 0.481547087430954, 0.4782642126083374, 0.48630091547966003, 0.5557174682617188, 0.5231923460960388, 0.47881606221199036, 0.47226619720458984, 0.480506032705307, 0.4905988574028015, 0.4811016321182251, 0.4805752635002136, 0.4763806462287903, 0.4775139093399048, 0.4687250256538391, 0.47235390543937683, 0.4785228967666626, 0.4884692132472992, 0.48399946093559265, 0.4782554805278778, 0.4799952805042267, 0.47540056705474854, 0.47905832529067993, 0.47606584429740906, 0.47815853357315063, 0.49090680480003357, 0.4803329110145569, 0.487451434135437, 0.4867686629295349, 0.4822934567928314, 0.4807928800582886, 0.49725571274757385, 0.48890313506126404], 'val_accuracy': [0.48491379618644714, 0.48491379618644714, 0.4892241358757019, 0.4967672526836395, 0.5096982717514038, 0.5161637663841248, 0.5334051847457886, 0.5517241358757019, 0.537715494632721, 0.5700430870056152, 0.5614224076271057, 0.6411637663841248, 0.6153017282485962, 0.6066810488700867, 0.6443965435028076, 0.7607758641242981, 0.8275862336158752, 0.8071120977401733, 0.8340517282485962, 0.8803879022598267, 0.8900862336158752, 0.892241358757019, 0.9073275923728943, 0.9094827771186829, 0.9181034564971924, 0.9213362336158752, 0.9191810488700867, 0.920258641242981, 0.9267241358757019, 0.9191810488700867, 0.920258641242981, 0.9181034564971924, 0.9191810488700867, 0.9267241358757019, 0.9224137663841248, 0.9224137663841248, 0.9256465435028076, 0.920258641242981, 0.9256465435028076, 0.920258641242981, 0.9170258641242981, 0.920258641242981, 0.9224137663841248, 0.9170258641242981, 0.920258641242981, 0.923491358757019, 0.9170258641242981, 0.9181034564971924, 0.923491358757019, 0.9159482717514038, 0.920258641242981, 0.923491358757019, 0.9191810488700867, 0.9256465435028076, 0.9224137663841248, 0.9213362336158752, 0.9148706793785095, 0.9159482717514038, 0.9191810488700867, 0.9170258641242981, 0.9170258641242981, 0.9137930870056152, 0.9159482717514038, 0.9159482717514038, 0.9213362336158752, 0.9191810488700867, 0.9137930870056152, 0.9137930870056152, 0.9137930870056152, 0.923491358757019, 0.9170258641242981, 0.9019396305084229, 0.9040948152542114, 0.920258641242981, 0.9170258641242981, 0.9181034564971924, 0.9170258641242981, 0.920258641242981, 0.9278017282485962, 0.920258641242981, 0.9224137663841248, 0.9159482717514038, 0.9170258641242981, 0.9170258641242981, 0.9191810488700867, 0.923491358757019, 0.9191810488700867, 0.9159482717514038, 0.9137930870056152, 0.920258641242981, 0.9170258641242981, 0.9137930870056152, 0.9181034564971924, 0.9181034564971924, 0.9224137663841248, 0.9170258641242981, 0.9170258641242981, 0.9170258641242981, 0.912715494632721, 0.9159482717514038]}\n","38/38 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - 4s 31ms/step - loss: 0.3773 - accuracy: 0.9556 - val_loss: 1.1921 - val_accuracy: 0.4955\n","Epoch 2/100\n"," 1/28 [>.............................] - ETA: 0s - loss: 0.3386 - accuracy: 0.9766"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["28/28 [==============================] - 0s 12ms/step - loss: 0.3282 - accuracy: 0.9799 - val_loss: 1.1366 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3207 - accuracy: 0.9819 - val_loss: 1.0842 - val_accuracy: 0.5000\n","Epoch 4/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.3166 - accuracy: 0.9833 - val_loss: 1.0379 - val_accuracy: 0.5090\n","Epoch 5/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3166 - accuracy: 0.9847 - val_loss: 0.9623 - val_accuracy: 0.5294\n","Epoch 6/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3105 - accuracy: 0.9884 - val_loss: 0.9304 - val_accuracy: 0.5430\n","Epoch 7/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3146 - accuracy: 0.9850 - val_loss: 0.9281 - val_accuracy: 0.5498\n","Epoch 8/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3073 - accuracy: 0.9890 - val_loss: 0.9443 - val_accuracy: 0.5532\n","Epoch 9/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.3068 - accuracy: 0.9912 - val_loss: 0.9167 - val_accuracy: 0.5713\n","Epoch 10/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3047 - accuracy: 0.9892 - val_loss: 0.9615 - val_accuracy: 0.5713\n","Epoch 11/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.3077 - accuracy: 0.9881 - val_loss: 0.9201 - val_accuracy: 0.5882\n","Epoch 12/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3073 - accuracy: 0.9890 - val_loss: 0.9687 - val_accuracy: 0.5871\n","Epoch 13/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3458 - accuracy: 0.9643 - val_loss: 0.8302 - val_accuracy: 0.6595\n","Epoch 14/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.3046 - accuracy: 0.9898 - val_loss: 0.8447 - val_accuracy: 0.6606\n","Epoch 15/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.3012 - accuracy: 0.9909 - val_loss: 0.8411 - val_accuracy: 0.6742\n","Epoch 16/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3054 - accuracy: 0.9884 - val_loss: 0.9695 - val_accuracy: 0.6448\n","Epoch 17/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3134 - accuracy: 0.9856 - val_loss: 0.7008 - val_accuracy: 0.7952\n","Epoch 18/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3010 - accuracy: 0.9921 - val_loss: 0.7774 - val_accuracy: 0.7489\n","Epoch 19/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.2962 - accuracy: 0.9938 - val_loss: 0.6349 - val_accuracy: 0.8371\n","Epoch 20/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2963 - accuracy: 0.9943 - val_loss: 0.5715 - val_accuracy: 0.8756\n","Epoch 21/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2950 - accuracy: 0.9938 - val_loss: 0.5647 - val_accuracy: 0.8857\n","Epoch 22/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.3018 - accuracy: 0.9887 - val_loss: 0.5865 - val_accuracy: 0.8710\n","Epoch 23/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.3018 - accuracy: 0.9909 - val_loss: 0.5631 - val_accuracy: 0.8778\n","Epoch 24/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2979 - accuracy: 0.9921 - val_loss: 0.5378 - val_accuracy: 0.9016\n","Epoch 25/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2895 - accuracy: 0.9966 - val_loss: 0.5249 - val_accuracy: 0.9016\n","Epoch 26/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2876 - accuracy: 0.9972 - val_loss: 0.5213 - val_accuracy: 0.9038\n","Epoch 27/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2907 - accuracy: 0.9960 - val_loss: 0.5146 - val_accuracy: 0.9163\n","Epoch 28/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2871 - accuracy: 0.9972 - val_loss: 0.5076 - val_accuracy: 0.9140\n","Epoch 29/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2858 - accuracy: 0.9975 - val_loss: 0.5109 - val_accuracy: 0.9186\n","Epoch 30/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2896 - accuracy: 0.9955 - val_loss: 0.5158 - val_accuracy: 0.9152\n","Epoch 31/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2881 - accuracy: 0.9955 - val_loss: 0.5119 - val_accuracy: 0.9208\n","Epoch 32/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2855 - accuracy: 0.9966 - val_loss: 0.5121 - val_accuracy: 0.9152\n","Epoch 33/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2853 - accuracy: 0.9969 - val_loss: 0.5250 - val_accuracy: 0.9050\n","Epoch 34/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2864 - accuracy: 0.9972 - val_loss: 0.5209 - val_accuracy: 0.9084\n","Epoch 35/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2853 - accuracy: 0.9960 - val_loss: 0.5146 - val_accuracy: 0.9129\n","Epoch 36/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2844 - accuracy: 0.9966 - val_loss: 0.5308 - val_accuracy: 0.9027\n","Epoch 37/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2828 - accuracy: 0.9980 - val_loss: 0.5211 - val_accuracy: 0.9140\n","Epoch 38/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2840 - accuracy: 0.9969 - val_loss: 0.5398 - val_accuracy: 0.9061\n","Epoch 39/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2821 - accuracy: 0.9980 - val_loss: 0.5405 - val_accuracy: 0.9005\n","Epoch 40/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2813 - accuracy: 0.9975 - val_loss: 0.5316 - val_accuracy: 0.9152\n","Epoch 41/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2843 - accuracy: 0.9969 - val_loss: 0.5311 - val_accuracy: 0.9095\n","Epoch 42/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2808 - accuracy: 0.9980 - val_loss: 0.5210 - val_accuracy: 0.9106\n","Epoch 43/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2808 - accuracy: 0.9975 - val_loss: 0.5441 - val_accuracy: 0.9050\n","Epoch 44/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2818 - accuracy: 0.9969 - val_loss: 0.5430 - val_accuracy: 0.9027\n","Epoch 45/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2799 - accuracy: 0.9975 - val_loss: 0.5295 - val_accuracy: 0.9140\n","Epoch 46/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2768 - accuracy: 0.9989 - val_loss: 0.5239 - val_accuracy: 0.9163\n","Epoch 47/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2779 - accuracy: 0.9980 - val_loss: 0.5271 - val_accuracy: 0.9140\n","Epoch 48/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2787 - accuracy: 0.9980 - val_loss: 0.5335 - val_accuracy: 0.9152\n","Epoch 49/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2797 - accuracy: 0.9966 - val_loss: 0.5232 - val_accuracy: 0.9095\n","Epoch 50/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2760 - accuracy: 0.9983 - val_loss: 0.5294 - val_accuracy: 0.9140\n","Epoch 51/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2750 - accuracy: 0.9989 - val_loss: 0.5246 - val_accuracy: 0.9095\n","Epoch 52/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2756 - accuracy: 0.9972 - val_loss: 0.5290 - val_accuracy: 0.9129\n","Epoch 53/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2743 - accuracy: 0.9986 - val_loss: 0.5429 - val_accuracy: 0.9061\n","Epoch 54/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2776 - accuracy: 0.9969 - val_loss: 0.5551 - val_accuracy: 0.8959\n","Epoch 55/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2882 - accuracy: 0.9929 - val_loss: 0.5341 - val_accuracy: 0.9061\n","Epoch 56/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2763 - accuracy: 0.9972 - val_loss: 0.5279 - val_accuracy: 0.9129\n","Epoch 57/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2714 - accuracy: 0.9992 - val_loss: 0.5308 - val_accuracy: 0.9106\n","Epoch 58/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2718 - accuracy: 0.9992 - val_loss: 0.5400 - val_accuracy: 0.9050\n","Epoch 59/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2723 - accuracy: 0.9986 - val_loss: 0.5924 - val_accuracy: 0.8925\n","Epoch 60/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2716 - accuracy: 0.9992 - val_loss: 0.5371 - val_accuracy: 0.9072\n","Epoch 61/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2803 - accuracy: 0.9949 - val_loss: 0.5458 - val_accuracy: 0.9027\n","Epoch 62/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2846 - accuracy: 0.9924 - val_loss: 0.5517 - val_accuracy: 0.9129\n","Epoch 63/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2900 - accuracy: 0.9881 - val_loss: 0.5671 - val_accuracy: 0.8959\n","Epoch 64/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2752 - accuracy: 0.9975 - val_loss: 0.5491 - val_accuracy: 0.8982\n","Epoch 65/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2720 - accuracy: 0.9969 - val_loss: 0.5644 - val_accuracy: 0.8971\n","Epoch 66/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2704 - accuracy: 0.9989 - val_loss: 0.5449 - val_accuracy: 0.9061\n","Epoch 67/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2687 - accuracy: 0.9992 - val_loss: 0.5669 - val_accuracy: 0.9050\n","Epoch 68/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2699 - accuracy: 0.9980 - val_loss: 0.5399 - val_accuracy: 0.9084\n","Epoch 69/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2675 - accuracy: 0.9992 - val_loss: 0.5796 - val_accuracy: 0.8971\n","Epoch 70/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2691 - accuracy: 0.9986 - val_loss: 0.5403 - val_accuracy: 0.9118\n","Epoch 71/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2663 - accuracy: 0.9992 - val_loss: 0.5467 - val_accuracy: 0.9095\n","Epoch 72/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2656 - accuracy: 0.9992 - val_loss: 0.5717 - val_accuracy: 0.8982\n","Epoch 73/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2656 - accuracy: 0.9992 - val_loss: 0.5478 - val_accuracy: 0.9129\n","Epoch 74/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2647 - accuracy: 0.9989 - val_loss: 0.5628 - val_accuracy: 0.8959\n","Epoch 75/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2671 - accuracy: 0.9989 - val_loss: 0.5495 - val_accuracy: 0.9027\n","Epoch 76/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2672 - accuracy: 0.9980 - val_loss: 0.5564 - val_accuracy: 0.9061\n","Epoch 77/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2673 - accuracy: 0.9986 - val_loss: 0.5787 - val_accuracy: 0.8959\n","Epoch 78/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2642 - accuracy: 0.9989 - val_loss: 0.5492 - val_accuracy: 0.9072\n","Epoch 79/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2640 - accuracy: 0.9983 - val_loss: 0.5438 - val_accuracy: 0.9072\n","Epoch 80/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2633 - accuracy: 0.9992 - val_loss: 0.5439 - val_accuracy: 0.9129\n","Epoch 81/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2633 - accuracy: 0.9989 - val_loss: 0.5688 - val_accuracy: 0.8903\n","Epoch 82/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2620 - accuracy: 0.9992 - val_loss: 0.5446 - val_accuracy: 0.9095\n","Epoch 83/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2627 - accuracy: 0.9986 - val_loss: 0.5544 - val_accuracy: 0.9129\n","Epoch 84/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2621 - accuracy: 0.9994 - val_loss: 0.5461 - val_accuracy: 0.9129\n","Epoch 85/100\n","28/28 [==============================] - 0s 11ms/step - loss: 0.2603 - accuracy: 0.9994 - val_loss: 0.5499 - val_accuracy: 0.9095\n","Epoch 86/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2593 - accuracy: 0.9992 - val_loss: 0.5490 - val_accuracy: 0.9140\n","Epoch 87/100\n","28/28 [==============================] - 0s 10ms/step - loss: 0.2600 - accuracy: 0.9994 - val_loss: 0.5623 - val_accuracy: 0.9016\n","Epoch 88/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2644 - accuracy: 0.9983 - val_loss: 0.5514 - val_accuracy: 0.9152\n","Epoch 89/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2625 - accuracy: 0.9986 - val_loss: 0.5495 - val_accuracy: 0.9118\n","Epoch 90/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2639 - accuracy: 0.9989 - val_loss: 0.5806 - val_accuracy: 0.8959\n","Epoch 91/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2599 - accuracy: 0.9992 - val_loss: 0.5486 - val_accuracy: 0.9072\n","Epoch 92/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2584 - accuracy: 0.9994 - val_loss: 0.5525 - val_accuracy: 0.9106\n","Epoch 93/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2583 - accuracy: 0.9994 - val_loss: 0.5562 - val_accuracy: 0.9095\n","Epoch 94/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2571 - accuracy: 0.9992 - val_loss: 0.5496 - val_accuracy: 0.9084\n","Epoch 95/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2565 - accuracy: 0.9994 - val_loss: 0.5555 - val_accuracy: 0.9118\n","Epoch 96/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2569 - accuracy: 0.9994 - val_loss: 0.5585 - val_accuracy: 0.9061\n","Epoch 97/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2559 - accuracy: 0.9994 - val_loss: 0.5522 - val_accuracy: 0.9084\n","Epoch 98/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2556 - accuracy: 0.9994 - val_loss: 0.5755 - val_accuracy: 0.8925\n","Epoch 99/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2558 - accuracy: 0.9994 - val_loss: 0.5532 - val_accuracy: 0.9118\n","Epoch 100/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.2550 - accuracy: 0.9994 - val_loss: 0.5548 - val_accuracy: 0.9061\n","{'loss': [0.37730732560157776, 0.3282392919063568, 0.3206583261489868, 0.316615492105484, 0.31663650274276733, 0.31047213077545166, 0.3146275579929352, 0.3073183000087738, 0.3068463206291199, 0.30472537875175476, 0.30774548649787903, 0.3072890639305115, 0.34580370783805847, 0.3045976161956787, 0.30122286081314087, 0.30535992980003357, 0.31344762444496155, 0.3009863495826721, 0.2962287664413452, 0.2962917387485504, 0.2950313985347748, 0.3018186092376709, 0.3017829954624176, 0.2978755235671997, 0.28951337933540344, 0.2876477837562561, 0.290722519159317, 0.28706473112106323, 0.28580543398857117, 0.289568692445755, 0.28813955187797546, 0.2855115532875061, 0.28525403141975403, 0.286418080329895, 0.28531745076179504, 0.28444772958755493, 0.28280723094940186, 0.28396522998809814, 0.2821374833583832, 0.2813412845134735, 0.2843259274959564, 0.28081434965133667, 0.28081777691841125, 0.28180181980133057, 0.27987855672836304, 0.27684980630874634, 0.2778756320476532, 0.27874141931533813, 0.2796722650527954, 0.276014506816864, 0.27498024702072144, 0.27558961510658264, 0.27428606152534485, 0.2775574028491974, 0.28819021582603455, 0.27634626626968384, 0.2713768482208252, 0.2717621922492981, 0.27232351899147034, 0.2715718150138855, 0.2803383767604828, 0.2846126854419708, 0.289958655834198, 0.2751598358154297, 0.272037148475647, 0.2704164385795593, 0.26870980858802795, 0.26988402009010315, 0.26750707626342773, 0.2691381275653839, 0.2662857174873352, 0.2655992805957794, 0.26564547419548035, 0.26469311118125916, 0.2670593559741974, 0.26721668243408203, 0.2673362195491791, 0.2641747295856476, 0.2639501392841339, 0.2632613778114319, 0.26327964663505554, 0.2620396912097931, 0.26273784041404724, 0.2621152698993683, 0.2602692246437073, 0.25934794545173645, 0.2600418031215668, 0.2643640339374542, 0.26245084404945374, 0.2638568878173828, 0.2599050998687744, 0.2583521604537964, 0.2583233118057251, 0.25709572434425354, 0.25648197531700134, 0.25690290331840515, 0.2559107840061188, 0.25564903020858765, 0.2558366358280182, 0.2549635171890259], 'accuracy': [0.9555743932723999, 0.9799094796180725, 0.9818902015686035, 0.983305037021637, 0.9847198724746704, 0.9883984327316284, 0.9850028157234192, 0.988964319229126, 0.9912280440330505, 0.9892473220825195, 0.9881154298782349, 0.988964319229126, 0.9643463492393494, 0.9898132681846619, 0.9909451007843018, 0.9883984327316284, 0.9855687618255615, 0.9920769929885864, 0.9937747716903687, 0.994340717792511, 0.9937747716903687, 0.9886813759803772, 0.9909451007843018, 0.9920769929885864, 0.9966044425964355, 0.9971703290939331, 0.9960384964942932, 0.9971703290939331, 0.9974533319473267, 0.9954725503921509, 0.9954725503921509, 0.9966044425964355, 0.9968873858451843, 0.9971703290939331, 0.9960384964942932, 0.9966044425964355, 0.9980192184448242, 0.9968873858451843, 0.9980192184448242, 0.9974533319473267, 0.9968873858451843, 0.9980192184448242, 0.9974533319473267, 0.9968873858451843, 0.9974533319473267, 0.9988681674003601, 0.9980192184448242, 0.9980192184448242, 0.9966044425964355, 0.9983022212982178, 0.9988681674003601, 0.9971703290939331, 0.9985851645469666, 0.9968873858451843, 0.9929258823394775, 0.9971703290939331, 0.9991511106491089, 0.9991511106491089, 0.9985851645469666, 0.9991511106491089, 0.9949066042900085, 0.9923599362373352, 0.9881154298782349, 0.9974533319473267, 0.9968873858451843, 0.9988681674003601, 0.9991511106491089, 0.9980192184448242, 0.9991511106491089, 0.9985851645469666, 0.9991511106491089, 0.9991511106491089, 0.9991511106491089, 0.9988681674003601, 0.9988681674003601, 0.9980192184448242, 0.9985851645469666, 0.9988681674003601, 0.9983022212982178, 0.9991511106491089, 0.9988681674003601, 0.9991511106491089, 0.9985851645469666, 0.9994340538978577, 0.9994340538978577, 0.9991511106491089, 0.9994340538978577, 0.9983022212982178, 0.9985851645469666, 0.9988681674003601, 0.9991511106491089, 0.9994340538978577, 0.9994340538978577, 0.9991511106491089, 0.9994340538978577, 0.9994340538978577, 0.9994340538978577, 0.9994340538978577, 0.9994340538978577, 0.9994340538978577], 'val_loss': [1.1921370029449463, 1.1365646123886108, 1.0842090845108032, 1.037937045097351, 0.962334394454956, 0.9303578734397888, 0.928077220916748, 0.9442506432533264, 0.9167161583900452, 0.9615024924278259, 0.9200832843780518, 0.9687306880950928, 0.8301562666893005, 0.8447051644325256, 0.8410961031913757, 0.9694722890853882, 0.7008039951324463, 0.7774267196655273, 0.6349205374717712, 0.5714631080627441, 0.5646764039993286, 0.5865363478660583, 0.5631150007247925, 0.5378258228302002, 0.5249439477920532, 0.5212541818618774, 0.5146033763885498, 0.5076338648796082, 0.5108706951141357, 0.5157750844955444, 0.5119144320487976, 0.5121408104896545, 0.5249930620193481, 0.5208818316459656, 0.514596700668335, 0.5308448672294617, 0.5210989713668823, 0.5398052930831909, 0.5405042767524719, 0.5315917134284973, 0.5310647487640381, 0.5209959745407104, 0.5440545082092285, 0.5430277585983276, 0.5294769406318665, 0.5238631963729858, 0.5271224975585938, 0.5334749817848206, 0.5231737494468689, 0.5294439792633057, 0.5245640277862549, 0.528992235660553, 0.5429171323776245, 0.5551146268844604, 0.5341171622276306, 0.527908205986023, 0.5307536721229553, 0.5399828553199768, 0.5923793911933899, 0.5371116399765015, 0.5457895994186401, 0.5516830682754517, 0.5671057105064392, 0.549055814743042, 0.564446747303009, 0.5449211001396179, 0.5669068098068237, 0.5398991703987122, 0.5796011686325073, 0.5403019189834595, 0.5466684699058533, 0.5716935992240906, 0.547822117805481, 0.5627508163452148, 0.5494827628135681, 0.5563616752624512, 0.5786826610565186, 0.5491675734519958, 0.5438359975814819, 0.5439293384552002, 0.5688184499740601, 0.5446118712425232, 0.5543850064277649, 0.5461233854293823, 0.5498623847961426, 0.5490317940711975, 0.5622687935829163, 0.5514187812805176, 0.5495288968086243, 0.5806199312210083, 0.5485535860061646, 0.5525003671646118, 0.5561715960502625, 0.5495734810829163, 0.5554950833320618, 0.5584861636161804, 0.5522379875183105, 0.5754672288894653, 0.5531834959983826, 0.5547508001327515], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.5, 0.5090497732162476, 0.529411792755127, 0.5429864525794983, 0.5497737526893616, 0.5531674027442932, 0.5712669491767883, 0.5712669491767883, 0.5882353186607361, 0.587104082107544, 0.6595022678375244, 0.6606335043907166, 0.6742081642150879, 0.6447963714599609, 0.7952488660812378, 0.7488687634468079, 0.837104082107544, 0.8755655884742737, 0.8857465982437134, 0.8710407018661499, 0.877828061580658, 0.901583731174469, 0.901583731174469, 0.9038461446762085, 0.9162895679473877, 0.9140271544456482, 0.918552041053772, 0.9151583909988403, 0.9208144545555115, 0.9151583909988403, 0.9049773812294006, 0.9083710312843323, 0.912895917892456, 0.9027149081230164, 0.9140271544456482, 0.9061086177825928, 0.9004524946212769, 0.9151583909988403, 0.9095022678375244, 0.9106335043907166, 0.9049773812294006, 0.9027149081230164, 0.9140271544456482, 0.9162895679473877, 0.9140271544456482, 0.9151583909988403, 0.9095022678375244, 0.9140271544456482, 0.9095022678375244, 0.912895917892456, 0.9061086177825928, 0.8959276080131531, 0.9061086177825928, 0.912895917892456, 0.9106335043907166, 0.9049773812294006, 0.8925339579582214, 0.9072397947311401, 0.9027149081230164, 0.912895917892456, 0.8959276080131531, 0.8981900215148926, 0.8970588445663452, 0.9061086177825928, 0.9049773812294006, 0.9083710312843323, 0.8970588445663452, 0.9117646813392639, 0.9095022678375244, 0.8981900215148926, 0.912895917892456, 0.8959276080131531, 0.9027149081230164, 0.9061086177825928, 0.8959276080131531, 0.9072397947311401, 0.9072397947311401, 0.912895917892456, 0.8902714848518372, 0.9095022678375244, 0.912895917892456, 0.912895917892456, 0.9095022678375244, 0.9140271544456482, 0.901583731174469, 0.9151583909988403, 0.9117646813392639, 0.8959276080131531, 0.9072397947311401, 0.9106335043907166, 0.9095022678375244, 0.9083710312843323, 0.9117646813392639, 0.9061086177825928, 0.9083710312843323, 0.8925339579582214, 0.9117646813392639, 0.9061086177825928]}\n","45/45 [==============================] - 0s 2ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," flatten (Flatten)           (None, 1024)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               524800    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2852417 (10.88 MB)\n","Trainable params: 2851905 (10.88 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 3s 27ms/step - loss: 0.3636 - accuracy: 0.9641 - val_loss: 1.2186 - val_accuracy: 0.4855\n","Epoch 2/100\n"," 1/31 [..............................] - ETA: 0s - loss: 0.3003 - accuracy: 1.0000"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["31/31 [==============================] - 1s 18ms/step - loss: 0.3150 - accuracy: 0.9845 - val_loss: 1.1578 - val_accuracy: 0.4866\n","Epoch 3/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.3122 - accuracy: 0.9871 - val_loss: 1.0600 - val_accuracy: 0.4979\n","Epoch 4/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3063 - accuracy: 0.9915 - val_loss: 0.9724 - val_accuracy: 0.5155\n","Epoch 5/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.3062 - accuracy: 0.9891 - val_loss: 0.9275 - val_accuracy: 0.5320\n","Epoch 6/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3046 - accuracy: 0.9886 - val_loss: 0.9513 - val_accuracy: 0.5310\n","Epoch 7/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.3188 - accuracy: 0.9801 - val_loss: 0.9073 - val_accuracy: 0.5579\n","Epoch 8/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3038 - accuracy: 0.9907 - val_loss: 0.9335 - val_accuracy: 0.5496\n","Epoch 9/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.3045 - accuracy: 0.9910 - val_loss: 0.8872 - val_accuracy: 0.5806\n","Epoch 10/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3027 - accuracy: 0.9899 - val_loss: 0.8283 - val_accuracy: 0.6498\n","Epoch 11/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3037 - accuracy: 0.9897 - val_loss: 1.0601 - val_accuracy: 0.5548\n","Epoch 12/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3006 - accuracy: 0.9904 - val_loss: 0.9486 - val_accuracy: 0.6002\n","Epoch 13/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2995 - accuracy: 0.9897 - val_loss: 1.1475 - val_accuracy: 0.5651\n","Epoch 14/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.3120 - accuracy: 0.9860 - val_loss: 0.8418 - val_accuracy: 0.7138\n","Epoch 15/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2963 - accuracy: 0.9933 - val_loss: 0.6742 - val_accuracy: 0.8048\n","Epoch 16/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3104 - accuracy: 0.9858 - val_loss: 0.5908 - val_accuracy: 0.8678\n","Epoch 17/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.2979 - accuracy: 0.9915 - val_loss: 0.6002 - val_accuracy: 0.8698\n","Epoch 18/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.2920 - accuracy: 0.9943 - val_loss: 0.5743 - val_accuracy: 0.8812\n","Epoch 19/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2946 - accuracy: 0.9922 - val_loss: 0.6196 - val_accuracy: 0.8564\n","Epoch 20/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2956 - accuracy: 0.9928 - val_loss: 0.5558 - val_accuracy: 0.8936\n","Epoch 21/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2920 - accuracy: 0.9948 - val_loss: 0.6018 - val_accuracy: 0.8802\n","Epoch 22/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2978 - accuracy: 0.9897 - val_loss: 0.5466 - val_accuracy: 0.8998\n","Epoch 23/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2938 - accuracy: 0.9928 - val_loss: 0.5674 - val_accuracy: 0.8926\n","Epoch 24/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.3011 - accuracy: 0.9889 - val_loss: 0.5740 - val_accuracy: 0.8957\n","Epoch 25/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3018 - accuracy: 0.9881 - val_loss: 0.5622 - val_accuracy: 0.8998\n","Epoch 26/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2867 - accuracy: 0.9959 - val_loss: 0.5324 - val_accuracy: 0.9091\n","Epoch 27/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2852 - accuracy: 0.9966 - val_loss: 0.5298 - val_accuracy: 0.9101\n","Epoch 28/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2841 - accuracy: 0.9972 - val_loss: 0.5265 - val_accuracy: 0.9081\n","Epoch 29/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2864 - accuracy: 0.9953 - val_loss: 0.5352 - val_accuracy: 0.9101\n","Epoch 30/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2829 - accuracy: 0.9969 - val_loss: 0.5486 - val_accuracy: 0.9101\n","Epoch 31/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2815 - accuracy: 0.9969 - val_loss: 0.5463 - val_accuracy: 0.9081\n","Epoch 32/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2865 - accuracy: 0.9953 - val_loss: 0.5518 - val_accuracy: 0.9039\n","Epoch 33/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2812 - accuracy: 0.9974 - val_loss: 0.5654 - val_accuracy: 0.8998\n","Epoch 34/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2839 - accuracy: 0.9964 - val_loss: 0.5430 - val_accuracy: 0.9019\n","Epoch 35/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2834 - accuracy: 0.9956 - val_loss: 0.5852 - val_accuracy: 0.8946\n","Epoch 36/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2837 - accuracy: 0.9951 - val_loss: 0.5777 - val_accuracy: 0.8884\n","Epoch 37/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2943 - accuracy: 0.9904 - val_loss: 0.5431 - val_accuracy: 0.9070\n","Epoch 38/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2792 - accuracy: 0.9972 - val_loss: 0.5448 - val_accuracy: 0.9060\n","Epoch 39/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2776 - accuracy: 0.9969 - val_loss: 0.5499 - val_accuracy: 0.9060\n","Epoch 40/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2796 - accuracy: 0.9961 - val_loss: 0.5508 - val_accuracy: 0.9008\n","Epoch 41/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2810 - accuracy: 0.9974 - val_loss: 0.5600 - val_accuracy: 0.9008\n","Epoch 42/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2774 - accuracy: 0.9972 - val_loss: 0.5484 - val_accuracy: 0.9029\n","Epoch 43/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2757 - accuracy: 0.9977 - val_loss: 0.5522 - val_accuracy: 0.9019\n","Epoch 44/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2755 - accuracy: 0.9977 - val_loss: 0.5621 - val_accuracy: 0.8988\n","Epoch 45/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2756 - accuracy: 0.9979 - val_loss: 0.5637 - val_accuracy: 0.9091\n","Epoch 46/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.2771 - accuracy: 0.9961 - val_loss: 0.5639 - val_accuracy: 0.9008\n","Epoch 47/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2744 - accuracy: 0.9979 - val_loss: 0.5619 - val_accuracy: 0.9029\n","Epoch 48/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2728 - accuracy: 0.9984 - val_loss: 0.5518 - val_accuracy: 0.9019\n","Epoch 49/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2725 - accuracy: 0.9990 - val_loss: 0.5694 - val_accuracy: 0.9050\n","Epoch 50/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2731 - accuracy: 0.9979 - val_loss: 0.5562 - val_accuracy: 0.9029\n","Epoch 51/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2738 - accuracy: 0.9979 - val_loss: 0.5592 - val_accuracy: 0.9039\n","Epoch 52/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.2719 - accuracy: 0.9977 - val_loss: 0.5718 - val_accuracy: 0.9112\n","Epoch 53/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2838 - accuracy: 0.9915 - val_loss: 0.5689 - val_accuracy: 0.9070\n","Epoch 54/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2751 - accuracy: 0.9959 - val_loss: 0.5597 - val_accuracy: 0.9039\n","Epoch 55/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2764 - accuracy: 0.9959 - val_loss: 0.6003 - val_accuracy: 0.8946\n","Epoch 56/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2779 - accuracy: 0.9964 - val_loss: 0.5659 - val_accuracy: 0.9050\n","Epoch 57/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2690 - accuracy: 0.9990 - val_loss: 0.5539 - val_accuracy: 0.9050\n","Epoch 58/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2714 - accuracy: 0.9982 - val_loss: 0.5819 - val_accuracy: 0.9050\n","Epoch 59/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2706 - accuracy: 0.9984 - val_loss: 0.5547 - val_accuracy: 0.9050\n","Epoch 60/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2685 - accuracy: 0.9974 - val_loss: 0.5828 - val_accuracy: 0.8988\n","Epoch 61/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2675 - accuracy: 0.9984 - val_loss: 0.6102 - val_accuracy: 0.8957\n","Epoch 62/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2695 - accuracy: 0.9979 - val_loss: 0.5835 - val_accuracy: 0.8998\n","Epoch 63/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2688 - accuracy: 0.9982 - val_loss: 0.5641 - val_accuracy: 0.8988\n","Epoch 64/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2673 - accuracy: 0.9984 - val_loss: 0.5714 - val_accuracy: 0.8998\n","Epoch 65/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2647 - accuracy: 0.9992 - val_loss: 0.5735 - val_accuracy: 0.8977\n","Epoch 66/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2643 - accuracy: 0.9984 - val_loss: 0.5756 - val_accuracy: 0.9019\n","Epoch 67/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2650 - accuracy: 0.9987 - val_loss: 0.5622 - val_accuracy: 0.9029\n","Epoch 68/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2626 - accuracy: 0.9995 - val_loss: 0.5808 - val_accuracy: 0.9039\n","Epoch 69/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2628 - accuracy: 0.9990 - val_loss: 0.5721 - val_accuracy: 0.8977\n","Epoch 70/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2665 - accuracy: 0.9974 - val_loss: 0.5810 - val_accuracy: 0.9060\n","Epoch 71/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2650 - accuracy: 0.9979 - val_loss: 0.5745 - val_accuracy: 0.8957\n","Epoch 72/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2638 - accuracy: 0.9987 - val_loss: 0.5747 - val_accuracy: 0.8977\n","Epoch 73/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2602 - accuracy: 0.9992 - val_loss: 0.5789 - val_accuracy: 0.8977\n","Epoch 74/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2615 - accuracy: 0.9992 - val_loss: 0.5863 - val_accuracy: 0.8998\n","Epoch 75/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2609 - accuracy: 0.9992 - val_loss: 0.5804 - val_accuracy: 0.9070\n","Epoch 76/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2614 - accuracy: 0.9984 - val_loss: 0.5910 - val_accuracy: 0.8895\n","Epoch 77/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2706 - accuracy: 0.9948 - val_loss: 0.5999 - val_accuracy: 0.8988\n","Epoch 78/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2660 - accuracy: 0.9969 - val_loss: 0.6113 - val_accuracy: 0.9019\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2660 - accuracy: 0.9966 - val_loss: 0.5935 - val_accuracy: 0.8926\n","Epoch 80/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2587 - accuracy: 0.9995 - val_loss: 0.5960 - val_accuracy: 0.9008\n","Epoch 81/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2593 - accuracy: 0.9987 - val_loss: 0.5845 - val_accuracy: 0.8977\n","Epoch 82/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2592 - accuracy: 0.9984 - val_loss: 0.5851 - val_accuracy: 0.9019\n","Epoch 83/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2591 - accuracy: 0.9990 - val_loss: 0.5864 - val_accuracy: 0.8998\n","Epoch 84/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2572 - accuracy: 0.9992 - val_loss: 0.5899 - val_accuracy: 0.8936\n","Epoch 85/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2563 - accuracy: 0.9997 - val_loss: 0.5763 - val_accuracy: 0.9019\n","Epoch 86/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2550 - accuracy: 0.9995 - val_loss: 0.5845 - val_accuracy: 0.9060\n","Epoch 87/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2555 - accuracy: 0.9997 - val_loss: 0.5881 - val_accuracy: 0.9019\n","Epoch 88/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2553 - accuracy: 0.9995 - val_loss: 0.5790 - val_accuracy: 0.9060\n","Epoch 89/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2539 - accuracy: 0.9995 - val_loss: 0.5879 - val_accuracy: 0.8977\n","Epoch 90/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2555 - accuracy: 0.9995 - val_loss: 0.5885 - val_accuracy: 0.9008\n","Epoch 91/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2558 - accuracy: 0.9990 - val_loss: 0.6158 - val_accuracy: 0.9029\n","Epoch 92/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2559 - accuracy: 0.9990 - val_loss: 0.6079 - val_accuracy: 0.8998\n","Epoch 93/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2761 - accuracy: 0.9889 - val_loss: 1.1885 - val_accuracy: 0.7820\n","Epoch 94/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.3524 - accuracy: 0.9576 - val_loss: 0.5900 - val_accuracy: 0.8884\n","Epoch 95/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.2609 - accuracy: 0.9964 - val_loss: 0.5912 - val_accuracy: 0.8936\n","Epoch 96/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2544 - accuracy: 0.9995 - val_loss: 0.5772 - val_accuracy: 0.8988\n","Epoch 97/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2522 - accuracy: 0.9995 - val_loss: 0.5787 - val_accuracy: 0.8988\n","Epoch 98/100\n","31/31 [==============================] - 0s 10ms/step - loss: 0.2522 - accuracy: 0.9992 - val_loss: 0.5808 - val_accuracy: 0.8967\n","Epoch 99/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2514 - accuracy: 0.9992 - val_loss: 0.6189 - val_accuracy: 0.8977\n","Epoch 100/100\n","31/31 [==============================] - 0s 11ms/step - loss: 0.2550 - accuracy: 0.9992 - val_loss: 0.6045 - val_accuracy: 0.8998\n","{'loss': [0.3636306822299957, 0.31504547595977783, 0.3122069239616394, 0.30631643533706665, 0.3062495291233063, 0.30460429191589355, 0.3187958300113678, 0.3038323223590851, 0.3045336604118347, 0.30269908905029297, 0.3036617934703827, 0.3006044626235962, 0.29948076605796814, 0.3120342195034027, 0.2963440716266632, 0.3104325532913208, 0.2978633940219879, 0.2919754981994629, 0.2946479022502899, 0.29555919766426086, 0.2920007109642029, 0.29775145649909973, 0.29380160570144653, 0.3011331856250763, 0.30180442333221436, 0.286747545003891, 0.28518033027648926, 0.28410083055496216, 0.2864297330379486, 0.28294968605041504, 0.2814818024635315, 0.28646284341812134, 0.2811512053012848, 0.2838869094848633, 0.28337129950523376, 0.2837165892124176, 0.2943059504032135, 0.2792356014251709, 0.27758705615997314, 0.2795597314834595, 0.28096285462379456, 0.27742549777030945, 0.27565205097198486, 0.27547773718833923, 0.27562767267227173, 0.2771429121494293, 0.27440693974494934, 0.2728194296360016, 0.2725469172000885, 0.2731355130672455, 0.27384138107299805, 0.2719053030014038, 0.2838064134120941, 0.275135338306427, 0.2763821482658386, 0.2779111862182617, 0.2689747214317322, 0.2713507115840912, 0.27061980962753296, 0.26846837997436523, 0.26753661036491394, 0.2694590389728546, 0.2687554955482483, 0.26734814047813416, 0.2647387683391571, 0.2643294334411621, 0.2650196850299835, 0.2625684440135956, 0.2627904415130615, 0.26646187901496887, 0.2649541199207306, 0.2638145685195923, 0.26015031337738037, 0.2615434527397156, 0.2609490752220154, 0.2614161968231201, 0.27056270837783813, 0.26604703068733215, 0.2660440504550934, 0.2586849629878998, 0.25934505462646484, 0.25924429297447205, 0.25911298394203186, 0.25716158747673035, 0.256316602230072, 0.2550218403339386, 0.2554594874382019, 0.2553418278694153, 0.25390490889549255, 0.25550493597984314, 0.2557707726955414, 0.2559487819671631, 0.27609431743621826, 0.3523595333099365, 0.2608996331691742, 0.2543851137161255, 0.25223812460899353, 0.252226322889328, 0.2513764500617981, 0.25499019026756287], 'accuracy': [0.964082658290863, 0.9844961166381836, 0.9870800971984863, 0.9914728403091431, 0.9891473054885864, 0.988630473613739, 0.9801033735275269, 0.9906976819038391, 0.9909560680389404, 0.9899224638938904, 0.9896640777587891, 0.9904392957687378, 0.9896640777587891, 0.9860464930534363, 0.9932816624641418, 0.985788106918335, 0.9914728403091431, 0.9943152666091919, 0.9922480583190918, 0.9927648305892944, 0.9948320388793945, 0.9896640777587891, 0.9927648305892944, 0.9888888597488403, 0.9881137013435364, 0.9958656430244446, 0.9966408014297485, 0.997157633304596, 0.9953488111495972, 0.9968992471694946, 0.9968992471694946, 0.9953488111495972, 0.9974160194396973, 0.9963824152946472, 0.9956072568893433, 0.9950904250144958, 0.9904392957687378, 0.997157633304596, 0.9968992471694946, 0.9961240291595459, 0.9974160194396973, 0.997157633304596, 0.9976744055747986, 0.9976744055747986, 0.9979327917098999, 0.9961240291595459, 0.9979327917098999, 0.9984496235847473, 0.99896639585495, 0.9979327917098999, 0.9979327917098999, 0.9976744055747986, 0.9914728403091431, 0.9958656430244446, 0.9958656430244446, 0.9963824152946472, 0.99896639585495, 0.998191237449646, 0.9984496235847473, 0.9974160194396973, 0.9984496235847473, 0.9979327917098999, 0.998191237449646, 0.9984496235847473, 0.9992247819900513, 0.9984496235847473, 0.9987080097198486, 0.9994832277297974, 0.99896639585495, 0.9974160194396973, 0.9979327917098999, 0.9987080097198486, 0.9992247819900513, 0.9992247819900513, 0.9992247819900513, 0.9984496235847473, 0.9948320388793945, 0.9968992471694946, 0.9966408014297485, 0.9994832277297974, 0.9987080097198486, 0.9984496235847473, 0.99896639585495, 0.9992247819900513, 0.9997416138648987, 0.9994832277297974, 0.9997416138648987, 0.9994832277297974, 0.9994832277297974, 0.9994832277297974, 0.99896639585495, 0.99896639585495, 0.9888888597488403, 0.957622766494751, 0.9963824152946472, 0.9994832277297974, 0.9994832277297974, 0.9992247819900513, 0.9992247819900513, 0.9992247819900513], 'val_loss': [1.2185862064361572, 1.1577942371368408, 1.059976577758789, 0.9724268913269043, 0.927452802658081, 0.9513455033302307, 0.907301127910614, 0.933454155921936, 0.8871750831604004, 0.8283491730690002, 1.0600639581680298, 0.9486261606216431, 1.1474753618240356, 0.841844379901886, 0.6741745471954346, 0.5908264517784119, 0.6001555919647217, 0.5742757320404053, 0.6196168661117554, 0.5557611584663391, 0.6017621159553528, 0.5465689301490784, 0.5674439668655396, 0.5740443468093872, 0.5621959567070007, 0.5324033498764038, 0.5298306941986084, 0.5265328884124756, 0.5352067947387695, 0.5486078858375549, 0.5463355779647827, 0.5517786741256714, 0.5653644800186157, 0.5429809093475342, 0.5852035284042358, 0.577734112739563, 0.5430704951286316, 0.5448416471481323, 0.5498777627944946, 0.5507602095603943, 0.5600088834762573, 0.5484277606010437, 0.5521500110626221, 0.562083899974823, 0.5637110471725464, 0.5638884902000427, 0.5619085431098938, 0.551803469657898, 0.5694311261177063, 0.5561823844909668, 0.5591846108436584, 0.5718337893486023, 0.5688554644584656, 0.559678852558136, 0.6002668738365173, 0.5659268498420715, 0.5539345145225525, 0.5819414854049683, 0.5547024011611938, 0.5828005075454712, 0.6101818084716797, 0.583472490310669, 0.564124584197998, 0.57144695520401, 0.5734565854072571, 0.5755650401115417, 0.5622372031211853, 0.5808460712432861, 0.5720865726470947, 0.5810008645057678, 0.5745264291763306, 0.5746734142303467, 0.5789014101028442, 0.5863078832626343, 0.5803942680358887, 0.5910494327545166, 0.5998887419700623, 0.6113227605819702, 0.5935173630714417, 0.5959577560424805, 0.5844867825508118, 0.5850707292556763, 0.5863907933235168, 0.589858889579773, 0.5763292908668518, 0.5844798684120178, 0.5880784392356873, 0.5789594054222107, 0.5878596305847168, 0.5884738564491272, 0.6158434748649597, 0.6079034209251404, 1.1885178089141846, 0.5900259017944336, 0.5911942720413208, 0.5772222280502319, 0.5787032842636108, 0.580809473991394, 0.6188520789146423, 0.6045049428939819], 'val_accuracy': [0.48553720116615295, 0.48657023906707764, 0.49793389439582825, 0.5154958963394165, 0.5320248007774353, 0.5309917330741882, 0.557851254940033, 0.5495867729187012, 0.5805785059928894, 0.6497933864593506, 0.5547520518302917, 0.6002066135406494, 0.5650826692581177, 0.7138429880142212, 0.8047520518302917, 0.8677685856819153, 0.8698347210884094, 0.8811983466148376, 0.8564049601554871, 0.8935950398445129, 0.8801652789115906, 0.8997933864593506, 0.8925619721412659, 0.8956611752510071, 0.8997933864593506, 0.9090909361839294, 0.9101239442825317, 0.9080578684806824, 0.9101239442825317, 0.9101239442825317, 0.9080578684806824, 0.9039255976676941, 0.8997933864593506, 0.9018595218658447, 0.89462810754776, 0.8884297609329224, 0.9070248007774353, 0.9059917330741882, 0.9059917330741882, 0.9008264541625977, 0.9008264541625977, 0.9028925895690918, 0.9018595218658447, 0.8987603187561035, 0.9090909361839294, 0.9008264541625977, 0.9028925895690918, 0.9018595218658447, 0.9049586653709412, 0.9028925895690918, 0.9039255976676941, 0.9111570119857788, 0.9070248007774353, 0.9039255976676941, 0.89462810754776, 0.9049586653709412, 0.9049586653709412, 0.9049586653709412, 0.9049586653709412, 0.8987603187561035, 0.8956611752510071, 0.8997933864593506, 0.8987603187561035, 0.8997933864593506, 0.8977272510528564, 0.9018595218658447, 0.9028925895690918, 0.9039255976676941, 0.8977272510528564, 0.9059917330741882, 0.8956611752510071, 0.8977272510528564, 0.8977272510528564, 0.8997933864593506, 0.9070248007774353, 0.8894628286361694, 0.8987603187561035, 0.9018595218658447, 0.8925619721412659, 0.9008264541625977, 0.8977272510528564, 0.9018595218658447, 0.8997933864593506, 0.8935950398445129, 0.9018595218658447, 0.9059917330741882, 0.9018595218658447, 0.9059917330741882, 0.8977272510528564, 0.9008264541625977, 0.9028925895690918, 0.8997933864593506, 0.7820248007774353, 0.8884297609329224, 0.8935950398445129, 0.8987603187561035, 0.8987603187561035, 0.8966942429542542, 0.8977272510528564, 0.8997933864593506]}\n","32/32 [==============================] - 0s 3ms/step\n"]}],"source":["global_model_CNN, metrics_df_CNN = federated_learning(Theta_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1717398957546,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"y3RXIk-qZ7ts","outputId":"945c7aa3-2919-49d2-830d-190f8481551c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Client  Accuracy  Precision  Recall     F1  Sensitivity  Specificity  \\\n","0        0     0.755      0.759   0.745  0.752        0.745        0.764   \n","1        1     0.809      0.792   0.838  0.814        0.838        0.780   \n","2        2     0.728      0.718   0.751  0.734        0.751        0.705   \n","3        0     0.815      0.813   0.817  0.815        0.817        0.812   \n","4        1     0.833      0.793   0.901  0.843        0.901        0.764   \n","5        2     0.762      0.736   0.817  0.775        0.817        0.707   \n","6        0     0.844      0.831   0.864  0.847        0.864        0.824   \n","7        1     0.869      0.897   0.833  0.864        0.833        0.904   \n","8        2     0.797      0.747   0.898  0.816        0.898        0.697   \n","9        0     0.861      0.837   0.896  0.866        0.896        0.826   \n","10       1     0.895      0.900   0.888  0.894        0.888        0.901   \n","11       2     0.841      0.794   0.922  0.853        0.922        0.761   \n","12       0     0.865      0.842   0.899  0.870        0.899        0.831   \n","13       1     0.899      0.891   0.910  0.900        0.910        0.888   \n","14       2     0.851      0.805   0.928  0.862        0.928        0.775   \n","\n","    Kappa  \n","0   0.509  \n","1   0.617  \n","2   0.456  \n","3   0.630  \n","4   0.665  \n","5   0.524  \n","6   0.688  \n","7   0.737  \n","8   0.594  \n","9   0.722  \n","10  0.790  \n","11  0.683  \n","12  0.730  \n","13  0.798  \n","14  0.703  "],"text/html":["\n","  <div id=\"df-ab1bd4b8-f60f-414c-bf4b-054704efb6bf\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Client</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Sensitivity</th>\n","      <th>Specificity</th>\n","      <th>Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.755</td>\n","      <td>0.759</td>\n","      <td>0.745</td>\n","      <td>0.752</td>\n","      <td>0.745</td>\n","      <td>0.764</td>\n","      <td>0.509</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.809</td>\n","      <td>0.792</td>\n","      <td>0.838</td>\n","      <td>0.814</td>\n","      <td>0.838</td>\n","      <td>0.780</td>\n","      <td>0.617</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.728</td>\n","      <td>0.718</td>\n","      <td>0.751</td>\n","      <td>0.734</td>\n","      <td>0.751</td>\n","      <td>0.705</td>\n","      <td>0.456</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.815</td>\n","      <td>0.813</td>\n","      <td>0.817</td>\n","      <td>0.815</td>\n","      <td>0.817</td>\n","      <td>0.812</td>\n","      <td>0.630</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.833</td>\n","      <td>0.793</td>\n","      <td>0.901</td>\n","      <td>0.843</td>\n","      <td>0.901</td>\n","      <td>0.764</td>\n","      <td>0.665</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2</td>\n","      <td>0.762</td>\n","      <td>0.736</td>\n","      <td>0.817</td>\n","      <td>0.775</td>\n","      <td>0.817</td>\n","      <td>0.707</td>\n","      <td>0.524</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0.844</td>\n","      <td>0.831</td>\n","      <td>0.864</td>\n","      <td>0.847</td>\n","      <td>0.864</td>\n","      <td>0.824</td>\n","      <td>0.688</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0.869</td>\n","      <td>0.897</td>\n","      <td>0.833</td>\n","      <td>0.864</td>\n","      <td>0.833</td>\n","      <td>0.904</td>\n","      <td>0.737</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>0.797</td>\n","      <td>0.747</td>\n","      <td>0.898</td>\n","      <td>0.816</td>\n","      <td>0.898</td>\n","      <td>0.697</td>\n","      <td>0.594</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>0.861</td>\n","      <td>0.837</td>\n","      <td>0.896</td>\n","      <td>0.866</td>\n","      <td>0.896</td>\n","      <td>0.826</td>\n","      <td>0.722</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0.895</td>\n","      <td>0.900</td>\n","      <td>0.888</td>\n","      <td>0.894</td>\n","      <td>0.888</td>\n","      <td>0.901</td>\n","      <td>0.790</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2</td>\n","      <td>0.841</td>\n","      <td>0.794</td>\n","      <td>0.922</td>\n","      <td>0.853</td>\n","      <td>0.922</td>\n","      <td>0.761</td>\n","      <td>0.683</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>0.865</td>\n","      <td>0.842</td>\n","      <td>0.899</td>\n","      <td>0.870</td>\n","      <td>0.899</td>\n","      <td>0.831</td>\n","      <td>0.730</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0.899</td>\n","      <td>0.891</td>\n","      <td>0.910</td>\n","      <td>0.900</td>\n","      <td>0.910</td>\n","      <td>0.888</td>\n","      <td>0.798</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2</td>\n","      <td>0.851</td>\n","      <td>0.805</td>\n","      <td>0.928</td>\n","      <td>0.862</td>\n","      <td>0.928</td>\n","      <td>0.775</td>\n","      <td>0.703</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab1bd4b8-f60f-414c-bf4b-054704efb6bf')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ab1bd4b8-f60f-414c-bf4b-054704efb6bf button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ab1bd4b8-f60f-414c-bf4b-054704efb6bf');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5cffe973-b887-4d69-96b4-7e009cf82494\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5cffe973-b887-4d69-96b4-7e009cf82494')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5cffe973-b887-4d69-96b4-7e009cf82494 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"metrics_df_CNN\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Client\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05058439435617424,\n        \"min\": 0.728,\n        \"max\": 0.899,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.861,\n          0.841,\n          0.755\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05721846188908246,\n        \"min\": 0.718,\n        \"max\": 0.9,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.837,\n          0.794,\n          0.759\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0583852558034826,\n        \"min\": 0.745,\n        \"max\": 0.928,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.888,\n          0.899,\n          0.745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04917848929688477,\n        \"min\": 0.734,\n        \"max\": 0.9,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.866,\n          0.853,\n          0.752\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sensitivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0583852558034826,\n        \"min\": 0.745,\n        \"max\": 0.928,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.888,\n          0.899,\n          0.745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Specificity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06776584334168302,\n        \"min\": 0.697,\n        \"max\": 0.904,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.901,\n          0.831,\n          0.764\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kappa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1012872859035836,\n        \"min\": 0.456,\n        \"max\": 0.798,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.722,\n          0.683,\n          0.509\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":17}],"source":["metrics_df_CNN.round(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iOLsKpkfzdG"},"outputs":[],"source":["metrics_df_CNN.round(4).to_csv('/content/drive/MyDrive/EEG Signal /results/FLresults/Time Domain/CNN/Theta_time_CNN.csv', index = False)"]},{"cell_type":"markdown","source":["#CNN_GRU"],"metadata":{"id":"Tm6qgVdxwoq4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ieXSN-9PI4Dx"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, cohen_kappa_score\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, LSTM, Dropout, GRU\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n","import tensorflow_addons as tfa\n","\n","class FederatedData:\n","    def __init__(self, data_path, num_clients, scaler_type='MinMax', test_size=0.2):\n","        self.data_path = data_path\n","        self.num_clients = num_clients\n","        self.scaler_type = scaler_type\n","        self.test_size = test_size\n","        self.partitions = []  # Initialize partitions attribute\n","        self.load_data()\n","        self.scale_data()\n","        self.create_partitions()\n","\n","    def load_data(self):\n","        try:\n","            data = np.load(self.data_path, allow_pickle=True)\n","            self.X = np.moveaxis(data['X'], 1, 2)  # Move axis here\n","            self.Y = data['Y']\n","            self.groups = data['Group']  # Assuming 'Group' contains group identifiers\n","        except KeyError as e:\n","            raise ValueError(f\"Missing expected data field: {e}\")\n","        except FileNotFoundError as e:\n","            raise ValueError(f\"Data file not found: {e}\")\n","\n","    def scale_data(self):\n","        # Reshape data to 2D array for scaling\n","        X_reshaped = self.X.reshape(-1, self.X.shape[-1])\n","\n","        # Select scaler based on input\n","        if self.scaler_type == 'Standard':\n","            scaler = StandardScaler()\n","        elif self.scaler_type == 'MinMax':\n","            scaler = MinMaxScaler(feature_range=(0, 1))\n","        else:\n","            raise ValueError(\"Unsupported scaler type. Choose either 'Standard' or 'MinMax'.\")\n","\n","        # Fit and transform the data\n","        scaled_data_reshaped = scaler.fit_transform(X_reshaped)\n","\n","        # Reshape back to original shape\n","        self.X = scaled_data_reshaped.reshape(self.X.shape)\n","\n","    def create_partitions(self):\n","        gss = GroupShuffleSplit(n_splits=self.num_clients, test_size=self.test_size, random_state=42)\n","        for train_index, test_index in gss.split(self.X, self.Y, self.groups):\n","            X_train, X_test = self.X[train_index], self.X[test_index]\n","            Y_train, Y_test = self.Y[train_index], self.Y[test_index]\n","            self.partitions.append((X_train, Y_train, X_test, Y_test))\n","            print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n","\n","    def get_training_and_validation_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        partition_X_train, partition_Y_train, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        X_train, X_val, Y_train, Y_val = train_test_split(partition_X_train, partition_Y_train, test_size=0.2, random_state=42)\n","        return X_train, X_val, Y_train, Y_val, partition_X_test, partition_Y_test\n","\n","    def get_testing_data(self, client_idx):\n","        if client_idx < 0 or client_idx >= len(self.partitions):\n","            raise ValueError(f\"Invalid client index. Must be between 0 and {len(self.partitions) - 1}.\")\n","\n","        _, _, partition_X_test, partition_Y_test = self.partitions[client_idx]\n","        return partition_X_test, partition_Y_test\n","\n","def build_sequential_model_gru(input_shape):\n","    clear_session()\n","    model = Sequential()\n","\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\", input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(256, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(512, 3, strides=1, activation='relu', padding=\"same\"))\n","    model.add(MaxPooling1D(2, padding=\"same\"))\n","    model.add(Conv1D(filters=1024, kernel_size=3, strides=1, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(GRU(256, return_sequences=True))\n","    model.add(GRU(256, return_sequences=True))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(256, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(32, activation='relu',kernel_regularizer=l2(0.001)))\n","    model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.001)))\n","\n","    # opt = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","    opt = tfa.optimizers.RectifiedAdam(learning_rate=0.00009)\n","    opt = tfa.optimizers.SWA(opt)\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","    return model\n","\n","def compute_metrics(y_true, y_pred):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    kappa = cohen_kappa_score(y_true, y_pred)\n","\n","    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","    specificity = tn / (tn + fp)\n","    sensitivity = recall\n","\n","    return accuracy, precision, recall, f1, sensitivity, specificity, kappa\n","\n","def federated_learning_gru(data_path):\n","    federated_data = FederatedData(data_path, num_clients=3, scaler_type='MinMax')\n","    federated_data.create_partitions()\n","\n","    # Get the input shape from the data\n","    input_shape = federated_data.X.shape[1:]\n","    global_model = build_sequential_model_gru(input_shape)\n","\n","    num_clients = 3\n","    local_epochs = 5\n","    global_optimizer = Adam(learning_rate=0.00009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","\n","    # Initialize m and v for Adam optimizer\n","    m = [np.zeros_like(w) for w in global_model.get_weights()]\n","    v = [np.zeros_like(w) for w in global_model.get_weights()]\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 1e-7\n","    t = 0\n","\n","    client_data = []\n","    for client_idx in range(num_clients):\n","        x_train, x_val, y_train, y_val, x_test, y_test = federated_data.get_training_and_validation_data(client_idx)\n","        client_data.append((x_train, x_val, y_train, y_val, x_test, y_test))\n","\n","    metrics_list = []\n","\n","    for epoch in range(local_epochs):\n","        client_models = []\n","\n","        for client in range(num_clients):\n","            x_train, x_val, y_train, y_val, x_test, y_test = client_data[client]\n","            client_model = build_sequential_model_gru(input_shape)\n","            client_model.set_weights(global_model.get_weights())  # Initialize with global weights\n","            opt = Adam(learning_rate=0.000009, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n","            client_model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","            # Set up callbacks\n","            run_name = f\"epoch_{epoch}_client_{client}\"\n","            checkpoint_filepath = f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time_domain/Total/GRU/best_model_{run_name}.h5'\n","            checkpoint_callback = ModelCheckpoint(\n","                checkpoint_filepath,\n","                monitor='val_accuracy',\n","                save_best_only=True,\n","                mode='max'\n","            )\n","            csv_logger = CSVLogger(f'/content/drive/MyDrive/EEG Signal /Epileptic seizure/Saved model/Time_domain/Total/GRU/training_log_{run_name}.csv', append=True, separator=';')\n","\n","            history = client_model.fit(x_train, y_train, epochs=100, batch_size=128, validation_data=(x_val, y_val), callbacks=[checkpoint_callback, csv_logger])\n","            client_models.append(client_model)\n","\n","            print(history.history)\n","\n","            # Load the best model before evaluation\n","            best_model = load_model(checkpoint_filepath)\n","\n","            # Evaluate client model\n","            y_pred = (best_model.predict(x_test) > 0.5).astype(\"int32\")\n","            accuracy, precision, recall, f1, sensitivity, specificity, kappa = compute_metrics(y_test, y_pred)\n","            metrics_list.append({\n","                'Client': client,\n","                'Accuracy': accuracy,\n","                'Precision': precision,\n","                'Recall': recall,\n","                'F1': f1,\n","                'Sensitivity': sensitivity,\n","                'Specificity': specificity,\n","                'Kappa': kappa\n","            })\n","\n","        global_weights = global_model.get_weights()\n","        layer_start_idx = 0\n","        for layer in global_model.layers:\n","            layer_weights = []\n","            layer_biases = []\n","            num_params = len(layer.get_weights())\n","            for i in range(num_clients):\n","                layer_params = client_models[i].get_weights()[layer_start_idx:layer_start_idx + num_params]\n","                if num_params > 0:\n","                    layer_weights.append(layer_params[0])\n","                if num_params > 1:\n","                    layer_biases.append(layer_params[1])\n","\n","            if len(layer_weights) > 0:\n","                averaged_layer_weights = np.mean(layer_weights, axis=0)\n","                if averaged_layer_weights.shape == global_weights[layer_start_idx].shape:\n","                    global_weights[layer_start_idx] = averaged_layer_weights\n","                else:\n","                    print(f\"Warning: Shape mismatch for weights at layer {layer.name}, expected {global_weights[layer_start_idx].shape} but got {averaged_layer_weights.shape}\")\n","            if len(layer_biases) > 0:\n","                averaged_layer_biases = np.mean(layer_biases, axis=0)\n","                if averaged_layer_biases.shape == global_weights[layer_start_idx + 1].shape:\n","                    global_weights[layer_start_idx + 1] = averaged_layer_biases\n","                else:\n","                    print(f\"Warning: Shape mismatch for biases at layer {layer.name}, expected {global_weights[layer_start_idx + 1].shape} but got {averaged_layer_biases.shape}\")\n","\n","            layer_start_idx += num_params\n","\n","        # Apply FedOpt (Adam) update to global weights\n","        t += 1\n","        for i in range(len(global_weights)):\n","            g_t = global_weights[i] - global_model.get_weights()[i]  # Gradient\n","            m[i] = beta1 * m[i] + (1 - beta1) * g_t\n","            v[i] = beta2 * v[i] + (1 - beta2) * (g_t ** 2)\n","            m_hat = m[i] / (1 - beta1 ** t)\n","            v_hat = v[i] / (1 - beta2 ** t)\n","            global_weights[i] -= global_optimizer.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n","\n","        global_model.set_weights(global_weights)\n","\n","    metrics_df = pd.DataFrame(metrics_list)\n","    return global_model, metrics_df\n","\n","# Example usage\n","# data_path = 'path/to/your/data.npz'\n","# global_model, metrics_df = federated_learning(data_path)\n","# print(metrics_df)\n"]},{"cell_type":"code","source":["global_model_gru, metrics_df_gru = federated_learning_gru(Theta_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_ZOjt0G0Bf3","executionInfo":{"status":"ok","timestamp":1717400189473,"user_tz":-360,"elapsed":1231945,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"}},"outputId":"99a993ff-f297-4eed-ab9f-b283f924f36e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (4640, 18, 29), Test shape: (1194, 18, 29)\n","Train shape: (4418, 18, 29), Test shape: (1416, 18, 29)\n","Train shape: (4838, 18, 29), Test shape: (996, 18, 29)\n","Train shape: (4640, 18, 29), Test shape: (1194, 18, 29)\n","Train shape: (4418, 18, 29), Test shape: (1416, 18, 29)\n","Train shape: (4838, 18, 29), Test shape: (996, 18, 29)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 7s 48ms/step - loss: 1.4292 - accuracy: 0.5582 - val_loss: 1.4280 - val_accuracy: 0.4849\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 1s 19ms/step - loss: 1.4180 - accuracy: 0.6487 - val_loss: 1.4223 - val_accuracy: 0.4957\n","Epoch 3/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.4015 - accuracy: 0.7010 - val_loss: 1.4160 - val_accuracy: 0.5733\n","Epoch 4/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.3731 - accuracy: 0.7204 - val_loss: 1.4078 - val_accuracy: 0.6897\n","Epoch 5/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.3364 - accuracy: 0.7352 - val_loss: 1.3967 - val_accuracy: 0.7177\n","Epoch 6/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.2966 - accuracy: 0.7384 - val_loss: 1.3822 - val_accuracy: 0.7317\n","Epoch 7/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.2671 - accuracy: 0.7425 - val_loss: 1.3649 - val_accuracy: 0.7597\n","Epoch 8/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.2470 - accuracy: 0.7460 - val_loss: 1.3494 - val_accuracy: 0.7554\n","Epoch 9/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.2311 - accuracy: 0.7503 - val_loss: 1.3340 - val_accuracy: 0.7672\n","Epoch 10/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.2185 - accuracy: 0.7592 - val_loss: 1.3175 - val_accuracy: 0.7716\n","Epoch 11/100\n","29/29 [==============================] - 1s 24ms/step - loss: 1.2043 - accuracy: 0.7654 - val_loss: 1.3008 - val_accuracy: 0.7726\n","Epoch 12/100\n","29/29 [==============================] - 1s 26ms/step - loss: 1.1899 - accuracy: 0.7697 - val_loss: 1.2829 - val_accuracy: 0.7877\n","Epoch 13/100\n","29/29 [==============================] - 0s 14ms/step - loss: 1.1837 - accuracy: 0.7751 - val_loss: 1.2628 - val_accuracy: 0.7877\n","Epoch 14/100\n","29/29 [==============================] - 0s 15ms/step - loss: 1.1691 - accuracy: 0.7767 - val_loss: 1.2421 - val_accuracy: 0.7812\n","Epoch 15/100\n","29/29 [==============================] - 1s 27ms/step - loss: 1.1619 - accuracy: 0.7775 - val_loss: 1.2219 - val_accuracy: 0.8017\n","Epoch 16/100\n","29/29 [==============================] - 0s 17ms/step - loss: 1.1543 - accuracy: 0.7856 - val_loss: 1.1996 - val_accuracy: 0.7953\n","Epoch 17/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.1434 - accuracy: 0.7831 - val_loss: 1.1811 - val_accuracy: 0.7920\n","Epoch 18/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.1392 - accuracy: 0.7872 - val_loss: 1.1575 - val_accuracy: 0.8017\n","Epoch 19/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.1271 - accuracy: 0.7907 - val_loss: 1.1448 - val_accuracy: 0.7963\n","Epoch 20/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.1198 - accuracy: 0.7939 - val_loss: 1.1227 - val_accuracy: 0.8114\n","Epoch 21/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.1176 - accuracy: 0.7909 - val_loss: 1.1207 - val_accuracy: 0.7996\n","Epoch 22/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.1043 - accuracy: 0.7974 - val_loss: 1.0946 - val_accuracy: 0.8103\n","Epoch 23/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0988 - accuracy: 0.8009 - val_loss: 1.0817 - val_accuracy: 0.8103\n","Epoch 24/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.0929 - accuracy: 0.7982 - val_loss: 1.0740 - val_accuracy: 0.8114\n","Epoch 25/100\n","29/29 [==============================] - 1s 21ms/step - loss: 1.0932 - accuracy: 0.7988 - val_loss: 1.0659 - val_accuracy: 0.8125\n","Epoch 26/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.0836 - accuracy: 0.8060 - val_loss: 1.0586 - val_accuracy: 0.8244\n","Epoch 27/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0839 - accuracy: 0.7942 - val_loss: 1.0583 - val_accuracy: 0.8082\n","Epoch 28/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.0740 - accuracy: 0.8050 - val_loss: 1.0555 - val_accuracy: 0.8060\n","Epoch 29/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.0708 - accuracy: 0.7942 - val_loss: 1.0440 - val_accuracy: 0.8179\n","Epoch 30/100\n","29/29 [==============================] - 1s 20ms/step - loss: 1.0617 - accuracy: 0.8058 - val_loss: 1.0350 - val_accuracy: 0.8276\n","Epoch 31/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0557 - accuracy: 0.8031 - val_loss: 1.0369 - val_accuracy: 0.8114\n","Epoch 32/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.0483 - accuracy: 0.8095 - val_loss: 1.0244 - val_accuracy: 0.8276\n","Epoch 33/100\n","29/29 [==============================] - 1s 19ms/step - loss: 1.0428 - accuracy: 0.8044 - val_loss: 1.0204 - val_accuracy: 0.8287\n","Epoch 34/100\n","29/29 [==============================] - 0s 12ms/step - loss: 1.0407 - accuracy: 0.8095 - val_loss: 1.0151 - val_accuracy: 0.8276\n","Epoch 35/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0329 - accuracy: 0.8112 - val_loss: 1.0120 - val_accuracy: 0.8254\n","Epoch 36/100\n","29/29 [==============================] - 0s 11ms/step - loss: 1.0328 - accuracy: 0.8055 - val_loss: 1.0120 - val_accuracy: 0.8222\n","Epoch 37/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0222 - accuracy: 0.8117 - val_loss: 1.0015 - val_accuracy: 0.8276\n","Epoch 38/100\n","29/29 [==============================] - 1s 23ms/step - loss: 1.0152 - accuracy: 0.8168 - val_loss: 0.9958 - val_accuracy: 0.8308\n","Epoch 39/100\n","29/29 [==============================] - 0s 13ms/step - loss: 1.0087 - accuracy: 0.8149 - val_loss: 0.9918 - val_accuracy: 0.8287\n","Epoch 40/100\n","29/29 [==============================] - 1s 22ms/step - loss: 1.0071 - accuracy: 0.8144 - val_loss: 0.9883 - val_accuracy: 0.8319\n","Epoch 41/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9992 - accuracy: 0.8187 - val_loss: 0.9838 - val_accuracy: 0.8308\n","Epoch 42/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.9985 - accuracy: 0.8141 - val_loss: 0.9929 - val_accuracy: 0.8308\n","Epoch 43/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.9913 - accuracy: 0.8184 - val_loss: 0.9766 - val_accuracy: 0.8341\n","Epoch 44/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.9860 - accuracy: 0.8227 - val_loss: 0.9700 - val_accuracy: 0.8351\n","Epoch 45/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.9820 - accuracy: 0.8233 - val_loss: 0.9657 - val_accuracy: 0.8362\n","Epoch 46/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.9752 - accuracy: 0.8252 - val_loss: 0.9626 - val_accuracy: 0.8287\n","Epoch 47/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9789 - accuracy: 0.8206 - val_loss: 0.9606 - val_accuracy: 0.8287\n","Epoch 48/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.9690 - accuracy: 0.8241 - val_loss: 0.9538 - val_accuracy: 0.8287\n","Epoch 49/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9616 - accuracy: 0.8265 - val_loss: 0.9498 - val_accuracy: 0.8341\n","Epoch 50/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9579 - accuracy: 0.8273 - val_loss: 0.9481 - val_accuracy: 0.8373\n","Epoch 51/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9532 - accuracy: 0.8276 - val_loss: 0.9412 - val_accuracy: 0.8341\n","Epoch 52/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9518 - accuracy: 0.8244 - val_loss: 0.9400 - val_accuracy: 0.8373\n","Epoch 53/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.9465 - accuracy: 0.8238 - val_loss: 0.9347 - val_accuracy: 0.8373\n","Epoch 54/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9393 - accuracy: 0.8332 - val_loss: 0.9383 - val_accuracy: 0.8319\n","Epoch 55/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.9372 - accuracy: 0.8297 - val_loss: 0.9270 - val_accuracy: 0.8384\n","Epoch 56/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.9348 - accuracy: 0.8335 - val_loss: 0.9232 - val_accuracy: 0.8362\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.9416 - accuracy: 0.8230 - val_loss: 0.9395 - val_accuracy: 0.8297\n","Epoch 58/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.9309 - accuracy: 0.8262 - val_loss: 0.9194 - val_accuracy: 0.8330\n","Epoch 59/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9204 - accuracy: 0.8343 - val_loss: 0.9136 - val_accuracy: 0.8384\n","Epoch 60/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.9151 - accuracy: 0.8351 - val_loss: 0.9101 - val_accuracy: 0.8384\n","Epoch 61/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.9111 - accuracy: 0.8362 - val_loss: 0.9079 - val_accuracy: 0.8416\n","Epoch 62/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.9045 - accuracy: 0.8389 - val_loss: 0.9018 - val_accuracy: 0.8384\n","Epoch 63/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.9014 - accuracy: 0.8384 - val_loss: 0.9075 - val_accuracy: 0.8287\n","Epoch 64/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.9031 - accuracy: 0.8367 - val_loss: 0.9078 - val_accuracy: 0.8308\n","Epoch 65/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.8987 - accuracy: 0.8373 - val_loss: 0.8943 - val_accuracy: 0.8427\n","Epoch 66/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8925 - accuracy: 0.8408 - val_loss: 0.8896 - val_accuracy: 0.8384\n","Epoch 67/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8851 - accuracy: 0.8432 - val_loss: 0.8892 - val_accuracy: 0.8416\n","Epoch 68/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8829 - accuracy: 0.8443 - val_loss: 0.8841 - val_accuracy: 0.8405\n","Epoch 69/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8803 - accuracy: 0.8459 - val_loss: 0.8835 - val_accuracy: 0.8405\n","Epoch 70/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8740 - accuracy: 0.8459 - val_loss: 0.8812 - val_accuracy: 0.8384\n","Epoch 71/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.8683 - accuracy: 0.8494 - val_loss: 0.8740 - val_accuracy: 0.8438\n","Epoch 72/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.8666 - accuracy: 0.8464 - val_loss: 0.8847 - val_accuracy: 0.8362\n","Epoch 73/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.8684 - accuracy: 0.8456 - val_loss: 0.8705 - val_accuracy: 0.8448\n","Epoch 74/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8577 - accuracy: 0.8483 - val_loss: 0.8661 - val_accuracy: 0.8427\n","Epoch 75/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8538 - accuracy: 0.8486 - val_loss: 0.8614 - val_accuracy: 0.8448\n","Epoch 76/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.8531 - accuracy: 0.8494 - val_loss: 0.8636 - val_accuracy: 0.8373\n","Epoch 77/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.8460 - accuracy: 0.8561 - val_loss: 0.8597 - val_accuracy: 0.8351\n","Epoch 78/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.8422 - accuracy: 0.8481 - val_loss: 0.8519 - val_accuracy: 0.8470\n","Epoch 79/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.8344 - accuracy: 0.8526 - val_loss: 0.8471 - val_accuracy: 0.8394\n","Epoch 80/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8323 - accuracy: 0.8578 - val_loss: 0.8460 - val_accuracy: 0.8394\n","Epoch 81/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8314 - accuracy: 0.8499 - val_loss: 0.8499 - val_accuracy: 0.8394\n","Epoch 82/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8309 - accuracy: 0.8502 - val_loss: 0.8417 - val_accuracy: 0.8384\n","Epoch 83/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8277 - accuracy: 0.8548 - val_loss: 0.8386 - val_accuracy: 0.8459\n","Epoch 84/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.8209 - accuracy: 0.8567 - val_loss: 0.8394 - val_accuracy: 0.8481\n","Epoch 85/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8172 - accuracy: 0.8548 - val_loss: 0.8320 - val_accuracy: 0.8470\n","Epoch 86/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.8151 - accuracy: 0.8548 - val_loss: 0.8385 - val_accuracy: 0.8438\n","Epoch 87/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.8110 - accuracy: 0.8583 - val_loss: 0.8268 - val_accuracy: 0.8459\n","Epoch 88/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7973 - accuracy: 0.8629 - val_loss: 0.8264 - val_accuracy: 0.8491\n","Epoch 89/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.8027 - accuracy: 0.8545 - val_loss: 0.8219 - val_accuracy: 0.8459\n","Epoch 90/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7982 - accuracy: 0.8629 - val_loss: 0.8247 - val_accuracy: 0.8448\n","Epoch 91/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7951 - accuracy: 0.8596 - val_loss: 0.8188 - val_accuracy: 0.8416\n","Epoch 92/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7933 - accuracy: 0.8613 - val_loss: 0.8145 - val_accuracy: 0.8459\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7859 - accuracy: 0.8615 - val_loss: 0.8170 - val_accuracy: 0.8438\n","Epoch 94/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7837 - accuracy: 0.8599 - val_loss: 0.8122 - val_accuracy: 0.8438\n","Epoch 95/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7785 - accuracy: 0.8661 - val_loss: 0.8071 - val_accuracy: 0.8459\n","Epoch 96/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7747 - accuracy: 0.8626 - val_loss: 0.8052 - val_accuracy: 0.8491\n","Epoch 97/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7743 - accuracy: 0.8610 - val_loss: 0.8221 - val_accuracy: 0.8351\n","Epoch 98/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7676 - accuracy: 0.8615 - val_loss: 0.8020 - val_accuracy: 0.8448\n","Epoch 99/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7649 - accuracy: 0.8610 - val_loss: 0.7988 - val_accuracy: 0.8470\n","Epoch 100/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7573 - accuracy: 0.8707 - val_loss: 0.7959 - val_accuracy: 0.8524\n","{'loss': [1.4291595220565796, 1.4179564714431763, 1.40149986743927, 1.373137354850769, 1.3364206552505493, 1.2966164350509644, 1.2670559883117676, 1.2470349073410034, 1.2310765981674194, 1.218506097793579, 1.2042689323425293, 1.1898967027664185, 1.1836789846420288, 1.169148325920105, 1.1618564128875732, 1.1542918682098389, 1.143438696861267, 1.1392215490341187, 1.1270931959152222, 1.1198103427886963, 1.1176323890686035, 1.104270577430725, 1.098766803741455, 1.092860221862793, 1.0931802988052368, 1.0835846662521362, 1.0839020013809204, 1.074032187461853, 1.0708428621292114, 1.0616552829742432, 1.0557364225387573, 1.0483123064041138, 1.042809009552002, 1.0406546592712402, 1.0329266786575317, 1.0328009128570557, 1.0221631526947021, 1.0152313709259033, 1.0087250471115112, 1.007073163986206, 0.9992359280586243, 0.9985302686691284, 0.9912587404251099, 0.9859719276428223, 0.9820271730422974, 0.9752082824707031, 0.978851318359375, 0.9690321087837219, 0.9615694284439087, 0.9578943848609924, 0.9532086849212646, 0.9518406391143799, 0.9465012550354004, 0.9392777681350708, 0.9371551275253296, 0.9348019361495972, 0.9416287541389465, 0.9308558106422424, 0.9204317331314087, 0.915144681930542, 0.9110672473907471, 0.9044741988182068, 0.9014467000961304, 0.9031487703323364, 0.8986579775810242, 0.8925259113311768, 0.8850802779197693, 0.882861316204071, 0.8802574872970581, 0.8739662170410156, 0.8683280944824219, 0.8666489720344543, 0.8683601021766663, 0.8576690554618835, 0.8538332581520081, 0.8530558347702026, 0.8460056781768799, 0.842223584651947, 0.8344457149505615, 0.8323388695716858, 0.8313899040222168, 0.8309381008148193, 0.8276513814926147, 0.8208803534507751, 0.8171729445457458, 0.8150856494903564, 0.8110466003417969, 0.797256588935852, 0.8027349710464478, 0.7981515526771545, 0.7950908541679382, 0.7933462262153625, 0.7858719229698181, 0.7836882472038269, 0.7784897089004517, 0.7747112512588501, 0.7742754220962524, 0.7675607800483704, 0.7648748755455017, 0.7572847604751587], 'accuracy': [0.5581896305084229, 0.6487069129943848, 0.7009698152542114, 0.720366358757019, 0.7351831793785095, 0.7384159564971924, 0.7424569129943848, 0.7459590435028076, 0.7502694129943848, 0.759159505367279, 0.7653555870056152, 0.7696659564971924, 0.775053858757019, 0.7766702771186829, 0.7774784564971924, 0.7855603694915771, 0.7831357717514038, 0.7871767282485962, 0.790678858757019, 0.7939116358757019, 0.7909482717514038, 0.7974137663841248, 0.8009159564971924, 0.798222005367279, 0.7987607717514038, 0.806034505367279, 0.7941810488700867, 0.8049569129943848, 0.7941810488700867, 0.8057650923728943, 0.803071141242981, 0.8095366358757019, 0.8044180870056152, 0.8095366358757019, 0.811152994632721, 0.8054956793785095, 0.8116918206214905, 0.8168103694915771, 0.8149245977401733, 0.8143857717514038, 0.818696141242981, 0.814116358757019, 0.8184267282485962, 0.8227370977401733, 0.8232758641242981, 0.8251616358757019, 0.8205819129943848, 0.8240840435028076, 0.826508641242981, 0.8273168206214905, 0.8275862336158752, 0.8243534564971924, 0.8238146305084229, 0.8332435488700867, 0.829741358757019, 0.8335129022598267, 0.8230064511299133, 0.8262392282485962, 0.834321141242981, 0.8351293206214905, 0.8362069129943848, 0.8389008641242981, 0.8383620977401733, 0.8367456793785095, 0.837284505367279, 0.8407866358757019, 0.8432112336158752, 0.8442887663841248, 0.8459051847457886, 0.8459051847457886, 0.8494073152542114, 0.8464439511299133, 0.8456357717514038, 0.8483297228813171, 0.8485991358757019, 0.8494073152542114, 0.8561422228813171, 0.8480603694915771, 0.8526400923728943, 0.857758641242981, 0.849946141242981, 0.850215494632721, 0.8547952771186829, 0.8566810488700867, 0.8547952771186829, 0.8547952771186829, 0.8582974076271057, 0.8628771305084229, 0.8545258641242981, 0.8628771305084229, 0.8596444129943848, 0.8612607717514038, 0.8615301847457886, 0.8599137663841248, 0.8661099076271057, 0.8626077771186829, 0.860991358757019, 0.8615301847457886, 0.860991358757019, 0.8706896305084229], 'val_loss': [1.4280163049697876, 1.4223297834396362, 1.4160062074661255, 1.4077978134155273, 1.3966730833053589, 1.3822394609451294, 1.364904522895813, 1.3493565320968628, 1.3340092897415161, 1.317525029182434, 1.3007892370224, 1.2829396724700928, 1.2628364562988281, 1.2420921325683594, 1.2219407558441162, 1.1996010541915894, 1.1811344623565674, 1.1575359106063843, 1.1447639465332031, 1.1227128505706787, 1.1206743717193604, 1.094623327255249, 1.0816998481750488, 1.074049711227417, 1.0658960342407227, 1.0586272478103638, 1.0583475828170776, 1.0554687976837158, 1.0439839363098145, 1.0350310802459717, 1.036893606185913, 1.0244288444519043, 1.020350694656372, 1.015086054801941, 1.011993169784546, 1.0120004415512085, 1.001503825187683, 0.9958160519599915, 0.9918078780174255, 0.9883092641830444, 0.9838226437568665, 0.9928896427154541, 0.9765713810920715, 0.9699653387069702, 0.9656972289085388, 0.9626235961914062, 0.9606254696846008, 0.9537990093231201, 0.9497741460800171, 0.9481266736984253, 0.9412330389022827, 0.9399699568748474, 0.934727132320404, 0.9382668733596802, 0.9269537329673767, 0.9231942296028137, 0.939456582069397, 0.9194170832633972, 0.9135971665382385, 0.9101065993309021, 0.9079021215438843, 0.9017674922943115, 0.9075395464897156, 0.9078413844108582, 0.8942737579345703, 0.8896395564079285, 0.8891758918762207, 0.8841253519058228, 0.8834563493728638, 0.8811983466148376, 0.8739702105522156, 0.8847132921218872, 0.8704903721809387, 0.8661195039749146, 0.8613924384117126, 0.8635805249214172, 0.8596739768981934, 0.8519337177276611, 0.8471134901046753, 0.8460243940353394, 0.8498753309249878, 0.8416653871536255, 0.8386054635047913, 0.8394370675086975, 0.8319766521453857, 0.8385183811187744, 0.8268370628356934, 0.8264473676681519, 0.8219138383865356, 0.8246931433677673, 0.8187956809997559, 0.8145452737808228, 0.816988468170166, 0.8122009634971619, 0.8070858716964722, 0.8051720857620239, 0.8220516443252563, 0.8020033240318298, 0.7988040447235107, 0.79585200548172], 'val_accuracy': [0.48491379618644714, 0.49568966031074524, 0.5732758641242981, 0.6896551847457886, 0.7176724076271057, 0.7316810488700867, 0.7596982717514038, 0.7553879022598267, 0.767241358757019, 0.7715517282485962, 0.7726293206214905, 0.787715494632721, 0.787715494632721, 0.78125, 0.8017241358757019, 0.795258641242981, 0.7920258641242981, 0.8017241358757019, 0.7963362336158752, 0.8114224076271057, 0.7995689511299133, 0.8103448152542114, 0.8103448152542114, 0.8114224076271057, 0.8125, 0.8243534564971924, 0.8081896305084229, 0.806034505367279, 0.8178879022598267, 0.8275862336158752, 0.8114224076271057, 0.8275862336158752, 0.8286637663841248, 0.8275862336158752, 0.8254310488700867, 0.8221982717514038, 0.8275862336158752, 0.8308189511299133, 0.8286637663841248, 0.8318965435028076, 0.8308189511299133, 0.8308189511299133, 0.8340517282485962, 0.8351293206214905, 0.8362069129943848, 0.8286637663841248, 0.8286637663841248, 0.8286637663841248, 0.8340517282485962, 0.837284505367279, 0.8340517282485962, 0.837284505367279, 0.837284505367279, 0.8318965435028076, 0.8383620977401733, 0.8362069129943848, 0.829741358757019, 0.8329741358757019, 0.8383620977401733, 0.8383620977401733, 0.8415948152542114, 0.8383620977401733, 0.8286637663841248, 0.8308189511299133, 0.8426724076271057, 0.8383620977401733, 0.8415948152542114, 0.8405172228813171, 0.8405172228813171, 0.8383620977401733, 0.84375, 0.8362069129943848, 0.8448275923728943, 0.8426724076271057, 0.8448275923728943, 0.837284505367279, 0.8351293206214905, 0.8469827771186829, 0.8394396305084229, 0.8394396305084229, 0.8394396305084229, 0.8383620977401733, 0.8459051847457886, 0.8480603694915771, 0.8469827771186829, 0.84375, 0.8459051847457886, 0.8491379022598267, 0.8459051847457886, 0.8448275923728943, 0.8415948152542114, 0.8459051847457886, 0.84375, 0.84375, 0.8459051847457886, 0.8491379022598267, 0.8351293206214905, 0.8448275923728943, 0.8469827771186829, 0.8523706793785095]}\n","38/38 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","27/28 [===========================>..] - ETA: 0s - loss: 1.4299 - accuracy: 0.5686"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 8s 75ms/step - loss: 1.4298 - accuracy: 0.5690 - val_loss: 1.4283 - val_accuracy: 0.5882\n","Epoch 2/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.4210 - accuracy: 0.6245 - val_loss: 1.4228 - val_accuracy: 0.5249\n","Epoch 3/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.4097 - accuracy: 0.6718 - val_loss: 1.4172 - val_accuracy: 0.6324\n","Epoch 4/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.3920 - accuracy: 0.6921 - val_loss: 1.4106 - val_accuracy: 0.6821\n","Epoch 5/100\n","28/28 [==============================] - 1s 23ms/step - loss: 1.3683 - accuracy: 0.6981 - val_loss: 1.4024 - val_accuracy: 0.7161\n","Epoch 6/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.3385 - accuracy: 0.7054 - val_loss: 1.3922 - val_accuracy: 0.6957\n","Epoch 7/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.3102 - accuracy: 0.7128 - val_loss: 1.3800 - val_accuracy: 0.7172\n","Epoch 8/100\n","28/28 [==============================] - 0s 16ms/step - loss: 1.2891 - accuracy: 0.7170 - val_loss: 1.3661 - val_accuracy: 0.7149\n","Epoch 9/100\n","28/28 [==============================] - 1s 27ms/step - loss: 1.2686 - accuracy: 0.7278 - val_loss: 1.3528 - val_accuracy: 0.7206\n","Epoch 10/100\n","28/28 [==============================] - 1s 36ms/step - loss: 1.2554 - accuracy: 0.7363 - val_loss: 1.3408 - val_accuracy: 0.7398\n","Epoch 11/100\n","28/28 [==============================] - 1s 30ms/step - loss: 1.2413 - accuracy: 0.7394 - val_loss: 1.3260 - val_accuracy: 0.7455\n","Epoch 12/100\n","28/28 [==============================] - 1s 27ms/step - loss: 1.2265 - accuracy: 0.7501 - val_loss: 1.3098 - val_accuracy: 0.7489\n","Epoch 13/100\n","28/28 [==============================] - 1s 23ms/step - loss: 1.2158 - accuracy: 0.7561 - val_loss: 1.2943 - val_accuracy: 0.7568\n","Epoch 14/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.2013 - accuracy: 0.7598 - val_loss: 1.2764 - val_accuracy: 0.7613\n","Epoch 15/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1857 - accuracy: 0.7691 - val_loss: 1.2641 - val_accuracy: 0.7330\n","Epoch 16/100\n","28/28 [==============================] - 0s 12ms/step - loss: 1.1762 - accuracy: 0.7660 - val_loss: 1.2420 - val_accuracy: 0.7432\n","Epoch 17/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.1646 - accuracy: 0.7736 - val_loss: 1.2239 - val_accuracy: 0.7443\n","Epoch 18/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.1564 - accuracy: 0.7753 - val_loss: 1.2111 - val_accuracy: 0.7421\n","Epoch 19/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.1474 - accuracy: 0.7762 - val_loss: 1.1842 - val_accuracy: 0.7636\n","Epoch 20/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1349 - accuracy: 0.7804 - val_loss: 1.1941 - val_accuracy: 0.7410\n","Epoch 21/100\n","28/28 [==============================] - 1s 18ms/step - loss: 1.1323 - accuracy: 0.7790 - val_loss: 1.1588 - val_accuracy: 0.7613\n","Epoch 22/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.1150 - accuracy: 0.7915 - val_loss: 1.1665 - val_accuracy: 0.7523\n","Epoch 23/100\n","28/28 [==============================] - 1s 26ms/step - loss: 1.1105 - accuracy: 0.7892 - val_loss: 1.1387 - val_accuracy: 0.7670\n","Epoch 24/100\n","28/28 [==============================] - 1s 19ms/step - loss: 1.1049 - accuracy: 0.7920 - val_loss: 1.1334 - val_accuracy: 0.7658\n","Epoch 25/100\n","28/28 [==============================] - 0s 17ms/step - loss: 1.1036 - accuracy: 0.7926 - val_loss: 1.1316 - val_accuracy: 0.7658\n","Epoch 26/100\n","28/28 [==============================] - 1s 24ms/step - loss: 1.0927 - accuracy: 0.7866 - val_loss: 1.1163 - val_accuracy: 0.7771\n","Epoch 27/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.0842 - accuracy: 0.8016 - val_loss: 1.1113 - val_accuracy: 0.7749\n","Epoch 28/100\n","28/28 [==============================] - 1s 22ms/step - loss: 1.0805 - accuracy: 0.7974 - val_loss: 1.0960 - val_accuracy: 0.7907\n","Epoch 29/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.0727 - accuracy: 0.7999 - val_loss: 1.1026 - val_accuracy: 0.7783\n","Epoch 30/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.0652 - accuracy: 0.8065 - val_loss: 1.0860 - val_accuracy: 0.7873\n","Epoch 31/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.0623 - accuracy: 0.7997 - val_loss: 1.0784 - val_accuracy: 0.7919\n","Epoch 32/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.0513 - accuracy: 0.8104 - val_loss: 1.0775 - val_accuracy: 0.7862\n","Epoch 33/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.0458 - accuracy: 0.8101 - val_loss: 1.0745 - val_accuracy: 0.7862\n","Epoch 34/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.0406 - accuracy: 0.8079 - val_loss: 1.0880 - val_accuracy: 0.7738\n","Epoch 35/100\n","28/28 [==============================] - 0s 12ms/step - loss: 1.0432 - accuracy: 0.8022 - val_loss: 1.0651 - val_accuracy: 0.7919\n","Epoch 36/100\n","28/28 [==============================] - 0s 14ms/step - loss: 1.0328 - accuracy: 0.8209 - val_loss: 1.0596 - val_accuracy: 0.7896\n","Epoch 37/100\n","28/28 [==============================] - 1s 20ms/step - loss: 1.0216 - accuracy: 0.8158 - val_loss: 1.0552 - val_accuracy: 0.7952\n","Epoch 38/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.0234 - accuracy: 0.8132 - val_loss: 1.0520 - val_accuracy: 0.7930\n","Epoch 39/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.0105 - accuracy: 0.8198 - val_loss: 1.0456 - val_accuracy: 0.7975\n","Epoch 40/100\n","28/28 [==============================] - 1s 21ms/step - loss: 1.0061 - accuracy: 0.8178 - val_loss: 1.0444 - val_accuracy: 0.8009\n","Epoch 41/100\n","28/28 [==============================] - 0s 13ms/step - loss: 1.0054 - accuracy: 0.8149 - val_loss: 1.0394 - val_accuracy: 0.8009\n","Epoch 42/100\n","28/28 [==============================] - 0s 15ms/step - loss: 1.0037 - accuracy: 0.8172 - val_loss: 1.0527 - val_accuracy: 0.7873\n","Epoch 43/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.9964 - accuracy: 0.8161 - val_loss: 1.0329 - val_accuracy: 0.8009\n","Epoch 44/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9919 - accuracy: 0.8169 - val_loss: 1.0274 - val_accuracy: 0.8009\n","Epoch 45/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.9829 - accuracy: 0.8229 - val_loss: 1.0280 - val_accuracy: 0.8020\n","Epoch 46/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.9833 - accuracy: 0.8237 - val_loss: 1.0203 - val_accuracy: 0.8077\n","Epoch 47/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.9742 - accuracy: 0.8328 - val_loss: 1.0234 - val_accuracy: 0.7964\n","Epoch 48/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.9703 - accuracy: 0.8265 - val_loss: 1.0152 - val_accuracy: 0.8043\n","Epoch 49/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9660 - accuracy: 0.8237 - val_loss: 1.0403 - val_accuracy: 0.7738\n","Epoch 50/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.9770 - accuracy: 0.8200 - val_loss: 1.0203 - val_accuracy: 0.7873\n","Epoch 51/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9584 - accuracy: 0.8291 - val_loss: 1.0093 - val_accuracy: 0.7964\n","Epoch 52/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9511 - accuracy: 0.8336 - val_loss: 1.0043 - val_accuracy: 0.8009\n","Epoch 53/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.9637 - accuracy: 0.8181 - val_loss: 1.0051 - val_accuracy: 0.8020\n","Epoch 54/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.9500 - accuracy: 0.8339 - val_loss: 0.9975 - val_accuracy: 0.8111\n","Epoch 55/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.9415 - accuracy: 0.8308 - val_loss: 0.9922 - val_accuracy: 0.8066\n","Epoch 56/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9390 - accuracy: 0.8297 - val_loss: 0.9894 - val_accuracy: 0.8066\n","Epoch 57/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.9318 - accuracy: 0.8407 - val_loss: 0.9872 - val_accuracy: 0.8032\n","Epoch 58/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9248 - accuracy: 0.8384 - val_loss: 0.9824 - val_accuracy: 0.8100\n","Epoch 59/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9280 - accuracy: 0.8359 - val_loss: 0.9801 - val_accuracy: 0.8043\n","Epoch 60/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.9169 - accuracy: 0.8379 - val_loss: 0.9766 - val_accuracy: 0.8088\n","Epoch 61/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9130 - accuracy: 0.8427 - val_loss: 0.9771 - val_accuracy: 0.8032\n","Epoch 62/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.9109 - accuracy: 0.8393 - val_loss: 0.9720 - val_accuracy: 0.8122\n","Epoch 63/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.9052 - accuracy: 0.8407 - val_loss: 0.9674 - val_accuracy: 0.8145\n","Epoch 64/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.9035 - accuracy: 0.8413 - val_loss: 0.9688 - val_accuracy: 0.8043\n","Epoch 65/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.9070 - accuracy: 0.8314 - val_loss: 0.9700 - val_accuracy: 0.8032\n","Epoch 66/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8997 - accuracy: 0.8410 - val_loss: 0.9657 - val_accuracy: 0.8043\n","Epoch 67/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.8894 - accuracy: 0.8447 - val_loss: 0.9572 - val_accuracy: 0.8077\n","Epoch 68/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8837 - accuracy: 0.8469 - val_loss: 0.9555 - val_accuracy: 0.8111\n","Epoch 69/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8877 - accuracy: 0.8427 - val_loss: 0.9517 - val_accuracy: 0.8088\n","Epoch 70/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8802 - accuracy: 0.8486 - val_loss: 0.9487 - val_accuracy: 0.8100\n","Epoch 71/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8715 - accuracy: 0.8463 - val_loss: 0.9453 - val_accuracy: 0.8077\n","Epoch 72/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8788 - accuracy: 0.8407 - val_loss: 0.9644 - val_accuracy: 0.7919\n","Epoch 73/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8725 - accuracy: 0.8486 - val_loss: 0.9393 - val_accuracy: 0.8088\n","Epoch 74/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8637 - accuracy: 0.8497 - val_loss: 0.9382 - val_accuracy: 0.8066\n","Epoch 75/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8582 - accuracy: 0.8492 - val_loss: 0.9393 - val_accuracy: 0.8066\n","Epoch 76/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8598 - accuracy: 0.8432 - val_loss: 0.9350 - val_accuracy: 0.8088\n","Epoch 77/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8528 - accuracy: 0.8534 - val_loss: 0.9294 - val_accuracy: 0.8066\n","Epoch 78/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.8547 - accuracy: 0.8503 - val_loss: 0.9250 - val_accuracy: 0.8088\n","Epoch 79/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8414 - accuracy: 0.8557 - val_loss: 0.9321 - val_accuracy: 0.8077\n","Epoch 80/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8436 - accuracy: 0.8472 - val_loss: 0.9275 - val_accuracy: 0.8133\n","Epoch 81/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8410 - accuracy: 0.8472 - val_loss: 0.9177 - val_accuracy: 0.8122\n","Epoch 82/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.8327 - accuracy: 0.8577 - val_loss: 0.9158 - val_accuracy: 0.8088\n","Epoch 83/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.8274 - accuracy: 0.8554 - val_loss: 0.9191 - val_accuracy: 0.8077\n","Epoch 84/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.8217 - accuracy: 0.8599 - val_loss: 0.9146 - val_accuracy: 0.8100\n","Epoch 85/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.8283 - accuracy: 0.8531 - val_loss: 0.9098 - val_accuracy: 0.8066\n","Epoch 86/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8227 - accuracy: 0.8565 - val_loss: 0.9095 - val_accuracy: 0.8066\n","Epoch 87/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.8156 - accuracy: 0.8585 - val_loss: 0.9051 - val_accuracy: 0.8100\n","Epoch 88/100\n","28/28 [==============================] - 1s 41ms/step - loss: 0.8127 - accuracy: 0.8554 - val_loss: 0.9059 - val_accuracy: 0.8156\n","Epoch 89/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8106 - accuracy: 0.8588 - val_loss: 0.8975 - val_accuracy: 0.8133\n","Epoch 90/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8035 - accuracy: 0.8613 - val_loss: 0.8955 - val_accuracy: 0.8100\n","Epoch 91/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7992 - accuracy: 0.8608 - val_loss: 0.9028 - val_accuracy: 0.8032\n","Epoch 92/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7987 - accuracy: 0.8596 - val_loss: 0.8966 - val_accuracy: 0.8088\n","Epoch 93/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.8006 - accuracy: 0.8512 - val_loss: 0.8954 - val_accuracy: 0.8054\n","Epoch 94/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7866 - accuracy: 0.8594 - val_loss: 0.8883 - val_accuracy: 0.8100\n","Epoch 95/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7845 - accuracy: 0.8681 - val_loss: 0.8866 - val_accuracy: 0.8133\n","Epoch 96/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7826 - accuracy: 0.8642 - val_loss: 0.9022 - val_accuracy: 0.8122\n","Epoch 97/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7817 - accuracy: 0.8636 - val_loss: 0.8862 - val_accuracy: 0.8088\n","Epoch 98/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7769 - accuracy: 0.8642 - val_loss: 0.8838 - val_accuracy: 0.8122\n","Epoch 99/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7687 - accuracy: 0.8676 - val_loss: 0.8772 - val_accuracy: 0.8156\n","Epoch 100/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7722 - accuracy: 0.8633 - val_loss: 0.8737 - val_accuracy: 0.8156\n","{'loss': [1.4298186302185059, 1.4210296869277954, 1.4096803665161133, 1.3919748067855835, 1.3683464527130127, 1.3384770154953003, 1.310180425643921, 1.289109706878662, 1.2685790061950684, 1.255397081375122, 1.2413398027420044, 1.2264645099639893, 1.2157607078552246, 1.2013354301452637, 1.1857017278671265, 1.1762356758117676, 1.164571762084961, 1.1564451456069946, 1.1473517417907715, 1.134887456893921, 1.1323219537734985, 1.1149979829788208, 1.110507607460022, 1.1048600673675537, 1.1036298274993896, 1.0927104949951172, 1.0842442512512207, 1.0805208683013916, 1.0726633071899414, 1.0652364492416382, 1.062288761138916, 1.0512983798980713, 1.0457829236984253, 1.0406391620635986, 1.0431973934173584, 1.0327858924865723, 1.0215764045715332, 1.0234291553497314, 1.0104573965072632, 1.0061203241348267, 1.0054430961608887, 1.0036815404891968, 0.9963718056678772, 0.9918732643127441, 0.9829276204109192, 0.9833336472511292, 0.9742040038108826, 0.9702674746513367, 0.9660062789916992, 0.9770230650901794, 0.9584366679191589, 0.951092004776001, 0.9637467265129089, 0.9500037431716919, 0.941504955291748, 0.9389908313751221, 0.9318348169326782, 0.9248241186141968, 0.9280318021774292, 0.9168901443481445, 0.9130316972732544, 0.9109413027763367, 0.9051612019538879, 0.9035210013389587, 0.9070374369621277, 0.8997123837471008, 0.889423668384552, 0.8837056756019592, 0.887679398059845, 0.8801737427711487, 0.8714916110038757, 0.8787742257118225, 0.8725098371505737, 0.8636631369590759, 0.8582320213317871, 0.8597795367240906, 0.852779746055603, 0.8547164797782898, 0.8413845896720886, 0.8435744643211365, 0.8409566283226013, 0.8327302932739258, 0.8273590207099915, 0.8216789960861206, 0.8283082842826843, 0.8226996660232544, 0.8155514597892761, 0.812658429145813, 0.8105815649032593, 0.8034737706184387, 0.7991864085197449, 0.7986851930618286, 0.8005680441856384, 0.7865933775901794, 0.7844837307929993, 0.7825806736946106, 0.781679093837738, 0.7769001126289368, 0.7686871290206909, 0.7721746563911438], 'accuracy': [0.5690435767173767, 0.624504804611206, 0.6717600226402283, 0.6921335458755493, 0.6980758309364319, 0.7054329514503479, 0.7127900123596191, 0.7170345187187195, 0.7277871966362, 0.7362761497497559, 0.7393888235092163, 0.7501415014266968, 0.7560837864875793, 0.7597622871398926, 0.7691001892089844, 0.7659875750541687, 0.7736276388168335, 0.7753254175186157, 0.7761743068695068, 0.7804188132286072, 0.7790039777755737, 0.7914544343948364, 0.7891907095909119, 0.7920203804969788, 0.7925863265991211, 0.7866440415382385, 0.8016412258148193, 0.797396719455719, 0.7999433875083923, 0.8064516186714172, 0.7996604442596436, 0.810413122177124, 0.8101301789283752, 0.8078664541244507, 0.8022071123123169, 0.8208828568458557, 0.8157894611358643, 0.8132427930831909, 0.819750964641571, 0.81777024269104, 0.8149405717849731, 0.8172042965888977, 0.8160724639892578, 0.8169213533401489, 0.8228636384010315, 0.8237125277519226, 0.8327674269676208, 0.8265421390533447, 0.8237125277519226, 0.8200339674949646, 0.8290888667106628, 0.833616316318512, 0.8180531859397888, 0.8338992595672607, 0.8307866454124451, 0.8296547532081604, 0.8406904339790344, 0.8384267091751099, 0.8358800411224365, 0.8378607630729675, 0.8426712155342102, 0.839275598526001, 0.8406904339790344, 0.8412563800811768, 0.8313525915145874, 0.8409733772277832, 0.8446519374847412, 0.8469156622886658, 0.8426712155342102, 0.848613440990448, 0.8463497161865234, 0.8406904339790344, 0.848613440990448, 0.8497453331947327, 0.8491793870925903, 0.8432371020317078, 0.8534238934516907, 0.850311279296875, 0.8556876182556152, 0.8471986651420593, 0.8471986651420593, 0.8576683402061462, 0.8554046154022217, 0.8599320650100708, 0.8531408905982971, 0.8565365076065063, 0.8585172891616821, 0.8554046154022217, 0.8588002324104309, 0.8613469004631042, 0.8607810139656067, 0.859649121761322, 0.8511601686477661, 0.8593661785125732, 0.8681380748748779, 0.8641765713691711, 0.8636106252670288, 0.8641765713691711, 0.8675721287727356, 0.86332768201828], 'val_loss': [1.4282903671264648, 1.4228140115737915, 1.4171509742736816, 1.4105650186538696, 1.4023693799972534, 1.3922325372695923, 1.3800008296966553, 1.3660800457000732, 1.352807641029358, 1.3407927751541138, 1.326036810874939, 1.3097950220108032, 1.2943462133407593, 1.2764344215393066, 1.264059066772461, 1.2419660091400146, 1.2239327430725098, 1.2111395597457886, 1.1841659545898438, 1.1940962076187134, 1.1588375568389893, 1.1665430068969727, 1.1386675834655762, 1.1333647966384888, 1.1316485404968262, 1.1163339614868164, 1.111279010772705, 1.0960322618484497, 1.1026297807693481, 1.0860265493392944, 1.0784071683883667, 1.077534556388855, 1.0745285749435425, 1.0879769325256348, 1.0650932788848877, 1.059633731842041, 1.0552092790603638, 1.0519652366638184, 1.0455520153045654, 1.0444080829620361, 1.0393633842468262, 1.0526686906814575, 1.032855749130249, 1.0274285078048706, 1.0280344486236572, 1.0203218460083008, 1.0234098434448242, 1.0151921510696411, 1.0403318405151367, 1.0202728509902954, 1.0093477964401245, 1.0042717456817627, 1.0050899982452393, 0.9974669218063354, 0.9922358393669128, 0.9893644452095032, 0.9872454404830933, 0.9823830127716064, 0.9801069498062134, 0.9765642285346985, 0.9770943522453308, 0.972015917301178, 0.967376172542572, 0.9688082933425903, 0.9700025916099548, 0.9656534194946289, 0.9572213888168335, 0.9555065631866455, 0.9517180323600769, 0.9486594796180725, 0.9452823400497437, 0.9644129872322083, 0.9393441677093506, 0.9381879568099976, 0.9392849802970886, 0.9349914789199829, 0.9294201731681824, 0.9250491857528687, 0.9320662021636963, 0.9274954795837402, 0.9177232384681702, 0.9157988429069519, 0.9190945029258728, 0.9146069288253784, 0.9097973704338074, 0.9094502925872803, 0.9051330089569092, 0.9059246182441711, 0.8975130915641785, 0.8954722285270691, 0.9027907848358154, 0.8966058492660522, 0.8953944444656372, 0.8883170485496521, 0.886643648147583, 0.9021856188774109, 0.8861697316169739, 0.8838422298431396, 0.8772203922271729, 0.8736729025840759], 'val_accuracy': [0.5882353186607361, 0.5248869061470032, 0.6323529481887817, 0.6821267008781433, 0.7160633206367493, 0.6957013607025146, 0.7171945571899414, 0.7149321436882019, 0.720588207244873, 0.7398189902305603, 0.7454751133918762, 0.7488687634468079, 0.7567873597145081, 0.7613122463226318, 0.733031690120697, 0.7432126402854919, 0.7443438768386841, 0.7420814633369446, 0.7635746598243713, 0.7409502267837524, 0.7613122463226318, 0.7522624731063843, 0.766968309879303, 0.7658371329307556, 0.7658371329307556, 0.7771493196487427, 0.7748869061470032, 0.790723979473114, 0.7782805562019348, 0.7873303294181824, 0.7918552160263062, 0.7861990928649902, 0.7861990928649902, 0.773755669593811, 0.7918552160263062, 0.7895927429199219, 0.7952488660812378, 0.7929864525794983, 0.7975113391876221, 0.8009049892425537, 0.8009049892425537, 0.7873303294181824, 0.8009049892425537, 0.8009049892425537, 0.8020362257957458, 0.807692289352417, 0.7963801026344299, 0.8042986392974854, 0.773755669593811, 0.7873303294181824, 0.7963801026344299, 0.8009049892425537, 0.8020362257957458, 0.8110859990119934, 0.8065611124038696, 0.8065611124038696, 0.8031674027442932, 0.8099547624588013, 0.8042986392974854, 0.8088235259056091, 0.8031674027442932, 0.8122171759605408, 0.814479649066925, 0.8042986392974854, 0.8031674027442932, 0.8042986392974854, 0.807692289352417, 0.8110859990119934, 0.8088235259056091, 0.8099547624588013, 0.807692289352417, 0.7918552160263062, 0.8088235259056091, 0.8065611124038696, 0.8065611124038696, 0.8088235259056091, 0.8065611124038696, 0.8088235259056091, 0.807692289352417, 0.8133484125137329, 0.8122171759605408, 0.8088235259056091, 0.807692289352417, 0.8099547624588013, 0.8065611124038696, 0.8065611124038696, 0.8099547624588013, 0.8156108856201172, 0.8133484125137329, 0.8099547624588013, 0.8031674027442932, 0.8088235259056091, 0.8054298758506775, 0.8099547624588013, 0.8133484125137329, 0.8122171759605408, 0.8088235259056091, 0.8122171759605408, 0.8156108856201172, 0.8156108856201172]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - 8s 86ms/step - loss: 1.4295 - accuracy: 0.5592 - val_loss: 1.4278 - val_accuracy: 0.4855\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.4192 - accuracy: 0.6323 - val_loss: 1.4218 - val_accuracy: 0.4845\n","Epoch 3/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.4053 - accuracy: 0.6628 - val_loss: 1.4157 - val_accuracy: 0.4845\n","Epoch 4/100\n","31/31 [==============================] - 1s 26ms/step - loss: 1.3839 - accuracy: 0.6829 - val_loss: 1.4083 - val_accuracy: 0.6436\n","Epoch 5/100\n","31/31 [==============================] - 1s 26ms/step - loss: 1.3542 - accuracy: 0.6941 - val_loss: 1.3990 - val_accuracy: 0.6808\n","Epoch 6/100\n","31/31 [==============================] - 1s 28ms/step - loss: 1.3217 - accuracy: 0.7054 - val_loss: 1.3865 - val_accuracy: 0.7097\n","Epoch 7/100\n","31/31 [==============================] - 1s 35ms/step - loss: 1.2906 - accuracy: 0.7168 - val_loss: 1.3732 - val_accuracy: 0.7159\n","Epoch 8/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.2657 - accuracy: 0.7274 - val_loss: 1.3583 - val_accuracy: 0.7304\n","Epoch 9/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.2413 - accuracy: 0.7408 - val_loss: 1.3435 - val_accuracy: 0.7355\n","Epoch 10/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.2220 - accuracy: 0.7509 - val_loss: 1.3258 - val_accuracy: 0.7355\n","Epoch 11/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.2009 - accuracy: 0.7685 - val_loss: 1.3085 - val_accuracy: 0.7438\n","Epoch 12/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1870 - accuracy: 0.7703 - val_loss: 1.2850 - val_accuracy: 0.7624\n","Epoch 13/100\n","31/31 [==============================] - 0s 15ms/step - loss: 1.1734 - accuracy: 0.7674 - val_loss: 1.2653 - val_accuracy: 0.7552\n","Epoch 14/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1563 - accuracy: 0.7827 - val_loss: 1.2464 - val_accuracy: 0.7562\n","Epoch 15/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.1418 - accuracy: 0.7899 - val_loss: 1.2189 - val_accuracy: 0.7686\n","Epoch 16/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1337 - accuracy: 0.7845 - val_loss: 1.1937 - val_accuracy: 0.7779\n","Epoch 17/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1223 - accuracy: 0.7930 - val_loss: 1.1785 - val_accuracy: 0.7779\n","Epoch 18/100\n","31/31 [==============================] - 1s 21ms/step - loss: 1.1213 - accuracy: 0.7868 - val_loss: 1.1598 - val_accuracy: 0.7831\n","Epoch 19/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.1082 - accuracy: 0.7935 - val_loss: 1.1628 - val_accuracy: 0.7665\n","Epoch 20/100\n","31/31 [==============================] - 1s 22ms/step - loss: 1.1014 - accuracy: 0.7987 - val_loss: 1.1405 - val_accuracy: 0.7862\n","Epoch 21/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0926 - accuracy: 0.7992 - val_loss: 1.1232 - val_accuracy: 0.7893\n","Epoch 22/100\n","31/31 [==============================] - 0s 13ms/step - loss: 1.0851 - accuracy: 0.8028 - val_loss: 1.1243 - val_accuracy: 0.7872\n","Epoch 23/100\n","31/31 [==============================] - 1s 19ms/step - loss: 1.0808 - accuracy: 0.8034 - val_loss: 1.1164 - val_accuracy: 0.7913\n","Epoch 24/100\n","31/31 [==============================] - 1s 24ms/step - loss: 1.0671 - accuracy: 0.8124 - val_loss: 1.0878 - val_accuracy: 0.8017\n","Epoch 25/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0637 - accuracy: 0.8093 - val_loss: 1.0819 - val_accuracy: 0.7965\n","Epoch 26/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0616 - accuracy: 0.8101 - val_loss: 1.0792 - val_accuracy: 0.8017\n","Epoch 27/100\n","31/31 [==============================] - 1s 24ms/step - loss: 1.0552 - accuracy: 0.8067 - val_loss: 1.0687 - val_accuracy: 0.8110\n","Epoch 28/100\n","31/31 [==============================] - 1s 16ms/step - loss: 1.0472 - accuracy: 0.8171 - val_loss: 1.0686 - val_accuracy: 0.8068\n","Epoch 29/100\n","31/31 [==============================] - 1s 18ms/step - loss: 1.0468 - accuracy: 0.8132 - val_loss: 1.0588 - val_accuracy: 0.8089\n","Epoch 30/100\n","31/31 [==============================] - 1s 17ms/step - loss: 1.0328 - accuracy: 0.8129 - val_loss: 1.0615 - val_accuracy: 0.8017\n","Epoch 31/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0296 - accuracy: 0.8178 - val_loss: 1.0566 - val_accuracy: 0.8037\n","Epoch 32/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0207 - accuracy: 0.8191 - val_loss: 1.0460 - val_accuracy: 0.8058\n","Epoch 33/100\n","31/31 [==============================] - 0s 12ms/step - loss: 1.0116 - accuracy: 0.8284 - val_loss: 1.0397 - val_accuracy: 0.8068\n","Epoch 34/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0114 - accuracy: 0.8189 - val_loss: 1.0524 - val_accuracy: 0.8037\n","Epoch 35/100\n","31/31 [==============================] - 0s 14ms/step - loss: 1.0041 - accuracy: 0.8220 - val_loss: 1.0331 - val_accuracy: 0.8058\n","Epoch 36/100\n","31/31 [==============================] - 1s 20ms/step - loss: 1.0006 - accuracy: 0.8233 - val_loss: 1.0227 - val_accuracy: 0.8120\n","Epoch 37/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9938 - accuracy: 0.8220 - val_loss: 1.0257 - val_accuracy: 0.8068\n","Epoch 38/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9890 - accuracy: 0.8222 - val_loss: 1.0236 - val_accuracy: 0.8079\n","Epoch 39/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.9850 - accuracy: 0.8261 - val_loss: 1.0342 - val_accuracy: 0.8089\n","Epoch 40/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.9787 - accuracy: 0.8256 - val_loss: 1.0062 - val_accuracy: 0.8130\n","Epoch 41/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.9710 - accuracy: 0.8320 - val_loss: 1.0052 - val_accuracy: 0.8151\n","Epoch 42/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9708 - accuracy: 0.8282 - val_loss: 0.9975 - val_accuracy: 0.8140\n","Epoch 43/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.9638 - accuracy: 0.8287 - val_loss: 0.9953 - val_accuracy: 0.8151\n","Epoch 44/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.9554 - accuracy: 0.8344 - val_loss: 1.0102 - val_accuracy: 0.8048\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9501 - accuracy: 0.8372 - val_loss: 1.0026 - val_accuracy: 0.8089\n","Epoch 46/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9483 - accuracy: 0.8333 - val_loss: 0.9895 - val_accuracy: 0.8130\n","Epoch 47/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9462 - accuracy: 0.8339 - val_loss: 0.9784 - val_accuracy: 0.8151\n","Epoch 48/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.9421 - accuracy: 0.8367 - val_loss: 0.9877 - val_accuracy: 0.8110\n","Epoch 49/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.9300 - accuracy: 0.8463 - val_loss: 0.9712 - val_accuracy: 0.8161\n","Epoch 50/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.9327 - accuracy: 0.8421 - val_loss: 0.9705 - val_accuracy: 0.8182\n","Epoch 51/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.9269 - accuracy: 0.8406 - val_loss: 0.9630 - val_accuracy: 0.8161\n","Epoch 52/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9196 - accuracy: 0.8434 - val_loss: 0.9616 - val_accuracy: 0.8171\n","Epoch 53/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.9141 - accuracy: 0.8416 - val_loss: 0.9574 - val_accuracy: 0.8192\n","Epoch 54/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.9100 - accuracy: 0.8411 - val_loss: 0.9531 - val_accuracy: 0.8182\n","Epoch 55/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.9046 - accuracy: 0.8457 - val_loss: 0.9485 - val_accuracy: 0.8202\n","Epoch 56/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.9006 - accuracy: 0.8450 - val_loss: 0.9592 - val_accuracy: 0.8140\n","Epoch 57/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.9018 - accuracy: 0.8457 - val_loss: 0.9441 - val_accuracy: 0.8213\n","Epoch 58/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8931 - accuracy: 0.8444 - val_loss: 0.9481 - val_accuracy: 0.8161\n","Epoch 59/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8908 - accuracy: 0.8496 - val_loss: 0.9385 - val_accuracy: 0.8192\n","Epoch 60/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.8874 - accuracy: 0.8475 - val_loss: 0.9322 - val_accuracy: 0.8223\n","Epoch 61/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8869 - accuracy: 0.8429 - val_loss: 0.9305 - val_accuracy: 0.8192\n","Epoch 62/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8772 - accuracy: 0.8501 - val_loss: 0.9254 - val_accuracy: 0.8223\n","Epoch 63/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8761 - accuracy: 0.8494 - val_loss: 0.9216 - val_accuracy: 0.8223\n","Epoch 64/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8773 - accuracy: 0.8491 - val_loss: 0.9179 - val_accuracy: 0.8223\n","Epoch 65/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.8740 - accuracy: 0.8447 - val_loss: 0.9196 - val_accuracy: 0.8233\n","Epoch 66/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8696 - accuracy: 0.8460 - val_loss: 0.9241 - val_accuracy: 0.8223\n","Epoch 67/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.8612 - accuracy: 0.8491 - val_loss: 0.9091 - val_accuracy: 0.8244\n","Epoch 68/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8541 - accuracy: 0.8525 - val_loss: 0.9240 - val_accuracy: 0.8171\n","Epoch 69/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8520 - accuracy: 0.8514 - val_loss: 0.9175 - val_accuracy: 0.8171\n","Epoch 70/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8445 - accuracy: 0.8522 - val_loss: 0.9043 - val_accuracy: 0.8233\n","Epoch 71/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8412 - accuracy: 0.8576 - val_loss: 0.9023 - val_accuracy: 0.8223\n","Epoch 72/100\n","31/31 [==============================] - 1s 31ms/step - loss: 0.8367 - accuracy: 0.8540 - val_loss: 0.8936 - val_accuracy: 0.8275\n","Epoch 73/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8308 - accuracy: 0.8550 - val_loss: 0.8903 - val_accuracy: 0.8264\n","Epoch 74/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8311 - accuracy: 0.8561 - val_loss: 0.8913 - val_accuracy: 0.8223\n","Epoch 75/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8315 - accuracy: 0.8558 - val_loss: 0.8878 - val_accuracy: 0.8275\n","Epoch 76/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.8214 - accuracy: 0.8599 - val_loss: 0.8968 - val_accuracy: 0.8254\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8240 - accuracy: 0.8550 - val_loss: 0.8772 - val_accuracy: 0.8264\n","Epoch 78/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.8135 - accuracy: 0.8561 - val_loss: 0.8886 - val_accuracy: 0.8264\n","Epoch 79/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8107 - accuracy: 0.8587 - val_loss: 0.8845 - val_accuracy: 0.8213\n","Epoch 80/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.8102 - accuracy: 0.8610 - val_loss: 0.8773 - val_accuracy: 0.8202\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.8069 - accuracy: 0.8633 - val_loss: 0.8672 - val_accuracy: 0.8275\n","Epoch 82/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.7975 - accuracy: 0.8628 - val_loss: 0.8690 - val_accuracy: 0.8202\n","Epoch 83/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7980 - accuracy: 0.8605 - val_loss: 0.8653 - val_accuracy: 0.8233\n","Epoch 84/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.7921 - accuracy: 0.8605 - val_loss: 0.8614 - val_accuracy: 0.8295\n","Epoch 85/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.7914 - accuracy: 0.8610 - val_loss: 0.8603 - val_accuracy: 0.8244\n","Epoch 86/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7878 - accuracy: 0.8664 - val_loss: 0.8616 - val_accuracy: 0.8316\n","Epoch 87/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7870 - accuracy: 0.8599 - val_loss: 0.8485 - val_accuracy: 0.8275\n","Epoch 88/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7763 - accuracy: 0.8638 - val_loss: 0.8478 - val_accuracy: 0.8295\n","Epoch 89/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7731 - accuracy: 0.8711 - val_loss: 0.8548 - val_accuracy: 0.8244\n","Epoch 90/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7703 - accuracy: 0.8664 - val_loss: 0.8550 - val_accuracy: 0.8202\n","Epoch 91/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7790 - accuracy: 0.8610 - val_loss: 0.8469 - val_accuracy: 0.8347\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7677 - accuracy: 0.8721 - val_loss: 0.8375 - val_accuracy: 0.8337\n","Epoch 93/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7582 - accuracy: 0.8700 - val_loss: 0.8388 - val_accuracy: 0.8275\n","Epoch 94/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7580 - accuracy: 0.8677 - val_loss: 0.8610 - val_accuracy: 0.8254\n","Epoch 95/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7649 - accuracy: 0.8646 - val_loss: 0.8265 - val_accuracy: 0.8244\n","Epoch 96/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7489 - accuracy: 0.8739 - val_loss: 0.8275 - val_accuracy: 0.8306\n","Epoch 97/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7465 - accuracy: 0.8752 - val_loss: 0.8245 - val_accuracy: 0.8316\n","Epoch 98/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.7558 - accuracy: 0.8656 - val_loss: 0.8248 - val_accuracy: 0.8295\n","Epoch 99/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7375 - accuracy: 0.8693 - val_loss: 0.8502 - val_accuracy: 0.8254\n","Epoch 100/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.7381 - accuracy: 0.8708 - val_loss: 0.8202 - val_accuracy: 0.8264\n","{'loss': [1.4294514656066895, 1.4192030429840088, 1.4052764177322388, 1.3838706016540527, 1.354195475578308, 1.321740746498108, 1.2905664443969727, 1.265741229057312, 1.2413403987884521, 1.2220412492752075, 1.2008720636367798, 1.1869747638702393, 1.1733559370040894, 1.1562854051589966, 1.1418125629425049, 1.1337028741836548, 1.1222820281982422, 1.121345043182373, 1.108182430267334, 1.1013764142990112, 1.0926285982131958, 1.0851082801818848, 1.0808054208755493, 1.0671125650405884, 1.0637388229370117, 1.0615891218185425, 1.055240273475647, 1.0472179651260376, 1.0468448400497437, 1.032801628112793, 1.0295873880386353, 1.0207027196884155, 1.011605978012085, 1.0113567113876343, 1.0041413307189941, 1.000569462776184, 0.9938464164733887, 0.9890292286872864, 0.9849941730499268, 0.9787183403968811, 0.9710426330566406, 0.9707896113395691, 0.9638140797615051, 0.9553540349006653, 0.9500815868377686, 0.9483076930046082, 0.9462347030639648, 0.9420868754386902, 0.9300420880317688, 0.9327185153961182, 0.9269337058067322, 0.9195833802223206, 0.914111316204071, 0.9099863171577454, 0.9046331644058228, 0.9006326198577881, 0.9017516374588013, 0.8930879831314087, 0.8907522559165955, 0.8873936533927917, 0.8868502378463745, 0.8772244453430176, 0.8761202096939087, 0.877348005771637, 0.8740255832672119, 0.8695966601371765, 0.8611592650413513, 0.8540827035903931, 0.8519997596740723, 0.844470202922821, 0.8411857485771179, 0.8366899490356445, 0.8308197855949402, 0.8310688138008118, 0.83149653673172, 0.8214223384857178, 0.8239941000938416, 0.813541054725647, 0.8107122778892517, 0.8102285265922546, 0.8069437742233276, 0.7974662780761719, 0.7979944944381714, 0.7920708656311035, 0.7913963794708252, 0.7877717614173889, 0.7870458960533142, 0.7763229608535767, 0.7731325030326843, 0.7702715396881104, 0.7789919972419739, 0.7677067518234253, 0.7582141757011414, 0.7580133080482483, 0.7648894190788269, 0.7488993406295776, 0.7464994788169861, 0.7557775378227234, 0.7375465035438538, 0.7381060123443604], 'accuracy': [0.5591731071472168, 0.6322997212409973, 0.6627907156944275, 0.682945728302002, 0.6940568685531616, 0.7054263353347778, 0.7167958617210388, 0.7273901700973511, 0.7408268451690674, 0.750904381275177, 0.7684754729270935, 0.7702842354774475, 0.7674418687820435, 0.7826873660087585, 0.7899224758148193, 0.7844961285591125, 0.7930232286453247, 0.786821722984314, 0.7935400605201721, 0.7987080216407776, 0.7992247939109802, 0.802842378616333, 0.8033591508865356, 0.8124030828475952, 0.8093023300170898, 0.8100775480270386, 0.8067183494567871, 0.817054271697998, 0.813178300857544, 0.8129199147224426, 0.817829430103302, 0.8191214203834534, 0.828423798084259, 0.818863034248352, 0.8219638466835022, 0.8232558369636536, 0.8219638466835022, 0.8222222328186035, 0.8260982036590576, 0.8255813717842102, 0.832041323184967, 0.8281653523445129, 0.8286821842193604, 0.8343669176101685, 0.8372092843055725, 0.8333333134651184, 0.8338501453399658, 0.8366925120353699, 0.8462532162666321, 0.8421188592910767, 0.840568482875824, 0.843410849571228, 0.841602087020874, 0.8410852551460266, 0.8457364439964294, 0.8449612259864807, 0.8457364439964294, 0.8444444537162781, 0.8496124148368835, 0.8475452065467834, 0.8428940773010254, 0.8501291871070862, 0.8493540287017822, 0.8490955829620361, 0.8447028398513794, 0.8459948301315308, 0.8490955829620361, 0.8524547815322876, 0.8514211773872375, 0.8521963953971863, 0.8576227426528931, 0.8540051579475403, 0.8550387620925903, 0.8560723662376404, 0.8558139801025391, 0.8599483370780945, 0.8550387620925903, 0.8560723662376404, 0.8586563467979431, 0.8609819412231445, 0.8633074760437012, 0.8627907037734985, 0.8604651093482971, 0.8604651093482971, 0.8609819412231445, 0.8664082884788513, 0.8599483370780945, 0.8638243079185486, 0.8710594177246094, 0.8664082884788513, 0.8609819412231445, 0.8720930218696594, 0.8700258135795593, 0.8677002787590027, 0.8645994663238525, 0.8739017844200134, 0.8751937747001648, 0.8656330704689026, 0.8692506551742554, 0.8708010315895081], 'val_loss': [1.427762508392334, 1.421830177307129, 1.4157227277755737, 1.408272385597229, 1.3989542722702026, 1.3864991664886475, 1.3731930255889893, 1.3582907915115356, 1.3434622287750244, 1.3258074522018433, 1.30851149559021, 1.2850052118301392, 1.265302300453186, 1.2464487552642822, 1.218923807144165, 1.1937451362609863, 1.1784683465957642, 1.15981125831604, 1.1628365516662598, 1.140511393547058, 1.1231567859649658, 1.124336838722229, 1.1164326667785645, 1.0877535343170166, 1.0819014310836792, 1.0791590213775635, 1.0687059164047241, 1.0685962438583374, 1.0587925910949707, 1.0615298748016357, 1.0565751791000366, 1.04600191116333, 1.0396733283996582, 1.0523509979248047, 1.0331482887268066, 1.0227049589157104, 1.0256710052490234, 1.0236393213272095, 1.0342024564743042, 1.006179928779602, 1.00518798828125, 0.9974564909934998, 0.9952608346939087, 1.0101542472839355, 1.0025883913040161, 0.989452064037323, 0.9783791899681091, 0.9876694083213806, 0.9712489247322083, 0.9704568386077881, 0.9630362391471863, 0.9615899920463562, 0.9573799967765808, 0.9531163573265076, 0.9484611749649048, 0.9592471122741699, 0.9440690279006958, 0.9480816125869751, 0.9384857416152954, 0.9322012662887573, 0.9305387139320374, 0.9254180788993835, 0.9216428399085999, 0.9179372787475586, 0.9196246266365051, 0.9241368174552917, 0.9090913534164429, 0.9239608645439148, 0.9175050258636475, 0.9042940735816956, 0.9022643566131592, 0.8935588002204895, 0.8903189301490784, 0.8913472294807434, 0.8877618908882141, 0.8968220353126526, 0.8771870136260986, 0.8885622024536133, 0.8844550848007202, 0.8773163557052612, 0.8672429323196411, 0.8689735531806946, 0.8653091788291931, 0.861422061920166, 0.86034095287323, 0.8616052269935608, 0.8485422730445862, 0.8477675914764404, 0.8547991514205933, 0.855008602142334, 0.8469092845916748, 0.8374768495559692, 0.8387869000434875, 0.8609991073608398, 0.8264920711517334, 0.827465295791626, 0.8245109915733337, 0.8248343467712402, 0.8502216935157776, 0.8202137351036072], 'val_accuracy': [0.48553720116615295, 0.4845041334629059, 0.4845041334629059, 0.6435950398445129, 0.6807851195335388, 0.7097107172012329, 0.7159090638160706, 0.73037189245224, 0.7355371713638306, 0.7355371713638306, 0.7438016533851624, 0.7623966932296753, 0.7551652789115906, 0.7561983466148376, 0.7685950398445129, 0.7778925895690918, 0.7778925895690918, 0.7830578684806824, 0.7665289044380188, 0.7861570119857788, 0.78925621509552, 0.7871900796890259, 0.7913222908973694, 0.8016529083251953, 0.7964876294136047, 0.8016529083251953, 0.8109503984451294, 0.8068181872367859, 0.80888432264328, 0.8016529083251953, 0.8037189841270447, 0.8057851195335388, 0.8068181872367859, 0.8037189841270447, 0.8057851195335388, 0.8119834661483765, 0.8068181872367859, 0.807851254940033, 0.80888432264328, 0.8130165338516235, 0.8150826692581177, 0.8140496015548706, 0.8150826692581177, 0.8047520518302917, 0.80888432264328, 0.8130165338516235, 0.8150826692581177, 0.8109503984451294, 0.81611567735672, 0.8181818127632141, 0.81611567735672, 0.817148745059967, 0.8192148804664612, 0.8181818127632141, 0.8202479481697083, 0.8140496015548706, 0.8212810158729553, 0.81611567735672, 0.8192148804664612, 0.8223140239715576, 0.8192148804664612, 0.8223140239715576, 0.8223140239715576, 0.8223140239715576, 0.8233470916748047, 0.8223140239715576, 0.8243801593780518, 0.817148745059967, 0.817148745059967, 0.8233470916748047, 0.8223140239715576, 0.827479362487793, 0.8264462947845459, 0.8223140239715576, 0.827479362487793, 0.8254132270812988, 0.8264462947845459, 0.8264462947845459, 0.8212810158729553, 0.8202479481697083, 0.827479362487793, 0.8202479481697083, 0.8233470916748047, 0.8295454382896423, 0.8243801593780518, 0.8316115736961365, 0.827479362487793, 0.8295454382896423, 0.8243801593780518, 0.8202479481697083, 0.8347107172012329, 0.8336777091026306, 0.827479362487793, 0.8254132270812988, 0.8243801593780518, 0.8305785059928894, 0.8316115736961365, 0.8295454382896423, 0.8254132270812988, 0.8264462947845459]}\n","32/32 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 7s 49ms/step - loss: 0.8099 - accuracy: 0.8376 - val_loss: 1.1288 - val_accuracy: 0.4849\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["29/29 [==============================] - 0s 13ms/step - loss: 0.7928 - accuracy: 0.8489 - val_loss: 1.1135 - val_accuracy: 0.4849\n","Epoch 3/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.7896 - accuracy: 0.8502 - val_loss: 1.1001 - val_accuracy: 0.4849\n","Epoch 4/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.7867 - accuracy: 0.8475 - val_loss: 1.0838 - val_accuracy: 0.4957\n","Epoch 5/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.7853 - accuracy: 0.8435 - val_loss: 1.0740 - val_accuracy: 0.5097\n","Epoch 6/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.7781 - accuracy: 0.8502 - val_loss: 1.0590 - val_accuracy: 0.6692\n","Epoch 7/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.7788 - accuracy: 0.8505 - val_loss: 1.0417 - val_accuracy: 0.8211\n","Epoch 8/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7726 - accuracy: 0.8494 - val_loss: 1.0296 - val_accuracy: 0.7716\n","Epoch 9/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.7635 - accuracy: 0.8508 - val_loss: 1.0125 - val_accuracy: 0.8362\n","Epoch 10/100\n","29/29 [==============================] - 1s 45ms/step - loss: 0.7586 - accuracy: 0.8553 - val_loss: 0.9918 - val_accuracy: 0.8373\n","Epoch 11/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.7543 - accuracy: 0.8548 - val_loss: 0.9746 - val_accuracy: 0.8427\n","Epoch 12/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7565 - accuracy: 0.8526 - val_loss: 0.9530 - val_accuracy: 0.8362\n","Epoch 13/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.7512 - accuracy: 0.8529 - val_loss: 0.9319 - val_accuracy: 0.8459\n","Epoch 14/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7511 - accuracy: 0.8513 - val_loss: 0.9031 - val_accuracy: 0.8384\n","Epoch 15/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7409 - accuracy: 0.8586 - val_loss: 0.8781 - val_accuracy: 0.8459\n","Epoch 16/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.7332 - accuracy: 0.8648 - val_loss: 0.8625 - val_accuracy: 0.8179\n","Epoch 17/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.7324 - accuracy: 0.8615 - val_loss: 0.8315 - val_accuracy: 0.8233\n","Epoch 18/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7298 - accuracy: 0.8596 - val_loss: 0.8170 - val_accuracy: 0.8211\n","Epoch 19/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7212 - accuracy: 0.8640 - val_loss: 0.7881 - val_accuracy: 0.8384\n","Epoch 20/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.7208 - accuracy: 0.8650 - val_loss: 0.7871 - val_accuracy: 0.8222\n","Epoch 21/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7185 - accuracy: 0.8621 - val_loss: 0.7828 - val_accuracy: 0.8222\n","Epoch 22/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7119 - accuracy: 0.8661 - val_loss: 0.7648 - val_accuracy: 0.8330\n","Epoch 23/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.7073 - accuracy: 0.8650 - val_loss: 0.7560 - val_accuracy: 0.8362\n","Epoch 24/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.7030 - accuracy: 0.8685 - val_loss: 0.7420 - val_accuracy: 0.8394\n","Epoch 25/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6978 - accuracy: 0.8707 - val_loss: 0.7307 - val_accuracy: 0.8438\n","Epoch 26/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6979 - accuracy: 0.8734 - val_loss: 0.7211 - val_accuracy: 0.8524\n","Epoch 27/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6953 - accuracy: 0.8642 - val_loss: 0.7590 - val_accuracy: 0.8287\n","Epoch 28/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.6949 - accuracy: 0.8685 - val_loss: 0.7141 - val_accuracy: 0.8599\n","Epoch 29/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6843 - accuracy: 0.8772 - val_loss: 0.7131 - val_accuracy: 0.8502\n","Epoch 30/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6814 - accuracy: 0.8774 - val_loss: 0.7297 - val_accuracy: 0.8405\n","Epoch 31/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6835 - accuracy: 0.8728 - val_loss: 0.7094 - val_accuracy: 0.8502\n","Epoch 32/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.6741 - accuracy: 0.8782 - val_loss: 0.7082 - val_accuracy: 0.8599\n","Epoch 33/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.6714 - accuracy: 0.8763 - val_loss: 0.7028 - val_accuracy: 0.8610\n","Epoch 34/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6656 - accuracy: 0.8774 - val_loss: 0.7028 - val_accuracy: 0.8578\n","Epoch 35/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6676 - accuracy: 0.8788 - val_loss: 0.7009 - val_accuracy: 0.8578\n","Epoch 36/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6603 - accuracy: 0.8774 - val_loss: 0.6967 - val_accuracy: 0.8588\n","Epoch 37/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6567 - accuracy: 0.8817 - val_loss: 0.6983 - val_accuracy: 0.8578\n","Epoch 38/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6532 - accuracy: 0.8785 - val_loss: 0.6943 - val_accuracy: 0.8578\n","Epoch 39/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6535 - accuracy: 0.8825 - val_loss: 0.7079 - val_accuracy: 0.8556\n","Epoch 40/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6513 - accuracy: 0.8828 - val_loss: 0.6936 - val_accuracy: 0.8588\n","Epoch 41/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6424 - accuracy: 0.8866 - val_loss: 0.7004 - val_accuracy: 0.8513\n","Epoch 42/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6411 - accuracy: 0.8839 - val_loss: 0.6882 - val_accuracy: 0.8599\n","Epoch 43/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6365 - accuracy: 0.8871 - val_loss: 0.6956 - val_accuracy: 0.8534\n","Epoch 44/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6350 - accuracy: 0.8860 - val_loss: 0.6946 - val_accuracy: 0.8545\n","Epoch 45/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6311 - accuracy: 0.8858 - val_loss: 0.6891 - val_accuracy: 0.8556\n","Epoch 46/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6249 - accuracy: 0.8936 - val_loss: 0.7049 - val_accuracy: 0.8578\n","Epoch 47/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6262 - accuracy: 0.8901 - val_loss: 0.6967 - val_accuracy: 0.8470\n","Epoch 48/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.6247 - accuracy: 0.8885 - val_loss: 0.6906 - val_accuracy: 0.8513\n","Epoch 49/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.6251 - accuracy: 0.8823 - val_loss: 0.6798 - val_accuracy: 0.8621\n","Epoch 50/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.6180 - accuracy: 0.8922 - val_loss: 0.6886 - val_accuracy: 0.8534\n","Epoch 51/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.6092 - accuracy: 0.8963 - val_loss: 0.6783 - val_accuracy: 0.8631\n","Epoch 52/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6034 - accuracy: 0.8955 - val_loss: 0.6792 - val_accuracy: 0.8631\n","Epoch 53/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.6055 - accuracy: 0.8944 - val_loss: 0.6797 - val_accuracy: 0.8578\n","Epoch 54/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.6027 - accuracy: 0.8901 - val_loss: 0.6745 - val_accuracy: 0.8567\n","Epoch 55/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5942 - accuracy: 0.9006 - val_loss: 0.6707 - val_accuracy: 0.8642\n","Epoch 56/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5928 - accuracy: 0.8998 - val_loss: 0.6880 - val_accuracy: 0.8545\n","Epoch 57/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5957 - accuracy: 0.8982 - val_loss: 0.6865 - val_accuracy: 0.8534\n","Epoch 58/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5808 - accuracy: 0.9062 - val_loss: 0.6742 - val_accuracy: 0.8556\n","Epoch 59/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5898 - accuracy: 0.8960 - val_loss: 0.6804 - val_accuracy: 0.8556\n","Epoch 60/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.5836 - accuracy: 0.8998 - val_loss: 0.6704 - val_accuracy: 0.8599\n","Epoch 61/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5820 - accuracy: 0.8990 - val_loss: 0.6934 - val_accuracy: 0.8416\n","Epoch 62/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5827 - accuracy: 0.8992 - val_loss: 0.6665 - val_accuracy: 0.8642\n","Epoch 63/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.5713 - accuracy: 0.9038 - val_loss: 0.6636 - val_accuracy: 0.8664\n","Epoch 64/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5731 - accuracy: 0.9025 - val_loss: 0.6630 - val_accuracy: 0.8653\n","Epoch 65/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5724 - accuracy: 0.9049 - val_loss: 0.7254 - val_accuracy: 0.8308\n","Epoch 66/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5649 - accuracy: 0.9049 - val_loss: 0.6681 - val_accuracy: 0.8653\n","Epoch 67/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5557 - accuracy: 0.9089 - val_loss: 0.6637 - val_accuracy: 0.8599\n","Epoch 68/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5568 - accuracy: 0.9106 - val_loss: 0.6562 - val_accuracy: 0.8631\n","Epoch 69/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5469 - accuracy: 0.9098 - val_loss: 0.6609 - val_accuracy: 0.8642\n","Epoch 70/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5516 - accuracy: 0.9111 - val_loss: 0.6779 - val_accuracy: 0.8502\n","Epoch 71/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5525 - accuracy: 0.9038 - val_loss: 0.6840 - val_accuracy: 0.8459\n","Epoch 72/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5659 - accuracy: 0.9006 - val_loss: 0.6538 - val_accuracy: 0.8653\n","Epoch 73/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5440 - accuracy: 0.9103 - val_loss: 0.6626 - val_accuracy: 0.8610\n","Epoch 74/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5400 - accuracy: 0.9124 - val_loss: 0.6522 - val_accuracy: 0.8653\n","Epoch 75/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5380 - accuracy: 0.9197 - val_loss: 0.6536 - val_accuracy: 0.8664\n","Epoch 76/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5322 - accuracy: 0.9141 - val_loss: 0.6605 - val_accuracy: 0.8621\n","Epoch 77/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5319 - accuracy: 0.9149 - val_loss: 0.6492 - val_accuracy: 0.8599\n","Epoch 78/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5286 - accuracy: 0.9157 - val_loss: 0.6472 - val_accuracy: 0.8664\n","Epoch 79/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5230 - accuracy: 0.9189 - val_loss: 0.6522 - val_accuracy: 0.8610\n","Epoch 80/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5257 - accuracy: 0.9184 - val_loss: 0.6499 - val_accuracy: 0.8664\n","Epoch 81/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5167 - accuracy: 0.9186 - val_loss: 0.6621 - val_accuracy: 0.8556\n","Epoch 82/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5267 - accuracy: 0.9146 - val_loss: 0.6472 - val_accuracy: 0.8599\n","Epoch 83/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5177 - accuracy: 0.9205 - val_loss: 0.6657 - val_accuracy: 0.8610\n","Epoch 84/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5205 - accuracy: 0.9157 - val_loss: 0.6471 - val_accuracy: 0.8588\n","Epoch 85/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.5052 - accuracy: 0.9208 - val_loss: 0.6485 - val_accuracy: 0.8621\n","Epoch 86/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.5005 - accuracy: 0.9246 - val_loss: 0.6569 - val_accuracy: 0.8631\n","Epoch 87/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4977 - accuracy: 0.9192 - val_loss: 0.6602 - val_accuracy: 0.8621\n","Epoch 88/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5145 - accuracy: 0.9089 - val_loss: 0.6615 - val_accuracy: 0.8631\n","Epoch 89/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4900 - accuracy: 0.9297 - val_loss: 0.6565 - val_accuracy: 0.8621\n","Epoch 90/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5056 - accuracy: 0.9211 - val_loss: 0.6481 - val_accuracy: 0.8621\n","Epoch 91/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.5079 - accuracy: 0.9181 - val_loss: 0.7066 - val_accuracy: 0.8405\n","Epoch 92/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4932 - accuracy: 0.9259 - val_loss: 0.6456 - val_accuracy: 0.8642\n","Epoch 93/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4843 - accuracy: 0.9305 - val_loss: 0.6452 - val_accuracy: 0.8588\n","Epoch 94/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4836 - accuracy: 0.9275 - val_loss: 0.6647 - val_accuracy: 0.8599\n","Epoch 95/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4944 - accuracy: 0.9227 - val_loss: 0.6533 - val_accuracy: 0.8642\n","Epoch 96/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4789 - accuracy: 0.9305 - val_loss: 0.6457 - val_accuracy: 0.8610\n","Epoch 97/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4886 - accuracy: 0.9208 - val_loss: 0.6627 - val_accuracy: 0.8599\n","Epoch 98/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4764 - accuracy: 0.9294 - val_loss: 0.6430 - val_accuracy: 0.8610\n","Epoch 99/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4677 - accuracy: 0.9356 - val_loss: 0.6411 - val_accuracy: 0.8642\n","Epoch 100/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4761 - accuracy: 0.9256 - val_loss: 0.6416 - val_accuracy: 0.8588\n","{'loss': [0.809852659702301, 0.7927951216697693, 0.7896280884742737, 0.7866776585578918, 0.7852692604064941, 0.7781370878219604, 0.7788258194923401, 0.7726199626922607, 0.7634568214416504, 0.7585994601249695, 0.754305899143219, 0.7565057277679443, 0.7511981725692749, 0.7511334419250488, 0.7409483790397644, 0.7331600189208984, 0.732405960559845, 0.7297738194465637, 0.7211673259735107, 0.7208473086357117, 0.7185345888137817, 0.711854875087738, 0.7072741389274597, 0.7030328512191772, 0.6977764368057251, 0.6978774666786194, 0.6953380107879639, 0.6949080228805542, 0.6842910051345825, 0.6813833117485046, 0.6835135221481323, 0.6741271615028381, 0.6714054346084595, 0.6655799150466919, 0.6675520539283752, 0.6603405475616455, 0.6567366123199463, 0.6531693935394287, 0.6535404920578003, 0.6512898802757263, 0.6424247622489929, 0.641083836555481, 0.6364985108375549, 0.6349799633026123, 0.6311011910438538, 0.6249463558197021, 0.6262032985687256, 0.6246865391731262, 0.6250737309455872, 0.6179841756820679, 0.6092385053634644, 0.603422224521637, 0.6054558157920837, 0.6026948094367981, 0.5942041873931885, 0.5927556157112122, 0.5956693291664124, 0.5807704925537109, 0.5897873640060425, 0.5836438536643982, 0.5819830298423767, 0.5826626420021057, 0.5712729692459106, 0.5730665922164917, 0.5724361538887024, 0.5649214386940002, 0.5556533336639404, 0.5567875504493713, 0.5469292998313904, 0.5516397953033447, 0.5525320768356323, 0.5658570528030396, 0.5439624190330505, 0.5400480628013611, 0.537973940372467, 0.532201886177063, 0.5319019556045532, 0.5285592675209045, 0.5230284333229065, 0.5256807208061218, 0.5166669487953186, 0.5266846418380737, 0.5177392363548279, 0.52048659324646, 0.5051681399345398, 0.500543475151062, 0.49765563011169434, 0.5145047903060913, 0.48998701572418213, 0.5056106448173523, 0.507935643196106, 0.4932384192943573, 0.4843124449253082, 0.48360419273376465, 0.4943828582763672, 0.4789315462112427, 0.48864829540252686, 0.47636914253234863, 0.4676680266857147, 0.4760594069957733], 'accuracy': [0.837553858757019, 0.8488685488700867, 0.850215494632721, 0.8475215435028076, 0.8434805870056152, 0.850215494632721, 0.8504849076271057, 0.8494073152542114, 0.8507543206214905, 0.8553340435028076, 0.8547952771186829, 0.8526400923728943, 0.852909505367279, 0.8512930870056152, 0.8585668206214905, 0.8647629022598267, 0.8615301847457886, 0.8596444129943848, 0.8639547228813171, 0.8650323152542114, 0.8620689511299133, 0.8661099076271057, 0.8650323152542114, 0.868534505367279, 0.8706896305084229, 0.873383641242981, 0.8642241358757019, 0.868534505367279, 0.8771551847457886, 0.8774245977401733, 0.8728448152542114, 0.8782327771186829, 0.876347005367279, 0.8774245977401733, 0.8787715435028076, 0.8774245977401733, 0.8817349076271057, 0.8785021305084229, 0.8825430870056152, 0.8828125, 0.8865840435028076, 0.8838900923728943, 0.8871228694915771, 0.8860452771186829, 0.8857758641242981, 0.8935883641242981, 0.8900862336158752, 0.8884698152542114, 0.8822737336158752, 0.892241358757019, 0.8962823152542114, 0.8954741358757019, 0.8943965435028076, 0.8900862336158752, 0.9005926847457886, 0.899784505367279, 0.8981680870056152, 0.90625, 0.8960129022598267, 0.899784505367279, 0.8989762663841248, 0.8992456793785095, 0.9038254022598267, 0.9024784564971924, 0.904902994632721, 0.904902994632721, 0.9089439511299133, 0.9105603694915771, 0.9097521305084229, 0.9110991358757019, 0.9038254022598267, 0.9005926847457886, 0.9102909564971924, 0.912446141242981, 0.9197198152542114, 0.9140625, 0.9148706793785095, 0.915678858757019, 0.9189116358757019, 0.9183728694915771, 0.9186422228813171, 0.9146012663841248, 0.920527994632721, 0.915678858757019, 0.9207974076271057, 0.9245689511299133, 0.9191810488700867, 0.9089439511299133, 0.9296875, 0.9210668206214905, 0.9181034564971924, 0.9259159564971924, 0.9304956793785095, 0.9275323152542114, 0.9226831793785095, 0.9304956793785095, 0.9207974076271057, 0.9294180870056152, 0.9356142282485962, 0.9256465435028076], 'val_loss': [1.128781795501709, 1.113548994064331, 1.1000956296920776, 1.083801507949829, 1.0740422010421753, 1.059020757675171, 1.0416808128356934, 1.0296063423156738, 1.0124900341033936, 0.9918316602706909, 0.9745824337005615, 0.952969491481781, 0.9319329857826233, 0.9030600786209106, 0.8780540227890015, 0.8625063300132751, 0.8314944505691528, 0.8170353174209595, 0.7881178855895996, 0.7871031761169434, 0.7828016877174377, 0.7648345232009888, 0.7560164928436279, 0.7420273423194885, 0.7307067513465881, 0.7210850119590759, 0.75901859998703, 0.7140935063362122, 0.7131242156028748, 0.7296937108039856, 0.7094255089759827, 0.708175539970398, 0.7028002142906189, 0.7028483748435974, 0.7008897066116333, 0.6966932415962219, 0.6983404755592346, 0.6942906379699707, 0.7078837752342224, 0.6935799717903137, 0.7004377841949463, 0.6882243156433105, 0.695558488368988, 0.6945685148239136, 0.6890943050384521, 0.7048929929733276, 0.6966655850410461, 0.6905567049980164, 0.6797944903373718, 0.6886322498321533, 0.6782601475715637, 0.6791641116142273, 0.6796546578407288, 0.6744856834411621, 0.6707049012184143, 0.6880049705505371, 0.6864593625068665, 0.6742108464241028, 0.6803861856460571, 0.6703702807426453, 0.6933954954147339, 0.6665155291557312, 0.6636295318603516, 0.6629989743232727, 0.7253825068473816, 0.6681126356124878, 0.663650393486023, 0.6561963558197021, 0.660925567150116, 0.6779348850250244, 0.6839759349822998, 0.6537967324256897, 0.6626324653625488, 0.6522396802902222, 0.6535830497741699, 0.6604962944984436, 0.6491571664810181, 0.6471574902534485, 0.6522494554519653, 0.6498538255691528, 0.6620997786521912, 0.6472461223602295, 0.6656792163848877, 0.6470577716827393, 0.6485272645950317, 0.6569129824638367, 0.6601695418357849, 0.6615253686904907, 0.6565210223197937, 0.6480600237846375, 0.7066032886505127, 0.6456288695335388, 0.645195484161377, 0.6647294759750366, 0.6532666683197021, 0.6456573009490967, 0.6627368330955505, 0.6430053114891052, 0.6410775780677795, 0.641609787940979], 'val_accuracy': [0.48491379618644714, 0.48491379618644714, 0.48491379618644714, 0.49568966031074524, 0.5096982717514038, 0.6691810488700867, 0.8211206793785095, 0.7715517282485962, 0.8362069129943848, 0.837284505367279, 0.8426724076271057, 0.8362069129943848, 0.8459051847457886, 0.8383620977401733, 0.8459051847457886, 0.8178879022598267, 0.8232758641242981, 0.8211206793785095, 0.8383620977401733, 0.8221982717514038, 0.8221982717514038, 0.8329741358757019, 0.8362069129943848, 0.8394396305084229, 0.84375, 0.8523706793785095, 0.8286637663841248, 0.8599137663841248, 0.850215494632721, 0.8405172228813171, 0.850215494632721, 0.8599137663841248, 0.860991358757019, 0.857758641242981, 0.857758641242981, 0.8588362336158752, 0.857758641242981, 0.857758641242981, 0.8556034564971924, 0.8588362336158752, 0.8512930870056152, 0.8599137663841248, 0.8534482717514038, 0.8545258641242981, 0.8556034564971924, 0.857758641242981, 0.8469827771186829, 0.8512930870056152, 0.8620689511299133, 0.8534482717514038, 0.8631465435028076, 0.8631465435028076, 0.857758641242981, 0.8566810488700867, 0.8642241358757019, 0.8545258641242981, 0.8534482717514038, 0.8556034564971924, 0.8556034564971924, 0.8599137663841248, 0.8415948152542114, 0.8642241358757019, 0.8663793206214905, 0.8653017282485962, 0.8308189511299133, 0.8653017282485962, 0.8599137663841248, 0.8631465435028076, 0.8642241358757019, 0.850215494632721, 0.8459051847457886, 0.8653017282485962, 0.860991358757019, 0.8653017282485962, 0.8663793206214905, 0.8620689511299133, 0.8599137663841248, 0.8663793206214905, 0.860991358757019, 0.8663793206214905, 0.8556034564971924, 0.8599137663841248, 0.860991358757019, 0.8588362336158752, 0.8620689511299133, 0.8631465435028076, 0.8620689511299133, 0.8631465435028076, 0.8620689511299133, 0.8620689511299133, 0.8405172228813171, 0.8642241358757019, 0.8588362336158752, 0.8599137663841248, 0.8642241358757019, 0.860991358757019, 0.8599137663841248, 0.860991358757019, 0.8642241358757019, 0.8588362336158752]}\n","38/38 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","24/28 [========================>.....] - ETA: 0s - loss: 0.8180 - accuracy: 0.8398"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 8s 54ms/step - loss: 0.8174 - accuracy: 0.8387 - val_loss: 1.1287 - val_accuracy: 0.4955\n","Epoch 2/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.8061 - accuracy: 0.8384 - val_loss: 1.1140 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7986 - accuracy: 0.8461 - val_loss: 1.1001 - val_accuracy: 0.4955\n","Epoch 4/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7965 - accuracy: 0.8435 - val_loss: 1.0886 - val_accuracy: 0.4966\n","Epoch 5/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7854 - accuracy: 0.8500 - val_loss: 1.0752 - val_accuracy: 0.5588\n","Epoch 6/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.7797 - accuracy: 0.8478 - val_loss: 1.0646 - val_accuracy: 0.5928\n","Epoch 7/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7791 - accuracy: 0.8503 - val_loss: 1.0535 - val_accuracy: 0.6900\n","Epoch 8/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.7789 - accuracy: 0.8427 - val_loss: 1.0357 - val_accuracy: 0.8111\n","Epoch 9/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.7693 - accuracy: 0.8497 - val_loss: 1.0247 - val_accuracy: 0.7851\n","Epoch 10/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7664 - accuracy: 0.8517 - val_loss: 1.0090 - val_accuracy: 0.8224\n","Epoch 11/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.7603 - accuracy: 0.8565 - val_loss: 0.9918 - val_accuracy: 0.8224\n","Epoch 12/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7557 - accuracy: 0.8512 - val_loss: 0.9777 - val_accuracy: 0.8213\n","Epoch 13/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7553 - accuracy: 0.8506 - val_loss: 0.9567 - val_accuracy: 0.8077\n","Epoch 14/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7476 - accuracy: 0.8557 - val_loss: 0.9322 - val_accuracy: 0.8167\n","Epoch 15/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7407 - accuracy: 0.8591 - val_loss: 0.9125 - val_accuracy: 0.8167\n","Epoch 16/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7423 - accuracy: 0.8537 - val_loss: 0.8948 - val_accuracy: 0.7998\n","Epoch 17/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.7309 - accuracy: 0.8664 - val_loss: 0.8778 - val_accuracy: 0.7998\n","Epoch 18/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7323 - accuracy: 0.8602 - val_loss: 0.8606 - val_accuracy: 0.8009\n","Epoch 19/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7259 - accuracy: 0.8611 - val_loss: 0.8403 - val_accuracy: 0.8111\n","Epoch 20/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.7239 - accuracy: 0.8594 - val_loss: 0.8467 - val_accuracy: 0.7885\n","Epoch 21/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.7170 - accuracy: 0.8633 - val_loss: 0.8180 - val_accuracy: 0.8122\n","Epoch 22/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.7178 - accuracy: 0.8622 - val_loss: 0.8074 - val_accuracy: 0.8167\n","Epoch 23/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.7181 - accuracy: 0.8616 - val_loss: 0.8125 - val_accuracy: 0.8088\n","Epoch 24/100\n","28/28 [==============================] - 1s 49ms/step - loss: 0.7060 - accuracy: 0.8704 - val_loss: 0.7830 - val_accuracy: 0.8281\n","Epoch 25/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.7102 - accuracy: 0.8636 - val_loss: 0.7950 - val_accuracy: 0.8122\n","Epoch 26/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.6987 - accuracy: 0.8701 - val_loss: 0.7707 - val_accuracy: 0.8360\n","Epoch 27/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6972 - accuracy: 0.8698 - val_loss: 0.7717 - val_accuracy: 0.8337\n","Epoch 28/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.7085 - accuracy: 0.8591 - val_loss: 0.7678 - val_accuracy: 0.8337\n","Epoch 29/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6914 - accuracy: 0.8693 - val_loss: 0.7616 - val_accuracy: 0.8337\n","Epoch 30/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.6826 - accuracy: 0.8741 - val_loss: 0.7649 - val_accuracy: 0.8382\n","Epoch 31/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6818 - accuracy: 0.8744 - val_loss: 0.7668 - val_accuracy: 0.8348\n","Epoch 32/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6775 - accuracy: 0.8741 - val_loss: 0.7613 - val_accuracy: 0.8371\n","Epoch 33/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6741 - accuracy: 0.8749 - val_loss: 0.7665 - val_accuracy: 0.8258\n","Epoch 34/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6725 - accuracy: 0.8727 - val_loss: 0.7635 - val_accuracy: 0.8314\n","Epoch 35/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6707 - accuracy: 0.8749 - val_loss: 0.7613 - val_accuracy: 0.8326\n","Epoch 36/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6640 - accuracy: 0.8761 - val_loss: 0.7590 - val_accuracy: 0.8303\n","Epoch 37/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6666 - accuracy: 0.8741 - val_loss: 0.7537 - val_accuracy: 0.8314\n","Epoch 38/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.6562 - accuracy: 0.8775 - val_loss: 0.7537 - val_accuracy: 0.8326\n","Epoch 39/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6595 - accuracy: 0.8741 - val_loss: 0.7552 - val_accuracy: 0.8337\n","Epoch 40/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6554 - accuracy: 0.8772 - val_loss: 0.7497 - val_accuracy: 0.8337\n","Epoch 41/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6481 - accuracy: 0.8803 - val_loss: 0.7496 - val_accuracy: 0.8314\n","Epoch 42/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6616 - accuracy: 0.8715 - val_loss: 0.7510 - val_accuracy: 0.8258\n","Epoch 43/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.6472 - accuracy: 0.8761 - val_loss: 0.7461 - val_accuracy: 0.8337\n","Epoch 44/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6416 - accuracy: 0.8814 - val_loss: 0.7473 - val_accuracy: 0.8360\n","Epoch 45/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6375 - accuracy: 0.8860 - val_loss: 0.7487 - val_accuracy: 0.8326\n","Epoch 46/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6329 - accuracy: 0.8846 - val_loss: 0.7453 - val_accuracy: 0.8281\n","Epoch 47/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6353 - accuracy: 0.8865 - val_loss: 0.7629 - val_accuracy: 0.8292\n","Epoch 48/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.6305 - accuracy: 0.8826 - val_loss: 0.7373 - val_accuracy: 0.8337\n","Epoch 49/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.6228 - accuracy: 0.8846 - val_loss: 0.7334 - val_accuracy: 0.8405\n","Epoch 50/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.6201 - accuracy: 0.8877 - val_loss: 0.7521 - val_accuracy: 0.8224\n","Epoch 51/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6259 - accuracy: 0.8829 - val_loss: 0.7381 - val_accuracy: 0.8371\n","Epoch 52/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6132 - accuracy: 0.8894 - val_loss: 0.7340 - val_accuracy: 0.8348\n","Epoch 53/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6209 - accuracy: 0.8843 - val_loss: 0.7330 - val_accuracy: 0.8326\n","Epoch 54/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.6134 - accuracy: 0.8854 - val_loss: 0.7385 - val_accuracy: 0.8326\n","Epoch 55/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.6110 - accuracy: 0.8882 - val_loss: 0.7282 - val_accuracy: 0.8360\n","Epoch 56/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.6016 - accuracy: 0.8871 - val_loss: 0.7260 - val_accuracy: 0.8371\n","Epoch 57/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5975 - accuracy: 0.8956 - val_loss: 0.7301 - val_accuracy: 0.8326\n","Epoch 58/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5975 - accuracy: 0.8908 - val_loss: 0.7290 - val_accuracy: 0.8371\n","Epoch 59/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5913 - accuracy: 0.8978 - val_loss: 0.7243 - val_accuracy: 0.8371\n","Epoch 60/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5884 - accuracy: 0.8936 - val_loss: 0.7322 - val_accuracy: 0.8360\n","Epoch 61/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5988 - accuracy: 0.8885 - val_loss: 0.7257 - val_accuracy: 0.8348\n","Epoch 62/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5854 - accuracy: 0.8967 - val_loss: 0.7211 - val_accuracy: 0.8326\n","Epoch 63/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5801 - accuracy: 0.9001 - val_loss: 0.7188 - val_accuracy: 0.8394\n","Epoch 64/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5816 - accuracy: 0.8973 - val_loss: 0.7236 - val_accuracy: 0.8314\n","Epoch 65/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5788 - accuracy: 0.9007 - val_loss: 0.7264 - val_accuracy: 0.8337\n","Epoch 66/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5756 - accuracy: 0.8973 - val_loss: 0.7279 - val_accuracy: 0.8303\n","Epoch 67/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5683 - accuracy: 0.8973 - val_loss: 0.7287 - val_accuracy: 0.8258\n","Epoch 68/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5741 - accuracy: 0.8962 - val_loss: 0.7179 - val_accuracy: 0.8382\n","Epoch 69/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5690 - accuracy: 0.9029 - val_loss: 0.7512 - val_accuracy: 0.8156\n","Epoch 70/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5744 - accuracy: 0.8978 - val_loss: 0.7122 - val_accuracy: 0.8292\n","Epoch 71/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5599 - accuracy: 0.9080 - val_loss: 0.7096 - val_accuracy: 0.8371\n","Epoch 72/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5546 - accuracy: 0.9010 - val_loss: 0.7229 - val_accuracy: 0.8360\n","Epoch 73/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5508 - accuracy: 0.9097 - val_loss: 0.7156 - val_accuracy: 0.8348\n","Epoch 74/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5480 - accuracy: 0.9078 - val_loss: 0.7240 - val_accuracy: 0.8371\n","Epoch 75/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5739 - accuracy: 0.8964 - val_loss: 0.7325 - val_accuracy: 0.8213\n","Epoch 76/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5501 - accuracy: 0.9106 - val_loss: 0.7114 - val_accuracy: 0.8348\n","Epoch 77/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5439 - accuracy: 0.9083 - val_loss: 0.7061 - val_accuracy: 0.8371\n","Epoch 78/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5416 - accuracy: 0.9095 - val_loss: 0.7263 - val_accuracy: 0.8269\n","Epoch 79/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5495 - accuracy: 0.9029 - val_loss: 0.7028 - val_accuracy: 0.8382\n","Epoch 80/100\n","28/28 [==============================] - 1s 38ms/step - loss: 0.5334 - accuracy: 0.9148 - val_loss: 0.7081 - val_accuracy: 0.8439\n","Epoch 81/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.5368 - accuracy: 0.9106 - val_loss: 0.7147 - val_accuracy: 0.8337\n","Epoch 82/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5250 - accuracy: 0.9148 - val_loss: 0.7057 - val_accuracy: 0.8394\n","Epoch 83/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5216 - accuracy: 0.9208 - val_loss: 0.7236 - val_accuracy: 0.8314\n","Epoch 84/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5188 - accuracy: 0.9194 - val_loss: 0.7132 - val_accuracy: 0.8303\n","Epoch 85/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5288 - accuracy: 0.9103 - val_loss: 0.7089 - val_accuracy: 0.8394\n","Epoch 86/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5159 - accuracy: 0.9202 - val_loss: 0.7040 - val_accuracy: 0.8360\n","Epoch 87/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.5167 - accuracy: 0.9151 - val_loss: 0.7028 - val_accuracy: 0.8382\n","Epoch 88/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5196 - accuracy: 0.9140 - val_loss: 0.7036 - val_accuracy: 0.8405\n","Epoch 89/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5099 - accuracy: 0.9202 - val_loss: 0.7409 - val_accuracy: 0.8247\n","Epoch 90/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.5143 - accuracy: 0.9143 - val_loss: 0.7088 - val_accuracy: 0.8303\n","Epoch 91/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.5069 - accuracy: 0.9196 - val_loss: 0.6993 - val_accuracy: 0.8439\n","Epoch 92/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.5049 - accuracy: 0.9185 - val_loss: 0.7015 - val_accuracy: 0.8416\n","Epoch 93/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.5000 - accuracy: 0.9208 - val_loss: 0.7193 - val_accuracy: 0.8269\n","Epoch 94/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4926 - accuracy: 0.9250 - val_loss: 0.7115 - val_accuracy: 0.8348\n","Epoch 95/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5040 - accuracy: 0.9160 - val_loss: 0.7166 - val_accuracy: 0.8303\n","Epoch 96/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4917 - accuracy: 0.9233 - val_loss: 0.7473 - val_accuracy: 0.8235\n","Epoch 97/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4956 - accuracy: 0.9202 - val_loss: 0.7083 - val_accuracy: 0.8382\n","Epoch 98/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4808 - accuracy: 0.9307 - val_loss: 0.7010 - val_accuracy: 0.8450\n","Epoch 99/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4932 - accuracy: 0.9264 - val_loss: 0.7356 - val_accuracy: 0.8303\n","Epoch 100/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.4999 - accuracy: 0.9148 - val_loss: 0.7033 - val_accuracy: 0.8394\n","{'loss': [0.8174350261688232, 0.8060609698295593, 0.7986400723457336, 0.796494722366333, 0.7854111790657043, 0.7797061204910278, 0.7790521383285522, 0.7788528203964233, 0.7693392634391785, 0.7664211988449097, 0.7603356838226318, 0.7557200193405151, 0.755272626876831, 0.747575044631958, 0.7406628727912903, 0.7423056960105896, 0.7309454083442688, 0.7322888970375061, 0.7259407639503479, 0.7238694429397583, 0.7170372009277344, 0.7177605628967285, 0.7180988192558289, 0.7060424089431763, 0.710249125957489, 0.6987194418907166, 0.6971703767776489, 0.7084557414054871, 0.691402018070221, 0.6825604438781738, 0.681774377822876, 0.6775393486022949, 0.6741473078727722, 0.6725466847419739, 0.6707097291946411, 0.6640360951423645, 0.6666460633277893, 0.6562055945396423, 0.6594556570053101, 0.6554333567619324, 0.6481242179870605, 0.6615802049636841, 0.6471994519233704, 0.641588568687439, 0.637514591217041, 0.6328533291816711, 0.6353034377098083, 0.6305037140846252, 0.6227749586105347, 0.6201146245002747, 0.6258700489997864, 0.613243818283081, 0.6209404468536377, 0.6134440898895264, 0.6109752058982849, 0.6015517115592957, 0.5974664688110352, 0.5974595546722412, 0.5913417339324951, 0.5884318351745605, 0.5988069772720337, 0.5853996276855469, 0.580087423324585, 0.5815767645835876, 0.578823447227478, 0.5756144523620605, 0.5682913064956665, 0.5740804672241211, 0.5689685940742493, 0.5743760466575623, 0.5599062442779541, 0.5545594692230225, 0.5508225560188293, 0.5479768514633179, 0.5739214420318604, 0.5500760078430176, 0.543861985206604, 0.5415653586387634, 0.5494906902313232, 0.5333899855613708, 0.5368366241455078, 0.5250475406646729, 0.5216283202171326, 0.5187678337097168, 0.5288172364234924, 0.5159224271774292, 0.5167160034179688, 0.5195663571357727, 0.5099282264709473, 0.5142768621444702, 0.5068801641464233, 0.504888653755188, 0.5000385046005249, 0.4926479458808899, 0.5040308237075806, 0.49173086881637573, 0.4955691695213318, 0.48079392313957214, 0.493163138628006, 0.4999447464942932], 'accuracy': [0.8387096524238586, 0.8384267091751099, 0.8460667729377747, 0.8435201048851013, 0.8500282764434814, 0.8477645516395569, 0.850311279296875, 0.8426712155342102, 0.8497453331947327, 0.8517261147499084, 0.8565365076065063, 0.8511601686477661, 0.8505942225456238, 0.8556876182556152, 0.8590831756591797, 0.8537068367004395, 0.8664402961730957, 0.8602150678634644, 0.8610639572143555, 0.8593661785125732, 0.86332768201828, 0.8621957898139954, 0.8616299033164978, 0.8704017996788025, 0.8636106252670288, 0.8701188564300537, 0.8698358535766602, 0.8590831756591797, 0.8692699670791626, 0.8740803599357605, 0.8743633031845093, 0.8740803599357605, 0.8749292492866516, 0.872665524482727, 0.8749292492866516, 0.8760611414909363, 0.8740803599357605, 0.8774759769439697, 0.8740803599357605, 0.8771929740905762, 0.8803055882453918, 0.8715336918830872, 0.8760611414909363, 0.8814374804496765, 0.8859649300575256, 0.8845500946044922, 0.8865308165550232, 0.8825693130493164, 0.8845500946044922, 0.8876627087593079, 0.88285231590271, 0.8893604874610901, 0.8842670917510986, 0.8853989839553833, 0.8882286548614502, 0.8870967626571655, 0.8955857157707214, 0.8907753229141235, 0.897849440574646, 0.8936049938201904, 0.888511598110199, 0.8967176079750061, 0.9001131653785706, 0.8972835540771484, 0.9006791114807129, 0.8972835540771484, 0.8972835540771484, 0.8961516618728638, 0.9029428362846375, 0.897849440574646, 0.9080362319946289, 0.9009620547294617, 0.9097340106964111, 0.9077532291412354, 0.8964346647262573, 0.9105829000473022, 0.9083191752433777, 0.9094510674476624, 0.9029428362846375, 0.9148274064064026, 0.9105829000473022, 0.9148274064064026, 0.9207696914672852, 0.9193548560142517, 0.9102999567985535, 0.9202037453651428, 0.9151103496551514, 0.9139785170555115, 0.9202037453651428, 0.9142614603042603, 0.9196377992630005, 0.9185059666633606, 0.9207696914672852, 0.9250141382217407, 0.9159592390060425, 0.9233163595199585, 0.9202037453651428, 0.9306734800338745, 0.9264289736747742, 0.9148274064064026], 'val_loss': [1.1286603212356567, 1.1139776706695557, 1.1000603437423706, 1.0885674953460693, 1.0751564502716064, 1.0645580291748047, 1.0534664392471313, 1.0356556177139282, 1.024705171585083, 1.0090056657791138, 0.9917767643928528, 0.9776623845100403, 0.9567359685897827, 0.9322003126144409, 0.912493884563446, 0.8948083519935608, 0.8778119087219238, 0.8605778813362122, 0.8402989506721497, 0.8467299342155457, 0.8180171847343445, 0.8073737621307373, 0.8125357031822205, 0.7830079793930054, 0.7949811220169067, 0.7706652283668518, 0.7716585993766785, 0.7678230404853821, 0.7616119980812073, 0.7648729681968689, 0.7668070197105408, 0.7612916231155396, 0.766451895236969, 0.7635087966918945, 0.7612934112548828, 0.7589808106422424, 0.7537268996238708, 0.7536982297897339, 0.7551831007003784, 0.7497490048408508, 0.7496002316474915, 0.7510421276092529, 0.7461427450180054, 0.7472538948059082, 0.7486687302589417, 0.7453187108039856, 0.762882649898529, 0.7372851967811584, 0.7334476113319397, 0.7521377205848694, 0.7381361126899719, 0.7339664697647095, 0.7329561114311218, 0.738461434841156, 0.7282087206840515, 0.7259634733200073, 0.7301025986671448, 0.7289761304855347, 0.7243192791938782, 0.7322439551353455, 0.7256529927253723, 0.7210714817047119, 0.7188123464584351, 0.7235574722290039, 0.7264329195022583, 0.7279174327850342, 0.7287271022796631, 0.7178676128387451, 0.7512372732162476, 0.7121535539627075, 0.7095746397972107, 0.7229406237602234, 0.7156171798706055, 0.7239655256271362, 0.7324874401092529, 0.7114351987838745, 0.7061474919319153, 0.7263481020927429, 0.7027938365936279, 0.7081006765365601, 0.7146841883659363, 0.7056919932365417, 0.7235651016235352, 0.7131972312927246, 0.7089099287986755, 0.7039628028869629, 0.7028010487556458, 0.7036451101303101, 0.740871787071228, 0.7088443040847778, 0.6993278861045837, 0.7015031576156616, 0.7192602157592773, 0.711517333984375, 0.7166459560394287, 0.7472684979438782, 0.7083094716072083, 0.7010142207145691, 0.7356384992599487, 0.703349232673645], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.49660632014274597, 0.5588235259056091, 0.5927602052688599, 0.6900452375411987, 0.8110859990119934, 0.7850678563117981, 0.8223981857299805, 0.8223981857299805, 0.8212669491767883, 0.807692289352417, 0.8167420625686646, 0.8167420625686646, 0.7997737526893616, 0.7997737526893616, 0.8009049892425537, 0.8110859990119934, 0.7884615659713745, 0.8122171759605408, 0.8167420625686646, 0.8088235259056091, 0.8280543088912964, 0.8122171759605408, 0.8359728455543518, 0.8337104320526123, 0.8337104320526123, 0.8337104320526123, 0.8382353186607361, 0.8348416090011597, 0.837104082107544, 0.8257918357849121, 0.831447958946228, 0.8325791954994202, 0.8303167223930359, 0.831447958946228, 0.8325791954994202, 0.8337104320526123, 0.8337104320526123, 0.831447958946228, 0.8257918357849121, 0.8337104320526123, 0.8359728455543518, 0.8325791954994202, 0.8280543088912964, 0.8291855454444885, 0.8337104320526123, 0.8404977321624756, 0.8223981857299805, 0.837104082107544, 0.8348416090011597, 0.8325791954994202, 0.8325791954994202, 0.8359728455543518, 0.837104082107544, 0.8325791954994202, 0.837104082107544, 0.837104082107544, 0.8359728455543518, 0.8348416090011597, 0.8325791954994202, 0.8393664956092834, 0.831447958946228, 0.8337104320526123, 0.8303167223930359, 0.8257918357849121, 0.8382353186607361, 0.8156108856201172, 0.8291855454444885, 0.837104082107544, 0.8359728455543518, 0.8348416090011597, 0.837104082107544, 0.8212669491767883, 0.8348416090011597, 0.837104082107544, 0.8269230723381042, 0.8382353186607361, 0.8438913822174072, 0.8337104320526123, 0.8393664956092834, 0.831447958946228, 0.8303167223930359, 0.8393664956092834, 0.8359728455543518, 0.8382353186607361, 0.8404977321624756, 0.8246606588363647, 0.8303167223930359, 0.8438913822174072, 0.8416289687156677, 0.8269230723381042, 0.8348416090011597, 0.8303167223930359, 0.8235294222831726, 0.8382353186607361, 0.8450226187705994, 0.8303167223930359, 0.8393664956092834]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - ETA: 0s - loss: 0.8061 - accuracy: 0.8408"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 8s 67ms/step - loss: 0.8061 - accuracy: 0.8408 - val_loss: 1.1278 - val_accuracy: 0.4855\n","Epoch 2/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7958 - accuracy: 0.8375 - val_loss: 1.1110 - val_accuracy: 0.4855\n","Epoch 3/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7841 - accuracy: 0.8450 - val_loss: 1.1000 - val_accuracy: 0.4855\n","Epoch 4/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7807 - accuracy: 0.8486 - val_loss: 1.0832 - val_accuracy: 0.5165\n","Epoch 5/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7724 - accuracy: 0.8496 - val_loss: 1.0689 - val_accuracy: 0.5950\n","Epoch 6/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.7666 - accuracy: 0.8494 - val_loss: 1.0562 - val_accuracy: 0.6529\n","Epoch 7/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7647 - accuracy: 0.8475 - val_loss: 1.0469 - val_accuracy: 0.6281\n","Epoch 8/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7580 - accuracy: 0.8556 - val_loss: 1.0241 - val_accuracy: 0.7676\n","Epoch 9/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7586 - accuracy: 0.8527 - val_loss: 1.0091 - val_accuracy: 0.7769\n","Epoch 10/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7569 - accuracy: 0.8537 - val_loss: 0.9831 - val_accuracy: 0.8357\n","Epoch 11/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7557 - accuracy: 0.8535 - val_loss: 0.9646 - val_accuracy: 0.8326\n","Epoch 12/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7443 - accuracy: 0.8553 - val_loss: 0.9428 - val_accuracy: 0.8264\n","Epoch 13/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7407 - accuracy: 0.8543 - val_loss: 0.9164 - val_accuracy: 0.8337\n","Epoch 14/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.7365 - accuracy: 0.8594 - val_loss: 0.8947 - val_accuracy: 0.8399\n","Epoch 15/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7281 - accuracy: 0.8571 - val_loss: 0.8714 - val_accuracy: 0.8306\n","Epoch 16/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7227 - accuracy: 0.8633 - val_loss: 0.8512 - val_accuracy: 0.8213\n","Epoch 17/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7278 - accuracy: 0.8589 - val_loss: 0.8238 - val_accuracy: 0.8378\n","Epoch 18/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7193 - accuracy: 0.8680 - val_loss: 0.8258 - val_accuracy: 0.8099\n","Epoch 19/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.7137 - accuracy: 0.8625 - val_loss: 0.8228 - val_accuracy: 0.8058\n","Epoch 20/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.7181 - accuracy: 0.8630 - val_loss: 0.8033 - val_accuracy: 0.8192\n","Epoch 21/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.7109 - accuracy: 0.8659 - val_loss: 0.7671 - val_accuracy: 0.8419\n","Epoch 22/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.7040 - accuracy: 0.8664 - val_loss: 0.7559 - val_accuracy: 0.8461\n","Epoch 23/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6992 - accuracy: 0.8669 - val_loss: 0.7568 - val_accuracy: 0.8409\n","Epoch 24/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.6949 - accuracy: 0.8703 - val_loss: 0.7492 - val_accuracy: 0.8471\n","Epoch 25/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.6897 - accuracy: 0.8734 - val_loss: 0.7412 - val_accuracy: 0.8440\n","Epoch 26/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.6912 - accuracy: 0.8705 - val_loss: 0.7425 - val_accuracy: 0.8471\n","Epoch 27/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6864 - accuracy: 0.8708 - val_loss: 0.7364 - val_accuracy: 0.8471\n","Epoch 28/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6815 - accuracy: 0.8755 - val_loss: 0.7347 - val_accuracy: 0.8440\n","Epoch 29/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6792 - accuracy: 0.8718 - val_loss: 0.7525 - val_accuracy: 0.8347\n","Epoch 30/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6745 - accuracy: 0.8786 - val_loss: 0.7304 - val_accuracy: 0.8419\n","Epoch 31/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6794 - accuracy: 0.8667 - val_loss: 0.7541 - val_accuracy: 0.8347\n","Epoch 32/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6709 - accuracy: 0.8736 - val_loss: 0.7441 - val_accuracy: 0.8409\n","Epoch 33/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6694 - accuracy: 0.8783 - val_loss: 0.7281 - val_accuracy: 0.8419\n","Epoch 34/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6650 - accuracy: 0.8721 - val_loss: 0.7226 - val_accuracy: 0.8481\n","Epoch 35/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6664 - accuracy: 0.8757 - val_loss: 0.7283 - val_accuracy: 0.8450\n","Epoch 36/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6634 - accuracy: 0.8703 - val_loss: 0.7217 - val_accuracy: 0.8481\n","Epoch 37/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6456 - accuracy: 0.8848 - val_loss: 0.7211 - val_accuracy: 0.8430\n","Epoch 38/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.6377 - accuracy: 0.8910 - val_loss: 0.7159 - val_accuracy: 0.8512\n","Epoch 39/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6458 - accuracy: 0.8827 - val_loss: 0.7197 - val_accuracy: 0.8450\n","Epoch 40/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6483 - accuracy: 0.8747 - val_loss: 0.7165 - val_accuracy: 0.8481\n","Epoch 41/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6425 - accuracy: 0.8845 - val_loss: 0.7189 - val_accuracy: 0.8450\n","Epoch 42/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6367 - accuracy: 0.8829 - val_loss: 0.7280 - val_accuracy: 0.8450\n","Epoch 43/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.6358 - accuracy: 0.8819 - val_loss: 0.7175 - val_accuracy: 0.8450\n","Epoch 44/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6257 - accuracy: 0.8829 - val_loss: 0.7052 - val_accuracy: 0.8461\n","Epoch 45/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.6245 - accuracy: 0.8881 - val_loss: 0.7051 - val_accuracy: 0.8523\n","Epoch 46/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6245 - accuracy: 0.8863 - val_loss: 0.6999 - val_accuracy: 0.8512\n","Epoch 47/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6167 - accuracy: 0.8891 - val_loss: 0.6986 - val_accuracy: 0.8523\n","Epoch 48/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6159 - accuracy: 0.8876 - val_loss: 0.7272 - val_accuracy: 0.8388\n","Epoch 49/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6154 - accuracy: 0.8897 - val_loss: 0.6955 - val_accuracy: 0.8471\n","Epoch 50/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.6112 - accuracy: 0.8879 - val_loss: 0.7288 - val_accuracy: 0.8347\n","Epoch 51/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6100 - accuracy: 0.8894 - val_loss: 0.6969 - val_accuracy: 0.8471\n","Epoch 52/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6030 - accuracy: 0.8938 - val_loss: 0.6953 - val_accuracy: 0.8419\n","Epoch 53/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.6018 - accuracy: 0.8941 - val_loss: 0.7171 - val_accuracy: 0.8430\n","Epoch 54/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6052 - accuracy: 0.8910 - val_loss: 0.6962 - val_accuracy: 0.8378\n","Epoch 55/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5927 - accuracy: 0.8886 - val_loss: 0.6935 - val_accuracy: 0.8419\n","Epoch 56/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5904 - accuracy: 0.8907 - val_loss: 0.7111 - val_accuracy: 0.8440\n","Epoch 57/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.6092 - accuracy: 0.8860 - val_loss: 0.7014 - val_accuracy: 0.8440\n","Epoch 58/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5902 - accuracy: 0.8930 - val_loss: 0.7150 - val_accuracy: 0.8347\n","Epoch 59/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5825 - accuracy: 0.8946 - val_loss: 0.7248 - val_accuracy: 0.8388\n","Epoch 60/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5781 - accuracy: 0.8935 - val_loss: 0.6839 - val_accuracy: 0.8440\n","Epoch 61/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5744 - accuracy: 0.9008 - val_loss: 0.6845 - val_accuracy: 0.8419\n","Epoch 62/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.5692 - accuracy: 0.9041 - val_loss: 0.6902 - val_accuracy: 0.8430\n","Epoch 63/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5694 - accuracy: 0.9023 - val_loss: 0.6880 - val_accuracy: 0.8430\n","Epoch 64/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5647 - accuracy: 0.9034 - val_loss: 0.6776 - val_accuracy: 0.8419\n","Epoch 65/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5656 - accuracy: 0.8997 - val_loss: 0.6838 - val_accuracy: 0.8419\n","Epoch 66/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5628 - accuracy: 0.9008 - val_loss: 0.6824 - val_accuracy: 0.8430\n","Epoch 67/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5584 - accuracy: 0.9000 - val_loss: 0.6796 - val_accuracy: 0.8481\n","Epoch 68/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5655 - accuracy: 0.9003 - val_loss: 0.6751 - val_accuracy: 0.8440\n","Epoch 69/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5572 - accuracy: 0.8982 - val_loss: 0.6763 - val_accuracy: 0.8419\n","Epoch 70/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5521 - accuracy: 0.9054 - val_loss: 0.6745 - val_accuracy: 0.8450\n","Epoch 71/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5408 - accuracy: 0.9114 - val_loss: 0.6737 - val_accuracy: 0.8481\n","Epoch 72/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5403 - accuracy: 0.9098 - val_loss: 0.6794 - val_accuracy: 0.8502\n","Epoch 73/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5474 - accuracy: 0.9057 - val_loss: 0.6789 - val_accuracy: 0.8492\n","Epoch 74/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5357 - accuracy: 0.9093 - val_loss: 0.6778 - val_accuracy: 0.8430\n","Epoch 75/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5420 - accuracy: 0.9070 - val_loss: 0.6884 - val_accuracy: 0.8450\n","Epoch 76/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5275 - accuracy: 0.9124 - val_loss: 0.6731 - val_accuracy: 0.8502\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5289 - accuracy: 0.9134 - val_loss: 0.6790 - val_accuracy: 0.8450\n","Epoch 78/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5356 - accuracy: 0.9098 - val_loss: 0.6715 - val_accuracy: 0.8481\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5300 - accuracy: 0.9067 - val_loss: 0.6948 - val_accuracy: 0.8399\n","Epoch 80/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5373 - accuracy: 0.9026 - val_loss: 0.6697 - val_accuracy: 0.8461\n","Epoch 81/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5190 - accuracy: 0.9152 - val_loss: 0.6683 - val_accuracy: 0.8523\n","Epoch 82/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5187 - accuracy: 0.9116 - val_loss: 0.6770 - val_accuracy: 0.8440\n","Epoch 83/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5100 - accuracy: 0.9189 - val_loss: 0.6761 - val_accuracy: 0.8461\n","Epoch 84/100\n","31/31 [==============================] - 1s 40ms/step - loss: 0.5187 - accuracy: 0.9103 - val_loss: 0.6677 - val_accuracy: 0.8533\n","Epoch 85/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.5156 - accuracy: 0.9127 - val_loss: 0.7381 - val_accuracy: 0.8285\n","Epoch 86/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5172 - accuracy: 0.9111 - val_loss: 0.6619 - val_accuracy: 0.8512\n","Epoch 87/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5026 - accuracy: 0.9155 - val_loss: 0.6649 - val_accuracy: 0.8533\n","Epoch 88/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.5058 - accuracy: 0.9137 - val_loss: 0.7264 - val_accuracy: 0.8285\n","Epoch 89/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5153 - accuracy: 0.9083 - val_loss: 0.6962 - val_accuracy: 0.8378\n","Epoch 90/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5104 - accuracy: 0.9103 - val_loss: 0.6732 - val_accuracy: 0.8471\n","Epoch 91/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.5082 - accuracy: 0.9124 - val_loss: 0.7103 - val_accuracy: 0.8275\n","Epoch 92/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.5062 - accuracy: 0.9114 - val_loss: 0.6570 - val_accuracy: 0.8554\n","Epoch 93/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4915 - accuracy: 0.9233 - val_loss: 0.6554 - val_accuracy: 0.8502\n","Epoch 94/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4881 - accuracy: 0.9240 - val_loss: 0.6628 - val_accuracy: 0.8512\n","Epoch 95/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4862 - accuracy: 0.9253 - val_loss: 0.6596 - val_accuracy: 0.8533\n","Epoch 96/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4826 - accuracy: 0.9230 - val_loss: 0.6629 - val_accuracy: 0.8543\n","Epoch 97/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4819 - accuracy: 0.9202 - val_loss: 0.6650 - val_accuracy: 0.8502\n","Epoch 98/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4974 - accuracy: 0.9124 - val_loss: 0.6842 - val_accuracy: 0.8409\n","Epoch 99/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4983 - accuracy: 0.9142 - val_loss: 0.6563 - val_accuracy: 0.8543\n","Epoch 100/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4770 - accuracy: 0.9253 - val_loss: 0.6645 - val_accuracy: 0.8461\n","{'loss': [0.8061152100563049, 0.7958486080169678, 0.7841030359268188, 0.7807413339614868, 0.7724193930625916, 0.766566812992096, 0.7646659016609192, 0.7579781413078308, 0.7586325407028198, 0.7568919062614441, 0.7557480335235596, 0.7442969679832458, 0.7407442331314087, 0.7365356087684631, 0.7280902862548828, 0.7227353453636169, 0.727821409702301, 0.7192818522453308, 0.7137298583984375, 0.7180849313735962, 0.7109262943267822, 0.7040254473686218, 0.6991553902626038, 0.6948534846305847, 0.6896753311157227, 0.6911579966545105, 0.6863593459129333, 0.6815117597579956, 0.6791860461235046, 0.6745250225067139, 0.6793543100357056, 0.670894980430603, 0.6693974733352661, 0.6649807095527649, 0.6664162278175354, 0.6634180545806885, 0.6455823183059692, 0.6376944184303284, 0.6458220481872559, 0.6482564210891724, 0.6424859762191772, 0.6367062330245972, 0.6358268857002258, 0.6256846189498901, 0.6244838833808899, 0.6245144605636597, 0.6167108416557312, 0.6159266829490662, 0.6154178380966187, 0.6111619472503662, 0.6099721193313599, 0.6030080318450928, 0.601778507232666, 0.6052355170249939, 0.5927438139915466, 0.5904201865196228, 0.6092481017112732, 0.5902121663093567, 0.5824854373931885, 0.5781494379043579, 0.5743681788444519, 0.5691569447517395, 0.5694471597671509, 0.5646602511405945, 0.5656417608261108, 0.5627551078796387, 0.5583913922309875, 0.5654934048652649, 0.557210385799408, 0.5521096587181091, 0.5408468246459961, 0.5402777791023254, 0.5473624467849731, 0.5357192158699036, 0.5420178174972534, 0.5274804830551147, 0.5288535952568054, 0.5356159806251526, 0.5300044417381287, 0.537325918674469, 0.5189952850341797, 0.518697202205658, 0.5099690556526184, 0.5187109112739563, 0.5155766010284424, 0.517234742641449, 0.5026417970657349, 0.5057575702667236, 0.5153173208236694, 0.510434091091156, 0.5081568360328674, 0.5062305331230164, 0.4914710819721222, 0.4880515933036804, 0.4861641228199005, 0.48258650302886963, 0.481857568025589, 0.4974454641342163, 0.4982988238334656, 0.47701501846313477], 'accuracy': [0.8408268690109253, 0.8374677300453186, 0.8449612259864807, 0.8485788106918335, 0.8496124148368835, 0.8493540287017822, 0.8475452065467834, 0.855555534362793, 0.8527131676673889, 0.853746771812439, 0.8534883856773376, 0.8552971482276917, 0.8542635440826416, 0.8594315052032471, 0.8571059703826904, 0.8633074760437012, 0.8589147329330444, 0.867958664894104, 0.8625323176383972, 0.8630490899085999, 0.8658914566040039, 0.8664082884788513, 0.866925060749054, 0.8702842593193054, 0.8733850121498108, 0.8705426454544067, 0.8708010315895081, 0.8754522204399109, 0.8718346357345581, 0.8785529732704163, 0.8666666746139526, 0.8736433982849121, 0.8782945871353149, 0.8720930218696594, 0.8757106065750122, 0.8702842593193054, 0.8847545385360718, 0.8909560441970825, 0.8826873302459717, 0.8746770024299622, 0.8844961524009705, 0.882945716381073, 0.8819121718406677, 0.882945716381073, 0.8881136775016785, 0.8863049149513245, 0.8891472816467285, 0.8875969052314758, 0.8896640539169312, 0.8878552913665771, 0.8894056677818298, 0.8937984704971313, 0.8940568566322327, 0.8909560441970825, 0.8886305093765259, 0.8906976580619812, 0.8860465288162231, 0.8930232524871826, 0.8945736289024353, 0.8935400247573853, 0.9007751941680908, 0.9041343927383423, 0.9023255705833435, 0.9033591747283936, 0.8997415900230408, 0.9007751941680908, 0.8999999761581421, 0.9002584218978882, 0.8981912136077881, 0.9054263830184937, 0.9113695025444031, 0.9098191261291504, 0.905684769153595, 0.9093023538589478, 0.9069767594337463, 0.9124031066894531, 0.9134367108345032, 0.9098191261291504, 0.906718373298645, 0.9025839567184448, 0.9152454733848572, 0.9116278886795044, 0.91886305809021, 0.910335898399353, 0.9126614928245544, 0.9111111164093018, 0.9155038595199585, 0.9136950969696045, 0.9082687497138977, 0.910335898399353, 0.9124031066894531, 0.9113695025444031, 0.9232558012008667, 0.9240310192108154, 0.9253230094909668, 0.9229974150657654, 0.9201550483703613, 0.9124031066894531, 0.9142118692398071, 0.9253230094909668], 'val_loss': [1.1277624368667603, 1.1109662055969238, 1.10004460811615, 1.0831692218780518, 1.068896770477295, 1.0562107563018799, 1.0468518733978271, 1.024119257926941, 1.0091228485107422, 0.9830688238143921, 0.9645748734474182, 0.9428318738937378, 0.9163705110549927, 0.8947122097015381, 0.8714460134506226, 0.8512367010116577, 0.8238193988800049, 0.8258031010627747, 0.8227837681770325, 0.8033132553100586, 0.7670847773551941, 0.7558754086494446, 0.7568119168281555, 0.7491674423217773, 0.7412367463111877, 0.7425314784049988, 0.7363892197608948, 0.7347217202186584, 0.7525019645690918, 0.7303545475006104, 0.7541112899780273, 0.7440918684005737, 0.7281404733657837, 0.7225589752197266, 0.7282667756080627, 0.7216858863830566, 0.7211428284645081, 0.7159377336502075, 0.7196614146232605, 0.7164772748947144, 0.7189136743545532, 0.7280195355415344, 0.7175180912017822, 0.7051576375961304, 0.7051385641098022, 0.6999311447143555, 0.6986203789710999, 0.7271758317947388, 0.6954541802406311, 0.7288260459899902, 0.6968799829483032, 0.6952653527259827, 0.7170535922050476, 0.6962369680404663, 0.6934893727302551, 0.7111173272132874, 0.7013518810272217, 0.7149670720100403, 0.7248112559318542, 0.6839155554771423, 0.6845085620880127, 0.6902128458023071, 0.6879552006721497, 0.6775588393211365, 0.683838963508606, 0.6823612451553345, 0.6795760989189148, 0.6751170754432678, 0.6762821674346924, 0.6745455265045166, 0.673664391040802, 0.6793758869171143, 0.6788939237594604, 0.6778247952461243, 0.6884437203407288, 0.6730596423149109, 0.6789646148681641, 0.671464741230011, 0.6948205828666687, 0.669746994972229, 0.6683371663093567, 0.6769691109657288, 0.6761225461959839, 0.6677166819572449, 0.7380500435829163, 0.6618928909301758, 0.6649396419525146, 0.726408839225769, 0.6961525082588196, 0.6731507778167725, 0.7103098630905151, 0.6569585800170898, 0.6553908586502075, 0.6628338694572449, 0.6596435904502869, 0.6629347205162048, 0.6649700999259949, 0.6842222213745117, 0.6563143134117126, 0.6644645929336548], 'val_accuracy': [0.48553720116615295, 0.48553720116615295, 0.48553720116615295, 0.5165289044380188, 0.5950413346290588, 0.6528925895690918, 0.6280992031097412, 0.7675619721412659, 0.7768595218658447, 0.83574378490448, 0.8326446413993835, 0.8264462947845459, 0.8336777091026306, 0.8398760557174683, 0.8305785059928894, 0.8212810158729553, 0.8378099203109741, 0.8099173307418823, 0.8057851195335388, 0.8192148804664612, 0.8419421315193176, 0.8460744023323059, 0.8409090638160706, 0.8471074104309082, 0.8440082669258118, 0.8471074104309082, 0.8471074104309082, 0.8440082669258118, 0.8347107172012329, 0.8419421315193176, 0.8347107172012329, 0.8409090638160706, 0.8419421315193176, 0.8481404781341553, 0.8450413346290588, 0.8481404781341553, 0.8429751992225647, 0.8512396812438965, 0.8450413346290588, 0.8481404781341553, 0.8450413346290588, 0.8450413346290588, 0.8450413346290588, 0.8460744023323059, 0.8522727489471436, 0.8512396812438965, 0.8522727489471436, 0.8388429880142212, 0.8471074104309082, 0.8347107172012329, 0.8471074104309082, 0.8419421315193176, 0.8429751992225647, 0.8378099203109741, 0.8419421315193176, 0.8440082669258118, 0.8440082669258118, 0.8347107172012329, 0.8388429880142212, 0.8440082669258118, 0.8419421315193176, 0.8429751992225647, 0.8429751992225647, 0.8419421315193176, 0.8419421315193176, 0.8429751992225647, 0.8481404781341553, 0.8440082669258118, 0.8419421315193176, 0.8450413346290588, 0.8481404781341553, 0.8502066135406494, 0.8491735458374023, 0.8429751992225647, 0.8450413346290588, 0.8502066135406494, 0.8450413346290588, 0.8481404781341553, 0.8398760557174683, 0.8460744023323059, 0.8522727489471436, 0.8440082669258118, 0.8460744023323059, 0.8533057570457458, 0.8285123705863953, 0.8512396812438965, 0.8533057570457458, 0.8285123705863953, 0.8378099203109741, 0.8471074104309082, 0.827479362487793, 0.85537189245224, 0.8502066135406494, 0.8512396812438965, 0.8533057570457458, 0.8543388247489929, 0.8502066135406494, 0.8409090638160706, 0.8543388247489929, 0.8460744023323059]}\n","32/32 [==============================] - 1s 5ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/29 [===========================>..] - ETA: 0s - loss: 0.5372 - accuracy: 0.8973"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 6s 49ms/step - loss: 0.5351 - accuracy: 0.8982 - val_loss: 0.9914 - val_accuracy: 0.4849\n","Epoch 2/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.5207 - accuracy: 0.9111 - val_loss: 0.9640 - val_accuracy: 0.4849\n","Epoch 3/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.5192 - accuracy: 0.9068 - val_loss: 0.9409 - val_accuracy: 0.4871\n","Epoch 4/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.5188 - accuracy: 0.9073 - val_loss: 0.9191 - val_accuracy: 0.5140\n","Epoch 5/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.5112 - accuracy: 0.9122 - val_loss: 0.9108 - val_accuracy: 0.5334\n","Epoch 6/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.5075 - accuracy: 0.9124 - val_loss: 0.8984 - val_accuracy: 0.5668\n","Epoch 7/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.5122 - accuracy: 0.9041 - val_loss: 0.8807 - val_accuracy: 0.6239\n","Epoch 8/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.4946 - accuracy: 0.9181 - val_loss: 0.8642 - val_accuracy: 0.6832\n","Epoch 9/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.4985 - accuracy: 0.9133 - val_loss: 0.8433 - val_accuracy: 0.7586\n","Epoch 10/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.4923 - accuracy: 0.9168 - val_loss: 0.8190 - val_accuracy: 0.7963\n","Epoch 11/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4942 - accuracy: 0.9146 - val_loss: 0.8021 - val_accuracy: 0.7953\n","Epoch 12/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.4821 - accuracy: 0.9211 - val_loss: 0.7718 - val_accuracy: 0.8351\n","Epoch 13/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4801 - accuracy: 0.9219 - val_loss: 0.7513 - val_accuracy: 0.8341\n","Epoch 14/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4760 - accuracy: 0.9238 - val_loss: 0.7242 - val_accuracy: 0.8384\n","Epoch 15/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.4750 - accuracy: 0.9238 - val_loss: 0.6967 - val_accuracy: 0.8470\n","Epoch 16/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4742 - accuracy: 0.9213 - val_loss: 0.6677 - val_accuracy: 0.8481\n","Epoch 17/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4677 - accuracy: 0.9262 - val_loss: 0.6398 - val_accuracy: 0.8567\n","Epoch 18/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4730 - accuracy: 0.9197 - val_loss: 0.6136 - val_accuracy: 0.8556\n","Epoch 19/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4653 - accuracy: 0.9267 - val_loss: 0.6131 - val_accuracy: 0.8502\n","Epoch 20/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.4525 - accuracy: 0.9310 - val_loss: 0.5873 - val_accuracy: 0.8675\n","Epoch 21/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4626 - accuracy: 0.9251 - val_loss: 0.5911 - val_accuracy: 0.8588\n","Epoch 22/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4579 - accuracy: 0.9294 - val_loss: 0.5845 - val_accuracy: 0.8642\n","Epoch 23/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4547 - accuracy: 0.9283 - val_loss: 0.5947 - val_accuracy: 0.8578\n","Epoch 24/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4513 - accuracy: 0.9310 - val_loss: 0.5782 - val_accuracy: 0.8739\n","Epoch 25/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.4467 - accuracy: 0.9291 - val_loss: 0.5746 - val_accuracy: 0.8675\n","Epoch 26/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4509 - accuracy: 0.9291 - val_loss: 0.6425 - val_accuracy: 0.8427\n","Epoch 27/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4531 - accuracy: 0.9256 - val_loss: 0.6118 - val_accuracy: 0.8567\n","Epoch 28/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.4382 - accuracy: 0.9351 - val_loss: 0.5637 - val_accuracy: 0.8772\n","Epoch 29/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.4403 - accuracy: 0.9281 - val_loss: 0.5549 - val_accuracy: 0.8804\n","Epoch 30/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4402 - accuracy: 0.9332 - val_loss: 0.5574 - val_accuracy: 0.8772\n","Epoch 31/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4385 - accuracy: 0.9364 - val_loss: 0.5598 - val_accuracy: 0.8793\n","Epoch 32/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4341 - accuracy: 0.9353 - val_loss: 0.5530 - val_accuracy: 0.8772\n","Epoch 33/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4302 - accuracy: 0.9380 - val_loss: 0.5814 - val_accuracy: 0.8750\n","Epoch 34/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4311 - accuracy: 0.9356 - val_loss: 0.5618 - val_accuracy: 0.8804\n","Epoch 35/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.4281 - accuracy: 0.9353 - val_loss: 0.5670 - val_accuracy: 0.8750\n","Epoch 36/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4334 - accuracy: 0.9308 - val_loss: 0.5608 - val_accuracy: 0.8772\n","Epoch 37/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.4290 - accuracy: 0.9353 - val_loss: 0.5612 - val_accuracy: 0.8804\n","Epoch 38/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.4286 - accuracy: 0.9343 - val_loss: 0.5626 - val_accuracy: 0.8804\n","Epoch 39/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.4165 - accuracy: 0.9440 - val_loss: 0.5563 - val_accuracy: 0.8815\n","Epoch 40/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4201 - accuracy: 0.9375 - val_loss: 0.6309 - val_accuracy: 0.8642\n","Epoch 41/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4294 - accuracy: 0.9308 - val_loss: 0.5716 - val_accuracy: 0.8815\n","Epoch 42/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4227 - accuracy: 0.9370 - val_loss: 0.5657 - val_accuracy: 0.8772\n","Epoch 43/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4033 - accuracy: 0.9461 - val_loss: 0.5564 - val_accuracy: 0.8793\n","Epoch 44/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4003 - accuracy: 0.9456 - val_loss: 0.5610 - val_accuracy: 0.8815\n","Epoch 45/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4069 - accuracy: 0.9413 - val_loss: 0.5686 - val_accuracy: 0.8804\n","Epoch 46/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4086 - accuracy: 0.9442 - val_loss: 0.5945 - val_accuracy: 0.8642\n","Epoch 47/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.4063 - accuracy: 0.9421 - val_loss: 0.5933 - val_accuracy: 0.8739\n","Epoch 48/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.4191 - accuracy: 0.9324 - val_loss: 0.5677 - val_accuracy: 0.8858\n","Epoch 49/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4131 - accuracy: 0.9388 - val_loss: 0.5753 - val_accuracy: 0.8685\n","Epoch 50/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3988 - accuracy: 0.9467 - val_loss: 0.5566 - val_accuracy: 0.8728\n","Epoch 51/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3859 - accuracy: 0.9542 - val_loss: 0.5576 - val_accuracy: 0.8761\n","Epoch 52/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3918 - accuracy: 0.9491 - val_loss: 0.5650 - val_accuracy: 0.8782\n","Epoch 53/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3874 - accuracy: 0.9483 - val_loss: 0.5578 - val_accuracy: 0.8793\n","Epoch 54/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.4003 - accuracy: 0.9442 - val_loss: 0.6188 - val_accuracy: 0.8631\n","Epoch 55/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3957 - accuracy: 0.9445 - val_loss: 0.5526 - val_accuracy: 0.8804\n","Epoch 56/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3807 - accuracy: 0.9566 - val_loss: 0.5592 - val_accuracy: 0.8815\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3791 - accuracy: 0.9512 - val_loss: 0.5642 - val_accuracy: 0.8836\n","Epoch 58/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3777 - accuracy: 0.9515 - val_loss: 0.5630 - val_accuracy: 0.8825\n","Epoch 59/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3815 - accuracy: 0.9464 - val_loss: 0.6582 - val_accuracy: 0.8588\n","Epoch 60/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3895 - accuracy: 0.9477 - val_loss: 0.5667 - val_accuracy: 0.8836\n","Epoch 61/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3803 - accuracy: 0.9494 - val_loss: 0.5721 - val_accuracy: 0.8750\n","Epoch 62/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3785 - accuracy: 0.9518 - val_loss: 0.6039 - val_accuracy: 0.8728\n","Epoch 63/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3808 - accuracy: 0.9483 - val_loss: 0.5543 - val_accuracy: 0.8782\n","Epoch 64/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3699 - accuracy: 0.9577 - val_loss: 0.5522 - val_accuracy: 0.8815\n","Epoch 65/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3667 - accuracy: 0.9585 - val_loss: 0.5588 - val_accuracy: 0.8825\n","Epoch 66/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3779 - accuracy: 0.9485 - val_loss: 0.5753 - val_accuracy: 0.8772\n","Epoch 67/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.3692 - accuracy: 0.9531 - val_loss: 0.5620 - val_accuracy: 0.8782\n","Epoch 68/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3597 - accuracy: 0.9615 - val_loss: 0.5600 - val_accuracy: 0.8793\n","Epoch 69/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3566 - accuracy: 0.9615 - val_loss: 0.5583 - val_accuracy: 0.8815\n","Epoch 70/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3645 - accuracy: 0.9510 - val_loss: 0.5566 - val_accuracy: 0.8772\n","Epoch 71/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3652 - accuracy: 0.9555 - val_loss: 0.5618 - val_accuracy: 0.8772\n","Epoch 72/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3561 - accuracy: 0.9585 - val_loss: 0.5554 - val_accuracy: 0.8804\n","Epoch 73/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3528 - accuracy: 0.9609 - val_loss: 0.5618 - val_accuracy: 0.8804\n","Epoch 74/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3559 - accuracy: 0.9601 - val_loss: 0.5609 - val_accuracy: 0.8761\n","Epoch 75/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3494 - accuracy: 0.9655 - val_loss: 0.5649 - val_accuracy: 0.8761\n","Epoch 76/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3693 - accuracy: 0.9483 - val_loss: 0.5704 - val_accuracy: 0.8782\n","Epoch 77/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3535 - accuracy: 0.9588 - val_loss: 0.5977 - val_accuracy: 0.8772\n","Epoch 78/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3525 - accuracy: 0.9612 - val_loss: 0.5644 - val_accuracy: 0.8761\n","Epoch 79/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3523 - accuracy: 0.9582 - val_loss: 0.6063 - val_accuracy: 0.8728\n","Epoch 80/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3502 - accuracy: 0.9582 - val_loss: 0.5964 - val_accuracy: 0.8772\n","Epoch 81/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3556 - accuracy: 0.9553 - val_loss: 0.5988 - val_accuracy: 0.8718\n","Epoch 82/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3557 - accuracy: 0.9555 - val_loss: 0.5660 - val_accuracy: 0.8825\n","Epoch 83/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3467 - accuracy: 0.9617 - val_loss: 0.5909 - val_accuracy: 0.8782\n","Epoch 84/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3498 - accuracy: 0.9601 - val_loss: 0.5591 - val_accuracy: 0.8750\n","Epoch 85/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3446 - accuracy: 0.9604 - val_loss: 0.5926 - val_accuracy: 0.8761\n","Epoch 86/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3444 - accuracy: 0.9634 - val_loss: 0.5769 - val_accuracy: 0.8782\n","Epoch 87/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3487 - accuracy: 0.9574 - val_loss: 0.5656 - val_accuracy: 0.8772\n","Epoch 88/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3417 - accuracy: 0.9620 - val_loss: 0.5686 - val_accuracy: 0.8782\n","Epoch 89/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3335 - accuracy: 0.9644 - val_loss: 0.5709 - val_accuracy: 0.8782\n","Epoch 90/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3341 - accuracy: 0.9644 - val_loss: 0.5777 - val_accuracy: 0.8847\n","Epoch 91/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3331 - accuracy: 0.9663 - val_loss: 0.5826 - val_accuracy: 0.8761\n","Epoch 92/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3292 - accuracy: 0.9709 - val_loss: 0.5716 - val_accuracy: 0.8836\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3241 - accuracy: 0.9696 - val_loss: 0.5750 - val_accuracy: 0.8761\n","Epoch 94/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3284 - accuracy: 0.9671 - val_loss: 0.5753 - val_accuracy: 0.8858\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3286 - accuracy: 0.9674 - val_loss: 0.6304 - val_accuracy: 0.8750\n","Epoch 96/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3399 - accuracy: 0.9574 - val_loss: 0.5752 - val_accuracy: 0.8793\n","Epoch 97/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3360 - accuracy: 0.9639 - val_loss: 0.5792 - val_accuracy: 0.8793\n","Epoch 98/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3319 - accuracy: 0.9682 - val_loss: 0.6156 - val_accuracy: 0.8728\n","Epoch 99/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3238 - accuracy: 0.9655 - val_loss: 0.5707 - val_accuracy: 0.8825\n","Epoch 100/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3161 - accuracy: 0.9723 - val_loss: 0.5826 - val_accuracy: 0.8793\n","{'loss': [0.5350502133369446, 0.5206553936004639, 0.5191934704780579, 0.5187860131263733, 0.51118004322052, 0.5074940919876099, 0.5121644735336304, 0.49463972449302673, 0.49850577116012573, 0.4922991096973419, 0.49419647455215454, 0.48206812143325806, 0.48014479875564575, 0.47602298855781555, 0.475047767162323, 0.4741913080215454, 0.46773824095726013, 0.472988486289978, 0.4653126001358032, 0.45246782898902893, 0.46260401606559753, 0.45785707235336304, 0.4546769857406616, 0.4512840211391449, 0.4467005431652069, 0.4509095847606659, 0.4530738592147827, 0.4382357895374298, 0.4403242766857147, 0.4402283728122711, 0.43849658966064453, 0.43414729833602905, 0.4302315413951874, 0.431144654750824, 0.4280518591403961, 0.4334072768688202, 0.4290456771850586, 0.4285646677017212, 0.4164585769176483, 0.4200642704963684, 0.42941081523895264, 0.422676682472229, 0.4032663106918335, 0.40029147267341614, 0.4069262444972992, 0.4085874855518341, 0.4063263535499573, 0.4190811216831207, 0.4131399691104889, 0.3988269865512848, 0.3859326243400574, 0.39184558391571045, 0.3873744606971741, 0.4002998471260071, 0.39566293358802795, 0.3806624114513397, 0.37911227345466614, 0.37766075134277344, 0.3815356492996216, 0.3894994258880615, 0.3802672028541565, 0.3784900903701782, 0.38078421354293823, 0.3699217736721039, 0.36673828959465027, 0.37789878249168396, 0.3691534101963043, 0.35971203446388245, 0.3565810024738312, 0.3644710183143616, 0.3651570677757263, 0.3560870885848999, 0.3528277277946472, 0.35591921210289, 0.34942856431007385, 0.3692930042743683, 0.3534911274909973, 0.35251736640930176, 0.35226327180862427, 0.35020795464515686, 0.3556288480758667, 0.3556697368621826, 0.34666764736175537, 0.3497735261917114, 0.34456774592399597, 0.3444376587867737, 0.3487071096897125, 0.3417404294013977, 0.33349257707595825, 0.3341354727745056, 0.33307284116744995, 0.32918858528137207, 0.32411935925483704, 0.3283902406692505, 0.3285650908946991, 0.33988457918167114, 0.3359808027744293, 0.33191654086112976, 0.3237968385219574, 0.31614401936531067], 'accuracy': [0.8981680870056152, 0.9110991358757019, 0.9067887663841248, 0.9073275923728943, 0.9121767282485962, 0.912446141242981, 0.9040948152542114, 0.9181034564971924, 0.9132543206214905, 0.9167564511299133, 0.9146012663841248, 0.9210668206214905, 0.921875, 0.9237607717514038, 0.9237607717514038, 0.9213362336158752, 0.9261853694915771, 0.9197198152542114, 0.9267241358757019, 0.931034505367279, 0.9251077771186829, 0.9294180870056152, 0.928340494632721, 0.931034505367279, 0.9291487336158752, 0.9291487336158752, 0.9256465435028076, 0.9350754022598267, 0.928071141242981, 0.9331896305084229, 0.9364224076271057, 0.9353448152542114, 0.9380387663841248, 0.9356142282485962, 0.9353448152542114, 0.9307650923728943, 0.9353448152542114, 0.9342672228813171, 0.943965494632721, 0.9375, 0.9307650923728943, 0.9369612336158752, 0.9461206793785095, 0.9455819129943848, 0.9412715435028076, 0.9442349076271057, 0.9420797228813171, 0.9323814511299133, 0.938847005367279, 0.946659505367279, 0.9542025923728943, 0.9490840435028076, 0.9482758641242981, 0.9442349076271057, 0.9445043206214905, 0.9566271305084229, 0.9512392282485962, 0.951508641242981, 0.9463900923728943, 0.9477370977401733, 0.9493534564971924, 0.951777994632721, 0.9482758641242981, 0.9577047228813171, 0.9585129022598267, 0.9485452771186829, 0.953125, 0.9614762663841248, 0.9614762663841248, 0.9509698152542114, 0.9555495977401733, 0.9585129022598267, 0.9609375, 0.9601293206214905, 0.9655172228813171, 0.9482758641242981, 0.9587823152542114, 0.9612069129943848, 0.9582435488700867, 0.9582435488700867, 0.9552801847457886, 0.9555495977401733, 0.9617456793785095, 0.9601293206214905, 0.9603987336158752, 0.9633620977401733, 0.9574353694915771, 0.9620150923728943, 0.9644396305084229, 0.9644396305084229, 0.9663254022598267, 0.9709051847457886, 0.9695581793785095, 0.967133641242981, 0.967402994632721, 0.9574353694915771, 0.9639008641242981, 0.9682112336158752, 0.9655172228813171, 0.9722521305084229], 'val_loss': [0.9913791418075562, 0.963954508304596, 0.9409164786338806, 0.9191015958786011, 0.9108024835586548, 0.8983813524246216, 0.8806743025779724, 0.8642445802688599, 0.8432577252388, 0.8190076351165771, 0.802074670791626, 0.7718362212181091, 0.7513496279716492, 0.7241844534873962, 0.6967189311981201, 0.6676843166351318, 0.6398184299468994, 0.6136455535888672, 0.6131373643875122, 0.587268054485321, 0.5911296606063843, 0.5844674706459045, 0.594740092754364, 0.5782024264335632, 0.5745910406112671, 0.6425372362136841, 0.6117697358131409, 0.5636959671974182, 0.5549373030662537, 0.5574200749397278, 0.5597798824310303, 0.5530006289482117, 0.581413209438324, 0.5617921352386475, 0.566955029964447, 0.5608487129211426, 0.5612059235572815, 0.5626112818717957, 0.556340754032135, 0.6309077143669128, 0.5715584754943848, 0.5657430291175842, 0.5564407706260681, 0.5610304474830627, 0.5686492919921875, 0.5944597125053406, 0.593337893486023, 0.5677395462989807, 0.5753222107887268, 0.55656498670578, 0.557624101638794, 0.5649749636650085, 0.5577985048294067, 0.6188104152679443, 0.5526022911071777, 0.5591691732406616, 0.5642208456993103, 0.5629634261131287, 0.6582022309303284, 0.566748321056366, 0.5721003413200378, 0.603876531124115, 0.5542582869529724, 0.552208662033081, 0.5588394403457642, 0.5753249526023865, 0.5619813203811646, 0.5600125789642334, 0.5582836270332336, 0.5565752983093262, 0.561797022819519, 0.5554249286651611, 0.5617833733558655, 0.560926616191864, 0.5649018287658691, 0.5704416036605835, 0.597710907459259, 0.5644493699073792, 0.6063279509544373, 0.5963846445083618, 0.5987924933433533, 0.5659889578819275, 0.5909491777420044, 0.5590533018112183, 0.5925832986831665, 0.5768551826477051, 0.5656065344810486, 0.5685742497444153, 0.5708728432655334, 0.5776598453521729, 0.5826022624969482, 0.5715924501419067, 0.5749805569648743, 0.5752972364425659, 0.6303758025169373, 0.5752133727073669, 0.5791735053062439, 0.6155655980110168, 0.570733368396759, 0.5825818777084351], 'val_accuracy': [0.48491379618644714, 0.48491379618644714, 0.48706895112991333, 0.514008641242981, 0.5334051847457886, 0.5668103694915771, 0.6239224076271057, 0.6831896305084229, 0.7586206793785095, 0.7963362336158752, 0.795258641242981, 0.8351293206214905, 0.8340517282485962, 0.8383620977401733, 0.8469827771186829, 0.8480603694915771, 0.8566810488700867, 0.8556034564971924, 0.850215494632721, 0.8674569129943848, 0.8588362336158752, 0.8642241358757019, 0.857758641242981, 0.8739224076271057, 0.8674569129943848, 0.8426724076271057, 0.8566810488700867, 0.8771551847457886, 0.8803879022598267, 0.8771551847457886, 0.8793103694915771, 0.8771551847457886, 0.875, 0.8803879022598267, 0.875, 0.8771551847457886, 0.8803879022598267, 0.8803879022598267, 0.881465494632721, 0.8642241358757019, 0.881465494632721, 0.8771551847457886, 0.8793103694915771, 0.881465494632721, 0.8803879022598267, 0.8642241358757019, 0.8739224076271057, 0.8857758641242981, 0.868534505367279, 0.8728448152542114, 0.8760775923728943, 0.8782327771186829, 0.8793103694915771, 0.8631465435028076, 0.8803879022598267, 0.881465494632721, 0.8836206793785095, 0.8825430870056152, 0.8588362336158752, 0.8836206793785095, 0.875, 0.8728448152542114, 0.8782327771186829, 0.881465494632721, 0.8825430870056152, 0.8771551847457886, 0.8782327771186829, 0.8793103694915771, 0.881465494632721, 0.8771551847457886, 0.8771551847457886, 0.8803879022598267, 0.8803879022598267, 0.8760775923728943, 0.8760775923728943, 0.8782327771186829, 0.8771551847457886, 0.8760775923728943, 0.8728448152542114, 0.8771551847457886, 0.8717672228813171, 0.8825430870056152, 0.8782327771186829, 0.875, 0.8760775923728943, 0.8782327771186829, 0.8771551847457886, 0.8782327771186829, 0.8782327771186829, 0.8846982717514038, 0.8760775923728943, 0.8836206793785095, 0.8760775923728943, 0.8857758641242981, 0.875, 0.8793103694915771, 0.8793103694915771, 0.8728448152542114, 0.8825430870056152, 0.8793103694915771]}\n","38/38 [==============================] - 1s 4ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - ETA: 0s - loss: 0.5490 - accuracy: 0.8973"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 7s 52ms/step - loss: 0.5490 - accuracy: 0.8973 - val_loss: 0.9923 - val_accuracy: 0.4955\n","Epoch 2/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.5382 - accuracy: 0.8953 - val_loss: 0.9695 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.5302 - accuracy: 0.8973 - val_loss: 0.9467 - val_accuracy: 0.4955\n","Epoch 4/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5213 - accuracy: 0.9055 - val_loss: 0.9307 - val_accuracy: 0.5023\n","Epoch 5/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.5124 - accuracy: 0.9075 - val_loss: 0.9234 - val_accuracy: 0.5147\n","Epoch 6/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.5122 - accuracy: 0.9078 - val_loss: 0.9035 - val_accuracy: 0.5803\n","Epoch 7/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.5069 - accuracy: 0.9111 - val_loss: 0.8964 - val_accuracy: 0.5871\n","Epoch 8/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.5011 - accuracy: 0.9100 - val_loss: 0.8841 - val_accuracy: 0.6176\n","Epoch 9/100\n","28/28 [==============================] - 1s 27ms/step - loss: 0.5062 - accuracy: 0.9120 - val_loss: 0.8698 - val_accuracy: 0.6425\n","Epoch 10/100\n","28/28 [==============================] - 1s 50ms/step - loss: 0.5043 - accuracy: 0.9063 - val_loss: 0.8496 - val_accuracy: 0.7070\n","Epoch 11/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4978 - accuracy: 0.9100 - val_loss: 0.8425 - val_accuracy: 0.6934\n","Epoch 12/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4948 - accuracy: 0.9143 - val_loss: 0.8044 - val_accuracy: 0.7771\n","Epoch 13/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4994 - accuracy: 0.9072 - val_loss: 0.7960 - val_accuracy: 0.7319\n","Epoch 14/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.5071 - accuracy: 0.9010 - val_loss: 0.7557 - val_accuracy: 0.8213\n","Epoch 15/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4912 - accuracy: 0.9137 - val_loss: 0.7560 - val_accuracy: 0.7749\n","Epoch 16/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4781 - accuracy: 0.9228 - val_loss: 0.7219 - val_accuracy: 0.8122\n","Epoch 17/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.4763 - accuracy: 0.9162 - val_loss: 0.6903 - val_accuracy: 0.8394\n","Epoch 18/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4736 - accuracy: 0.9247 - val_loss: 0.6734 - val_accuracy: 0.8235\n","Epoch 19/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4726 - accuracy: 0.9196 - val_loss: 0.6596 - val_accuracy: 0.8371\n","Epoch 20/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4653 - accuracy: 0.9270 - val_loss: 0.6476 - val_accuracy: 0.8314\n","Epoch 21/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4711 - accuracy: 0.9202 - val_loss: 0.6404 - val_accuracy: 0.8303\n","Epoch 22/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.4630 - accuracy: 0.9222 - val_loss: 0.6164 - val_accuracy: 0.8529\n","Epoch 23/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4682 - accuracy: 0.9270 - val_loss: 0.6079 - val_accuracy: 0.8450\n","Epoch 24/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4594 - accuracy: 0.9273 - val_loss: 0.6266 - val_accuracy: 0.8428\n","Epoch 25/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4526 - accuracy: 0.9290 - val_loss: 0.6633 - val_accuracy: 0.8314\n","Epoch 26/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4644 - accuracy: 0.9244 - val_loss: 0.7115 - val_accuracy: 0.8167\n","Epoch 27/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4781 - accuracy: 0.9114 - val_loss: 0.6430 - val_accuracy: 0.8382\n","Epoch 28/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4603 - accuracy: 0.9213 - val_loss: 0.6687 - val_accuracy: 0.8337\n","Epoch 29/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.4562 - accuracy: 0.9253 - val_loss: 0.6107 - val_accuracy: 0.8563\n","Epoch 30/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.4492 - accuracy: 0.9307 - val_loss: 0.5998 - val_accuracy: 0.8597\n","Epoch 31/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4417 - accuracy: 0.9332 - val_loss: 0.6420 - val_accuracy: 0.8462\n","Epoch 32/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4467 - accuracy: 0.9307 - val_loss: 0.6234 - val_accuracy: 0.8484\n","Epoch 33/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4334 - accuracy: 0.9372 - val_loss: 0.6190 - val_accuracy: 0.8541\n","Epoch 34/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4413 - accuracy: 0.9315 - val_loss: 0.6347 - val_accuracy: 0.8563\n","Epoch 35/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.4412 - accuracy: 0.9310 - val_loss: 0.6199 - val_accuracy: 0.8620\n","Epoch 36/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4350 - accuracy: 0.9346 - val_loss: 0.6068 - val_accuracy: 0.8620\n","Epoch 37/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4333 - accuracy: 0.9346 - val_loss: 0.6919 - val_accuracy: 0.8382\n","Epoch 38/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.4336 - accuracy: 0.9315 - val_loss: 0.6127 - val_accuracy: 0.8654\n","Epoch 39/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4275 - accuracy: 0.9349 - val_loss: 0.6496 - val_accuracy: 0.8484\n","Epoch 40/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.4274 - accuracy: 0.9335 - val_loss: 0.6113 - val_accuracy: 0.8586\n","Epoch 41/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.4261 - accuracy: 0.9298 - val_loss: 0.6118 - val_accuracy: 0.8575\n","Epoch 42/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4299 - accuracy: 0.9349 - val_loss: 0.6482 - val_accuracy: 0.8541\n","Epoch 43/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4288 - accuracy: 0.9369 - val_loss: 0.6310 - val_accuracy: 0.8609\n","Epoch 44/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.4158 - accuracy: 0.9394 - val_loss: 0.6119 - val_accuracy: 0.8609\n","Epoch 45/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4137 - accuracy: 0.9414 - val_loss: 0.6152 - val_accuracy: 0.8586\n","Epoch 46/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4139 - accuracy: 0.9377 - val_loss: 0.6968 - val_accuracy: 0.8224\n","Epoch 47/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4302 - accuracy: 0.9284 - val_loss: 0.6694 - val_accuracy: 0.8326\n","Epoch 48/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4150 - accuracy: 0.9397 - val_loss: 0.6259 - val_accuracy: 0.8552\n","Epoch 49/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4373 - accuracy: 0.9261 - val_loss: 0.6407 - val_accuracy: 0.8405\n","Epoch 50/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4088 - accuracy: 0.9392 - val_loss: 0.6066 - val_accuracy: 0.8563\n","Epoch 51/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.4031 - accuracy: 0.9428 - val_loss: 0.6239 - val_accuracy: 0.8563\n","Epoch 52/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.4078 - accuracy: 0.9409 - val_loss: 0.6147 - val_accuracy: 0.8609\n","Epoch 53/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3973 - accuracy: 0.9454 - val_loss: 0.6270 - val_accuracy: 0.8541\n","Epoch 54/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3995 - accuracy: 0.9440 - val_loss: 0.6231 - val_accuracy: 0.8552\n","Epoch 55/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3975 - accuracy: 0.9457 - val_loss: 0.6467 - val_accuracy: 0.8552\n","Epoch 56/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3920 - accuracy: 0.9485 - val_loss: 0.6308 - val_accuracy: 0.8586\n","Epoch 57/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3940 - accuracy: 0.9468 - val_loss: 0.6476 - val_accuracy: 0.8495\n","Epoch 58/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3999 - accuracy: 0.9406 - val_loss: 0.6186 - val_accuracy: 0.8609\n","Epoch 59/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.4043 - accuracy: 0.9417 - val_loss: 0.6224 - val_accuracy: 0.8575\n","Epoch 60/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3900 - accuracy: 0.9479 - val_loss: 0.6357 - val_accuracy: 0.8575\n","Epoch 61/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3844 - accuracy: 0.9510 - val_loss: 0.6371 - val_accuracy: 0.8529\n","Epoch 62/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3818 - accuracy: 0.9479 - val_loss: 0.6301 - val_accuracy: 0.8507\n","Epoch 63/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3801 - accuracy: 0.9516 - val_loss: 0.6332 - val_accuracy: 0.8563\n","Epoch 64/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3810 - accuracy: 0.9474 - val_loss: 0.6327 - val_accuracy: 0.8552\n","Epoch 65/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3775 - accuracy: 0.9519 - val_loss: 0.6325 - val_accuracy: 0.8575\n","Epoch 66/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3832 - accuracy: 0.9491 - val_loss: 0.6187 - val_accuracy: 0.8620\n","Epoch 67/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3705 - accuracy: 0.9536 - val_loss: 0.6292 - val_accuracy: 0.8563\n","Epoch 68/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3706 - accuracy: 0.9539 - val_loss: 0.6261 - val_accuracy: 0.8609\n","Epoch 69/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3744 - accuracy: 0.9505 - val_loss: 0.6252 - val_accuracy: 0.8575\n","Epoch 70/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.3835 - accuracy: 0.9468 - val_loss: 0.6265 - val_accuracy: 0.8620\n","Epoch 71/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3667 - accuracy: 0.9519 - val_loss: 0.6192 - val_accuracy: 0.8631\n","Epoch 72/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3625 - accuracy: 0.9561 - val_loss: 0.6395 - val_accuracy: 0.8541\n","Epoch 73/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3624 - accuracy: 0.9564 - val_loss: 0.6294 - val_accuracy: 0.8563\n","Epoch 74/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.3773 - accuracy: 0.9482 - val_loss: 0.6265 - val_accuracy: 0.8586\n","Epoch 75/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3607 - accuracy: 0.9567 - val_loss: 0.6258 - val_accuracy: 0.8552\n","Epoch 76/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3623 - accuracy: 0.9561 - val_loss: 0.6593 - val_accuracy: 0.8416\n","Epoch 77/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3599 - accuracy: 0.9539 - val_loss: 0.6383 - val_accuracy: 0.8507\n","Epoch 78/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3553 - accuracy: 0.9593 - val_loss: 0.6648 - val_accuracy: 0.8371\n","Epoch 79/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3590 - accuracy: 0.9576 - val_loss: 0.6557 - val_accuracy: 0.8484\n","Epoch 80/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3508 - accuracy: 0.9598 - val_loss: 0.6417 - val_accuracy: 0.8575\n","Epoch 81/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3507 - accuracy: 0.9587 - val_loss: 0.6592 - val_accuracy: 0.8507\n","Epoch 82/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3541 - accuracy: 0.9564 - val_loss: 0.6392 - val_accuracy: 0.8575\n","Epoch 83/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3469 - accuracy: 0.9598 - val_loss: 0.6516 - val_accuracy: 0.8507\n","Epoch 84/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3561 - accuracy: 0.9553 - val_loss: 0.6353 - val_accuracy: 0.8586\n","Epoch 85/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3447 - accuracy: 0.9590 - val_loss: 0.6397 - val_accuracy: 0.8575\n","Epoch 86/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3548 - accuracy: 0.9553 - val_loss: 0.6319 - val_accuracy: 0.8541\n","Epoch 87/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3412 - accuracy: 0.9669 - val_loss: 0.6428 - val_accuracy: 0.8541\n","Epoch 88/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3504 - accuracy: 0.9556 - val_loss: 0.6472 - val_accuracy: 0.8552\n","Epoch 89/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3715 - accuracy: 0.9482 - val_loss: 0.6388 - val_accuracy: 0.8597\n","Epoch 90/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3365 - accuracy: 0.9672 - val_loss: 0.6318 - val_accuracy: 0.8552\n","Epoch 91/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3402 - accuracy: 0.9598 - val_loss: 0.6452 - val_accuracy: 0.8552\n","Epoch 92/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3509 - accuracy: 0.9559 - val_loss: 0.6560 - val_accuracy: 0.8439\n","Epoch 93/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3354 - accuracy: 0.9638 - val_loss: 0.6454 - val_accuracy: 0.8552\n","Epoch 94/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3322 - accuracy: 0.9621 - val_loss: 0.6760 - val_accuracy: 0.8360\n","Epoch 95/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3446 - accuracy: 0.9595 - val_loss: 0.6855 - val_accuracy: 0.8371\n","Epoch 96/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3400 - accuracy: 0.9595 - val_loss: 0.6834 - val_accuracy: 0.8371\n","Epoch 97/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3372 - accuracy: 0.9632 - val_loss: 0.6381 - val_accuracy: 0.8541\n","Epoch 98/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3256 - accuracy: 0.9697 - val_loss: 0.6877 - val_accuracy: 0.8382\n","Epoch 99/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3427 - accuracy: 0.9621 - val_loss: 0.6474 - val_accuracy: 0.8484\n","Epoch 100/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3323 - accuracy: 0.9677 - val_loss: 0.6549 - val_accuracy: 0.8518\n","{'loss': [0.5490111708641052, 0.5381794571876526, 0.5302252173423767, 0.5213236212730408, 0.5123936533927917, 0.5122469663619995, 0.5068750977516174, 0.5010632276535034, 0.5061901211738586, 0.5043270587921143, 0.4977650046348572, 0.49484941363334656, 0.499356210231781, 0.507125735282898, 0.49118733406066895, 0.4780506193637848, 0.4762968420982361, 0.4735703468322754, 0.47262945771217346, 0.4652978777885437, 0.47105100750923157, 0.4630194306373596, 0.4681786596775055, 0.4594115912914276, 0.4526241421699524, 0.4644019603729248, 0.47813680768013, 0.46031156182289124, 0.4562362730503082, 0.449180006980896, 0.44174084067344666, 0.44673430919647217, 0.4334471821784973, 0.4413180947303772, 0.44115734100341797, 0.434962660074234, 0.43329039216041565, 0.4336022138595581, 0.42752763628959656, 0.42736443877220154, 0.42611363530158997, 0.4299306273460388, 0.42882880568504333, 0.4157552719116211, 0.41371950507164, 0.4138902723789215, 0.43015751242637634, 0.4149889647960663, 0.4373059868812561, 0.40883663296699524, 0.4031466841697693, 0.40776097774505615, 0.3973056674003601, 0.3995213508605957, 0.3974592685699463, 0.3920218050479889, 0.39402973651885986, 0.3999144434928894, 0.4042908847332001, 0.38999971747398376, 0.3843730390071869, 0.38184764981269836, 0.3800930380821228, 0.380991667509079, 0.37748488783836365, 0.3831643760204315, 0.37052083015441895, 0.3706139922142029, 0.37435561418533325, 0.3835248649120331, 0.36674964427948, 0.36245566606521606, 0.36237457394599915, 0.3772895038127899, 0.36074140667915344, 0.3623269200325012, 0.359892874956131, 0.3553229868412018, 0.3590129017829895, 0.3508218824863434, 0.350697785615921, 0.35408273339271545, 0.3468780815601349, 0.3560771048069, 0.3446730375289917, 0.3548499345779419, 0.3411885201931, 0.35042113065719604, 0.37150147557258606, 0.3364565968513489, 0.34024202823638916, 0.3508811295032501, 0.33536067605018616, 0.3321922719478607, 0.34456124901771545, 0.3399903476238251, 0.3371758460998535, 0.3256399631500244, 0.3427245318889618, 0.33233416080474854], 'accuracy': [0.8972835540771484, 0.8953027725219727, 0.8972835540771484, 0.9054895043373108, 0.9074702858924866, 0.9077532291412354, 0.9111488461494446, 0.9100169539451599, 0.9119977355003357, 0.9063384532928467, 0.9100169539451599, 0.9142614603042603, 0.9071873426437378, 0.9009620547294617, 0.9136955142021179, 0.9227504134178162, 0.916242241859436, 0.9247311949729919, 0.9196377992630005, 0.9269949197769165, 0.9202037453651428, 0.9221844673156738, 0.9269949197769165, 0.9272778630256653, 0.9289756417274475, 0.9244481921195984, 0.9114317893981934, 0.9213355779647827, 0.9252971410751343, 0.9306734800338745, 0.9332201480865479, 0.9306734800338745, 0.9371816515922546, 0.9315223693847656, 0.9309564232826233, 0.9346349835395813, 0.9346349835395813, 0.9315223693847656, 0.9349179267883301, 0.9335030913352966, 0.9298245906829834, 0.9349179267883301, 0.9368987083435059, 0.9394453763961792, 0.941426157951355, 0.937747597694397, 0.92840975522995, 0.9397283792495728, 0.9261460304260254, 0.9391624331474304, 0.9428409934043884, 0.9408602118492126, 0.9453876614570618, 0.9439728260040283, 0.9456706047058105, 0.9485002756118774, 0.9468024969100952, 0.9405772686004639, 0.9417091012001038, 0.9479343295097351, 0.9510469436645508, 0.9479343295097351, 0.9516128897666931, 0.9473684430122375, 0.9518958926200867, 0.9490662217140198, 0.9535936713218689, 0.9538766145706177, 0.9504810571670532, 0.9468024969100952, 0.9518958926200867, 0.9561403393745422, 0.9564233422279358, 0.9482173323631287, 0.9567062854766846, 0.9561403393745422, 0.9538766145706177, 0.9592529535293579, 0.9575551748275757, 0.9598188996315002, 0.9586870670318604, 0.9564233422279358, 0.9598188996315002, 0.9552914500236511, 0.9589700102806091, 0.9552914500236511, 0.9668930172920227, 0.9555743932723999, 0.9482173323631287, 0.9671760201454163, 0.9598188996315002, 0.9558573961257935, 0.963780403137207, 0.9620826244354248, 0.9595359563827515, 0.9595359563827515, 0.9632145166397095, 0.9697226881980896, 0.9620826244354248, 0.9677419066429138], 'val_loss': [0.992267370223999, 0.969535231590271, 0.9467101693153381, 0.9307472109794617, 0.9234135150909424, 0.9035114049911499, 0.8964198231697083, 0.8840799927711487, 0.8697584271430969, 0.849648654460907, 0.842488706111908, 0.8043535947799683, 0.7960212826728821, 0.7556838393211365, 0.7559967637062073, 0.7219082713127136, 0.690252423286438, 0.6733508706092834, 0.6595736742019653, 0.6475526690483093, 0.6404370069503784, 0.616396963596344, 0.6079065203666687, 0.6266049146652222, 0.6633464694023132, 0.7114890217781067, 0.6430025696754456, 0.6686710119247437, 0.6106509566307068, 0.5997693538665771, 0.6420398354530334, 0.6234073042869568, 0.6189693212509155, 0.6347355246543884, 0.619879424571991, 0.6068377494812012, 0.6918815970420837, 0.6127356886863708, 0.6496445536613464, 0.6112959384918213, 0.6118347644805908, 0.6481642127037048, 0.6310170888900757, 0.6118512153625488, 0.6151523590087891, 0.6968215107917786, 0.6693866848945618, 0.6258544325828552, 0.6407135725021362, 0.6066033840179443, 0.6238865852355957, 0.6147446632385254, 0.6269717812538147, 0.623117983341217, 0.6467050313949585, 0.6308013796806335, 0.6475705504417419, 0.6186293959617615, 0.6224207878112793, 0.6356536746025085, 0.6371137499809265, 0.6301060914993286, 0.6332095265388489, 0.6326922178268433, 0.6325084567070007, 0.6186676621437073, 0.6291784644126892, 0.6261404156684875, 0.625196099281311, 0.6264690160751343, 0.619174599647522, 0.6395105123519897, 0.6293753981590271, 0.6264998316764832, 0.6257835626602173, 0.6592885255813599, 0.6383219361305237, 0.6648180484771729, 0.6557103395462036, 0.6416996717453003, 0.6592201590538025, 0.6392391324043274, 0.6515543460845947, 0.6352561712265015, 0.6396967768669128, 0.6318729519844055, 0.6427607536315918, 0.647243857383728, 0.6387568116188049, 0.6318068504333496, 0.6451737880706787, 0.6559557914733887, 0.6454246640205383, 0.6759992837905884, 0.6855288147926331, 0.6833904385566711, 0.6380764842033386, 0.6877154111862183, 0.6474312543869019, 0.6549350619316101], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.5022624731063843, 0.5147058963775635, 0.5803167223930359, 0.587104082107544, 0.6176470518112183, 0.6425339579582214, 0.7070135474205017, 0.6934388875961304, 0.7771493196487427, 0.7319004535675049, 0.8212669491767883, 0.7748869061470032, 0.8122171759605408, 0.8393664956092834, 0.8235294222831726, 0.837104082107544, 0.831447958946228, 0.8303167223930359, 0.8529411554336548, 0.8450226187705994, 0.8427602052688599, 0.831447958946228, 0.8167420625686646, 0.8382353186607361, 0.8337104320526123, 0.8563348650932312, 0.8597285151481628, 0.8461538553237915, 0.848416268825531, 0.8540723919868469, 0.8563348650932312, 0.8619909286499023, 0.8619909286499023, 0.8382353186607361, 0.8653846383094788, 0.848416268825531, 0.8585972785949707, 0.8574660420417786, 0.8540723919868469, 0.860859751701355, 0.860859751701355, 0.8585972785949707, 0.8223981857299805, 0.8325791954994202, 0.8552036285400391, 0.8404977321624756, 0.8563348650932312, 0.8563348650932312, 0.860859751701355, 0.8540723919868469, 0.8552036285400391, 0.8552036285400391, 0.8585972785949707, 0.8495475053787231, 0.860859751701355, 0.8574660420417786, 0.8574660420417786, 0.8529411554336548, 0.8506787419319153, 0.8563348650932312, 0.8552036285400391, 0.8574660420417786, 0.8619909286499023, 0.8563348650932312, 0.860859751701355, 0.8574660420417786, 0.8619909286499023, 0.8631221652030945, 0.8540723919868469, 0.8563348650932312, 0.8585972785949707, 0.8552036285400391, 0.8416289687156677, 0.8506787419319153, 0.837104082107544, 0.848416268825531, 0.8574660420417786, 0.8506787419319153, 0.8574660420417786, 0.8506787419319153, 0.8585972785949707, 0.8574660420417786, 0.8540723919868469, 0.8540723919868469, 0.8552036285400391, 0.8597285151481628, 0.8552036285400391, 0.8552036285400391, 0.8438913822174072, 0.8552036285400391, 0.8359728455543518, 0.837104082107544, 0.837104082107544, 0.8540723919868469, 0.8382353186607361, 0.848416268825531, 0.8518099784851074]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/31 [===========================>..] - ETA: 0s - loss: 0.5506 - accuracy: 0.8887"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 7s 50ms/step - loss: 0.5510 - accuracy: 0.8881 - val_loss: 0.9926 - val_accuracy: 0.4855\n","Epoch 2/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.5209 - accuracy: 0.9044 - val_loss: 0.9647 - val_accuracy: 0.4855\n","Epoch 3/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.5178 - accuracy: 0.9036 - val_loss: 0.9476 - val_accuracy: 0.4866\n","Epoch 4/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.5208 - accuracy: 0.9026 - val_loss: 0.9217 - val_accuracy: 0.5279\n","Epoch 5/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.5063 - accuracy: 0.9106 - val_loss: 0.9045 - val_accuracy: 0.5775\n","Epoch 6/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.5095 - accuracy: 0.9075 - val_loss: 0.8872 - val_accuracy: 0.6405\n","Epoch 7/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.5000 - accuracy: 0.9078 - val_loss: 0.8676 - val_accuracy: 0.7149\n","Epoch 8/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.4956 - accuracy: 0.9132 - val_loss: 0.8514 - val_accuracy: 0.7273\n","Epoch 9/100\n","31/31 [==============================] - 1s 23ms/step - loss: 0.4994 - accuracy: 0.9150 - val_loss: 0.8263 - val_accuracy: 0.7800\n","Epoch 10/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4900 - accuracy: 0.9183 - val_loss: 0.8121 - val_accuracy: 0.7696\n","Epoch 11/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4879 - accuracy: 0.9140 - val_loss: 0.7753 - val_accuracy: 0.8347\n","Epoch 12/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4865 - accuracy: 0.9124 - val_loss: 0.7624 - val_accuracy: 0.8017\n","Epoch 13/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4913 - accuracy: 0.9078 - val_loss: 0.7318 - val_accuracy: 0.8264\n","Epoch 14/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4794 - accuracy: 0.9189 - val_loss: 0.7085 - val_accuracy: 0.8347\n","Epoch 15/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.4796 - accuracy: 0.9163 - val_loss: 0.6788 - val_accuracy: 0.8399\n","Epoch 16/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4771 - accuracy: 0.9173 - val_loss: 0.6566 - val_accuracy: 0.8450\n","Epoch 17/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4762 - accuracy: 0.9207 - val_loss: 0.6414 - val_accuracy: 0.8419\n","Epoch 18/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4725 - accuracy: 0.9158 - val_loss: 0.6503 - val_accuracy: 0.8254\n","Epoch 19/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.4741 - accuracy: 0.9233 - val_loss: 0.6129 - val_accuracy: 0.8502\n","Epoch 20/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4603 - accuracy: 0.9238 - val_loss: 0.6097 - val_accuracy: 0.8461\n","Epoch 21/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4621 - accuracy: 0.9243 - val_loss: 0.6145 - val_accuracy: 0.8388\n","Epoch 22/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4619 - accuracy: 0.9245 - val_loss: 0.7150 - val_accuracy: 0.8110\n","Epoch 23/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4748 - accuracy: 0.9140 - val_loss: 0.6782 - val_accuracy: 0.8254\n","Epoch 24/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4656 - accuracy: 0.9230 - val_loss: 0.6647 - val_accuracy: 0.8306\n","Epoch 25/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4570 - accuracy: 0.9238 - val_loss: 0.6810 - val_accuracy: 0.8285\n","Epoch 26/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.4726 - accuracy: 0.9142 - val_loss: 0.5768 - val_accuracy: 0.8667\n","Epoch 27/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4468 - accuracy: 0.9331 - val_loss: 0.5995 - val_accuracy: 0.8605\n","Epoch 28/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.4444 - accuracy: 0.9292 - val_loss: 0.5772 - val_accuracy: 0.8678\n","Epoch 29/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4477 - accuracy: 0.9331 - val_loss: 0.6096 - val_accuracy: 0.8574\n","Epoch 30/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4448 - accuracy: 0.9279 - val_loss: 0.5825 - val_accuracy: 0.8636\n","Epoch 31/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4386 - accuracy: 0.9305 - val_loss: 0.5752 - val_accuracy: 0.8636\n","Epoch 32/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4402 - accuracy: 0.9331 - val_loss: 0.5915 - val_accuracy: 0.8678\n","Epoch 33/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4454 - accuracy: 0.9243 - val_loss: 0.6180 - val_accuracy: 0.8554\n","Epoch 34/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4363 - accuracy: 0.9297 - val_loss: 0.5759 - val_accuracy: 0.8667\n","Epoch 35/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.4238 - accuracy: 0.9395 - val_loss: 0.5788 - val_accuracy: 0.8709\n","Epoch 36/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.4355 - accuracy: 0.9315 - val_loss: 0.6077 - val_accuracy: 0.8533\n","Epoch 37/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4280 - accuracy: 0.9300 - val_loss: 0.5732 - val_accuracy: 0.8698\n","Epoch 38/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4247 - accuracy: 0.9333 - val_loss: 0.6042 - val_accuracy: 0.8647\n","Epoch 39/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4241 - accuracy: 0.9362 - val_loss: 0.5846 - val_accuracy: 0.8729\n","Epoch 40/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.4226 - accuracy: 0.9344 - val_loss: 0.5920 - val_accuracy: 0.8616\n","Epoch 41/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4130 - accuracy: 0.9442 - val_loss: 0.6164 - val_accuracy: 0.8605\n","Epoch 42/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4261 - accuracy: 0.9336 - val_loss: 0.5844 - val_accuracy: 0.8688\n","Epoch 43/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4238 - accuracy: 0.9341 - val_loss: 0.6498 - val_accuracy: 0.8471\n","Epoch 44/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4229 - accuracy: 0.9295 - val_loss: 0.5781 - val_accuracy: 0.8616\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4153 - accuracy: 0.9401 - val_loss: 0.5766 - val_accuracy: 0.8698\n","Epoch 46/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4148 - accuracy: 0.9382 - val_loss: 0.5800 - val_accuracy: 0.8688\n","Epoch 47/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.4121 - accuracy: 0.9398 - val_loss: 0.5832 - val_accuracy: 0.8626\n","Epoch 48/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.4025 - accuracy: 0.9419 - val_loss: 0.5834 - val_accuracy: 0.8688\n","Epoch 49/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4097 - accuracy: 0.9411 - val_loss: 0.5764 - val_accuracy: 0.8647\n","Epoch 50/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4019 - accuracy: 0.9437 - val_loss: 0.5928 - val_accuracy: 0.8678\n","Epoch 51/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4083 - accuracy: 0.9364 - val_loss: 0.6076 - val_accuracy: 0.8605\n","Epoch 52/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.4196 - accuracy: 0.9305 - val_loss: 0.5894 - val_accuracy: 0.8647\n","Epoch 53/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4116 - accuracy: 0.9354 - val_loss: 0.5720 - val_accuracy: 0.8678\n","Epoch 54/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.4213 - accuracy: 0.9326 - val_loss: 0.5689 - val_accuracy: 0.8678\n","Epoch 55/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3972 - accuracy: 0.9434 - val_loss: 0.5748 - val_accuracy: 0.8688\n","Epoch 56/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3902 - accuracy: 0.9468 - val_loss: 0.5758 - val_accuracy: 0.8657\n","Epoch 57/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.3929 - accuracy: 0.9465 - val_loss: 0.6040 - val_accuracy: 0.8605\n","Epoch 58/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.4087 - accuracy: 0.9349 - val_loss: 0.6820 - val_accuracy: 0.8399\n","Epoch 59/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.3983 - accuracy: 0.9429 - val_loss: 0.5730 - val_accuracy: 0.8688\n","Epoch 60/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.3884 - accuracy: 0.9491 - val_loss: 0.5911 - val_accuracy: 0.8657\n","Epoch 61/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3796 - accuracy: 0.9491 - val_loss: 0.5810 - val_accuracy: 0.8667\n","Epoch 62/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3762 - accuracy: 0.9535 - val_loss: 0.5967 - val_accuracy: 0.8616\n","Epoch 63/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3807 - accuracy: 0.9488 - val_loss: 0.5790 - val_accuracy: 0.8626\n","Epoch 64/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.3785 - accuracy: 0.9494 - val_loss: 0.6035 - val_accuracy: 0.8647\n","Epoch 65/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3852 - accuracy: 0.9434 - val_loss: 0.5930 - val_accuracy: 0.8688\n","Epoch 66/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3754 - accuracy: 0.9525 - val_loss: 0.6001 - val_accuracy: 0.8605\n","Epoch 67/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3791 - accuracy: 0.9470 - val_loss: 0.5891 - val_accuracy: 0.8647\n","Epoch 68/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3715 - accuracy: 0.9527 - val_loss: 0.5947 - val_accuracy: 0.8667\n","Epoch 69/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3744 - accuracy: 0.9527 - val_loss: 0.5879 - val_accuracy: 0.8647\n","Epoch 70/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3727 - accuracy: 0.9522 - val_loss: 0.5905 - val_accuracy: 0.8657\n","Epoch 71/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3654 - accuracy: 0.9556 - val_loss: 0.5948 - val_accuracy: 0.8647\n","Epoch 72/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3686 - accuracy: 0.9509 - val_loss: 0.5931 - val_accuracy: 0.8657\n","Epoch 73/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3720 - accuracy: 0.9499 - val_loss: 0.5784 - val_accuracy: 0.8636\n","Epoch 74/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3651 - accuracy: 0.9519 - val_loss: 0.5963 - val_accuracy: 0.8616\n","Epoch 75/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3727 - accuracy: 0.9483 - val_loss: 0.5956 - val_accuracy: 0.8636\n","Epoch 76/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3579 - accuracy: 0.9556 - val_loss: 0.6110 - val_accuracy: 0.8574\n","Epoch 77/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3691 - accuracy: 0.9488 - val_loss: 0.6013 - val_accuracy: 0.8647\n","Epoch 78/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3732 - accuracy: 0.9470 - val_loss: 0.5770 - val_accuracy: 0.8729\n","Epoch 79/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3532 - accuracy: 0.9568 - val_loss: 0.5876 - val_accuracy: 0.8678\n","Epoch 80/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3543 - accuracy: 0.9571 - val_loss: 0.5950 - val_accuracy: 0.8657\n","Epoch 81/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3593 - accuracy: 0.9540 - val_loss: 0.6399 - val_accuracy: 0.8554\n","Epoch 82/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3557 - accuracy: 0.9553 - val_loss: 0.5895 - val_accuracy: 0.8626\n","Epoch 83/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3547 - accuracy: 0.9599 - val_loss: 0.5871 - val_accuracy: 0.8647\n","Epoch 84/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3504 - accuracy: 0.9605 - val_loss: 0.6168 - val_accuracy: 0.8626\n","Epoch 85/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3611 - accuracy: 0.9514 - val_loss: 0.6038 - val_accuracy: 0.8626\n","Epoch 86/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3512 - accuracy: 0.9561 - val_loss: 0.5922 - val_accuracy: 0.8657\n","Epoch 87/100\n","31/31 [==============================] - 0s 12ms/step - loss: 0.3434 - accuracy: 0.9602 - val_loss: 0.5906 - val_accuracy: 0.8647\n","Epoch 88/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3405 - accuracy: 0.9623 - val_loss: 0.6017 - val_accuracy: 0.8719\n","Epoch 89/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3359 - accuracy: 0.9641 - val_loss: 0.6005 - val_accuracy: 0.8636\n","Epoch 90/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3374 - accuracy: 0.9623 - val_loss: 0.6126 - val_accuracy: 0.8605\n","Epoch 91/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3633 - accuracy: 0.9455 - val_loss: 0.5882 - val_accuracy: 0.8667\n","Epoch 92/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.3363 - accuracy: 0.9667 - val_loss: 0.5888 - val_accuracy: 0.8657\n","Epoch 93/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3424 - accuracy: 0.9587 - val_loss: 0.6493 - val_accuracy: 0.8523\n","Epoch 94/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3566 - accuracy: 0.9514 - val_loss: 0.5860 - val_accuracy: 0.8678\n","Epoch 95/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.3329 - accuracy: 0.9636 - val_loss: 0.5940 - val_accuracy: 0.8667\n","Epoch 96/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3316 - accuracy: 0.9651 - val_loss: 0.5959 - val_accuracy: 0.8647\n","Epoch 97/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3278 - accuracy: 0.9672 - val_loss: 0.6013 - val_accuracy: 0.8626\n","Epoch 98/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3299 - accuracy: 0.9633 - val_loss: 0.6057 - val_accuracy: 0.8657\n","Epoch 99/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3304 - accuracy: 0.9643 - val_loss: 0.6055 - val_accuracy: 0.8698\n","Epoch 100/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3332 - accuracy: 0.9638 - val_loss: 0.6212 - val_accuracy: 0.8616\n","{'loss': [0.5509694814682007, 0.5208756923675537, 0.5178093910217285, 0.5207843780517578, 0.5063278079032898, 0.5095357894897461, 0.5000177025794983, 0.4956366717815399, 0.49939706921577454, 0.4900079667568207, 0.48787403106689453, 0.4865478277206421, 0.4912564158439636, 0.47940242290496826, 0.47962847352027893, 0.47707661986351013, 0.47624826431274414, 0.47246912121772766, 0.4740573465824127, 0.4602922201156616, 0.4620782136917114, 0.46190422773361206, 0.4748208820819855, 0.4655683636665344, 0.45701873302459717, 0.4726160764694214, 0.4467724859714508, 0.444436639547348, 0.4477391541004181, 0.4447607696056366, 0.4386202096939087, 0.44022923707962036, 0.4453558027744293, 0.4362761974334717, 0.4238012731075287, 0.43546438217163086, 0.42798373103141785, 0.4247078001499176, 0.4241282641887665, 0.4226418137550354, 0.4130131006240845, 0.4261135160923004, 0.4237603545188904, 0.4228903353214264, 0.4153381586074829, 0.4147699475288391, 0.4120910167694092, 0.4024978578090668, 0.40970396995544434, 0.40189486742019653, 0.4083337187767029, 0.4196200370788574, 0.4116255044937134, 0.4212700128555298, 0.3972369432449341, 0.39019083976745605, 0.39292997121810913, 0.40870174765586853, 0.3983193337917328, 0.3884333670139313, 0.379633367061615, 0.37621355056762695, 0.38068893551826477, 0.3785356283187866, 0.38521310687065125, 0.3754200041294098, 0.37908264994621277, 0.37154656648635864, 0.3744264543056488, 0.3727336823940277, 0.36536678671836853, 0.3685511350631714, 0.3719630241394043, 0.36506661772727966, 0.37271782755851746, 0.35786497592926025, 0.3690546154975891, 0.3732110261917114, 0.3531644344329834, 0.3542778491973877, 0.35933512449264526, 0.3556676506996155, 0.3546738624572754, 0.3504042625427246, 0.36106204986572266, 0.35117384791374207, 0.34337881207466125, 0.3404906094074249, 0.33590614795684814, 0.33737504482269287, 0.36329570412635803, 0.33628764748573303, 0.3423975706100464, 0.3566408157348633, 0.332946240901947, 0.3315921127796173, 0.32779744267463684, 0.3299483358860016, 0.33041051030158997, 0.3332406282424927], 'accuracy': [0.8881136775016785, 0.9043927788734436, 0.9036175608634949, 0.9025839567184448, 0.9105943441390991, 0.907493531703949, 0.9077519178390503, 0.9131782650947571, 0.9149870872497559, 0.9183462262153625, 0.9139534831047058, 0.9124031066894531, 0.9077519178390503, 0.91886305809021, 0.9162790775299072, 0.9173126816749573, 0.920671820640564, 0.9157622456550598, 0.9232558012008667, 0.9237726330757141, 0.9242894053459167, 0.9245477914810181, 0.9139534831047058, 0.9229974150657654, 0.9237726330757141, 0.9142118692398071, 0.933074951171875, 0.9291989803314209, 0.933074951171875, 0.9279069900512695, 0.9304909706115723, 0.933074951171875, 0.9242894053459167, 0.9297157526016235, 0.9395349025726318, 0.9315245747566223, 0.9299741387367249, 0.9333333373069763, 0.9361757040023804, 0.9343669414520264, 0.9441860318183899, 0.9335917234420776, 0.934108555316925, 0.9294573664665222, 0.9400516748428345, 0.9382429122924805, 0.9397932887077332, 0.9418604373931885, 0.9410852789878845, 0.9436692595481873, 0.9364340901374817, 0.9304909706115723, 0.9354005455970764, 0.9325581192970276, 0.9434108734130859, 0.9467700123786926, 0.9465116262435913, 0.934883713722229, 0.9428940415382385, 0.949095606803894, 0.949095606803894, 0.9534883499145508, 0.9488372206687927, 0.9493539929389954, 0.9434108734130859, 0.9524548053741455, 0.947028398513794, 0.9527131915092468, 0.9527131915092468, 0.9521963596343994, 0.9555555582046509, 0.950904369354248, 0.9498708248138428, 0.9519379734992981, 0.9483203887939453, 0.9555555582046509, 0.9488372206687927, 0.947028398513794, 0.9568475484848022, 0.9571059346199036, 0.9540051817893982, 0.9552971720695496, 0.9599483013153076, 0.960465133190155, 0.9514212012290955, 0.9560723304748535, 0.9602067470550537, 0.962273895740509, 0.964082658290863, 0.962273895740509, 0.9454780220985413, 0.9666666388511658, 0.9586563110351562, 0.9514212012290955, 0.9635658860206604, 0.9651162624359131, 0.9671834707260132, 0.9633074998855591, 0.9643411040306091, 0.9638242721557617], 'val_loss': [0.9926086664199829, 0.9646733999252319, 0.9476458430290222, 0.9217399954795837, 0.9045202732086182, 0.8872470855712891, 0.8676472902297974, 0.851419985294342, 0.8262931108474731, 0.8120691180229187, 0.7753401398658752, 0.7624000310897827, 0.7317877411842346, 0.7085132598876953, 0.678757905960083, 0.6565569043159485, 0.6413701176643372, 0.6502746939659119, 0.6128584742546082, 0.6096538305282593, 0.6144939661026001, 0.7150025963783264, 0.6781756281852722, 0.6647316217422485, 0.6809778809547424, 0.576803982257843, 0.5995087027549744, 0.5772356986999512, 0.6096132397651672, 0.5825479030609131, 0.57520991563797, 0.5915163159370422, 0.6179962754249573, 0.5758864879608154, 0.5787653923034668, 0.6077402234077454, 0.5731779336929321, 0.6042435765266418, 0.5846233367919922, 0.5920128226280212, 0.6164423823356628, 0.5843784213066101, 0.6497597694396973, 0.5781350135803223, 0.5766080617904663, 0.5799513459205627, 0.5832346677780151, 0.5833945274353027, 0.5763787031173706, 0.5928454399108887, 0.6076411008834839, 0.589379608631134, 0.5719788074493408, 0.5688787698745728, 0.5748199820518494, 0.5757885575294495, 0.603996992111206, 0.6819500923156738, 0.5729539394378662, 0.5910619497299194, 0.5809834003448486, 0.5966967344284058, 0.5790114402770996, 0.6035082340240479, 0.592984676361084, 0.6001419425010681, 0.5891034603118896, 0.5946765542030334, 0.5878536105155945, 0.5904674530029297, 0.5947788953781128, 0.5931193232536316, 0.5783615112304688, 0.596280038356781, 0.5956240296363831, 0.6110456585884094, 0.6013491153717041, 0.576964259147644, 0.5875552296638489, 0.5949961543083191, 0.6398976445198059, 0.5895264148712158, 0.5870991349220276, 0.6167830228805542, 0.6038295030593872, 0.5922378301620483, 0.590624213218689, 0.6017408967018127, 0.6004705429077148, 0.6125615835189819, 0.588150143623352, 0.5888381600379944, 0.6492907404899597, 0.586001455783844, 0.5940002799034119, 0.5958892703056335, 0.6013151407241821, 0.6057019829750061, 0.6055153608322144, 0.6212209463119507], 'val_accuracy': [0.48553720116615295, 0.48553720116615295, 0.48657023906707764, 0.5278925895690918, 0.577479362487793, 0.6404958963394165, 0.7148760557174683, 0.7272727489471436, 0.7799586653709412, 0.76962810754776, 0.8347107172012329, 0.8016529083251953, 0.8264462947845459, 0.8347107172012329, 0.8398760557174683, 0.8450413346290588, 0.8419421315193176, 0.8254132270812988, 0.8502066135406494, 0.8460744023323059, 0.8388429880142212, 0.8109503984451294, 0.8254132270812988, 0.8305785059928894, 0.8285123705863953, 0.8667355179786682, 0.8605371713638306, 0.8677685856819153, 0.8574380278587341, 0.8636363744735718, 0.8636363744735718, 0.8677685856819153, 0.85537189245224, 0.8667355179786682, 0.8708677887916565, 0.8533057570457458, 0.8698347210884094, 0.8646694421768188, 0.8729338645935059, 0.8615702390670776, 0.8605371713638306, 0.8688016533851624, 0.8471074104309082, 0.8615702390670776, 0.8698347210884094, 0.8688016533851624, 0.8626033067703247, 0.8688016533851624, 0.8646694421768188, 0.8677685856819153, 0.8605371713638306, 0.8646694421768188, 0.8677685856819153, 0.8677685856819153, 0.8688016533851624, 0.8657024502754211, 0.8605371713638306, 0.8398760557174683, 0.8688016533851624, 0.8657024502754211, 0.8667355179786682, 0.8615702390670776, 0.8626033067703247, 0.8646694421768188, 0.8688016533851624, 0.8605371713638306, 0.8646694421768188, 0.8667355179786682, 0.8646694421768188, 0.8657024502754211, 0.8646694421768188, 0.8657024502754211, 0.8636363744735718, 0.8615702390670776, 0.8636363744735718, 0.8574380278587341, 0.8646694421768188, 0.8729338645935059, 0.8677685856819153, 0.8657024502754211, 0.85537189245224, 0.8626033067703247, 0.8646694421768188, 0.8626033067703247, 0.8626033067703247, 0.8657024502754211, 0.8646694421768188, 0.8719007968902588, 0.8636363744735718, 0.8605371713638306, 0.8667355179786682, 0.8657024502754211, 0.8522727489471436, 0.8677685856819153, 0.8667355179786682, 0.8646694421768188, 0.8626033067703247, 0.8657024502754211, 0.8698347210884094, 0.8615702390670776]}\n","32/32 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","29/29 [==============================] - 7s 48ms/step - loss: 0.3994 - accuracy: 0.9351 - val_loss: 0.9718 - val_accuracy: 0.4849\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.3880 - accuracy: 0.9378 - val_loss: 0.9289 - val_accuracy: 0.4860\n","Epoch 3/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3715 - accuracy: 0.9448 - val_loss: 0.9055 - val_accuracy: 0.4860\n","Epoch 4/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.3709 - accuracy: 0.9429 - val_loss: 0.8854 - val_accuracy: 0.4968\n","Epoch 5/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.3675 - accuracy: 0.9459 - val_loss: 0.8579 - val_accuracy: 0.5216\n","Epoch 6/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.3569 - accuracy: 0.9561 - val_loss: 0.8413 - val_accuracy: 0.5474\n","Epoch 7/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.3536 - accuracy: 0.9566 - val_loss: 0.8280 - val_accuracy: 0.5700\n","Epoch 8/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3544 - accuracy: 0.9529 - val_loss: 0.8273 - val_accuracy: 0.5668\n","Epoch 9/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.3555 - accuracy: 0.9518 - val_loss: 0.8015 - val_accuracy: 0.6099\n","Epoch 10/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.3490 - accuracy: 0.9545 - val_loss: 0.7999 - val_accuracy: 0.6110\n","Epoch 11/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3483 - accuracy: 0.9572 - val_loss: 0.8023 - val_accuracy: 0.6056\n","Epoch 12/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.3399 - accuracy: 0.9620 - val_loss: 0.7685 - val_accuracy: 0.6455\n","Epoch 13/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.3414 - accuracy: 0.9601 - val_loss: 0.7127 - val_accuracy: 0.7295\n","Epoch 14/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3436 - accuracy: 0.9566 - val_loss: 0.7068 - val_accuracy: 0.7231\n","Epoch 15/100\n","29/29 [==============================] - 1s 25ms/step - loss: 0.3423 - accuracy: 0.9588 - val_loss: 0.6550 - val_accuracy: 0.7845\n","Epoch 16/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.3368 - accuracy: 0.9604 - val_loss: 0.6274 - val_accuracy: 0.8028\n","Epoch 17/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.3441 - accuracy: 0.9561 - val_loss: 0.5574 - val_accuracy: 0.8642\n","Epoch 18/100\n","29/29 [==============================] - 1s 17ms/step - loss: 0.3372 - accuracy: 0.9607 - val_loss: 0.5373 - val_accuracy: 0.8578\n","Epoch 19/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.3447 - accuracy: 0.9534 - val_loss: 0.5199 - val_accuracy: 0.8761\n","Epoch 20/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3277 - accuracy: 0.9688 - val_loss: 0.5186 - val_accuracy: 0.8653\n","Epoch 21/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3258 - accuracy: 0.9669 - val_loss: 0.5231 - val_accuracy: 0.8685\n","Epoch 22/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.3224 - accuracy: 0.9663 - val_loss: 0.5718 - val_accuracy: 0.8588\n","Epoch 23/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3268 - accuracy: 0.9617 - val_loss: 0.5463 - val_accuracy: 0.8728\n","Epoch 24/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3231 - accuracy: 0.9661 - val_loss: 0.5359 - val_accuracy: 0.8761\n","Epoch 25/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.3252 - accuracy: 0.9652 - val_loss: 0.5100 - val_accuracy: 0.8858\n","Epoch 26/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3333 - accuracy: 0.9604 - val_loss: 0.5196 - val_accuracy: 0.8836\n","Epoch 27/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.3267 - accuracy: 0.9650 - val_loss: 0.4645 - val_accuracy: 0.8987\n","Epoch 28/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3164 - accuracy: 0.9688 - val_loss: 0.4765 - val_accuracy: 0.8944\n","Epoch 29/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.3116 - accuracy: 0.9709 - val_loss: 0.4649 - val_accuracy: 0.9019\n","Epoch 30/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3165 - accuracy: 0.9671 - val_loss: 0.4624 - val_accuracy: 0.9009\n","Epoch 31/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.3120 - accuracy: 0.9698 - val_loss: 0.4653 - val_accuracy: 0.9041\n","Epoch 32/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3114 - accuracy: 0.9679 - val_loss: 0.4621 - val_accuracy: 0.8987\n","Epoch 33/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3149 - accuracy: 0.9682 - val_loss: 0.4899 - val_accuracy: 0.8998\n","Epoch 34/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3307 - accuracy: 0.9615 - val_loss: 0.5178 - val_accuracy: 0.8987\n","Epoch 35/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3095 - accuracy: 0.9690 - val_loss: 0.4919 - val_accuracy: 0.9041\n","Epoch 36/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.3112 - accuracy: 0.9696 - val_loss: 0.4847 - val_accuracy: 0.9084\n","Epoch 37/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.3102 - accuracy: 0.9696 - val_loss: 0.4774 - val_accuracy: 0.9138\n","Epoch 38/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3035 - accuracy: 0.9720 - val_loss: 0.4754 - val_accuracy: 0.9095\n","Epoch 39/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3086 - accuracy: 0.9685 - val_loss: 0.4876 - val_accuracy: 0.8976\n","Epoch 40/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.3195 - accuracy: 0.9647 - val_loss: 0.5566 - val_accuracy: 0.8836\n","Epoch 41/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3062 - accuracy: 0.9706 - val_loss: 0.4634 - val_accuracy: 0.8966\n","Epoch 42/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.3043 - accuracy: 0.9704 - val_loss: 0.4874 - val_accuracy: 0.9149\n","Epoch 43/100\n","29/29 [==============================] - 1s 16ms/step - loss: 0.3025 - accuracy: 0.9701 - val_loss: 0.4859 - val_accuracy: 0.9009\n","Epoch 44/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2976 - accuracy: 0.9736 - val_loss: 0.4746 - val_accuracy: 0.9019\n","Epoch 45/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.3013 - accuracy: 0.9709 - val_loss: 0.4860 - val_accuracy: 0.9106\n","Epoch 46/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2984 - accuracy: 0.9744 - val_loss: 0.4765 - val_accuracy: 0.9095\n","Epoch 47/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2920 - accuracy: 0.9758 - val_loss: 0.4735 - val_accuracy: 0.9095\n","Epoch 48/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2952 - accuracy: 0.9736 - val_loss: 0.4839 - val_accuracy: 0.9095\n","Epoch 49/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.3211 - accuracy: 0.9593 - val_loss: 0.6004 - val_accuracy: 0.8761\n","Epoch 50/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.3213 - accuracy: 0.9607 - val_loss: 0.5115 - val_accuracy: 0.9030\n","Epoch 51/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.3112 - accuracy: 0.9658 - val_loss: 0.5239 - val_accuracy: 0.8869\n","Epoch 52/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3002 - accuracy: 0.9688 - val_loss: 0.4766 - val_accuracy: 0.9030\n","Epoch 53/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2905 - accuracy: 0.9731 - val_loss: 0.4943 - val_accuracy: 0.9052\n","Epoch 54/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2876 - accuracy: 0.9774 - val_loss: 0.5033 - val_accuracy: 0.9041\n","Epoch 55/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2840 - accuracy: 0.9779 - val_loss: 0.4844 - val_accuracy: 0.8966\n","Epoch 56/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2848 - accuracy: 0.9766 - val_loss: 0.4784 - val_accuracy: 0.9041\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2824 - accuracy: 0.9784 - val_loss: 0.4961 - val_accuracy: 0.9084\n","Epoch 58/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2803 - accuracy: 0.9801 - val_loss: 0.4896 - val_accuracy: 0.9062\n","Epoch 59/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3028 - accuracy: 0.9679 - val_loss: 0.4902 - val_accuracy: 0.9019\n","Epoch 60/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2919 - accuracy: 0.9712 - val_loss: 0.5122 - val_accuracy: 0.8933\n","Epoch 61/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2826 - accuracy: 0.9768 - val_loss: 0.4901 - val_accuracy: 0.9030\n","Epoch 62/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2815 - accuracy: 0.9784 - val_loss: 0.4996 - val_accuracy: 0.8966\n","Epoch 63/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2785 - accuracy: 0.9779 - val_loss: 0.4967 - val_accuracy: 0.8976\n","Epoch 64/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2842 - accuracy: 0.9760 - val_loss: 0.4924 - val_accuracy: 0.9073\n","Epoch 65/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2801 - accuracy: 0.9768 - val_loss: 0.5065 - val_accuracy: 0.8987\n","Epoch 66/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2767 - accuracy: 0.9784 - val_loss: 0.4964 - val_accuracy: 0.9041\n","Epoch 67/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2794 - accuracy: 0.9774 - val_loss: 0.5271 - val_accuracy: 0.8966\n","Epoch 68/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2850 - accuracy: 0.9755 - val_loss: 0.5236 - val_accuracy: 0.8858\n","Epoch 69/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2814 - accuracy: 0.9736 - val_loss: 0.4963 - val_accuracy: 0.9019\n","Epoch 70/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2709 - accuracy: 0.9822 - val_loss: 0.4906 - val_accuracy: 0.8998\n","Epoch 71/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2815 - accuracy: 0.9733 - val_loss: 0.5020 - val_accuracy: 0.8944\n","Epoch 72/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2758 - accuracy: 0.9774 - val_loss: 0.4911 - val_accuracy: 0.8987\n","Epoch 73/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2714 - accuracy: 0.9795 - val_loss: 0.5019 - val_accuracy: 0.8987\n","Epoch 74/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2730 - accuracy: 0.9795 - val_loss: 0.4994 - val_accuracy: 0.9030\n","Epoch 75/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2681 - accuracy: 0.9795 - val_loss: 0.5073 - val_accuracy: 0.9019\n","Epoch 76/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2667 - accuracy: 0.9822 - val_loss: 0.4969 - val_accuracy: 0.8998\n","Epoch 77/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.2713 - accuracy: 0.9809 - val_loss: 0.4962 - val_accuracy: 0.9019\n","Epoch 78/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.2664 - accuracy: 0.9814 - val_loss: 0.5115 - val_accuracy: 0.8976\n","Epoch 79/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2731 - accuracy: 0.9771 - val_loss: 0.5836 - val_accuracy: 0.8912\n","Epoch 80/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2817 - accuracy: 0.9731 - val_loss: 0.4955 - val_accuracy: 0.9106\n","Epoch 81/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2641 - accuracy: 0.9811 - val_loss: 0.5145 - val_accuracy: 0.9030\n","Epoch 82/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2634 - accuracy: 0.9806 - val_loss: 0.5035 - val_accuracy: 0.9009\n","Epoch 83/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2630 - accuracy: 0.9814 - val_loss: 0.5182 - val_accuracy: 0.9009\n","Epoch 84/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2604 - accuracy: 0.9830 - val_loss: 0.5066 - val_accuracy: 0.9030\n","Epoch 85/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2757 - accuracy: 0.9744 - val_loss: 0.5002 - val_accuracy: 0.9009\n","Epoch 86/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2606 - accuracy: 0.9814 - val_loss: 0.5000 - val_accuracy: 0.9052\n","Epoch 87/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2701 - accuracy: 0.9779 - val_loss: 0.5091 - val_accuracy: 0.8944\n","Epoch 88/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2668 - accuracy: 0.9793 - val_loss: 0.5046 - val_accuracy: 0.8933\n","Epoch 89/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2605 - accuracy: 0.9822 - val_loss: 0.5363 - val_accuracy: 0.9062\n","Epoch 90/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2833 - accuracy: 0.9685 - val_loss: 0.5172 - val_accuracy: 0.8966\n","Epoch 91/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2815 - accuracy: 0.9701 - val_loss: 0.5391 - val_accuracy: 0.8933\n","Epoch 92/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2789 - accuracy: 0.9709 - val_loss: 0.5016 - val_accuracy: 0.8987\n","Epoch 93/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2660 - accuracy: 0.9768 - val_loss: 0.5136 - val_accuracy: 0.8955\n","Epoch 94/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2640 - accuracy: 0.9793 - val_loss: 0.5485 - val_accuracy: 0.8966\n","Epoch 95/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2546 - accuracy: 0.9822 - val_loss: 0.5041 - val_accuracy: 0.9062\n","Epoch 96/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2523 - accuracy: 0.9841 - val_loss: 0.5484 - val_accuracy: 0.8998\n","Epoch 97/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2563 - accuracy: 0.9830 - val_loss: 0.5063 - val_accuracy: 0.9062\n","Epoch 98/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2549 - accuracy: 0.9814 - val_loss: 0.4969 - val_accuracy: 0.9009\n","Epoch 99/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2486 - accuracy: 0.9863 - val_loss: 0.6006 - val_accuracy: 0.8847\n","Epoch 100/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2627 - accuracy: 0.9774 - val_loss: 0.5180 - val_accuracy: 0.8987\n","{'loss': [0.39936548471450806, 0.38797086477279663, 0.3715474307537079, 0.3709186315536499, 0.3675497770309448, 0.35693731904029846, 0.35357865691185, 0.3543771505355835, 0.35552433133125305, 0.348979115486145, 0.34825363755226135, 0.3399321734905243, 0.34137189388275146, 0.3435716927051544, 0.3422597646713257, 0.33684417605400085, 0.3441433012485504, 0.33720800280570984, 0.34465163946151733, 0.32772332429885864, 0.32577160000801086, 0.322436660528183, 0.3267620801925659, 0.3230900466442108, 0.3252140283584595, 0.33333975076675415, 0.326677531003952, 0.3163876235485077, 0.3116499185562134, 0.31646084785461426, 0.3119876980781555, 0.31138110160827637, 0.31492361426353455, 0.3307078182697296, 0.30949077010154724, 0.3112156391143799, 0.3101891279220581, 0.30352145433425903, 0.30859461426734924, 0.3194637894630432, 0.30621248483657837, 0.30426016449928284, 0.30253976583480835, 0.2975541055202484, 0.30133265256881714, 0.2984284460544586, 0.2920196056365967, 0.29519742727279663, 0.32111746072769165, 0.32133522629737854, 0.31123828887939453, 0.30016881227493286, 0.29045259952545166, 0.2875549793243408, 0.2840493321418762, 0.2847791016101837, 0.28235748410224915, 0.2803443670272827, 0.3027622401714325, 0.29192104935646057, 0.28255122900009155, 0.2815072238445282, 0.27848392724990845, 0.2841644585132599, 0.2801011800765991, 0.2767098844051361, 0.2793881595134735, 0.28501424193382263, 0.2813926041126251, 0.27092814445495605, 0.2815020978450775, 0.2758098840713501, 0.2713727653026581, 0.27295365929603577, 0.26805540919303894, 0.26671701669692993, 0.2712751626968384, 0.2663663923740387, 0.27305757999420166, 0.281693696975708, 0.26413780450820923, 0.26335906982421875, 0.26300814747810364, 0.260415256023407, 0.27566611766815186, 0.2606388032436371, 0.2701193392276764, 0.26677873730659485, 0.26051685214042664, 0.2833319902420044, 0.28145545721054077, 0.27889981865882874, 0.2660110592842102, 0.2639760673046112, 0.25460392236709595, 0.25232475996017456, 0.25626832246780396, 0.2549116611480713, 0.2485838234424591, 0.2626969516277313], 'accuracy': [0.9350754022598267, 0.9377694129943848, 0.9447737336158752, 0.9428879022598267, 0.9458512663841248, 0.9560883641242981, 0.9566271305084229, 0.9528555870056152, 0.951777994632721, 0.954472005367279, 0.9571659564971924, 0.9620150923728943, 0.9601293206214905, 0.9566271305084229, 0.9587823152542114, 0.9603987336158752, 0.9560883641242981, 0.9606680870056152, 0.9533944129943848, 0.96875, 0.9668642282485962, 0.9663254022598267, 0.9617456793785095, 0.9660560488700867, 0.9652478694915771, 0.9603987336158752, 0.9649784564971924, 0.96875, 0.9709051847457886, 0.967133641242981, 0.9698275923728943, 0.9679418206214905, 0.9682112336158752, 0.9614762663841248, 0.9690194129943848, 0.9695581793785095, 0.9695581793785095, 0.9719827771186829, 0.9684805870056152, 0.9647090435028076, 0.9706357717514038, 0.970366358757019, 0.970097005367279, 0.9735991358757019, 0.9709051847457886, 0.9744073152542114, 0.9757543206214905, 0.9735991358757019, 0.959321141242981, 0.9606680870056152, 0.9657866358757019, 0.96875, 0.9730603694915771, 0.9773706793785095, 0.977909505367279, 0.9765625, 0.9784482717514038, 0.9800646305084229, 0.9679418206214905, 0.9711745977401733, 0.9768319129943848, 0.9784482717514038, 0.977909505367279, 0.9760237336158752, 0.9768319129943848, 0.9784482717514038, 0.9773706793785095, 0.9754849076271057, 0.9735991358757019, 0.9822198152542114, 0.9733297228813171, 0.9773706793785095, 0.9795258641242981, 0.9795258641242981, 0.9795258641242981, 0.9822198152542114, 0.9808728694915771, 0.9814116358757019, 0.9771012663841248, 0.9730603694915771, 0.9811422228813171, 0.9806034564971924, 0.9814116358757019, 0.983027994632721, 0.9744073152542114, 0.9814116358757019, 0.977909505367279, 0.9792564511299133, 0.9822198152542114, 0.9684805870056152, 0.970097005367279, 0.9709051847457886, 0.9768319129943848, 0.9792564511299133, 0.9822198152542114, 0.9841055870056152, 0.983027994632721, 0.9814116358757019, 0.9862607717514038, 0.9773706793785095], 'val_loss': [0.9717646837234497, 0.9289213418960571, 0.9054694175720215, 0.8853877782821655, 0.8578518033027649, 0.8413224220275879, 0.8279889225959778, 0.8273182511329651, 0.8015456795692444, 0.7998930215835571, 0.8023389577865601, 0.7684709429740906, 0.7126942276954651, 0.7068239450454712, 0.6550160050392151, 0.6273559927940369, 0.5574021935462952, 0.5372942090034485, 0.5198869109153748, 0.5186352133750916, 0.5230633616447449, 0.5718316435813904, 0.5463306307792664, 0.5359241366386414, 0.5100417137145996, 0.5195868611335754, 0.4644775986671448, 0.4764707088470459, 0.46493077278137207, 0.46242058277130127, 0.4653283953666687, 0.46206700801849365, 0.48988571763038635, 0.5178057551383972, 0.4918724596500397, 0.4847264885902405, 0.47742152214050293, 0.4753991365432739, 0.4875720143318176, 0.5566088557243347, 0.46342921257019043, 0.48744261264801025, 0.4859125316143036, 0.4746049642562866, 0.48604729771614075, 0.47645455598831177, 0.4735488295555115, 0.4839285910129547, 0.600387454032898, 0.5114988684654236, 0.5238604545593262, 0.47656211256980896, 0.49431654810905457, 0.5033007264137268, 0.4844483435153961, 0.4783770740032196, 0.49613964557647705, 0.4895731806755066, 0.4902424216270447, 0.5121649503707886, 0.49005454778671265, 0.4995679557323456, 0.49667251110076904, 0.4924001693725586, 0.5065213441848755, 0.49644818902015686, 0.5271487236022949, 0.5236213803291321, 0.4962800145149231, 0.4906478226184845, 0.5020078420639038, 0.4910706877708435, 0.50187087059021, 0.4994382858276367, 0.507306694984436, 0.496903657913208, 0.4961628019809723, 0.5115413069725037, 0.5835652947425842, 0.49550890922546387, 0.5144541263580322, 0.5034934282302856, 0.5182097554206848, 0.5065585374832153, 0.5002254843711853, 0.49996522068977356, 0.5090755820274353, 0.5046446323394775, 0.5362725853919983, 0.5171730518341064, 0.5390775203704834, 0.5016056895256042, 0.5136470198631287, 0.548538088798523, 0.5041311383247375, 0.5484102368354797, 0.5063124895095825, 0.49693015217781067, 0.6005764007568359, 0.5180453658103943], 'val_accuracy': [0.48491379618644714, 0.48599138855934143, 0.48599138855934143, 0.4967672526836395, 0.5215517282485962, 0.5474137663841248, 0.5700430870056152, 0.5668103694915771, 0.6099137663841248, 0.610991358757019, 0.6056034564971924, 0.6454741358757019, 0.7295258641242981, 0.7230603694915771, 0.7844827771186829, 0.8028017282485962, 0.8642241358757019, 0.857758641242981, 0.8760775923728943, 0.8653017282485962, 0.868534505367279, 0.8588362336158752, 0.8728448152542114, 0.8760775923728943, 0.8857758641242981, 0.8836206793785095, 0.8987069129943848, 0.8943965435028076, 0.9019396305084229, 0.9008620977401733, 0.9040948152542114, 0.8987069129943848, 0.899784505367279, 0.8987069129943848, 0.9040948152542114, 0.9084051847457886, 0.9137930870056152, 0.9094827771186829, 0.8976293206214905, 0.8836206793785095, 0.8965517282485962, 0.9148706793785095, 0.9008620977401733, 0.9019396305084229, 0.9105603694915771, 0.9094827771186829, 0.9094827771186829, 0.9094827771186829, 0.8760775923728943, 0.9030172228813171, 0.8868534564971924, 0.9030172228813171, 0.9051724076271057, 0.9040948152542114, 0.8965517282485962, 0.9040948152542114, 0.9084051847457886, 0.90625, 0.9019396305084229, 0.8933189511299133, 0.9030172228813171, 0.8965517282485962, 0.8976293206214905, 0.9073275923728943, 0.8987069129943848, 0.9040948152542114, 0.8965517282485962, 0.8857758641242981, 0.9019396305084229, 0.899784505367279, 0.8943965435028076, 0.8987069129943848, 0.8987069129943848, 0.9030172228813171, 0.9019396305084229, 0.899784505367279, 0.9019396305084229, 0.8976293206214905, 0.8911637663841248, 0.9105603694915771, 0.9030172228813171, 0.9008620977401733, 0.9008620977401733, 0.9030172228813171, 0.9008620977401733, 0.9051724076271057, 0.8943965435028076, 0.8933189511299133, 0.90625, 0.8965517282485962, 0.8933189511299133, 0.8987069129943848, 0.8954741358757019, 0.8965517282485962, 0.90625, 0.899784505367279, 0.90625, 0.9008620977401733, 0.8846982717514038, 0.8987069129943848]}\n","38/38 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","25/28 [=========================>....] - ETA: 0s - loss: 0.4150 - accuracy: 0.9250"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 8s 56ms/step - loss: 0.4107 - accuracy: 0.9281 - val_loss: 0.9574 - val_accuracy: 0.4955\n","Epoch 2/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3672 - accuracy: 0.9448 - val_loss: 0.9418 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3654 - accuracy: 0.9493 - val_loss: 0.9127 - val_accuracy: 0.4955\n","Epoch 4/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.3743 - accuracy: 0.9431 - val_loss: 0.8897 - val_accuracy: 0.5000\n","Epoch 5/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.3628 - accuracy: 0.9493 - val_loss: 0.8820 - val_accuracy: 0.5136\n","Epoch 6/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.3606 - accuracy: 0.9508 - val_loss: 0.8623 - val_accuracy: 0.5249\n","Epoch 7/100\n","28/28 [==============================] - 1s 24ms/step - loss: 0.3598 - accuracy: 0.9505 - val_loss: 0.8389 - val_accuracy: 0.5679\n","Epoch 8/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3575 - accuracy: 0.9527 - val_loss: 0.8468 - val_accuracy: 0.5543\n","Epoch 9/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3656 - accuracy: 0.9471 - val_loss: 0.8641 - val_accuracy: 0.5464\n","Epoch 10/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.3586 - accuracy: 0.9448 - val_loss: 0.8143 - val_accuracy: 0.5939\n","Epoch 11/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.3612 - accuracy: 0.9499 - val_loss: 0.8072 - val_accuracy: 0.6097\n","Epoch 12/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3497 - accuracy: 0.9553 - val_loss: 0.7992 - val_accuracy: 0.6222\n","Epoch 13/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.3412 - accuracy: 0.9590 - val_loss: 0.7446 - val_accuracy: 0.6878\n","Epoch 14/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.3450 - accuracy: 0.9561 - val_loss: 0.7312 - val_accuracy: 0.6968\n","Epoch 15/100\n","28/28 [==============================] - 1s 33ms/step - loss: 0.3406 - accuracy: 0.9587 - val_loss: 0.7103 - val_accuracy: 0.7262\n","Epoch 16/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.3361 - accuracy: 0.9570 - val_loss: 0.6930 - val_accuracy: 0.7398\n","Epoch 17/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.3431 - accuracy: 0.9533 - val_loss: 0.6203 - val_accuracy: 0.8213\n","Epoch 18/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.3299 - accuracy: 0.9626 - val_loss: 0.5838 - val_accuracy: 0.8473\n","Epoch 19/100\n","28/28 [==============================] - 1s 17ms/step - loss: 0.3531 - accuracy: 0.9491 - val_loss: 0.5697 - val_accuracy: 0.8473\n","Epoch 20/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3347 - accuracy: 0.9595 - val_loss: 0.5900 - val_accuracy: 0.8337\n","Epoch 21/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.3316 - accuracy: 0.9584 - val_loss: 0.5500 - val_accuracy: 0.8484\n","Epoch 22/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.3308 - accuracy: 0.9581 - val_loss: 0.5781 - val_accuracy: 0.8484\n","Epoch 23/100\n","28/28 [==============================] - 1s 25ms/step - loss: 0.3275 - accuracy: 0.9632 - val_loss: 0.5270 - val_accuracy: 0.8643\n","Epoch 24/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3262 - accuracy: 0.9624 - val_loss: 0.7042 - val_accuracy: 0.8247\n","Epoch 25/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3399 - accuracy: 0.9533 - val_loss: 0.5227 - val_accuracy: 0.8790\n","Epoch 26/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3216 - accuracy: 0.9677 - val_loss: 0.5414 - val_accuracy: 0.8744\n","Epoch 27/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3263 - accuracy: 0.9632 - val_loss: 0.5257 - val_accuracy: 0.8812\n","Epoch 28/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.3177 - accuracy: 0.9629 - val_loss: 0.5127 - val_accuracy: 0.8914\n","Epoch 29/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3149 - accuracy: 0.9641 - val_loss: 0.5219 - val_accuracy: 0.8880\n","Epoch 30/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3268 - accuracy: 0.9629 - val_loss: 0.5393 - val_accuracy: 0.8801\n","Epoch 31/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3222 - accuracy: 0.9652 - val_loss: 0.5762 - val_accuracy: 0.8733\n","Epoch 32/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3228 - accuracy: 0.9607 - val_loss: 0.6287 - val_accuracy: 0.8394\n","Epoch 33/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3739 - accuracy: 0.9346 - val_loss: 0.6477 - val_accuracy: 0.8416\n","Epoch 34/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3613 - accuracy: 0.9428 - val_loss: 0.5628 - val_accuracy: 0.8744\n","Epoch 35/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3148 - accuracy: 0.9646 - val_loss: 0.5314 - val_accuracy: 0.8857\n","Epoch 36/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3156 - accuracy: 0.9660 - val_loss: 0.5390 - val_accuracy: 0.8801\n","Epoch 37/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3089 - accuracy: 0.9675 - val_loss: 0.5458 - val_accuracy: 0.8778\n","Epoch 38/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.3087 - accuracy: 0.9680 - val_loss: 0.5485 - val_accuracy: 0.8835\n","Epoch 39/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3078 - accuracy: 0.9694 - val_loss: 0.5448 - val_accuracy: 0.8812\n","Epoch 40/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3058 - accuracy: 0.9692 - val_loss: 0.5509 - val_accuracy: 0.8790\n","Epoch 41/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2996 - accuracy: 0.9740 - val_loss: 0.5505 - val_accuracy: 0.8869\n","Epoch 42/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.3019 - accuracy: 0.9689 - val_loss: 0.5534 - val_accuracy: 0.8857\n","Epoch 43/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3021 - accuracy: 0.9720 - val_loss: 0.5469 - val_accuracy: 0.8857\n","Epoch 44/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2986 - accuracy: 0.9723 - val_loss: 0.5509 - val_accuracy: 0.8891\n","Epoch 45/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2995 - accuracy: 0.9697 - val_loss: 0.5647 - val_accuracy: 0.8812\n","Epoch 46/100\n","28/28 [==============================] - 0s 12ms/step - loss: 0.3256 - accuracy: 0.9570 - val_loss: 0.5798 - val_accuracy: 0.8710\n","Epoch 47/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.3079 - accuracy: 0.9649 - val_loss: 0.5656 - val_accuracy: 0.8835\n","Epoch 48/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2964 - accuracy: 0.9714 - val_loss: 0.5477 - val_accuracy: 0.8891\n","Epoch 49/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2941 - accuracy: 0.9740 - val_loss: 0.5937 - val_accuracy: 0.8676\n","Epoch 50/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3008 - accuracy: 0.9692 - val_loss: 0.5530 - val_accuracy: 0.8891\n","Epoch 51/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2969 - accuracy: 0.9731 - val_loss: 0.5514 - val_accuracy: 0.8903\n","Epoch 52/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.3007 - accuracy: 0.9677 - val_loss: 0.5613 - val_accuracy: 0.8778\n","Epoch 53/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2907 - accuracy: 0.9748 - val_loss: 0.5699 - val_accuracy: 0.8790\n","Epoch 54/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.2911 - accuracy: 0.9762 - val_loss: 0.6293 - val_accuracy: 0.8688\n","Epoch 55/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.3074 - accuracy: 0.9658 - val_loss: 0.5526 - val_accuracy: 0.8869\n","Epoch 56/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2866 - accuracy: 0.9748 - val_loss: 0.5540 - val_accuracy: 0.8869\n","Epoch 57/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2820 - accuracy: 0.9793 - val_loss: 0.5647 - val_accuracy: 0.8891\n","Epoch 58/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2791 - accuracy: 0.9799 - val_loss: 0.5606 - val_accuracy: 0.8869\n","Epoch 59/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2795 - accuracy: 0.9802 - val_loss: 0.5701 - val_accuracy: 0.8857\n","Epoch 60/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2855 - accuracy: 0.9748 - val_loss: 0.5552 - val_accuracy: 0.8869\n","Epoch 61/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2770 - accuracy: 0.9805 - val_loss: 0.5762 - val_accuracy: 0.8778\n","Epoch 62/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2851 - accuracy: 0.9768 - val_loss: 0.5700 - val_accuracy: 0.8778\n","Epoch 63/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2805 - accuracy: 0.9765 - val_loss: 0.5906 - val_accuracy: 0.8733\n","Epoch 64/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2897 - accuracy: 0.9709 - val_loss: 0.6210 - val_accuracy: 0.8744\n","Epoch 65/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2815 - accuracy: 0.9759 - val_loss: 0.6037 - val_accuracy: 0.8733\n","Epoch 66/100\n","28/28 [==============================] - 1s 41ms/step - loss: 0.2857 - accuracy: 0.9762 - val_loss: 0.5620 - val_accuracy: 0.8925\n","Epoch 67/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2786 - accuracy: 0.9768 - val_loss: 0.5546 - val_accuracy: 0.8880\n","Epoch 68/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2759 - accuracy: 0.9799 - val_loss: 0.5674 - val_accuracy: 0.8857\n","Epoch 69/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2782 - accuracy: 0.9762 - val_loss: 0.5776 - val_accuracy: 0.8756\n","Epoch 70/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2740 - accuracy: 0.9802 - val_loss: 0.6655 - val_accuracy: 0.8552\n","Epoch 71/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2937 - accuracy: 0.9703 - val_loss: 0.5718 - val_accuracy: 0.8891\n","Epoch 72/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2757 - accuracy: 0.9788 - val_loss: 0.5715 - val_accuracy: 0.8857\n","Epoch 73/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2879 - accuracy: 0.9720 - val_loss: 0.6311 - val_accuracy: 0.8699\n","Epoch 74/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2858 - accuracy: 0.9689 - val_loss: 0.5739 - val_accuracy: 0.8835\n","Epoch 75/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2735 - accuracy: 0.9774 - val_loss: 0.5609 - val_accuracy: 0.8812\n","Epoch 76/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2676 - accuracy: 0.9802 - val_loss: 0.6289 - val_accuracy: 0.8654\n","Epoch 77/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2728 - accuracy: 0.9771 - val_loss: 0.5684 - val_accuracy: 0.8857\n","Epoch 78/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2661 - accuracy: 0.9830 - val_loss: 0.5642 - val_accuracy: 0.8824\n","Epoch 79/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2747 - accuracy: 0.9805 - val_loss: 0.5771 - val_accuracy: 0.8767\n","Epoch 80/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2610 - accuracy: 0.9850 - val_loss: 0.5785 - val_accuracy: 0.8824\n","Epoch 81/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2570 - accuracy: 0.9859 - val_loss: 0.5668 - val_accuracy: 0.8835\n","Epoch 82/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2646 - accuracy: 0.9785 - val_loss: 0.5774 - val_accuracy: 0.8880\n","Epoch 83/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2630 - accuracy: 0.9836 - val_loss: 0.5799 - val_accuracy: 0.8824\n","Epoch 84/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2807 - accuracy: 0.9720 - val_loss: 0.6030 - val_accuracy: 0.8722\n","Epoch 85/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2672 - accuracy: 0.9810 - val_loss: 0.5676 - val_accuracy: 0.8857\n","Epoch 86/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.2675 - accuracy: 0.9799 - val_loss: 0.5875 - val_accuracy: 0.8846\n","Epoch 87/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.2592 - accuracy: 0.9836 - val_loss: 0.5786 - val_accuracy: 0.8846\n","Epoch 88/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2595 - accuracy: 0.9827 - val_loss: 0.5841 - val_accuracy: 0.8857\n","Epoch 89/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2574 - accuracy: 0.9836 - val_loss: 0.5784 - val_accuracy: 0.8846\n","Epoch 90/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2592 - accuracy: 0.9819 - val_loss: 0.6395 - val_accuracy: 0.8586\n","Epoch 91/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2626 - accuracy: 0.9796 - val_loss: 0.6218 - val_accuracy: 0.8756\n","Epoch 92/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2626 - accuracy: 0.9808 - val_loss: 0.5973 - val_accuracy: 0.8767\n","Epoch 93/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2559 - accuracy: 0.9833 - val_loss: 0.5883 - val_accuracy: 0.8812\n","Epoch 94/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2554 - accuracy: 0.9825 - val_loss: 0.6014 - val_accuracy: 0.8699\n","Epoch 95/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2562 - accuracy: 0.9810 - val_loss: 0.6233 - val_accuracy: 0.8710\n","Epoch 96/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2583 - accuracy: 0.9822 - val_loss: 0.5904 - val_accuracy: 0.8778\n","Epoch 97/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2483 - accuracy: 0.9867 - val_loss: 0.5842 - val_accuracy: 0.8835\n","Epoch 98/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2494 - accuracy: 0.9867 - val_loss: 0.5837 - val_accuracy: 0.8914\n","Epoch 99/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2544 - accuracy: 0.9833 - val_loss: 0.6319 - val_accuracy: 0.8688\n","Epoch 100/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2541 - accuracy: 0.9836 - val_loss: 0.6297 - val_accuracy: 0.8722\n","{'loss': [0.41067707538604736, 0.36718854308128357, 0.36538296937942505, 0.37430763244628906, 0.3628498613834381, 0.36063629388809204, 0.35976821184158325, 0.35753223299980164, 0.365561306476593, 0.3585554361343384, 0.36123061180114746, 0.3496808409690857, 0.3411986827850342, 0.34500715136528015, 0.3405607044696808, 0.336126446723938, 0.3431166112422943, 0.3298819661140442, 0.3530508577823639, 0.3347364068031311, 0.33163803815841675, 0.3308359682559967, 0.3275465965270996, 0.3262055814266205, 0.3399152457714081, 0.3216395974159241, 0.3263128399848938, 0.3177412152290344, 0.3149460256099701, 0.3267500698566437, 0.3221698999404907, 0.3227947950363159, 0.3739061653614044, 0.36126115918159485, 0.31483837962150574, 0.31555667519569397, 0.3088747560977936, 0.30873754620552063, 0.30775415897369385, 0.3058214485645294, 0.29964160919189453, 0.3018886148929596, 0.30209797620773315, 0.29859599471092224, 0.29952186346054077, 0.32560163736343384, 0.3078826069831848, 0.29639461636543274, 0.29405438899993896, 0.30075010657310486, 0.2969037592411041, 0.3006533086299896, 0.2906962037086487, 0.291135311126709, 0.30743467807769775, 0.28663909435272217, 0.28197604417800903, 0.2791379690170288, 0.27950385212898254, 0.28554344177246094, 0.2770160436630249, 0.2850929796695709, 0.28050509095191956, 0.2896880805492401, 0.2814522683620453, 0.2857489585876465, 0.2786436080932617, 0.27594882249832153, 0.27817270159721375, 0.2740253508090973, 0.29370787739753723, 0.27569636702537537, 0.2879440486431122, 0.2858192026615143, 0.27354860305786133, 0.26758143305778503, 0.2728079557418823, 0.26610150933265686, 0.2747182846069336, 0.2609638273715973, 0.25697198510169983, 0.26461079716682434, 0.2630101144313812, 0.280740350484848, 0.26717254519462585, 0.26746252179145813, 0.2591874599456787, 0.2594811022281647, 0.25737661123275757, 0.25922808051109314, 0.2625665068626404, 0.2626050114631653, 0.2558669149875641, 0.25540217757225037, 0.25618815422058105, 0.2582550346851349, 0.2483377307653427, 0.24941113591194153, 0.2543914020061493, 0.25409606099128723], 'accuracy': [0.9281267523765564, 0.9448217153549194, 0.9493491649627686, 0.9431239366531372, 0.9493491649627686, 0.950764000415802, 0.9504810571670532, 0.9527447819709778, 0.947085440158844, 0.9448217153549194, 0.9499151110649109, 0.9552914500236511, 0.9589700102806091, 0.9561403393745422, 0.9586870670318604, 0.9569892287254333, 0.9533106684684753, 0.9626485705375671, 0.9490662217140198, 0.9595359563827515, 0.9584040641784668, 0.958121120929718, 0.9632145166397095, 0.9623655676841736, 0.9533106684684753, 0.9677419066429138, 0.9632145166397095, 0.9629315137863159, 0.9640634059906006, 0.9629315137863159, 0.9651952385902405, 0.9606677889823914, 0.9346349835395813, 0.9428409934043884, 0.9646292924880981, 0.9660441279411316, 0.967458963394165, 0.9680249094963074, 0.9694397449493408, 0.9691567420959473, 0.9739671945571899, 0.9688737988471985, 0.9719864130020142, 0.9722693562507629, 0.9697226881980896, 0.9569892287254333, 0.9649122953414917, 0.9714204668998718, 0.9739671945571899, 0.9691567420959473, 0.9731183052062988, 0.9677419066429138, 0.974816083908081, 0.9762309193611145, 0.9657611846923828, 0.974816083908081, 0.9793435335159302, 0.9799094796180725, 0.9801924228668213, 0.974816083908081, 0.9804753661155701, 0.9767968058586121, 0.9765138626098633, 0.9708545804023743, 0.975947916507721, 0.9762309193611145, 0.9767968058586121, 0.9799094796180725, 0.9762309193611145, 0.9801924228668213, 0.9702886343002319, 0.9787775874137878, 0.9719864130020142, 0.9688737988471985, 0.9773627519607544, 0.9801924228668213, 0.9770798087120056, 0.9830220937728882, 0.9804753661155701, 0.9850028157234192, 0.9858517050743103, 0.9784946441650391, 0.9835879802703857, 0.9719864130020142, 0.9810413122177124, 0.9799094796180725, 0.9835879802703857, 0.9827390909194946, 0.9835879802703857, 0.9818902015686035, 0.979626476764679, 0.9807583689689636, 0.983305037021637, 0.9824561476707458, 0.9810413122177124, 0.9821732044219971, 0.9867005944252014, 0.9867005944252014, 0.983305037021637, 0.9835879802703857], 'val_loss': [0.9573983550071716, 0.9417784214019775, 0.9126651883125305, 0.8897024393081665, 0.8819646239280701, 0.8622835874557495, 0.8389298915863037, 0.8467606902122498, 0.8640950918197632, 0.8142921328544617, 0.8072375655174255, 0.7992064356803894, 0.7445910573005676, 0.7311554551124573, 0.710313081741333, 0.6930484175682068, 0.6202794909477234, 0.5837639570236206, 0.5696976184844971, 0.5899942517280579, 0.5499545931816101, 0.5781320333480835, 0.5270130038261414, 0.7041504383087158, 0.5226870775222778, 0.541397750377655, 0.5257406234741211, 0.5127385854721069, 0.521854817867279, 0.5393091440200806, 0.5761595964431763, 0.6287366151809692, 0.6477499604225159, 0.5627935528755188, 0.5313702821731567, 0.5389511585235596, 0.5457943081855774, 0.5484707355499268, 0.5448208451271057, 0.5508812665939331, 0.5504999756813049, 0.5533726215362549, 0.5469039082527161, 0.550870418548584, 0.5647067427635193, 0.5797798037528992, 0.5656012892723083, 0.5476831793785095, 0.5937375426292419, 0.552969217300415, 0.5514334440231323, 0.5612701773643494, 0.569869875907898, 0.6293361186981201, 0.5525671243667603, 0.5540279150009155, 0.5646759271621704, 0.560569703578949, 0.5701432228088379, 0.5551608204841614, 0.5761635303497314, 0.5699982047080994, 0.5906493067741394, 0.6209622621536255, 0.6037105917930603, 0.5620037317276001, 0.5546454787254333, 0.5674027800559998, 0.5775729417800903, 0.665481448173523, 0.5717844367027283, 0.5714507699012756, 0.6311048865318298, 0.5738996267318726, 0.5608978867530823, 0.6288856863975525, 0.5683797597885132, 0.5641766786575317, 0.5770679116249084, 0.5785223245620728, 0.5668380260467529, 0.5773694515228271, 0.5799328088760376, 0.6029585003852844, 0.567582368850708, 0.587463915348053, 0.5786226987838745, 0.584139883518219, 0.5783524513244629, 0.6394897103309631, 0.6217859387397766, 0.5972718000411987, 0.5882779359817505, 0.6014082431793213, 0.6233224272727966, 0.5904087424278259, 0.5841507911682129, 0.5837087035179138, 0.6319318413734436, 0.6296746730804443], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.5, 0.5135746598243713, 0.5248869061470032, 0.5678732991218567, 0.5542986392974854, 0.5463801026344299, 0.5938913822174072, 0.6097285151481628, 0.622171938419342, 0.6877828240394592, 0.6968325972557068, 0.726244330406189, 0.7398189902305603, 0.8212669491767883, 0.8472850918769836, 0.8472850918769836, 0.8337104320526123, 0.848416268825531, 0.848416268825531, 0.8642534017562866, 0.8246606588363647, 0.8789592981338501, 0.8744344115257263, 0.8812217116355896, 0.8914027214050293, 0.8880090713500977, 0.8800904750823975, 0.8733031749725342, 0.8393664956092834, 0.8416289687156677, 0.8744344115257263, 0.8857465982437134, 0.8800904750823975, 0.877828061580658, 0.8834841847419739, 0.8812217116355896, 0.8789592981338501, 0.8868778347969055, 0.8857465982437134, 0.8857465982437134, 0.889140248298645, 0.8812217116355896, 0.8710407018661499, 0.8834841847419739, 0.889140248298645, 0.8676470518112183, 0.889140248298645, 0.8902714848518372, 0.877828061580658, 0.8789592981338501, 0.8687782883644104, 0.8868778347969055, 0.8868778347969055, 0.889140248298645, 0.8868778347969055, 0.8857465982437134, 0.8868778347969055, 0.877828061580658, 0.877828061580658, 0.8733031749725342, 0.8744344115257263, 0.8733031749725342, 0.8925339579582214, 0.8880090713500977, 0.8857465982437134, 0.8755655884742737, 0.8552036285400391, 0.889140248298645, 0.8857465982437134, 0.8699095249176025, 0.8834841847419739, 0.8812217116355896, 0.8653846383094788, 0.8857465982437134, 0.8823529481887817, 0.8766968250274658, 0.8823529481887817, 0.8834841847419739, 0.8880090713500977, 0.8823529481887817, 0.872171938419342, 0.8857465982437134, 0.8846153616905212, 0.8846153616905212, 0.8857465982437134, 0.8846153616905212, 0.8585972785949707, 0.8755655884742737, 0.8766968250274658, 0.8812217116355896, 0.8699095249176025, 0.8710407018661499, 0.877828061580658, 0.8834841847419739, 0.8914027214050293, 0.8687782883644104, 0.872171938419342]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/31 [==========================>...] - ETA: 0s - loss: 0.3997 - accuracy: 0.9342"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 7s 61ms/step - loss: 0.3998 - accuracy: 0.9336 - val_loss: 0.9674 - val_accuracy: 0.4855\n","Epoch 2/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3763 - accuracy: 0.9442 - val_loss: 0.9448 - val_accuracy: 0.4855\n","Epoch 3/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.3740 - accuracy: 0.9408 - val_loss: 0.9160 - val_accuracy: 0.4866\n","Epoch 4/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.3600 - accuracy: 0.9506 - val_loss: 0.8776 - val_accuracy: 0.5114\n","Epoch 5/100\n","31/31 [==============================] - 1s 29ms/step - loss: 0.3647 - accuracy: 0.9444 - val_loss: 0.8564 - val_accuracy: 0.5300\n","Epoch 6/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.3668 - accuracy: 0.9447 - val_loss: 0.8397 - val_accuracy: 0.5579\n","Epoch 7/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3660 - accuracy: 0.9450 - val_loss: 0.8695 - val_accuracy: 0.5289\n","Epoch 8/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.3806 - accuracy: 0.9388 - val_loss: 0.8319 - val_accuracy: 0.5795\n","Epoch 9/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.3531 - accuracy: 0.9543 - val_loss: 0.8142 - val_accuracy: 0.6002\n","Epoch 10/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.3518 - accuracy: 0.9481 - val_loss: 0.7775 - val_accuracy: 0.6519\n","Epoch 11/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3509 - accuracy: 0.9499 - val_loss: 0.7828 - val_accuracy: 0.6457\n","Epoch 12/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3497 - accuracy: 0.9517 - val_loss: 0.8126 - val_accuracy: 0.6219\n","Epoch 13/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.3465 - accuracy: 0.9535 - val_loss: 0.6947 - val_accuracy: 0.7345\n","Epoch 14/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3487 - accuracy: 0.9537 - val_loss: 0.7274 - val_accuracy: 0.6973\n","Epoch 15/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.3582 - accuracy: 0.9450 - val_loss: 0.6898 - val_accuracy: 0.7355\n","Epoch 16/100\n","31/31 [==============================] - 1s 30ms/step - loss: 0.3722 - accuracy: 0.9385 - val_loss: 0.6055 - val_accuracy: 0.8182\n","Epoch 17/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.3458 - accuracy: 0.9556 - val_loss: 0.5600 - val_accuracy: 0.8554\n","Epoch 18/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3449 - accuracy: 0.9545 - val_loss: 0.5449 - val_accuracy: 0.8574\n","Epoch 19/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3370 - accuracy: 0.9576 - val_loss: 0.5321 - val_accuracy: 0.8616\n","Epoch 20/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3341 - accuracy: 0.9587 - val_loss: 0.5196 - val_accuracy: 0.8698\n","Epoch 21/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3355 - accuracy: 0.9592 - val_loss: 0.5148 - val_accuracy: 0.8688\n","Epoch 22/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3532 - accuracy: 0.9491 - val_loss: 0.5836 - val_accuracy: 0.8564\n","Epoch 23/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.3328 - accuracy: 0.9584 - val_loss: 0.5222 - val_accuracy: 0.8729\n","Epoch 24/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3255 - accuracy: 0.9638 - val_loss: 0.5346 - val_accuracy: 0.8709\n","Epoch 25/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.3345 - accuracy: 0.9545 - val_loss: 0.4952 - val_accuracy: 0.8791\n","Epoch 26/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.3308 - accuracy: 0.9597 - val_loss: 0.5578 - val_accuracy: 0.8729\n","Epoch 27/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.3437 - accuracy: 0.9517 - val_loss: 0.4934 - val_accuracy: 0.8874\n","Epoch 28/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.3355 - accuracy: 0.9587 - val_loss: 0.4933 - val_accuracy: 0.8822\n","Epoch 29/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3175 - accuracy: 0.9698 - val_loss: 0.4934 - val_accuracy: 0.8905\n","Epoch 30/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3223 - accuracy: 0.9651 - val_loss: 0.5233 - val_accuracy: 0.8853\n","Epoch 31/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3252 - accuracy: 0.9576 - val_loss: 0.5062 - val_accuracy: 0.8864\n","Epoch 32/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.3220 - accuracy: 0.9625 - val_loss: 0.5084 - val_accuracy: 0.8895\n","Epoch 33/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3191 - accuracy: 0.9623 - val_loss: 0.5228 - val_accuracy: 0.8843\n","Epoch 34/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3239 - accuracy: 0.9607 - val_loss: 0.5006 - val_accuracy: 0.8884\n","Epoch 35/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.3170 - accuracy: 0.9651 - val_loss: 0.5155 - val_accuracy: 0.8926\n","Epoch 36/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3128 - accuracy: 0.9667 - val_loss: 0.5062 - val_accuracy: 0.8874\n","Epoch 37/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3107 - accuracy: 0.9672 - val_loss: 0.5350 - val_accuracy: 0.8812\n","Epoch 38/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3218 - accuracy: 0.9620 - val_loss: 0.5019 - val_accuracy: 0.8905\n","Epoch 39/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3151 - accuracy: 0.9625 - val_loss: 0.5859 - val_accuracy: 0.8698\n","Epoch 40/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3270 - accuracy: 0.9579 - val_loss: 0.5050 - val_accuracy: 0.8843\n","Epoch 41/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3137 - accuracy: 0.9620 - val_loss: 0.5116 - val_accuracy: 0.8802\n","Epoch 42/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3155 - accuracy: 0.9628 - val_loss: 0.5101 - val_accuracy: 0.8926\n","Epoch 43/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3071 - accuracy: 0.9649 - val_loss: 0.5112 - val_accuracy: 0.8884\n","Epoch 44/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3039 - accuracy: 0.9693 - val_loss: 0.5087 - val_accuracy: 0.8926\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.3195 - accuracy: 0.9628 - val_loss: 0.5079 - val_accuracy: 0.8853\n","Epoch 46/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3084 - accuracy: 0.9646 - val_loss: 0.5168 - val_accuracy: 0.8864\n","Epoch 47/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2980 - accuracy: 0.9726 - val_loss: 0.5333 - val_accuracy: 0.8864\n","Epoch 48/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3020 - accuracy: 0.9659 - val_loss: 0.5100 - val_accuracy: 0.8895\n","Epoch 49/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2970 - accuracy: 0.9721 - val_loss: 0.5125 - val_accuracy: 0.8884\n","Epoch 50/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3076 - accuracy: 0.9646 - val_loss: 0.5237 - val_accuracy: 0.8915\n","Epoch 51/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.3102 - accuracy: 0.9633 - val_loss: 0.5229 - val_accuracy: 0.8812\n","Epoch 52/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.3099 - accuracy: 0.9630 - val_loss: 0.5200 - val_accuracy: 0.8853\n","Epoch 53/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.2940 - accuracy: 0.9729 - val_loss: 0.5144 - val_accuracy: 0.8833\n","Epoch 54/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.3017 - accuracy: 0.9680 - val_loss: 0.5723 - val_accuracy: 0.8750\n","Epoch 55/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.3096 - accuracy: 0.9610 - val_loss: 0.5105 - val_accuracy: 0.8905\n","Epoch 56/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2918 - accuracy: 0.9721 - val_loss: 0.5866 - val_accuracy: 0.8698\n","Epoch 57/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.3098 - accuracy: 0.9625 - val_loss: 0.5406 - val_accuracy: 0.8843\n","Epoch 58/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.2960 - accuracy: 0.9705 - val_loss: 0.5474 - val_accuracy: 0.8812\n","Epoch 59/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2870 - accuracy: 0.9739 - val_loss: 0.5163 - val_accuracy: 0.8905\n","Epoch 60/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.2824 - accuracy: 0.9760 - val_loss: 0.5335 - val_accuracy: 0.8853\n","Epoch 61/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2824 - accuracy: 0.9739 - val_loss: 0.5265 - val_accuracy: 0.8915\n","Epoch 62/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2859 - accuracy: 0.9739 - val_loss: 0.5360 - val_accuracy: 0.8812\n","Epoch 63/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.3013 - accuracy: 0.9643 - val_loss: 0.5578 - val_accuracy: 0.8802\n","Epoch 64/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2885 - accuracy: 0.9708 - val_loss: 0.5180 - val_accuracy: 0.8895\n","Epoch 65/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2832 - accuracy: 0.9765 - val_loss: 0.5852 - val_accuracy: 0.8750\n","Epoch 66/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2860 - accuracy: 0.9726 - val_loss: 0.5282 - val_accuracy: 0.8833\n","Epoch 67/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2785 - accuracy: 0.9773 - val_loss: 0.5305 - val_accuracy: 0.8843\n","Epoch 68/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2790 - accuracy: 0.9786 - val_loss: 0.5379 - val_accuracy: 0.8729\n","Epoch 69/100\n","31/31 [==============================] - 1s 38ms/step - loss: 0.2877 - accuracy: 0.9711 - val_loss: 0.5231 - val_accuracy: 0.8936\n","Epoch 70/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2808 - accuracy: 0.9749 - val_loss: 0.5232 - val_accuracy: 0.8864\n","Epoch 71/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2712 - accuracy: 0.9801 - val_loss: 0.5200 - val_accuracy: 0.8874\n","Epoch 72/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2797 - accuracy: 0.9747 - val_loss: 0.5324 - val_accuracy: 0.8812\n","Epoch 73/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2744 - accuracy: 0.9767 - val_loss: 0.5360 - val_accuracy: 0.8936\n","Epoch 74/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2783 - accuracy: 0.9765 - val_loss: 0.5874 - val_accuracy: 0.8760\n","Epoch 75/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2833 - accuracy: 0.9708 - val_loss: 0.5447 - val_accuracy: 0.8874\n","Epoch 76/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2815 - accuracy: 0.9752 - val_loss: 0.5989 - val_accuracy: 0.8719\n","Epoch 77/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2759 - accuracy: 0.9765 - val_loss: 0.5337 - val_accuracy: 0.8915\n","Epoch 78/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2853 - accuracy: 0.9698 - val_loss: 0.5368 - val_accuracy: 0.8864\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2722 - accuracy: 0.9780 - val_loss: 0.5828 - val_accuracy: 0.8729\n","Epoch 80/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2649 - accuracy: 0.9811 - val_loss: 0.5420 - val_accuracy: 0.8884\n","Epoch 81/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2692 - accuracy: 0.9786 - val_loss: 0.5357 - val_accuracy: 0.8812\n","Epoch 82/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2689 - accuracy: 0.9786 - val_loss: 0.5709 - val_accuracy: 0.8760\n","Epoch 83/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.2693 - accuracy: 0.9783 - val_loss: 0.5468 - val_accuracy: 0.8853\n","Epoch 84/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.2671 - accuracy: 0.9796 - val_loss: 0.5367 - val_accuracy: 0.8874\n","Epoch 85/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.2646 - accuracy: 0.9791 - val_loss: 0.5473 - val_accuracy: 0.8822\n","Epoch 86/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2698 - accuracy: 0.9752 - val_loss: 0.5661 - val_accuracy: 0.8802\n","Epoch 87/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.2617 - accuracy: 0.9811 - val_loss: 0.5789 - val_accuracy: 0.8781\n","Epoch 88/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2651 - accuracy: 0.9786 - val_loss: 0.5890 - val_accuracy: 0.8771\n","Epoch 89/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2640 - accuracy: 0.9793 - val_loss: 0.5374 - val_accuracy: 0.8905\n","Epoch 90/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2641 - accuracy: 0.9791 - val_loss: 0.5517 - val_accuracy: 0.8895\n","Epoch 91/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2554 - accuracy: 0.9832 - val_loss: 0.5462 - val_accuracy: 0.8802\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2521 - accuracy: 0.9848 - val_loss: 0.5455 - val_accuracy: 0.8926\n","Epoch 93/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2636 - accuracy: 0.9801 - val_loss: 0.5934 - val_accuracy: 0.8771\n","Epoch 94/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2855 - accuracy: 0.9680 - val_loss: 0.5417 - val_accuracy: 0.8915\n","Epoch 95/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2574 - accuracy: 0.9827 - val_loss: 0.5421 - val_accuracy: 0.8812\n","Epoch 96/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2556 - accuracy: 0.9837 - val_loss: 0.5469 - val_accuracy: 0.8905\n","Epoch 97/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2509 - accuracy: 0.9848 - val_loss: 0.5568 - val_accuracy: 0.8791\n","Epoch 98/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2537 - accuracy: 0.9842 - val_loss: 0.5473 - val_accuracy: 0.8915\n","Epoch 99/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2514 - accuracy: 0.9827 - val_loss: 0.5547 - val_accuracy: 0.8864\n","Epoch 100/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2504 - accuracy: 0.9837 - val_loss: 0.5524 - val_accuracy: 0.8833\n","{'loss': [0.39977458119392395, 0.37628722190856934, 0.37399351596832275, 0.3599732518196106, 0.3646731674671173, 0.36676719784736633, 0.3660331666469574, 0.3805616796016693, 0.35310298204421997, 0.3518277108669281, 0.35088223218917847, 0.3496570885181427, 0.3465433418750763, 0.34866300225257874, 0.35818925499916077, 0.37215113639831543, 0.34581130743026733, 0.3449167013168335, 0.3369966745376587, 0.3340965807437897, 0.3354789912700653, 0.3531779646873474, 0.33284157514572144, 0.32551050186157227, 0.33448514342308044, 0.330770343542099, 0.3436776399612427, 0.3354521691799164, 0.3175438642501831, 0.3222954273223877, 0.3251558244228363, 0.3220427632331848, 0.31907227635383606, 0.3238550126552582, 0.31695032119750977, 0.31275326013565063, 0.3107304871082306, 0.3217698931694031, 0.315101683139801, 0.3270235061645508, 0.31367117166519165, 0.31546080112457275, 0.3071366250514984, 0.30386701226234436, 0.31947779655456543, 0.3084433674812317, 0.2979588210582733, 0.301959753036499, 0.29702672362327576, 0.3075772225856781, 0.3102196156978607, 0.3098725974559784, 0.29403695464134216, 0.30167779326438904, 0.30962154269218445, 0.2917831838130951, 0.30975982546806335, 0.2960473895072937, 0.2870015501976013, 0.282356321811676, 0.2824299931526184, 0.2858758568763733, 0.3013273775577545, 0.28848886489868164, 0.2831763029098511, 0.28601449728012085, 0.2785063087940216, 0.27901747822761536, 0.28771165013313293, 0.2807908356189728, 0.2711946666240692, 0.27969565987586975, 0.27439919114112854, 0.27834954857826233, 0.28331926465034485, 0.28148117661476135, 0.27586880326271057, 0.28529801964759827, 0.2721726596355438, 0.26489830017089844, 0.269222229719162, 0.2688920497894287, 0.26928067207336426, 0.26714345812797546, 0.26457688212394714, 0.26979437470436096, 0.2616603374481201, 0.265103280544281, 0.2640364170074463, 0.2640906870365143, 0.25544947385787964, 0.25207361578941345, 0.263621062040329, 0.2854929268360138, 0.25744321942329407, 0.2555883526802063, 0.2509116232395172, 0.2537170946598053, 0.2513698637485504, 0.25035905838012695], 'accuracy': [0.9335917234420776, 0.9441860318183899, 0.9408268928527832, 0.9506459832191467, 0.9444444179534912, 0.9447028636932373, 0.9449612498283386, 0.9387596845626831, 0.9542635679244995, 0.948062002658844, 0.9498708248138428, 0.9516795873641968, 0.9534883499145508, 0.9537467956542969, 0.9449612498283386, 0.9385012984275818, 0.9555555582046509, 0.9545219540596008, 0.957622766494751, 0.9586563110351562, 0.9591731429100037, 0.949095606803894, 0.9583979249000549, 0.9638242721557617, 0.9545219540596008, 0.9596899151802063, 0.9516795873641968, 0.9586563110351562, 0.9697674512863159, 0.9651162624359131, 0.957622766494751, 0.9625322818756104, 0.962273895740509, 0.9607235193252563, 0.9651162624359131, 0.9666666388511658, 0.9671834707260132, 0.9620155096054077, 0.9625322818756104, 0.9578811526298523, 0.9620155096054077, 0.9627906680107117, 0.9648578763008118, 0.9692506194114685, 0.9627906680107117, 0.9645994901657104, 0.97260981798172, 0.9658914804458618, 0.9720930457115173, 0.9645994901657104, 0.9633074998855591, 0.9630491137504578, 0.9728682041168213, 0.9679586291313171, 0.9609819054603577, 0.9720930457115173, 0.9625322818756104, 0.9705426096916199, 0.9739018082618713, 0.9759690165519714, 0.9739018082618713, 0.9739018082618713, 0.9643411040306091, 0.970801055431366, 0.9764857888221741, 0.97260981798172, 0.9772610068321228, 0.9785529971122742, 0.9710594415664673, 0.9749354124069214, 0.9801033735275269, 0.9746770262718201, 0.9767441749572754, 0.9764857888221741, 0.970801055431366, 0.9751937985420227, 0.9764857888221741, 0.9697674512863159, 0.9780361652374268, 0.9811369776725769, 0.9785529971122742, 0.9785529971122742, 0.9782945513725281, 0.9795865416526794, 0.9790697693824768, 0.9751937985420227, 0.9811369776725769, 0.9785529971122742, 0.9793281555175781, 0.9790697693824768, 0.9832041263580322, 0.9847545027732849, 0.9801033735275269, 0.9679586291313171, 0.9826873540878296, 0.9837209582328796, 0.9847545027732849, 0.9842377305030823, 0.9826873540878296, 0.9837209582328796], 'val_loss': [0.967441737651825, 0.9447592496871948, 0.9160262942314148, 0.8776105046272278, 0.8564059138298035, 0.8397493958473206, 0.8694901466369629, 0.8319317102432251, 0.8141774535179138, 0.7774665951728821, 0.782802164554596, 0.812572717666626, 0.6947083473205566, 0.7274277806282043, 0.6897792816162109, 0.6055223345756531, 0.560001015663147, 0.5449336171150208, 0.53212970495224, 0.5196421146392822, 0.5147868990898132, 0.5835683345794678, 0.5222434997558594, 0.534581184387207, 0.4951750934123993, 0.5578262209892273, 0.49344614148139954, 0.49327829480171204, 0.4933904707431793, 0.5232776999473572, 0.5061861872673035, 0.508362889289856, 0.5228003859519958, 0.5005815029144287, 0.5154994130134583, 0.5061877369880676, 0.5350441336631775, 0.5018554329872131, 0.5858727097511292, 0.5050048232078552, 0.5115832090377808, 0.510147213935852, 0.5112143158912659, 0.5086725354194641, 0.507911741733551, 0.5168442130088806, 0.5332806706428528, 0.5100338459014893, 0.5125430822372437, 0.5237075686454773, 0.5228725075721741, 0.519995391368866, 0.514384388923645, 0.5723219513893127, 0.5105307698249817, 0.586635410785675, 0.5405874848365784, 0.5473946928977966, 0.5163487195968628, 0.5335027575492859, 0.5265039205551147, 0.535991370677948, 0.5577956438064575, 0.5179898738861084, 0.5852364897727966, 0.5282090306282043, 0.530510663986206, 0.5379071831703186, 0.5231336355209351, 0.523246705532074, 0.5200150012969971, 0.5324259400367737, 0.5359664559364319, 0.5874126553535461, 0.5446776747703552, 0.5989025235176086, 0.5336781740188599, 0.5367753505706787, 0.5828135013580322, 0.5420264005661011, 0.5356731414794922, 0.5709062814712524, 0.5467800498008728, 0.5367280840873718, 0.5472948551177979, 0.5661430954933167, 0.5789071917533875, 0.5890493988990784, 0.5373584628105164, 0.5517464876174927, 0.546209990978241, 0.5454702973365784, 0.5933921337127686, 0.5416814684867859, 0.542087972164154, 0.5468805432319641, 0.5568123459815979, 0.547310471534729, 0.5546637773513794, 0.5524237751960754], 'val_accuracy': [0.48553720116615295, 0.48553720116615295, 0.48657023906707764, 0.5113636255264282, 0.5299586653709412, 0.557851254940033, 0.5289255976676941, 0.5795454382896423, 0.6002066135406494, 0.6518595218658447, 0.6456611752510071, 0.6219007968902588, 0.7345041036605835, 0.6973140239715576, 0.7355371713638306, 0.8181818127632141, 0.85537189245224, 0.8574380278587341, 0.8615702390670776, 0.8698347210884094, 0.8688016533851624, 0.8564049601554871, 0.8729338645935059, 0.8708677887916565, 0.8791322112083435, 0.8729338645935059, 0.8873966932296753, 0.8822314143180847, 0.8904958963394165, 0.8853305578231812, 0.8863636255264282, 0.8894628286361694, 0.8842975497245789, 0.8884297609329224, 0.8925619721412659, 0.8873966932296753, 0.8811983466148376, 0.8904958963394165, 0.8698347210884094, 0.8842975497245789, 0.8801652789115906, 0.8925619721412659, 0.8884297609329224, 0.8925619721412659, 0.8853305578231812, 0.8863636255264282, 0.8863636255264282, 0.8894628286361694, 0.8884297609329224, 0.8915289044380188, 0.8811983466148376, 0.8853305578231812, 0.8832644820213318, 0.875, 0.8904958963394165, 0.8698347210884094, 0.8842975497245789, 0.8811983466148376, 0.8904958963394165, 0.8853305578231812, 0.8915289044380188, 0.8811983466148376, 0.8801652789115906, 0.8894628286361694, 0.875, 0.8832644820213318, 0.8842975497245789, 0.8729338645935059, 0.8935950398445129, 0.8863636255264282, 0.8873966932296753, 0.8811983466148376, 0.8935950398445129, 0.8760330677032471, 0.8873966932296753, 0.8719007968902588, 0.8915289044380188, 0.8863636255264282, 0.8729338645935059, 0.8884297609329224, 0.8811983466148376, 0.8760330677032471, 0.8853305578231812, 0.8873966932296753, 0.8822314143180847, 0.8801652789115906, 0.8780992031097412, 0.8770661354064941, 0.8904958963394165, 0.8894628286361694, 0.8801652789115906, 0.8925619721412659, 0.8770661354064941, 0.8915289044380188, 0.8811983466148376, 0.8904958963394165, 0.8791322112083435, 0.8915289044380188, 0.8863636255264282, 0.8832644820213318]}\n","32/32 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/29 [===========================>..] - ETA: 0s - loss: 0.3061 - accuracy: 0.9615"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r29/29 [==============================] - 8s 51ms/step - loss: 0.3070 - accuracy: 0.9615 - val_loss: 0.9806 - val_accuracy: 0.4849\n","Epoch 2/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.3120 - accuracy: 0.9585 - val_loss: 0.9434 - val_accuracy: 0.4849\n","Epoch 3/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.2980 - accuracy: 0.9661 - val_loss: 0.9151 - val_accuracy: 0.4860\n","Epoch 4/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.2927 - accuracy: 0.9644 - val_loss: 0.8819 - val_accuracy: 0.4978\n","Epoch 5/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.2765 - accuracy: 0.9741 - val_loss: 0.8660 - val_accuracy: 0.5097\n","Epoch 6/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.2791 - accuracy: 0.9736 - val_loss: 0.8363 - val_accuracy: 0.5366\n","Epoch 7/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.2845 - accuracy: 0.9723 - val_loss: 0.8214 - val_accuracy: 0.5517\n","Epoch 8/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2917 - accuracy: 0.9655 - val_loss: 0.8492 - val_accuracy: 0.5399\n","Epoch 9/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.2773 - accuracy: 0.9723 - val_loss: 0.8510 - val_accuracy: 0.5550\n","Epoch 10/100\n","29/29 [==============================] - 1s 39ms/step - loss: 0.2720 - accuracy: 0.9774 - val_loss: 0.7919 - val_accuracy: 0.6078\n","Epoch 11/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.2748 - accuracy: 0.9731 - val_loss: 0.7561 - val_accuracy: 0.6336\n","Epoch 12/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.2670 - accuracy: 0.9787 - val_loss: 0.7611 - val_accuracy: 0.6433\n","Epoch 13/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.2663 - accuracy: 0.9776 - val_loss: 0.8045 - val_accuracy: 0.6293\n","Epoch 14/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.2635 - accuracy: 0.9809 - val_loss: 0.6801 - val_accuracy: 0.7241\n","Epoch 15/100\n","29/29 [==============================] - 1s 28ms/step - loss: 0.2608 - accuracy: 0.9817 - val_loss: 0.6188 - val_accuracy: 0.7866\n","Epoch 16/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.2672 - accuracy: 0.9771 - val_loss: 0.5954 - val_accuracy: 0.8103\n","Epoch 17/100\n","29/29 [==============================] - 1s 26ms/step - loss: 0.2749 - accuracy: 0.9731 - val_loss: 0.5877 - val_accuracy: 0.8168\n","Epoch 18/100\n","29/29 [==============================] - 1s 27ms/step - loss: 0.2662 - accuracy: 0.9747 - val_loss: 0.5066 - val_accuracy: 0.8685\n","Epoch 19/100\n","29/29 [==============================] - 1s 24ms/step - loss: 0.2632 - accuracy: 0.9787 - val_loss: 0.4867 - val_accuracy: 0.8761\n","Epoch 20/100\n","29/29 [==============================] - 1s 23ms/step - loss: 0.2716 - accuracy: 0.9728 - val_loss: 0.4828 - val_accuracy: 0.8782\n","Epoch 21/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.2774 - accuracy: 0.9688 - val_loss: 0.4573 - val_accuracy: 0.8901\n","Epoch 22/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2679 - accuracy: 0.9736 - val_loss: 0.5397 - val_accuracy: 0.8675\n","Epoch 23/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2518 - accuracy: 0.9820 - val_loss: 0.5048 - val_accuracy: 0.8815\n","Epoch 24/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.2578 - accuracy: 0.9809 - val_loss: 0.4268 - val_accuracy: 0.8966\n","Epoch 25/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.2568 - accuracy: 0.9806 - val_loss: 0.4389 - val_accuracy: 0.8998\n","Epoch 26/100\n","29/29 [==============================] - 1s 22ms/step - loss: 0.2604 - accuracy: 0.9779 - val_loss: 0.4286 - val_accuracy: 0.9084\n","Epoch 27/100\n","29/29 [==============================] - 1s 21ms/step - loss: 0.2554 - accuracy: 0.9795 - val_loss: 0.4003 - val_accuracy: 0.9170\n","Epoch 28/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2518 - accuracy: 0.9830 - val_loss: 0.4165 - val_accuracy: 0.9159\n","Epoch 29/100\n","29/29 [==============================] - 0s 11ms/step - loss: 0.2498 - accuracy: 0.9836 - val_loss: 0.5605 - val_accuracy: 0.8793\n","Epoch 30/100\n","29/29 [==============================] - 1s 19ms/step - loss: 0.2711 - accuracy: 0.9728 - val_loss: 0.4123 - val_accuracy: 0.9192\n","Epoch 31/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.2515 - accuracy: 0.9793 - val_loss: 0.3899 - val_accuracy: 0.9321\n","Epoch 32/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2441 - accuracy: 0.9844 - val_loss: 0.3988 - val_accuracy: 0.9278\n","Epoch 33/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2494 - accuracy: 0.9803 - val_loss: 0.4812 - val_accuracy: 0.9052\n","Epoch 34/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2653 - accuracy: 0.9736 - val_loss: 0.3960 - val_accuracy: 0.9289\n","Epoch 35/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2464 - accuracy: 0.9846 - val_loss: 0.4122 - val_accuracy: 0.9203\n","Epoch 36/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2447 - accuracy: 0.9828 - val_loss: 0.4884 - val_accuracy: 0.9084\n","Epoch 37/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2755 - accuracy: 0.9696 - val_loss: 0.4182 - val_accuracy: 0.9203\n","Epoch 38/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2451 - accuracy: 0.9833 - val_loss: 0.4030 - val_accuracy: 0.9321\n","Epoch 39/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2488 - accuracy: 0.9811 - val_loss: 0.4338 - val_accuracy: 0.9138\n","Epoch 40/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2459 - accuracy: 0.9833 - val_loss: 0.4129 - val_accuracy: 0.9203\n","Epoch 41/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.2393 - accuracy: 0.9871 - val_loss: 0.4818 - val_accuracy: 0.9052\n","Epoch 42/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2548 - accuracy: 0.9744 - val_loss: 0.4245 - val_accuracy: 0.9181\n","Epoch 43/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2366 - accuracy: 0.9857 - val_loss: 0.4122 - val_accuracy: 0.9246\n","Epoch 44/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.2465 - accuracy: 0.9820 - val_loss: 0.4105 - val_accuracy: 0.9256\n","Epoch 45/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2514 - accuracy: 0.9798 - val_loss: 0.4488 - val_accuracy: 0.9138\n","Epoch 46/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2414 - accuracy: 0.9838 - val_loss: 0.4147 - val_accuracy: 0.9256\n","Epoch 47/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2311 - accuracy: 0.9887 - val_loss: 0.4385 - val_accuracy: 0.9170\n","Epoch 48/100\n","29/29 [==============================] - 1s 20ms/step - loss: 0.2355 - accuracy: 0.9871 - val_loss: 0.4400 - val_accuracy: 0.9138\n","Epoch 49/100\n","29/29 [==============================] - 0s 17ms/step - loss: 0.2469 - accuracy: 0.9793 - val_loss: 0.5277 - val_accuracy: 0.8987\n","Epoch 50/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2599 - accuracy: 0.9758 - val_loss: 0.4461 - val_accuracy: 0.9149\n","Epoch 51/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2561 - accuracy: 0.9731 - val_loss: 0.4237 - val_accuracy: 0.9203\n","Epoch 52/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2340 - accuracy: 0.9871 - val_loss: 0.4303 - val_accuracy: 0.9192\n","Epoch 53/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2390 - accuracy: 0.9844 - val_loss: 0.4150 - val_accuracy: 0.9224\n","Epoch 54/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2347 - accuracy: 0.9865 - val_loss: 0.4474 - val_accuracy: 0.9116\n","Epoch 55/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2320 - accuracy: 0.9863 - val_loss: 0.4223 - val_accuracy: 0.9181\n","Epoch 56/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2296 - accuracy: 0.9887 - val_loss: 0.4307 - val_accuracy: 0.9181\n","Epoch 57/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2277 - accuracy: 0.9890 - val_loss: 0.4129 - val_accuracy: 0.9181\n","Epoch 58/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2418 - accuracy: 0.9811 - val_loss: 0.4220 - val_accuracy: 0.9235\n","Epoch 59/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2339 - accuracy: 0.9846 - val_loss: 0.4428 - val_accuracy: 0.9213\n","Epoch 60/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2313 - accuracy: 0.9841 - val_loss: 0.4392 - val_accuracy: 0.9213\n","Epoch 61/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2323 - accuracy: 0.9844 - val_loss: 0.4345 - val_accuracy: 0.9181\n","Epoch 62/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2254 - accuracy: 0.9900 - val_loss: 0.4210 - val_accuracy: 0.9181\n","Epoch 63/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2246 - accuracy: 0.9887 - val_loss: 0.4432 - val_accuracy: 0.9170\n","Epoch 64/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2234 - accuracy: 0.9898 - val_loss: 0.4355 - val_accuracy: 0.9170\n","Epoch 65/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2242 - accuracy: 0.9903 - val_loss: 0.4307 - val_accuracy: 0.9213\n","Epoch 66/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2248 - accuracy: 0.9892 - val_loss: 0.4449 - val_accuracy: 0.9203\n","Epoch 67/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2229 - accuracy: 0.9892 - val_loss: 0.4212 - val_accuracy: 0.9235\n","Epoch 68/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2231 - accuracy: 0.9887 - val_loss: 0.4256 - val_accuracy: 0.9256\n","Epoch 69/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2256 - accuracy: 0.9868 - val_loss: 0.4283 - val_accuracy: 0.9192\n","Epoch 70/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2246 - accuracy: 0.9890 - val_loss: 0.4355 - val_accuracy: 0.9149\n","Epoch 71/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2195 - accuracy: 0.9900 - val_loss: 0.4192 - val_accuracy: 0.9267\n","Epoch 72/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2151 - accuracy: 0.9925 - val_loss: 0.4233 - val_accuracy: 0.9267\n","Epoch 73/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2218 - accuracy: 0.9884 - val_loss: 0.4344 - val_accuracy: 0.9159\n","Epoch 74/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2248 - accuracy: 0.9881 - val_loss: 0.4444 - val_accuracy: 0.9224\n","Epoch 75/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2181 - accuracy: 0.9898 - val_loss: 0.4173 - val_accuracy: 0.9159\n","Epoch 76/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2191 - accuracy: 0.9900 - val_loss: 0.4352 - val_accuracy: 0.9159\n","Epoch 77/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2204 - accuracy: 0.9873 - val_loss: 0.4283 - val_accuracy: 0.9170\n","Epoch 78/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2150 - accuracy: 0.9911 - val_loss: 0.4926 - val_accuracy: 0.9106\n","Epoch 79/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2295 - accuracy: 0.9833 - val_loss: 0.4649 - val_accuracy: 0.9106\n","Epoch 80/100\n","29/29 [==============================] - 0s 14ms/step - loss: 0.2169 - accuracy: 0.9911 - val_loss: 0.4368 - val_accuracy: 0.9127\n","Epoch 81/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2226 - accuracy: 0.9863 - val_loss: 0.4626 - val_accuracy: 0.9116\n","Epoch 82/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2237 - accuracy: 0.9855 - val_loss: 0.4324 - val_accuracy: 0.9235\n","Epoch 83/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2154 - accuracy: 0.9890 - val_loss: 0.4570 - val_accuracy: 0.9213\n","Epoch 84/100\n","29/29 [==============================] - 0s 15ms/step - loss: 0.2185 - accuracy: 0.9868 - val_loss: 0.4489 - val_accuracy: 0.9181\n","Epoch 85/100\n","29/29 [==============================] - 0s 16ms/step - loss: 0.2378 - accuracy: 0.9784 - val_loss: 0.5350 - val_accuracy: 0.9009\n","Epoch 86/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2447 - accuracy: 0.9760 - val_loss: 0.4670 - val_accuracy: 0.9084\n","Epoch 87/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2421 - accuracy: 0.9760 - val_loss: 0.4415 - val_accuracy: 0.9181\n","Epoch 88/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2142 - accuracy: 0.9906 - val_loss: 0.4315 - val_accuracy: 0.9246\n","Epoch 89/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2257 - accuracy: 0.9849 - val_loss: 0.4280 - val_accuracy: 0.9159\n","Epoch 90/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2156 - accuracy: 0.9900 - val_loss: 0.4366 - val_accuracy: 0.9127\n","Epoch 91/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2150 - accuracy: 0.9890 - val_loss: 0.4442 - val_accuracy: 0.9159\n","Epoch 92/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2124 - accuracy: 0.9898 - val_loss: 0.4354 - val_accuracy: 0.9138\n","Epoch 93/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2104 - accuracy: 0.9914 - val_loss: 0.4396 - val_accuracy: 0.9203\n","Epoch 94/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2098 - accuracy: 0.9911 - val_loss: 0.4391 - val_accuracy: 0.9213\n","Epoch 95/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2144 - accuracy: 0.9881 - val_loss: 0.4762 - val_accuracy: 0.9138\n","Epoch 96/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2113 - accuracy: 0.9892 - val_loss: 0.4478 - val_accuracy: 0.9203\n","Epoch 97/100\n","29/29 [==============================] - 0s 13ms/step - loss: 0.2087 - accuracy: 0.9916 - val_loss: 0.4593 - val_accuracy: 0.9149\n","Epoch 98/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2047 - accuracy: 0.9933 - val_loss: 0.4571 - val_accuracy: 0.9138\n","Epoch 99/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2111 - accuracy: 0.9873 - val_loss: 0.4413 - val_accuracy: 0.9235\n","Epoch 100/100\n","29/29 [==============================] - 0s 12ms/step - loss: 0.2063 - accuracy: 0.9914 - val_loss: 0.4694 - val_accuracy: 0.9149\n","{'loss': [0.306965708732605, 0.3120146095752716, 0.2979758381843567, 0.29266357421875, 0.2764953672885895, 0.27914154529571533, 0.28449198603630066, 0.2916528880596161, 0.2773078382015228, 0.27196282148361206, 0.27484118938446045, 0.26697012782096863, 0.266318678855896, 0.2634996473789215, 0.26076075434684753, 0.2672436833381653, 0.2749371826648712, 0.2662031948566437, 0.2631537914276123, 0.2715931534767151, 0.2774086594581604, 0.2678791880607605, 0.2518051862716675, 0.2577836215496063, 0.2567708492279053, 0.26040947437286377, 0.25540104508399963, 0.2517938017845154, 0.24984073638916016, 0.2711268365383148, 0.251516193151474, 0.24406908452510834, 0.2493807077407837, 0.2652892470359802, 0.24638068675994873, 0.24474570155143738, 0.2754744589328766, 0.24512554705142975, 0.24884355068206787, 0.24585166573524475, 0.2392796277999878, 0.2548038363456726, 0.23659579455852509, 0.24645327031612396, 0.25136348605155945, 0.2413683831691742, 0.23114103078842163, 0.2355203777551651, 0.24690042436122894, 0.2598523497581482, 0.2560528516769409, 0.23404286801815033, 0.23903454840183258, 0.23466891050338745, 0.2319614589214325, 0.22961761057376862, 0.22772763669490814, 0.24182619154453278, 0.23390240967273712, 0.23128560185432434, 0.23229195177555084, 0.22540400922298431, 0.2246367186307907, 0.2234325110912323, 0.22415030002593994, 0.22483381628990173, 0.2229308933019638, 0.22308507561683655, 0.2255873829126358, 0.22455500066280365, 0.21947893500328064, 0.21505387127399445, 0.22178778052330017, 0.2247682809829712, 0.21811965107917786, 0.21907450258731842, 0.2203563004732132, 0.21498741209506989, 0.2295379489660263, 0.21688391268253326, 0.22262325882911682, 0.22374382615089417, 0.21541938185691833, 0.21852754056453705, 0.2378453016281128, 0.24474050104618073, 0.24208864569664001, 0.214192733168602, 0.22566060721874237, 0.21559694409370422, 0.21500083804130554, 0.21241652965545654, 0.21036513149738312, 0.20983563363552094, 0.2144041657447815, 0.21132585406303406, 0.20867474377155304, 0.2046763151884079, 0.21111690998077393, 0.2063073068857193], 'accuracy': [0.9614762663841248, 0.9585129022598267, 0.9660560488700867, 0.9644396305084229, 0.9741379022598267, 0.9735991358757019, 0.9722521305084229, 0.9655172228813171, 0.9722521305084229, 0.9773706793785095, 0.9730603694915771, 0.9787176847457886, 0.9776400923728943, 0.9808728694915771, 0.9816810488700867, 0.9771012663841248, 0.9730603694915771, 0.9746767282485962, 0.9787176847457886, 0.9727909564971924, 0.96875, 0.9735991358757019, 0.9819504022598267, 0.9808728694915771, 0.9806034564971924, 0.977909505367279, 0.9795258641242981, 0.983027994632721, 0.9835668206214905, 0.9727909564971924, 0.9792564511299133, 0.984375, 0.9803340435028076, 0.9735991358757019, 0.9846444129943848, 0.982758641242981, 0.9695581793785095, 0.9832974076271057, 0.9811422228813171, 0.9832974076271057, 0.9870689511299133, 0.9744073152542114, 0.985722005367279, 0.9819504022598267, 0.9797952771186829, 0.9838362336158752, 0.9886853694915771, 0.9870689511299133, 0.9792564511299133, 0.9757543206214905, 0.9730603694915771, 0.9870689511299133, 0.984375, 0.9865301847457886, 0.9862607717514038, 0.9886853694915771, 0.9889547228813171, 0.9811422228813171, 0.9846444129943848, 0.9841055870056152, 0.984375, 0.9900323152542114, 0.9886853694915771, 0.9897629022598267, 0.9903017282485962, 0.9892241358757019, 0.9892241358757019, 0.9886853694915771, 0.9867995977401733, 0.9889547228813171, 0.9900323152542114, 0.9924569129943848, 0.9884159564971924, 0.9881465435028076, 0.9897629022598267, 0.9900323152542114, 0.9873383641242981, 0.9911099076271057, 0.9832974076271057, 0.9911099076271057, 0.9862607717514038, 0.9854525923728943, 0.9889547228813171, 0.9867995977401733, 0.9784482717514038, 0.9760237336158752, 0.9760237336158752, 0.990571141242981, 0.9849137663841248, 0.9900323152542114, 0.9889547228813171, 0.9897629022598267, 0.9913793206214905, 0.9911099076271057, 0.9881465435028076, 0.9892241358757019, 0.9916487336158752, 0.9932650923728943, 0.9873383641242981, 0.9913793206214905], 'val_loss': [0.9805741906166077, 0.9434008598327637, 0.9151477813720703, 0.8818584084510803, 0.8660026788711548, 0.8363339900970459, 0.8213544487953186, 0.8492485284805298, 0.8509632349014282, 0.7918908596038818, 0.7560674548149109, 0.7610833644866943, 0.8044601082801819, 0.6800830364227295, 0.6187959909439087, 0.5954132676124573, 0.587678074836731, 0.5065925717353821, 0.48665398359298706, 0.4827752709388733, 0.45725950598716736, 0.5396594405174255, 0.5048407316207886, 0.4267866015434265, 0.43894264101982117, 0.4285876452922821, 0.4002876281738281, 0.41653797030448914, 0.560546875, 0.41233742237091064, 0.3899131715297699, 0.39882561564445496, 0.48120683431625366, 0.39603081345558167, 0.4121876060962677, 0.48838546872138977, 0.4182230532169342, 0.4030488431453705, 0.43375423550605774, 0.4128510057926178, 0.48176252841949463, 0.4244508743286133, 0.4122469127178192, 0.41049593687057495, 0.4487709701061249, 0.41472676396369934, 0.43852823972702026, 0.44000422954559326, 0.527678906917572, 0.446121484041214, 0.423719197511673, 0.43027496337890625, 0.4149967133998871, 0.4473913908004761, 0.42225581407546997, 0.430742084980011, 0.4129031300544739, 0.4219959080219269, 0.4428170323371887, 0.4391530454158783, 0.4345313012599945, 0.42103111743927, 0.4432256519794464, 0.43551233410835266, 0.4306614398956299, 0.4448903203010559, 0.4211711585521698, 0.4255909025669098, 0.4282749891281128, 0.43549659848213196, 0.41915568709373474, 0.4232546389102936, 0.43443745374679565, 0.4443628787994385, 0.41730886697769165, 0.4351733922958374, 0.4282585680484772, 0.49263566732406616, 0.46491557359695435, 0.43675875663757324, 0.4626121520996094, 0.43243661522865295, 0.45696908235549927, 0.44890618324279785, 0.5350468754768372, 0.4669889211654663, 0.4414973556995392, 0.4314717948436737, 0.4280133843421936, 0.4365907907485962, 0.44415080547332764, 0.435353547334671, 0.43962880969047546, 0.4390922784805298, 0.47616443037986755, 0.4477761685848236, 0.4593242108821869, 0.4570538103580475, 0.44134095311164856, 0.4693695604801178], 'val_accuracy': [0.48491379618644714, 0.48491379618644714, 0.48599138855934143, 0.4978448152542114, 0.5096982717514038, 0.5366379022598267, 0.5517241358757019, 0.5398706793785095, 0.5549569129943848, 0.607758641242981, 0.6336206793785095, 0.6433189511299133, 0.6293103694915771, 0.7241379022598267, 0.7866379022598267, 0.8103448152542114, 0.8168103694915771, 0.868534505367279, 0.8760775923728943, 0.8782327771186829, 0.8900862336158752, 0.8674569129943848, 0.881465494632721, 0.8965517282485962, 0.899784505367279, 0.9084051847457886, 0.9170258641242981, 0.9159482717514038, 0.8793103694915771, 0.9191810488700867, 0.9321120977401733, 0.9278017282485962, 0.9051724076271057, 0.9288793206214905, 0.920258641242981, 0.9084051847457886, 0.920258641242981, 0.9321120977401733, 0.9137930870056152, 0.920258641242981, 0.9051724076271057, 0.9181034564971924, 0.9245689511299133, 0.9256465435028076, 0.9137930870056152, 0.9256465435028076, 0.9170258641242981, 0.9137930870056152, 0.8987069129943848, 0.9148706793785095, 0.920258641242981, 0.9191810488700867, 0.9224137663841248, 0.9116379022598267, 0.9181034564971924, 0.9181034564971924, 0.9181034564971924, 0.923491358757019, 0.9213362336158752, 0.9213362336158752, 0.9181034564971924, 0.9181034564971924, 0.9170258641242981, 0.9170258641242981, 0.9213362336158752, 0.920258641242981, 0.923491358757019, 0.9256465435028076, 0.9191810488700867, 0.9148706793785095, 0.9267241358757019, 0.9267241358757019, 0.9159482717514038, 0.9224137663841248, 0.9159482717514038, 0.9159482717514038, 0.9170258641242981, 0.9105603694915771, 0.9105603694915771, 0.912715494632721, 0.9116379022598267, 0.923491358757019, 0.9213362336158752, 0.9181034564971924, 0.9008620977401733, 0.9084051847457886, 0.9181034564971924, 0.9245689511299133, 0.9159482717514038, 0.912715494632721, 0.9159482717514038, 0.9137930870056152, 0.920258641242981, 0.9213362336158752, 0.9137930870056152, 0.920258641242981, 0.9148706793785095, 0.9137930870056152, 0.923491358757019, 0.9148706793785095]}\n","38/38 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","28/28 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.9448"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r28/28 [==============================] - 10s 56ms/step - loss: 0.3464 - accuracy: 0.9448 - val_loss: 0.9540 - val_accuracy: 0.4955\n","Epoch 2/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.3066 - accuracy: 0.9607 - val_loss: 0.9435 - val_accuracy: 0.4955\n","Epoch 3/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2835 - accuracy: 0.9703 - val_loss: 0.9172 - val_accuracy: 0.4955\n","Epoch 4/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.2772 - accuracy: 0.9731 - val_loss: 0.8981 - val_accuracy: 0.4989\n","Epoch 5/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.2806 - accuracy: 0.9686 - val_loss: 0.8841 - val_accuracy: 0.5136\n","Epoch 6/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.2817 - accuracy: 0.9728 - val_loss: 0.8698 - val_accuracy: 0.5204\n","Epoch 7/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.2756 - accuracy: 0.9720 - val_loss: 0.8646 - val_accuracy: 0.5283\n","Epoch 8/100\n","28/28 [==============================] - 1s 22ms/step - loss: 0.2740 - accuracy: 0.9740 - val_loss: 0.8809 - val_accuracy: 0.5294\n","Epoch 9/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.2728 - accuracy: 0.9740 - val_loss: 0.8553 - val_accuracy: 0.5588\n","Epoch 10/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.2652 - accuracy: 0.9768 - val_loss: 0.8354 - val_accuracy: 0.5814\n","Epoch 11/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.2698 - accuracy: 0.9731 - val_loss: 0.8493 - val_accuracy: 0.5882\n","Epoch 12/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.2726 - accuracy: 0.9751 - val_loss: 0.8098 - val_accuracy: 0.6165\n","Epoch 13/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.2711 - accuracy: 0.9726 - val_loss: 0.7410 - val_accuracy: 0.6799\n","Epoch 14/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2740 - accuracy: 0.9723 - val_loss: 0.7980 - val_accuracy: 0.6448\n","Epoch 15/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.2618 - accuracy: 0.9779 - val_loss: 0.7216 - val_accuracy: 0.6968\n","Epoch 16/100\n","28/28 [==============================] - 1s 29ms/step - loss: 0.2659 - accuracy: 0.9768 - val_loss: 0.6651 - val_accuracy: 0.7590\n","Epoch 17/100\n","28/28 [==============================] - 1s 30ms/step - loss: 0.2670 - accuracy: 0.9734 - val_loss: 0.6155 - val_accuracy: 0.8054\n","Epoch 18/100\n","28/28 [==============================] - 1s 32ms/step - loss: 0.2610 - accuracy: 0.9745 - val_loss: 0.5583 - val_accuracy: 0.8405\n","Epoch 19/100\n","28/28 [==============================] - 1s 30ms/step - loss: 0.2528 - accuracy: 0.9810 - val_loss: 0.5327 - val_accuracy: 0.8484\n","Epoch 20/100\n","28/28 [==============================] - 1s 26ms/step - loss: 0.2562 - accuracy: 0.9796 - val_loss: 0.5181 - val_accuracy: 0.8643\n","Epoch 21/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.2498 - accuracy: 0.9830 - val_loss: 0.5106 - val_accuracy: 0.8688\n","Epoch 22/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2549 - accuracy: 0.9816 - val_loss: 0.5785 - val_accuracy: 0.8507\n","Epoch 23/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2593 - accuracy: 0.9771 - val_loss: 0.5174 - val_accuracy: 0.8620\n","Epoch 24/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2580 - accuracy: 0.9776 - val_loss: 0.5643 - val_accuracy: 0.8631\n","Epoch 25/100\n","28/28 [==============================] - 1s 21ms/step - loss: 0.2573 - accuracy: 0.9791 - val_loss: 0.5053 - val_accuracy: 0.8778\n","Epoch 26/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.2465 - accuracy: 0.9842 - val_loss: 0.5170 - val_accuracy: 0.8835\n","Epoch 27/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2598 - accuracy: 0.9782 - val_loss: 0.6074 - val_accuracy: 0.8676\n","Epoch 28/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.2566 - accuracy: 0.9802 - val_loss: 0.4847 - val_accuracy: 0.9084\n","Epoch 29/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2592 - accuracy: 0.9759 - val_loss: 0.5778 - val_accuracy: 0.8722\n","Epoch 30/100\n","28/28 [==============================] - 1s 23ms/step - loss: 0.2646 - accuracy: 0.9765 - val_loss: 0.4649 - val_accuracy: 0.9163\n","Epoch 31/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2449 - accuracy: 0.9842 - val_loss: 0.4845 - val_accuracy: 0.9095\n","Epoch 32/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2497 - accuracy: 0.9819 - val_loss: 0.4804 - val_accuracy: 0.9084\n","Epoch 33/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2394 - accuracy: 0.9881 - val_loss: 0.4936 - val_accuracy: 0.9027\n","Epoch 34/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2445 - accuracy: 0.9853 - val_loss: 0.4992 - val_accuracy: 0.9038\n","Epoch 35/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2572 - accuracy: 0.9757 - val_loss: 0.5065 - val_accuracy: 0.8982\n","Epoch 36/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2410 - accuracy: 0.9850 - val_loss: 0.4948 - val_accuracy: 0.9084\n","Epoch 37/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2520 - accuracy: 0.9771 - val_loss: 0.4936 - val_accuracy: 0.9163\n","Epoch 38/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2395 - accuracy: 0.9850 - val_loss: 0.5078 - val_accuracy: 0.9095\n","Epoch 39/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2415 - accuracy: 0.9842 - val_loss: 0.5120 - val_accuracy: 0.8993\n","Epoch 40/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2414 - accuracy: 0.9842 - val_loss: 0.5165 - val_accuracy: 0.9005\n","Epoch 41/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2351 - accuracy: 0.9875 - val_loss: 0.5092 - val_accuracy: 0.9038\n","Epoch 42/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2398 - accuracy: 0.9844 - val_loss: 0.5323 - val_accuracy: 0.8925\n","Epoch 43/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2373 - accuracy: 0.9842 - val_loss: 0.5552 - val_accuracy: 0.8846\n","Epoch 44/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.2393 - accuracy: 0.9836 - val_loss: 0.5265 - val_accuracy: 0.9005\n","Epoch 45/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.2349 - accuracy: 0.9870 - val_loss: 0.5073 - val_accuracy: 0.9095\n","Epoch 46/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2304 - accuracy: 0.9867 - val_loss: 0.5159 - val_accuracy: 0.9027\n","Epoch 47/100\n","28/28 [==============================] - 1s 28ms/step - loss: 0.2353 - accuracy: 0.9853 - val_loss: 0.5080 - val_accuracy: 0.9197\n","Epoch 48/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2358 - accuracy: 0.9859 - val_loss: 0.5062 - val_accuracy: 0.9072\n","Epoch 49/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.2495 - accuracy: 0.9774 - val_loss: 0.5462 - val_accuracy: 0.9005\n","Epoch 50/100\n","28/28 [==============================] - 1s 18ms/step - loss: 0.2421 - accuracy: 0.9850 - val_loss: 0.5259 - val_accuracy: 0.8993\n","Epoch 51/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2370 - accuracy: 0.9839 - val_loss: 0.5602 - val_accuracy: 0.9005\n","Epoch 52/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2384 - accuracy: 0.9822 - val_loss: 0.5229 - val_accuracy: 0.9005\n","Epoch 53/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2327 - accuracy: 0.9867 - val_loss: 0.5204 - val_accuracy: 0.8971\n","Epoch 54/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2333 - accuracy: 0.9856 - val_loss: 0.5280 - val_accuracy: 0.9016\n","Epoch 55/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2267 - accuracy: 0.9898 - val_loss: 0.5187 - val_accuracy: 0.9038\n","Epoch 56/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2307 - accuracy: 0.9853 - val_loss: 0.5495 - val_accuracy: 0.8982\n","Epoch 57/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2342 - accuracy: 0.9853 - val_loss: 0.5017 - val_accuracy: 0.9050\n","Epoch 58/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2395 - accuracy: 0.9813 - val_loss: 0.5529 - val_accuracy: 0.8971\n","Epoch 59/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2384 - accuracy: 0.9808 - val_loss: 0.5335 - val_accuracy: 0.9038\n","Epoch 60/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2251 - accuracy: 0.9870 - val_loss: 0.5130 - val_accuracy: 0.9016\n","Epoch 61/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2241 - accuracy: 0.9887 - val_loss: 0.5339 - val_accuracy: 0.9072\n","Epoch 62/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2220 - accuracy: 0.9895 - val_loss: 0.5347 - val_accuracy: 0.9038\n","Epoch 63/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2238 - accuracy: 0.9873 - val_loss: 0.5388 - val_accuracy: 0.8914\n","Epoch 64/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2278 - accuracy: 0.9878 - val_loss: 0.5172 - val_accuracy: 0.9084\n","Epoch 65/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2232 - accuracy: 0.9890 - val_loss: 0.5178 - val_accuracy: 0.9038\n","Epoch 66/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2290 - accuracy: 0.9859 - val_loss: 0.5361 - val_accuracy: 0.8959\n","Epoch 67/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2240 - accuracy: 0.9887 - val_loss: 0.6079 - val_accuracy: 0.8756\n","Epoch 68/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2439 - accuracy: 0.9754 - val_loss: 0.5283 - val_accuracy: 0.8982\n","Epoch 69/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2353 - accuracy: 0.9816 - val_loss: 0.5430 - val_accuracy: 0.8891\n","Epoch 70/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2383 - accuracy: 0.9810 - val_loss: 0.5102 - val_accuracy: 0.9095\n","Epoch 71/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2236 - accuracy: 0.9881 - val_loss: 0.5154 - val_accuracy: 0.9118\n","Epoch 72/100\n","28/28 [==============================] - 0s 16ms/step - loss: 0.2156 - accuracy: 0.9915 - val_loss: 0.5466 - val_accuracy: 0.8982\n","Epoch 73/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2171 - accuracy: 0.9907 - val_loss: 0.5472 - val_accuracy: 0.9038\n","Epoch 74/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2351 - accuracy: 0.9839 - val_loss: 0.5540 - val_accuracy: 0.8971\n","Epoch 75/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2232 - accuracy: 0.9881 - val_loss: 0.5242 - val_accuracy: 0.9140\n","Epoch 76/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.2279 - accuracy: 0.9856 - val_loss: 0.5862 - val_accuracy: 0.8790\n","Epoch 77/100\n","28/28 [==============================] - 1s 20ms/step - loss: 0.2357 - accuracy: 0.9796 - val_loss: 0.5114 - val_accuracy: 0.9016\n","Epoch 78/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2162 - accuracy: 0.9898 - val_loss: 0.5320 - val_accuracy: 0.9129\n","Epoch 79/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.2142 - accuracy: 0.9915 - val_loss: 0.5850 - val_accuracy: 0.8891\n","Epoch 80/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2214 - accuracy: 0.9847 - val_loss: 0.5189 - val_accuracy: 0.9174\n","Epoch 81/100\n","28/28 [==============================] - 0s 17ms/step - loss: 0.2101 - accuracy: 0.9943 - val_loss: 0.5400 - val_accuracy: 0.9118\n","Epoch 82/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.2160 - accuracy: 0.9895 - val_loss: 0.5251 - val_accuracy: 0.9163\n","Epoch 83/100\n","28/28 [==============================] - 1s 19ms/step - loss: 0.2131 - accuracy: 0.9890 - val_loss: 0.5216 - val_accuracy: 0.9072\n","Epoch 84/100\n","28/28 [==============================] - 0s 18ms/step - loss: 0.2256 - accuracy: 0.9833 - val_loss: 0.5177 - val_accuracy: 0.9106\n","Epoch 85/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2139 - accuracy: 0.9895 - val_loss: 0.7367 - val_accuracy: 0.8348\n","Epoch 86/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2192 - accuracy: 0.9881 - val_loss: 0.5302 - val_accuracy: 0.9118\n","Epoch 87/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2133 - accuracy: 0.9895 - val_loss: 0.5395 - val_accuracy: 0.8982\n","Epoch 88/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2114 - accuracy: 0.9921 - val_loss: 0.5566 - val_accuracy: 0.9005\n","Epoch 89/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2117 - accuracy: 0.9909 - val_loss: 0.6277 - val_accuracy: 0.8790\n","Epoch 90/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2092 - accuracy: 0.9915 - val_loss: 0.5523 - val_accuracy: 0.9027\n","Epoch 91/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2038 - accuracy: 0.9949 - val_loss: 0.5569 - val_accuracy: 0.8982\n","Epoch 92/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2050 - accuracy: 0.9921 - val_loss: 0.5436 - val_accuracy: 0.9186\n","Epoch 93/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2041 - accuracy: 0.9949 - val_loss: 0.5437 - val_accuracy: 0.9140\n","Epoch 94/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2072 - accuracy: 0.9924 - val_loss: 0.5635 - val_accuracy: 0.9016\n","Epoch 95/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2057 - accuracy: 0.9924 - val_loss: 0.5759 - val_accuracy: 0.8937\n","Epoch 96/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2090 - accuracy: 0.9915 - val_loss: 0.5680 - val_accuracy: 0.8937\n","Epoch 97/100\n","28/28 [==============================] - 0s 14ms/step - loss: 0.2039 - accuracy: 0.9918 - val_loss: 0.5626 - val_accuracy: 0.8971\n","Epoch 98/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2119 - accuracy: 0.9887 - val_loss: 0.5716 - val_accuracy: 0.8914\n","Epoch 99/100\n","28/28 [==============================] - 0s 13ms/step - loss: 0.2044 - accuracy: 0.9918 - val_loss: 0.6136 - val_accuracy: 0.8903\n","Epoch 100/100\n","28/28 [==============================] - 0s 15ms/step - loss: 0.2124 - accuracy: 0.9881 - val_loss: 0.5648 - val_accuracy: 0.8959\n","{'loss': [0.34637144207954407, 0.3065784275531769, 0.2835128903388977, 0.27716952562332153, 0.2806021571159363, 0.2817251682281494, 0.2756209969520569, 0.27402567863464355, 0.2728443443775177, 0.2652101218700409, 0.26976338028907776, 0.2726106643676758, 0.2711205780506134, 0.2739711105823517, 0.26178762316703796, 0.26587316393852234, 0.2669878900051117, 0.2609565258026123, 0.2527918815612793, 0.2562392055988312, 0.24982145428657532, 0.254947692155838, 0.2592908442020416, 0.258038192987442, 0.2572968602180481, 0.2465103715658188, 0.2598097324371338, 0.25664448738098145, 0.25915300846099854, 0.26461800932884216, 0.2448965162038803, 0.24969345331192017, 0.23941513895988464, 0.24453066289424896, 0.25721946358680725, 0.24095526337623596, 0.25197482109069824, 0.23945611715316772, 0.24151067435741425, 0.2413560152053833, 0.2350601702928543, 0.23977068066596985, 0.2373003363609314, 0.23926100134849548, 0.2349143922328949, 0.23040178418159485, 0.2352568507194519, 0.23579035699367523, 0.24951107800006866, 0.24206122756004333, 0.23701244592666626, 0.23843540251255035, 0.23265133798122406, 0.23331785202026367, 0.22666002810001373, 0.2306615263223648, 0.23422066867351532, 0.23950554430484772, 0.23843657970428467, 0.22511261701583862, 0.22414104640483856, 0.2220103144645691, 0.2237958014011383, 0.22781524062156677, 0.2232428342103958, 0.2290196567773819, 0.2239902913570404, 0.24393239617347717, 0.23526176810264587, 0.2383010983467102, 0.22357992827892303, 0.21560053527355194, 0.21706362068653107, 0.235126793384552, 0.2231772541999817, 0.22793294489383698, 0.23570412397384644, 0.21616482734680176, 0.21420273184776306, 0.22139699757099152, 0.21013455092906952, 0.2159806489944458, 0.21305909752845764, 0.22563500702381134, 0.2138575315475464, 0.2192280888557434, 0.2132970094680786, 0.2113688588142395, 0.21169130504131317, 0.20922115445137024, 0.20377270877361298, 0.20496803522109985, 0.20412108302116394, 0.20719482004642487, 0.20565523207187653, 0.20896272361278534, 0.20385058224201202, 0.2119235396385193, 0.20444683730602264, 0.21241052448749542], 'accuracy': [0.9448217153549194, 0.9606677889823914, 0.9702886343002319, 0.9731183052062988, 0.9685908555984497, 0.9728353023529053, 0.9719864130020142, 0.9739671945571899, 0.9739671945571899, 0.9767968058586121, 0.9731183052062988, 0.9750990271568298, 0.9725523591041565, 0.9722693562507629, 0.9779286980628967, 0.9767968058586121, 0.9734012484550476, 0.9745330810546875, 0.9810413122177124, 0.979626476764679, 0.9830220937728882, 0.9816072583198547, 0.9770798087120056, 0.977645754814148, 0.9790605306625366, 0.9841539263725281, 0.9782116413116455, 0.9801924228668213, 0.975947916507721, 0.9765138626098633, 0.9841539263725281, 0.9818902015686035, 0.9881154298782349, 0.9852858185768127, 0.9756649732589722, 0.9850028157234192, 0.9770798087120056, 0.9850028157234192, 0.9841539263725281, 0.9841539263725281, 0.9875495433807373, 0.9844368696212769, 0.9841539263725281, 0.9835879802703857, 0.986983597278595, 0.9867005944252014, 0.9852858185768127, 0.9858517050743103, 0.9773627519607544, 0.9850028157234192, 0.9838709831237793, 0.9821732044219971, 0.9867005944252014, 0.9855687618255615, 0.9898132681846619, 0.9852858185768127, 0.9852858185768127, 0.9813242554664612, 0.9807583689689636, 0.986983597278595, 0.9886813759803772, 0.9895302653312683, 0.9872665405273438, 0.9878324866294861, 0.988964319229126, 0.9858517050743103, 0.9886813759803772, 0.9753820300102234, 0.9816072583198547, 0.9810413122177124, 0.9881154298782349, 0.9915110468864441, 0.990662157535553, 0.9838709831237793, 0.9881154298782349, 0.9855687618255615, 0.979626476764679, 0.9898132681846619, 0.9915110468864441, 0.9847198724746704, 0.994340717792511, 0.9895302653312683, 0.988964319229126, 0.983305037021637, 0.9895302653312683, 0.9881154298782349, 0.9895302653312683, 0.9920769929885864, 0.9909451007843018, 0.9915110468864441, 0.9949066042900085, 0.9920769929885864, 0.9949066042900085, 0.9923599362373352, 0.9923599362373352, 0.9915110468864441, 0.9917939901351929, 0.9886813759803772, 0.9917939901351929, 0.9881154298782349], 'val_loss': [0.9539771676063538, 0.9435007572174072, 0.9171804189682007, 0.8980625867843628, 0.8841114044189453, 0.8697937726974487, 0.8646283745765686, 0.8809148669242859, 0.8552737832069397, 0.835445761680603, 0.8493181467056274, 0.8097773194313049, 0.7409951090812683, 0.7980219721794128, 0.721613883972168, 0.6650761365890503, 0.6155253052711487, 0.5582934617996216, 0.532706081867218, 0.5180966258049011, 0.5106166005134583, 0.5784659385681152, 0.5174046158790588, 0.5643369555473328, 0.5053089261054993, 0.5169514417648315, 0.6074231266975403, 0.4846877455711365, 0.5777558088302612, 0.46488437056541443, 0.4845196008682251, 0.48041731119155884, 0.4935859143733978, 0.4991687834262848, 0.506529688835144, 0.49482619762420654, 0.49359261989593506, 0.5077685117721558, 0.5120295286178589, 0.516521692276001, 0.5092372894287109, 0.5323255062103271, 0.5551971197128296, 0.5264707207679749, 0.5073291063308716, 0.5159403085708618, 0.5080412030220032, 0.5062477588653564, 0.5462274551391602, 0.5258811712265015, 0.5602430105209351, 0.5228525996208191, 0.520390510559082, 0.5280286073684692, 0.5186768174171448, 0.5494859218597412, 0.5016568303108215, 0.552903413772583, 0.5334879755973816, 0.5129541158676147, 0.533933699131012, 0.5347162485122681, 0.538784384727478, 0.5171672701835632, 0.5178300142288208, 0.5360878705978394, 0.6079218983650208, 0.5283055901527405, 0.5429849028587341, 0.5101546049118042, 0.5154241919517517, 0.5466148257255554, 0.5471634864807129, 0.553955078125, 0.5242099165916443, 0.5862178206443787, 0.5114427208900452, 0.5319854021072388, 0.5849900841712952, 0.5188900232315063, 0.5399999618530273, 0.5250564217567444, 0.5216422080993652, 0.5177197456359863, 0.7367274761199951, 0.5302273035049438, 0.5395137071609497, 0.5565537214279175, 0.6277185678482056, 0.5523195862770081, 0.5568910241127014, 0.5436137914657593, 0.5436685085296631, 0.5635239481925964, 0.5759134292602539, 0.5679898858070374, 0.5626360177993774, 0.57159823179245, 0.6136031746864319, 0.5647956728935242], 'val_accuracy': [0.4954751133918762, 0.4954751133918762, 0.4954751133918762, 0.49886876344680786, 0.5135746598243713, 0.5203620195388794, 0.5282805562019348, 0.529411792755127, 0.5588235259056091, 0.581447958946228, 0.5882353186607361, 0.6165158152580261, 0.679864227771759, 0.6447963714599609, 0.6968325972557068, 0.7590497732162476, 0.8054298758506775, 0.8404977321624756, 0.848416268825531, 0.8642534017562866, 0.8687782883644104, 0.8506787419319153, 0.8619909286499023, 0.8631221652030945, 0.877828061580658, 0.8834841847419739, 0.8676470518112183, 0.9083710312843323, 0.872171938419342, 0.9162895679473877, 0.9095022678375244, 0.9083710312843323, 0.9027149081230164, 0.9038461446762085, 0.8981900215148926, 0.9083710312843323, 0.9162895679473877, 0.9095022678375244, 0.8993212580680847, 0.9004524946212769, 0.9038461446762085, 0.8925339579582214, 0.8846153616905212, 0.9004524946212769, 0.9095022678375244, 0.9027149081230164, 0.9196832776069641, 0.9072397947311401, 0.9004524946212769, 0.8993212580680847, 0.9004524946212769, 0.9004524946212769, 0.8970588445663452, 0.901583731174469, 0.9038461446762085, 0.8981900215148926, 0.9049773812294006, 0.8970588445663452, 0.9038461446762085, 0.901583731174469, 0.9072397947311401, 0.9038461446762085, 0.8914027214050293, 0.9083710312843323, 0.9038461446762085, 0.8959276080131531, 0.8755655884742737, 0.8981900215148926, 0.889140248298645, 0.9095022678375244, 0.9117646813392639, 0.8981900215148926, 0.9038461446762085, 0.8970588445663452, 0.9140271544456482, 0.8789592981338501, 0.901583731174469, 0.912895917892456, 0.889140248298645, 0.9174208045005798, 0.9117646813392639, 0.9162895679473877, 0.9072397947311401, 0.9106335043907166, 0.8348416090011597, 0.9117646813392639, 0.8981900215148926, 0.9004524946212769, 0.8789592981338501, 0.9027149081230164, 0.8981900215148926, 0.918552041053772, 0.9140271544456482, 0.901583731174469, 0.8936651349067688, 0.8936651349067688, 0.8970588445663452, 0.8914027214050293, 0.8902714848518372, 0.8959276080131531]}\n","45/45 [==============================] - 1s 3ms/step\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1d (Conv1D)             (None, 18, 256)           22528     \n","                                                                 \n"," batch_normalization (Batch  (None, 18, 256)           1024      \n"," Normalization)                                                  \n","                                                                 \n"," max_pooling1d (MaxPooling1  (None, 9, 256)            0         \n"," D)                                                              \n","                                                                 \n"," conv1d_1 (Conv1D)           (None, 9, 256)            196864    \n","                                                                 \n"," max_pooling1d_1 (MaxPoolin  (None, 5, 256)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_2 (Conv1D)           (None, 5, 512)            393728    \n","                                                                 \n"," max_pooling1d_2 (MaxPoolin  (None, 3, 512)            0         \n"," g1D)                                                            \n","                                                                 \n"," conv1d_3 (Conv1D)           (None, 1, 1024)           1573888   \n","                                                                 \n"," dropout (Dropout)           (None, 1, 1024)           0         \n","                                                                 \n"," gru (GRU)                   (None, 1, 256)            984576    \n","                                                                 \n"," gru_1 (GRU)                 (None, 1, 256)            394752    \n","                                                                 \n"," flatten (Flatten)           (None, 256)               0         \n","                                                                 \n"," dense (Dense)               (None, 512)               131584    \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               131328    \n","                                                                 \n"," dense_2 (Dense)             (None, 32)                8224      \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 3838529 (14.64 MB)\n","Trainable params: 3838017 (14.64 MB)\n","Non-trainable params: 512 (2.00 KB)\n","_________________________________________________________________\n","Epoch 1/100\n","31/31 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.9571"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r31/31 [==============================] - 7s 65ms/step - loss: 0.3165 - accuracy: 0.9571 - val_loss: 0.9687 - val_accuracy: 0.4855\n","Epoch 2/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.3036 - accuracy: 0.9607 - val_loss: 0.9379 - val_accuracy: 0.4855\n","Epoch 3/100\n","31/31 [==============================] - 1s 26ms/step - loss: 0.2905 - accuracy: 0.9680 - val_loss: 0.8881 - val_accuracy: 0.4907\n","Epoch 4/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.2854 - accuracy: 0.9705 - val_loss: 0.8981 - val_accuracy: 0.4990\n","Epoch 5/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.2870 - accuracy: 0.9677 - val_loss: 0.8669 - val_accuracy: 0.5176\n","Epoch 6/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2799 - accuracy: 0.9721 - val_loss: 0.8407 - val_accuracy: 0.5341\n","Epoch 7/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2955 - accuracy: 0.9605 - val_loss: 0.8535 - val_accuracy: 0.5382\n","Epoch 8/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2765 - accuracy: 0.9721 - val_loss: 0.8141 - val_accuracy: 0.5919\n","Epoch 9/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2907 - accuracy: 0.9620 - val_loss: 0.9035 - val_accuracy: 0.5475\n","Epoch 10/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.3136 - accuracy: 0.9543 - val_loss: 0.8192 - val_accuracy: 0.6095\n","Epoch 11/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.2778 - accuracy: 0.9726 - val_loss: 0.7555 - val_accuracy: 0.6529\n","Epoch 12/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2711 - accuracy: 0.9749 - val_loss: 0.8293 - val_accuracy: 0.6291\n","Epoch 13/100\n","31/31 [==============================] - 1s 22ms/step - loss: 0.2734 - accuracy: 0.9736 - val_loss: 0.7874 - val_accuracy: 0.6560\n","Epoch 14/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2738 - accuracy: 0.9726 - val_loss: 0.7506 - val_accuracy: 0.6808\n","Epoch 15/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2825 - accuracy: 0.9698 - val_loss: 0.5884 - val_accuracy: 0.7996\n","Epoch 16/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2692 - accuracy: 0.9731 - val_loss: 0.5304 - val_accuracy: 0.8585\n","Epoch 17/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2796 - accuracy: 0.9698 - val_loss: 0.5196 - val_accuracy: 0.8647\n","Epoch 18/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2628 - accuracy: 0.9773 - val_loss: 0.5295 - val_accuracy: 0.8554\n","Epoch 19/100\n","31/31 [==============================] - 1s 20ms/step - loss: 0.2679 - accuracy: 0.9744 - val_loss: 0.5009 - val_accuracy: 0.8740\n","Epoch 20/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.2638 - accuracy: 0.9773 - val_loss: 0.4882 - val_accuracy: 0.8802\n","Epoch 21/100\n","31/31 [==============================] - 1s 24ms/step - loss: 0.2609 - accuracy: 0.9798 - val_loss: 0.4809 - val_accuracy: 0.8812\n","Epoch 22/100\n","31/31 [==============================] - 1s 28ms/step - loss: 0.2680 - accuracy: 0.9744 - val_loss: 0.4692 - val_accuracy: 0.8915\n","Epoch 23/100\n","31/31 [==============================] - 1s 27ms/step - loss: 0.2578 - accuracy: 0.9780 - val_loss: 0.4713 - val_accuracy: 0.8946\n","Epoch 24/100\n","31/31 [==============================] - 1s 25ms/step - loss: 0.2939 - accuracy: 0.9623 - val_loss: 0.4667 - val_accuracy: 0.8977\n","Epoch 25/100\n","31/31 [==============================] - 1s 30ms/step - loss: 0.2559 - accuracy: 0.9806 - val_loss: 0.4467 - val_accuracy: 0.9029\n","Epoch 26/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2500 - accuracy: 0.9814 - val_loss: 0.4683 - val_accuracy: 0.8977\n","Epoch 27/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2518 - accuracy: 0.9817 - val_loss: 0.5207 - val_accuracy: 0.8905\n","Epoch 28/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2582 - accuracy: 0.9788 - val_loss: 0.4571 - val_accuracy: 0.9008\n","Epoch 29/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2479 - accuracy: 0.9829 - val_loss: 0.5245 - val_accuracy: 0.8926\n","Epoch 30/100\n","31/31 [==============================] - 1s 21ms/step - loss: 0.2766 - accuracy: 0.9680 - val_loss: 0.4592 - val_accuracy: 0.9112\n","Epoch 31/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2536 - accuracy: 0.9809 - val_loss: 0.4542 - val_accuracy: 0.9070\n","Epoch 32/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2472 - accuracy: 0.9827 - val_loss: 0.4584 - val_accuracy: 0.9060\n","Epoch 33/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2427 - accuracy: 0.9860 - val_loss: 0.4661 - val_accuracy: 0.9039\n","Epoch 34/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2577 - accuracy: 0.9767 - val_loss: 0.6061 - val_accuracy: 0.8740\n","Epoch 35/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2699 - accuracy: 0.9682 - val_loss: 0.4677 - val_accuracy: 0.9050\n","Epoch 36/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2556 - accuracy: 0.9762 - val_loss: 0.4588 - val_accuracy: 0.9029\n","Epoch 37/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2522 - accuracy: 0.9778 - val_loss: 0.4642 - val_accuracy: 0.9019\n","Epoch 38/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2495 - accuracy: 0.9798 - val_loss: 0.4481 - val_accuracy: 0.9060\n","Epoch 39/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2428 - accuracy: 0.9829 - val_loss: 0.4604 - val_accuracy: 0.9008\n","Epoch 40/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2395 - accuracy: 0.9842 - val_loss: 0.4766 - val_accuracy: 0.9050\n","Epoch 41/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2428 - accuracy: 0.9832 - val_loss: 0.4773 - val_accuracy: 0.9070\n","Epoch 42/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2496 - accuracy: 0.9811 - val_loss: 0.4634 - val_accuracy: 0.9039\n","Epoch 43/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2571 - accuracy: 0.9765 - val_loss: 0.4761 - val_accuracy: 0.8998\n","Epoch 44/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2473 - accuracy: 0.9819 - val_loss: 0.4670 - val_accuracy: 0.9050\n","Epoch 45/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2424 - accuracy: 0.9824 - val_loss: 0.4825 - val_accuracy: 0.9008\n","Epoch 46/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2426 - accuracy: 0.9817 - val_loss: 0.4618 - val_accuracy: 0.9008\n","Epoch 47/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2409 - accuracy: 0.9832 - val_loss: 0.5359 - val_accuracy: 0.8967\n","Epoch 48/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2572 - accuracy: 0.9749 - val_loss: 0.4994 - val_accuracy: 0.8988\n","Epoch 49/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2397 - accuracy: 0.9832 - val_loss: 0.4778 - val_accuracy: 0.8988\n","Epoch 50/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2342 - accuracy: 0.9858 - val_loss: 0.4871 - val_accuracy: 0.9070\n","Epoch 51/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.2323 - accuracy: 0.9871 - val_loss: 0.4899 - val_accuracy: 0.8998\n","Epoch 52/100\n","31/31 [==============================] - 0s 16ms/step - loss: 0.2489 - accuracy: 0.9791 - val_loss: 0.4928 - val_accuracy: 0.8998\n","Epoch 53/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2395 - accuracy: 0.9842 - val_loss: 0.4762 - val_accuracy: 0.9060\n","Epoch 54/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.2298 - accuracy: 0.9873 - val_loss: 0.4842 - val_accuracy: 0.9039\n","Epoch 55/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2466 - accuracy: 0.9786 - val_loss: 0.4790 - val_accuracy: 0.9008\n","Epoch 56/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2362 - accuracy: 0.9835 - val_loss: 0.5283 - val_accuracy: 0.8936\n","Epoch 57/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.2324 - accuracy: 0.9863 - val_loss: 0.5034 - val_accuracy: 0.8977\n","Epoch 58/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2373 - accuracy: 0.9806 - val_loss: 0.5217 - val_accuracy: 0.9039\n","Epoch 59/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2547 - accuracy: 0.9729 - val_loss: 0.5781 - val_accuracy: 0.8864\n","Epoch 60/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2361 - accuracy: 0.9832 - val_loss: 0.4813 - val_accuracy: 0.9081\n","Epoch 61/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2811 - accuracy: 0.9630 - val_loss: 0.5255 - val_accuracy: 0.8957\n","Epoch 62/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2362 - accuracy: 0.9829 - val_loss: 0.4698 - val_accuracy: 0.9029\n","Epoch 63/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2315 - accuracy: 0.9860 - val_loss: 0.4718 - val_accuracy: 0.9029\n","Epoch 64/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2258 - accuracy: 0.9881 - val_loss: 0.5092 - val_accuracy: 0.8977\n","Epoch 65/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2292 - accuracy: 0.9860 - val_loss: 0.4819 - val_accuracy: 0.9039\n","Epoch 66/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2230 - accuracy: 0.9910 - val_loss: 0.5433 - val_accuracy: 0.8967\n","Epoch 67/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2309 - accuracy: 0.9853 - val_loss: 0.5676 - val_accuracy: 0.8905\n","Epoch 68/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2364 - accuracy: 0.9822 - val_loss: 0.4931 - val_accuracy: 0.8998\n","Epoch 69/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2308 - accuracy: 0.9853 - val_loss: 0.4848 - val_accuracy: 0.9029\n","Epoch 70/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2297 - accuracy: 0.9845 - val_loss: 0.5052 - val_accuracy: 0.8998\n","Epoch 71/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2296 - accuracy: 0.9835 - val_loss: 0.5050 - val_accuracy: 0.8998\n","Epoch 72/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2221 - accuracy: 0.9899 - val_loss: 0.4879 - val_accuracy: 0.8967\n","Epoch 73/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2221 - accuracy: 0.9879 - val_loss: 0.4782 - val_accuracy: 0.9050\n","Epoch 74/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2176 - accuracy: 0.9897 - val_loss: 0.4893 - val_accuracy: 0.9039\n","Epoch 75/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2269 - accuracy: 0.9863 - val_loss: 0.4769 - val_accuracy: 0.9019\n","Epoch 76/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2163 - accuracy: 0.9912 - val_loss: 0.4764 - val_accuracy: 0.9008\n","Epoch 77/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2200 - accuracy: 0.9884 - val_loss: 0.5176 - val_accuracy: 0.9008\n","Epoch 78/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2161 - accuracy: 0.9910 - val_loss: 0.5052 - val_accuracy: 0.8988\n","Epoch 79/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2171 - accuracy: 0.9907 - val_loss: 0.5239 - val_accuracy: 0.9019\n","Epoch 80/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.2164 - accuracy: 0.9917 - val_loss: 0.5053 - val_accuracy: 0.9029\n","Epoch 81/100\n","31/31 [==============================] - 1s 16ms/step - loss: 0.2277 - accuracy: 0.9809 - val_loss: 0.4902 - val_accuracy: 0.9019\n","Epoch 82/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2239 - accuracy: 0.9868 - val_loss: 0.5191 - val_accuracy: 0.9081\n","Epoch 83/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2357 - accuracy: 0.9809 - val_loss: 0.4954 - val_accuracy: 0.8998\n","Epoch 84/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2239 - accuracy: 0.9850 - val_loss: 0.4965 - val_accuracy: 0.9029\n","Epoch 85/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2156 - accuracy: 0.9891 - val_loss: 0.4812 - val_accuracy: 0.9008\n","Epoch 86/100\n","31/31 [==============================] - 1s 19ms/step - loss: 0.2165 - accuracy: 0.9904 - val_loss: 0.4971 - val_accuracy: 0.9019\n","Epoch 87/100\n","31/31 [==============================] - 1s 17ms/step - loss: 0.2127 - accuracy: 0.9920 - val_loss: 0.4976 - val_accuracy: 0.8998\n","Epoch 88/100\n","31/31 [==============================] - 1s 18ms/step - loss: 0.2166 - accuracy: 0.9894 - val_loss: 0.4898 - val_accuracy: 0.9019\n","Epoch 89/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2141 - accuracy: 0.9912 - val_loss: 0.4869 - val_accuracy: 0.9039\n","Epoch 90/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2105 - accuracy: 0.9902 - val_loss: 0.5035 - val_accuracy: 0.8998\n","Epoch 91/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2088 - accuracy: 0.9920 - val_loss: 0.5377 - val_accuracy: 0.8967\n","Epoch 92/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2238 - accuracy: 0.9840 - val_loss: 0.7127 - val_accuracy: 0.8564\n","Epoch 93/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2354 - accuracy: 0.9778 - val_loss: 0.4841 - val_accuracy: 0.8988\n","Epoch 94/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2234 - accuracy: 0.9832 - val_loss: 0.5118 - val_accuracy: 0.8988\n","Epoch 95/100\n","31/31 [==============================] - 0s 15ms/step - loss: 0.2118 - accuracy: 0.9881 - val_loss: 0.4911 - val_accuracy: 0.9029\n","Epoch 96/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2065 - accuracy: 0.9935 - val_loss: 0.5049 - val_accuracy: 0.8977\n","Epoch 97/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2077 - accuracy: 0.9922 - val_loss: 0.5083 - val_accuracy: 0.8946\n","Epoch 98/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2090 - accuracy: 0.9902 - val_loss: 0.5126 - val_accuracy: 0.9029\n","Epoch 99/100\n","31/31 [==============================] - 0s 14ms/step - loss: 0.2052 - accuracy: 0.9933 - val_loss: 0.4975 - val_accuracy: 0.9029\n","Epoch 100/100\n","31/31 [==============================] - 0s 13ms/step - loss: 0.2095 - accuracy: 0.9894 - val_loss: 0.5010 - val_accuracy: 0.8967\n","{'loss': [0.3165391981601715, 0.30364271998405457, 0.290500670671463, 0.2853941023349762, 0.2870122492313385, 0.27989232540130615, 0.29545626044273376, 0.27653926610946655, 0.29067111015319824, 0.3135939836502075, 0.2777600586414337, 0.27107882499694824, 0.27340900897979736, 0.273814857006073, 0.2825483977794647, 0.26917213201522827, 0.2796177864074707, 0.2627805471420288, 0.26785168051719666, 0.26380395889282227, 0.2608514130115509, 0.2680051624774933, 0.25782307982444763, 0.29387137293815613, 0.25588053464889526, 0.25004342198371887, 0.25179004669189453, 0.258196085691452, 0.2478591650724411, 0.2765861749649048, 0.2536294162273407, 0.24715737998485565, 0.2427060753107071, 0.2577493488788605, 0.26993808150291443, 0.25555405020713806, 0.2522428333759308, 0.24945969879627228, 0.24284076690673828, 0.23951250314712524, 0.24282465875148773, 0.2496059685945511, 0.25706595182418823, 0.2472752183675766, 0.24243949353694916, 0.24257341027259827, 0.2408611923456192, 0.2571580111980438, 0.2396955043077469, 0.23424240946769714, 0.23226480185985565, 0.2488926500082016, 0.23951455950737, 0.22978465259075165, 0.24655118584632874, 0.23622426390647888, 0.23244866728782654, 0.23727622628211975, 0.25473642349243164, 0.23612579703330994, 0.281114786863327, 0.23615896701812744, 0.23153379559516907, 0.22583980858325958, 0.2292405217885971, 0.22300520539283752, 0.23093581199645996, 0.23639173805713654, 0.23083578050136566, 0.2296687364578247, 0.22963206470012665, 0.22207044064998627, 0.22209401428699493, 0.2175980806350708, 0.22689668834209442, 0.21633058786392212, 0.21996860206127167, 0.21606014668941498, 0.21705752611160278, 0.21644318103790283, 0.22765010595321655, 0.22392268478870392, 0.2357279509305954, 0.22388528287410736, 0.21556444466114044, 0.2165471613407135, 0.21274133026599884, 0.216589093208313, 0.2141115814447403, 0.21052873134613037, 0.20878468453884125, 0.22377946972846985, 0.23542459309101105, 0.22340701520442963, 0.21180890500545502, 0.20645296573638916, 0.20774337649345398, 0.20896989107131958, 0.20524969696998596, 0.20951704680919647], 'accuracy': [0.9571059346199036, 0.9607235193252563, 0.9679586291313171, 0.9705426096916199, 0.9677002429962158, 0.9720930457115173, 0.960465133190155, 0.9720930457115173, 0.9620155096054077, 0.9542635679244995, 0.97260981798172, 0.9749354124069214, 0.97364342212677, 0.97260981798172, 0.9697674512863159, 0.9731265902519226, 0.9697674512863159, 0.9772610068321228, 0.974418580532074, 0.9772610068321228, 0.9798449873924255, 0.974418580532074, 0.9780361652374268, 0.962273895740509, 0.9806201457977295, 0.9813953638076782, 0.9816537499427795, 0.9788113832473755, 0.9829457402229309, 0.9679586291313171, 0.9808785319328308, 0.9826873540878296, 0.9860464930534363, 0.9767441749572754, 0.9682170748710632, 0.9762274026870728, 0.9777777791023254, 0.9798449873924255, 0.9829457402229309, 0.9842377305030823, 0.9832041263580322, 0.9811369776725769, 0.9764857888221741, 0.9819121360778809, 0.9824289679527283, 0.9816537499427795, 0.9832041263580322, 0.9749354124069214, 0.9832041263580322, 0.985788106918335, 0.9870800971984863, 0.9790697693824768, 0.9842377305030823, 0.9873384833335876, 0.9785529971122742, 0.9834625124931335, 0.9863049387931824, 0.9806201457977295, 0.9728682041168213, 0.9832041263580322, 0.9630491137504578, 0.9829457402229309, 0.9860464930534363, 0.9881137013435364, 0.9860464930534363, 0.9909560680389404, 0.9852713346481323, 0.9821705222129822, 0.9852713346481323, 0.9844961166381836, 0.9834625124931335, 0.9899224638938904, 0.9878553152084351, 0.9896640777587891, 0.9863049387931824, 0.9912144541740417, 0.9883720874786377, 0.9909560680389404, 0.9906976819038391, 0.9917312860488892, 0.9808785319328308, 0.986821711063385, 0.9808785319328308, 0.985012948513031, 0.9891473054885864, 0.9904392957687378, 0.9919896721839905, 0.9894056916236877, 0.9912144541740417, 0.9901808500289917, 0.9919896721839905, 0.983979344367981, 0.9777777791023254, 0.9832041263580322, 0.9881137013435364, 0.9935400485992432, 0.9922480583190918, 0.9901808500289917, 0.9932816624641418, 0.9894056916236877], 'val_loss': [0.9687221050262451, 0.9379088282585144, 0.8881229758262634, 0.8981199264526367, 0.8668898940086365, 0.8407113552093506, 0.8534683585166931, 0.814102828502655, 0.9035406112670898, 0.819164514541626, 0.7555332779884338, 0.8293249011039734, 0.7873608469963074, 0.750554621219635, 0.5883777141571045, 0.5304409861564636, 0.5196341276168823, 0.5295323133468628, 0.5009059906005859, 0.4881947338581085, 0.480917364358902, 0.46921291947364807, 0.4712502956390381, 0.4667273163795471, 0.4467028081417084, 0.4683190882205963, 0.520719051361084, 0.4570774734020233, 0.5245414972305298, 0.4592297375202179, 0.4542476236820221, 0.45836472511291504, 0.4660891592502594, 0.6061146259307861, 0.4677020311355591, 0.4588366150856018, 0.46415555477142334, 0.4481342136859894, 0.46041175723075867, 0.4765852391719818, 0.47725388407707214, 0.46344733238220215, 0.47605037689208984, 0.46704724431037903, 0.4824572503566742, 0.4618377089500427, 0.5358609557151794, 0.49938416481018066, 0.4777749478816986, 0.4871041774749756, 0.4899477958679199, 0.4927678108215332, 0.4761919677257538, 0.4841894805431366, 0.47895708680152893, 0.5282866358757019, 0.5034176707267761, 0.5217158794403076, 0.5781192779541016, 0.4812730550765991, 0.5255383849143982, 0.4698105454444885, 0.471823126077652, 0.5091933012008667, 0.481850802898407, 0.5432722568511963, 0.5675815939903259, 0.49314260482788086, 0.4848393499851227, 0.5051836371421814, 0.5049534440040588, 0.4878688156604767, 0.47816580533981323, 0.48932504653930664, 0.4768703579902649, 0.4764275550842285, 0.5175657868385315, 0.5052466988563538, 0.5239278078079224, 0.5052695870399475, 0.4902081787586212, 0.5190763473510742, 0.49541175365448, 0.4965086281299591, 0.4811652898788452, 0.4970925450325012, 0.4976419508457184, 0.48983338475227356, 0.48685353994369507, 0.5035499334335327, 0.537699282169342, 0.7126712203025818, 0.484052836894989, 0.5117584466934204, 0.49107447266578674, 0.504878580570221, 0.5082927942276001, 0.5126368403434753, 0.49751901626586914, 0.5010342597961426], 'val_accuracy': [0.48553720116615295, 0.48553720116615295, 0.49070248007774353, 0.49896693229675293, 0.5175619721412659, 0.5340909361839294, 0.538223147392273, 0.5919421315193176, 0.547520637512207, 0.6095041036605835, 0.6528925895690918, 0.6291322112083435, 0.6559917330741882, 0.6807851195335388, 0.7995867729187012, 0.8584710955619812, 0.8646694421768188, 0.85537189245224, 0.8739669322967529, 0.8801652789115906, 0.8811983466148376, 0.8915289044380188, 0.89462810754776, 0.8977272510528564, 0.9028925895690918, 0.8977272510528564, 0.8904958963394165, 0.9008264541625977, 0.8925619721412659, 0.9111570119857788, 0.9070248007774353, 0.9059917330741882, 0.9039255976676941, 0.8739669322967529, 0.9049586653709412, 0.9028925895690918, 0.9018595218658447, 0.9059917330741882, 0.9008264541625977, 0.9049586653709412, 0.9070248007774353, 0.9039255976676941, 0.8997933864593506, 0.9049586653709412, 0.9008264541625977, 0.9008264541625977, 0.8966942429542542, 0.8987603187561035, 0.8987603187561035, 0.9070248007774353, 0.8997933864593506, 0.8997933864593506, 0.9059917330741882, 0.9039255976676941, 0.9008264541625977, 0.8935950398445129, 0.8977272510528564, 0.9039255976676941, 0.8863636255264282, 0.9080578684806824, 0.8956611752510071, 0.9028925895690918, 0.9028925895690918, 0.8977272510528564, 0.9039255976676941, 0.8966942429542542, 0.8904958963394165, 0.8997933864593506, 0.9028925895690918, 0.8997933864593506, 0.8997933864593506, 0.8966942429542542, 0.9049586653709412, 0.9039255976676941, 0.9018595218658447, 0.9008264541625977, 0.9008264541625977, 0.8987603187561035, 0.9018595218658447, 0.9028925895690918, 0.9018595218658447, 0.9080578684806824, 0.8997933864593506, 0.9028925895690918, 0.9008264541625977, 0.9018595218658447, 0.8997933864593506, 0.9018595218658447, 0.9039255976676941, 0.8997933864593506, 0.8966942429542542, 0.8564049601554871, 0.8987603187561035, 0.8987603187561035, 0.9028925895690918, 0.8977272510528564, 0.89462810754776, 0.9028925895690918, 0.9028925895690918, 0.8966942429542542]}\n","32/32 [==============================] - 1s 4ms/step\n"]}]},{"cell_type":"code","source":["metrics_df_gru"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"9L8FIkc61T0q","executionInfo":{"status":"ok","timestamp":1717400189488,"user_tz":-360,"elapsed":117,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"}},"outputId":"468c903c-c0b7-4b00-8db7-237499b2a92f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Client  Accuracy  Precision    Recall        F1  Sensitivity  Specificity  \\\n","0        0  0.746231   0.750000  0.738693  0.744304     0.738693     0.753769   \n","1        1  0.798729   0.782377  0.827684  0.804393     0.827684     0.769774   \n","2        2  0.730924   0.733740  0.724900  0.729293     0.724900     0.736948   \n","3        0  0.793132   0.802768  0.777219  0.789787     0.777219     0.809045   \n","4        1  0.812853   0.788036  0.855932  0.820582     0.855932     0.769774   \n","5        2  0.760040   0.733333  0.817269  0.773029     0.817269     0.702811   \n","6        0  0.807370   0.771852  0.872697  0.819182     0.872697     0.742044   \n","7        1  0.842514   0.822045  0.874294  0.847365     0.874294     0.810734   \n","8        2  0.817269   0.791513  0.861446  0.825000     0.861446     0.773092   \n","9        0  0.831658   0.800912  0.882747  0.839841     0.882747     0.780570   \n","10       1  0.862994   0.841755  0.894068  0.867123     0.894068     0.831921   \n","11       2  0.839357   0.792388  0.919679  0.851301     0.919679     0.759036   \n","12       0  0.865997   0.861157  0.872697  0.866889     0.872697     0.859296   \n","13       1  0.876412   0.844761  0.922316  0.881837     0.922316     0.830508   \n","14       2  0.868474   0.827094  0.931727  0.876298     0.931727     0.805221   \n","\n","       Kappa  \n","0   0.492462  \n","1   0.597458  \n","2   0.461847  \n","3   0.586265  \n","4   0.625706  \n","5   0.520080  \n","6   0.614740  \n","7   0.685028  \n","8   0.634538  \n","9   0.663317  \n","10  0.725989  \n","11  0.678715  \n","12  0.731993  \n","13  0.752825  \n","14  0.736948  "],"text/html":["\n","  <div id=\"df-32b767ce-e1cd-4e16-b2ff-cb0238e58a3a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Client</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Sensitivity</th>\n","      <th>Specificity</th>\n","      <th>Kappa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.746231</td>\n","      <td>0.750000</td>\n","      <td>0.738693</td>\n","      <td>0.744304</td>\n","      <td>0.738693</td>\n","      <td>0.753769</td>\n","      <td>0.492462</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.798729</td>\n","      <td>0.782377</td>\n","      <td>0.827684</td>\n","      <td>0.804393</td>\n","      <td>0.827684</td>\n","      <td>0.769774</td>\n","      <td>0.597458</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.730924</td>\n","      <td>0.733740</td>\n","      <td>0.724900</td>\n","      <td>0.729293</td>\n","      <td>0.724900</td>\n","      <td>0.736948</td>\n","      <td>0.461847</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.793132</td>\n","      <td>0.802768</td>\n","      <td>0.777219</td>\n","      <td>0.789787</td>\n","      <td>0.777219</td>\n","      <td>0.809045</td>\n","      <td>0.586265</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0.812853</td>\n","      <td>0.788036</td>\n","      <td>0.855932</td>\n","      <td>0.820582</td>\n","      <td>0.855932</td>\n","      <td>0.769774</td>\n","      <td>0.625706</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2</td>\n","      <td>0.760040</td>\n","      <td>0.733333</td>\n","      <td>0.817269</td>\n","      <td>0.773029</td>\n","      <td>0.817269</td>\n","      <td>0.702811</td>\n","      <td>0.520080</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0.807370</td>\n","      <td>0.771852</td>\n","      <td>0.872697</td>\n","      <td>0.819182</td>\n","      <td>0.872697</td>\n","      <td>0.742044</td>\n","      <td>0.614740</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>0.842514</td>\n","      <td>0.822045</td>\n","      <td>0.874294</td>\n","      <td>0.847365</td>\n","      <td>0.874294</td>\n","      <td>0.810734</td>\n","      <td>0.685028</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2</td>\n","      <td>0.817269</td>\n","      <td>0.791513</td>\n","      <td>0.861446</td>\n","      <td>0.825000</td>\n","      <td>0.861446</td>\n","      <td>0.773092</td>\n","      <td>0.634538</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>0.831658</td>\n","      <td>0.800912</td>\n","      <td>0.882747</td>\n","      <td>0.839841</td>\n","      <td>0.882747</td>\n","      <td>0.780570</td>\n","      <td>0.663317</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>0.862994</td>\n","      <td>0.841755</td>\n","      <td>0.894068</td>\n","      <td>0.867123</td>\n","      <td>0.894068</td>\n","      <td>0.831921</td>\n","      <td>0.725989</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2</td>\n","      <td>0.839357</td>\n","      <td>0.792388</td>\n","      <td>0.919679</td>\n","      <td>0.851301</td>\n","      <td>0.919679</td>\n","      <td>0.759036</td>\n","      <td>0.678715</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>0.865997</td>\n","      <td>0.861157</td>\n","      <td>0.872697</td>\n","      <td>0.866889</td>\n","      <td>0.872697</td>\n","      <td>0.859296</td>\n","      <td>0.731993</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>0.876412</td>\n","      <td>0.844761</td>\n","      <td>0.922316</td>\n","      <td>0.881837</td>\n","      <td>0.922316</td>\n","      <td>0.830508</td>\n","      <td>0.752825</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2</td>\n","      <td>0.868474</td>\n","      <td>0.827094</td>\n","      <td>0.931727</td>\n","      <td>0.876298</td>\n","      <td>0.931727</td>\n","      <td>0.805221</td>\n","      <td>0.736948</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32b767ce-e1cd-4e16-b2ff-cb0238e58a3a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-32b767ce-e1cd-4e16-b2ff-cb0238e58a3a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-32b767ce-e1cd-4e16-b2ff-cb0238e58a3a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-ee568450-8836-4073-bfa0-682d8d2e07e3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee568450-8836-4073-bfa0-682d8d2e07e3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-ee568450-8836-4073-bfa0-682d8d2e07e3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_f18c34e2-e854-4120-8992-6b972a69db2b\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df_gru')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_f18c34e2-e854-4120-8992-6b972a69db2b button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('metrics_df_gru');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"metrics_df_gru","summary":"{\n  \"name\": \"metrics_df_gru\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"Client\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.045305672987340805,\n        \"min\": 0.7309236947791165,\n        \"max\": 0.876412429378531,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.8316582914572864,\n          0.8393574297188755,\n          0.7462311557788944\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03892812716738194,\n        \"min\": 0.7333333333333333,\n        \"max\": 0.8611570247933884,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.8009118541033434,\n          0.7923875432525952,\n          0.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06349031714271815,\n        \"min\": 0.7248995983935743,\n        \"max\": 0.9317269076305221,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.8827470686767169,\n          0.9196787148594378,\n          0.7386934673366834\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04689921428260388,\n        \"min\": 0.7292929292929293,\n        \"max\": 0.8818365968939905,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.8398406374501992,\n          0.8513011152416358,\n          0.7443037974683544\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sensitivity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06349031714271815,\n        \"min\": 0.7248995983935743,\n        \"max\": 0.9317269076305221,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.8827470686767169,\n          0.9196787148594378,\n          0.7386934673366834\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Specificity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.041847186606041184,\n        \"min\": 0.7028112449799196,\n        \"max\": 0.8592964824120602,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.8319209039548022,\n          0.8592964824120602,\n          0.7537688442211056\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kappa\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09061134597468161,\n        \"min\": 0.4618473895582329,\n        \"max\": 0.7528248587570622,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.6633165829145728,\n          0.678714859437751,\n          0.49246231155778897\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["metrics_df_gru.round(4).to_csv('/content/drive/MyDrive/EEG Signal /results/FLresults/Time Domain/CNN_GRU/time_CNN.csv', index = False)"],"metadata":{"id":"HmSxbjlU0LlC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YbK5vsE62IBZ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}