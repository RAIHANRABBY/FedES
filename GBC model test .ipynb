{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rBb4Qge7Jpji"},"outputs":[],"source":["%%capture\n","pip install mne"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mECGGdpyHt5E"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import mne\n","import pywt"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-B-iq3r7ixes"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9gslxn4FEE2V"},"outputs":[],"source":["%%capture\n","# folder = 'E:/Data WareHouse/1sec/Epileptic Seizure/equalized'\n","folder = '/content/drive/MyDrive/EEG Signal /Epileptic seizure/data/equalized epoch'\n","epochs_path = [os.path.join(folder,i) for i in os.listdir(folder) if i[-3:]=='fif']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EPV0WFNKzcL"},"outputs":[],"source":["%%capture\n","# folder = 'E:/Data WareHouse/1sec/Epileptic Seizure/equalized'\n","# folder = '/content/drive/MyDrive/EEG Signal /Epileptic seizure/data/equalized epoch'\n","# epochs_path = [os.path.join(folder,i) for i in os.listdir(folder) if i[-3:]=='fif']\n","\n","data= [mne.read_epochs(i).pick_types(eeg=True) for i in epochs_path]\n","labels = [mne.read_epochs(i).events[:,2] for i in epochs_path]\n","group = [[i]*len(j) for i,j in enumerate(data)]\n","X=np.vstack(data)\n","Y=np.hstack(labels)\n","group= np.hstack(group)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQJA9euEEn6S"},"outputs":[],"source":["print(X.shape,Y.shape,group.shape)"]},{"cell_type":"markdown","metadata":{"id":"Li9z4hcFz53K"},"source":["# Feature Extraction methods"]},{"cell_type":"markdown","metadata":{"id":"EcluaJK1z9hw"},"source":["##Time-domain Features\n","\n","1. Mean (mean_val): The average value of the signal amplitude, representing the signal's central tendency.\n","\n","2. Median (median_val): The middle value of the signal amplitude when sorted, representing the central tendency of the signal, less sensitive to outliers than the mean.\n","\n","3. Variance (var_val): Measures the spread of the signal's amplitude.\n","\n","4. Standard Deviation (std_dev): The square root of the variance, representing the dispersion of the signal's amplitude.\n","\n","5. Skewness (skewness): Measures the asymmetry of the signal's amplitude distribution around the mean.\n","\n","6. Kurtosis (kurt): Measures the 'tailedness' of the signal's amplitude distribution, indicating the presence of outliers.\n","\n","7. Zero Crossing Rate (zcr): The rate at which the signal changes sign, indicating frequency content.\n","\n","8. Root Mean Square (rms_val): Represents the square root of the average of the squares of the signal, indicative of the signal's magnitude.\n","\n","9. Signal Energy (energy): The sum of the squares of the signal values, indicative of the signal's power.\n","\n","10. Crest Factor (crest_fact): The ratio of the peak amplitude of the waveform to the RMS value, indicating the extremeness of peaks.\n","\n","11. Shape Factor (shape_fact): The ratio of the RMS value to the mean absolute value, indicative of the waveform shape.\n","\n","12. Entropy (signal_entropy): Measures the unpredictability or complexity of the signal.\n","\n","13. Peak Amplitude (peak_amp): The difference between the maximum and minimum amplitude, indicating the signal's range.\n","\n","14. Number of Peaks (num_peaks): The count of local maxima, indicating the frequency of oscillations.\n","\n","15. Average Peak-to-Peak Distance (peak_to_peak_distance): The average distance between consecutive peaks, related to the periodicity of the signal.\n","\n","16. Hjorth Parameters:\n","\n","  Activity: Indicates the signal power.\n","\n","  Mobility: Indicates the mean frequency or the proportion of the standard deviation of the power spectrum.\n","  \n","  Complexity: Indicates the bandwidth of the signal or the change in frequency.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbNE0IwEJbPg"},"outputs":[],"source":["# import numpy as np\n","# from scipy.stats import skew, kurtosis\n","# from scipy.signal import find_peaks\n","\n","# def zero_crossing_rate(signal):\n","#     zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n","#     return len(zero_crossings) / len(signal)\n","\n","# def hjorth_parameters(signal):\n","#     diff_input = np.diff(signal)\n","#     diff_diff_input = np.diff(diff_input)\n","\n","#     activity = np.var(signal)\n","#     mobility = np.sqrt(np.var(diff_input)/activity)\n","#     complexity = np.sqrt(np.var(diff_diff_input)/np.var(diff_input)) / mobility\n","\n","#     return activity, mobility, complexity\n","\n","\n","\n","\n","# def extract_time_domain_features(epochs):\n","#     features = []\n","\n","#     for epoch in epochs:\n","#         epoch_features = []\n","\n","#         for channel_data in epoch:\n","#             # Flatten the channel data\n","#             flattened_data = channel_data.flatten()\n","\n","#             # Basic Time-Domain Features\n","#             mean_val = np.mean(flattened_data)\n","#             median_val = np.median(flattened_data)\n","#             var_val = np.var(flattened_data)\n","#             std_dev = np.std(flattened_data)\n","#             skewness = skew(flattened_data)\n","#             kurt = kurtosis(flattened_data)\n","#             zcr = zero_crossing_rate(flattened_data)\n","#             peak_amp = np.ptp(flattened_data)\n","\n","#             # Hjorth Parameters\n","#             activity, mobility, complexity = hjorth_parameters(flattened_data)\n","\n","#             # Additional Features\n","#             num_waves = len(find_peaks(flattened_data)[0])\n","#             wave_duration = len(flattened_data) / num_waves if num_waves > 0 else 0\n","\n","#             channel_features = [\n","#                 mean_val, median_val, var_val, std_dev, skewness, kurt, zcr, num_waves,\n","#                 wave_duration, peak_amp, activity, mobility, complexity\n","#             ]\n","#             epoch_features.append(channel_features)\n","\n","#         features.append(epoch_features)\n","\n","#     return np.array(features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QMPhbvu0vAL"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import skew, kurtosis, entropy\n","from scipy.signal import find_peaks\n","\n","\n","def hjorth_parameters(signal):\n","    diff_input = np.diff(signal)\n","    diff_diff_input = np.diff(diff_input)\n","\n","    activity = np.var(signal)\n","    mobility = np.sqrt(np.var(diff_input)/activity)\n","    complexity = np.sqrt(np.var(diff_diff_input)/np.var(diff_input)) / mobility\n","\n","    return activity, mobility, complexity\n","\n","# def zero_crossing_rate(signal):\n","#     # Enhanced ZCR to handle noise\n","#     # Setting a threshold (eps) to consider as zero (to avoid detecting false crossings due to noise)\n","#     eps = 0.01 * np.std(signal)\n","#     zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n","#     close_to_zero = np.where(np.abs(signal) < eps)[0]\n","#     return (len(zero_crossings) + len(close_to_zero)) / len(signal)\n","\n","def zero_crossing_rate(signal):\n","    zero_crossings = np.where(np.diff(np.sign(signal)))[0]\n","    return len(zero_crossings) / len(signal)\n","\n","def safe_divide(numerator, denominator, default=0.0):\n","    \"\"\"Safely divide two numbers, returning a default value if the denominator is zero.\"\"\"\n","    if denominator == 0:\n","        return default\n","    else:\n","        return numerator / denominator\n","\n","def signal_entropy(signal):\n","    # Calculate histogram of the signal\n","    hist, bin_edges = np.histogram(signal, bins='auto', density=True)\n","    # Ensure non-zero histogram values by adding a small constant\n","    hist += np.finfo(float).eps\n","    # Normalize the histogram to get a probability distribution\n","    prob_dist = hist / hist.sum()\n","    # Calculate the entropy\n","    return entropy(prob_dist)\n","\n","def rms(signal):\n","    return np.sqrt(np.mean(signal**2))\n","\n","def signal_energy(signal):\n","    return np.sum(signal**2)\n","\n","\n","\n","def crest_factor(signal, rms_val=None):\n","    if rms_val is None:\n","        rms_val = rms(signal)\n","    return np.max(np.abs(signal)) / rms_val\n","\n","def shape_factor(signal, rms_val=None):\n","    if rms_val is None:\n","        rms_val = rms(signal)\n","    mean_abs = np.mean(np.abs(signal))\n","    return rms_val / mean_abs if mean_abs != 0 else 0\n","\n","def extract_time_domain_features(epochs):\n","    features = []\n","\n","    for epoch in epochs:\n","        epoch_features = []\n","\n","        for channel_data in epoch:\n","            # Flatten the channel data if needed\n","            flattened_data = np.ravel(channel_data)\n","\n","            # Basic Time-Domain Features\n","            mean_val = np.mean(flattened_data)\n","            median_val = np.median(flattened_data)\n","            var_val = np.var(flattened_data)\n","            std_dev = np.std(flattened_data)\n","            skewness = skew(flattened_data)\n","            kurt = kurtosis(flattened_data)\n","            zcr = zero_crossing_rate(flattened_data)\n","            rms_val = rms(flattened_data)\n","            energy = signal_energy(flattened_data)\n","            crest_fact = crest_factor(flattened_data, rms_val)\n","            shape_fact = shape_factor(flattened_data, rms_val)\n","            signal_entropy_val = signal_entropy(flattened_data)  # Use the defined function\n","            peak_amp = np.ptp(flattened_data)\n","\n","            # Hjorth Parameters\n","            activity, mobility, complexity = hjorth_parameters(flattened_data)\n","\n","            # Additional Features\n","            num_peaks = len(find_peaks(flattened_data)[0])\n","            peak_to_peak_distance = np.mean(np.diff(find_peaks(flattened_data)[0])) if num_peaks > 1 else 0\n","\n","            channel_features = [\n","                mean_val, median_val, var_val, std_dev, skewness, kurt, zcr, rms_val, energy,\n","                 crest_fact, shape_fact, signal_entropy_val,\n","                peak_amp, num_peaks, peak_to_peak_distance,\n","                activity, mobility, complexity\n","            ]\n","            epoch_features.append(channel_features)\n","\n","        features.append(epoch_features)\n","\n","    return np.array(features)"]},{"cell_type":"markdown","metadata":{"id":"usMNds7p2CQY"},"source":["## Frequency Domain Features\n","\n","\n","1. Power Spectral Density (PSD) Calculation:\n","\n","Utilizes the welch method to compute the power spectral density of the signal, which forms the basis for many of the following features.\n","\n","2. Basic Statistical Features from PSD:\n","\n","  Mean (mean_val): The average power in the spectrum.\n","\n","*   Median (median_val): The middle value of the power spectrum.\n","*   Variance (var_val): The variance of the power spectrum.\n","*   Standard Deviation (std_dev): The standard deviation of the power spectrum.\n","*   Standard Deviation (std_dev): The standard deviation of the power spectrum.\n","* Skewness (skewness): A measure of the asymmetry of the power spectrum.\n","* Kurtosis (kurt): A measure of the tailedness of the power spectrum.\n","\n",".\n","\n","3. Wavelet Coefficients:\n","\n","* Wavelet Coefficients (wave_coeffs): Computed using the Discrete Wavelet\n","* Transform (DWT) to provide a multi-resolution analysis of the signal.\n","\n","* Mean of Wavelet Coefficients (wave_coeffs_mean): The mean of the wavelet coefficients, providing a summary of the wavelet-transformed data.\n","\n","4. Band Power Features:\n","\n","Computed by summing the PSD within specified frequency bands. The bands considered are delta, theta, alpha, beta, gamma, and sigma.\n","\n","\n","5. Band Power Ratios:\n","\n","* Theta/Alpha Ratio (theta_alpha_ratio): The ratio of power in the theta band to the power in the alpha band.\n","\n","* Beta/Alpha Ratio (beta_alpha_ratio): The ratio of power in the beta band to the power in the alpha band.\n","\n","* (Theta + Alpha)/Beta Ratio (theta_alpha_beta_ratio): The ratio of the sum of power in the theta and alpha bands to the power in the beta band.\n","\n","* Theta/Beta Ratio (theta_beta_ratio): The ratio of power in the theta band to the power in the beta band.\n","\n","* (Theta + Alpha)/(Alpha + Beta) Ratio (theta_alpha_beta_alpha_ratio): The ratio of the sum of power in the theta and alpha bands to the sum of power in the alpha and beta bands.\n","\n","* Gamma/Delta Ratio (gamma_delta_ratio): The ratio of power in the gamma band to the power in the delta band.\n","\n","* (Gamma + Beta)/(Delta + Alpha) Ratio (gamma_beta_delta_alpha_ratio): The ratio of the sum of power in the gamma and beta bands to the sum of power in the delta and alpha bands.\n","\n","6. Additional Frequency Domain Features:\n","\n","* Spectral Entropy (spectral_entropy_val): Measures the entropy of the power spectrum, providing an index of the complexity or disorder in the frequency domain.\n","\n","* Spectral Edge Frequency (spectral_edge_freq): The frequency below which a certain percentage (e.g., 95%) of the power of the signal is contained, providing a cutoff frequency that encloses most of the signal's power."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AE_0hytQJbH0"},"outputs":[],"source":["# import numpy as np\n","# from scipy.stats import skew, kurtosis\n","# from scipy.signal import welch\n","\n","\n","\n","# def get_wavelet_coeffs(channel_data, wavelet='db4', level=5):\n","#     coeffs = pywt.wavedec(channel_data, wavelet, level=level)\n","#     return coeffs\n","\n","\n","# def extract_frequency_domain_features(epochs, sfreq,wavelet='db4', bands={'delta': (1.59, 4), 'theta': (4, 8), 'alpha': (8, 12), 'beta': (12, 30), 'gamma': (30, 100), 'sigma': (11, 16)}):\n","#     features = []\n","\n","#     for epoch in epochs:\n","#         epoch_features = []\n","\n","#         for channel_data in epoch:\n","#             # Compute the Power Spectral Density (PSD)\n","#             freqs, psd = welch(channel_data, sfreq, nperseg=256)\n","\n","#             # Frequency domain features\n","#             mean_val = np.mean(psd)\n","#             median_val = np.median(psd)\n","#             var_val = np.var(psd)\n","#             std_dev = np.std(psd)\n","#             skewness = skew(psd)\n","#             kurt = kurtosis(psd)\n","\n","#             # Compute wavelet coefficients\n","#             wave_coeffs = get_wavelet_coeffs(channel_data, wavelet, level=5)\n","#             wave_coeffs_mean = np.mean(wave_coeffs[0])\n","\n","#             # Band Power Features\n","#             band_powers = {}\n","#             for band, freq_range in bands.items():\n","#                 freq_mask = (freqs >= freq_range[0]) & (freqs <= freq_range[1])\n","#                 band_power = np.sum(psd[freq_mask])\n","#                 band_powers[band] = band_power\n","\n","#             # Band Power Ratios\n","#             theta_alpha_ratio = band_powers['theta'] / band_powers['alpha']\n","#             beta_alpha_ratio = band_powers['beta'] / band_powers['alpha']\n","#             theta_alpha_beta_ratio = (band_powers['theta'] + band_powers['alpha']) / band_powers['beta']\n","\n","#             # Additional Band Power Ratios\n","#             theta_beta_ratio = band_powers['theta'] / band_powers['beta']\n","#             theta_alpha_beta_alpha_ratio = (band_powers['theta'] + band_powers['alpha']) / (band_powers['alpha'] + band_powers['beta'])\n","#             gamma_delta_ratio = band_powers['gamma'] / band_powers['delta']\n","#             gamma_beta_delta_alpha_ratio = (band_powers['gamma'] + band_powers['beta']) / (band_powers['delta'] + band_powers['alpha'])\n","\n","\n","#             channel_features = [\n","#                 mean_val, median_val, var_val, std_dev, skewness, kurt,\n","#                 band_powers['delta'], band_powers['theta'], band_powers['alpha'],\n","#                 band_powers['beta'], band_powers['gamma'], band_powers['sigma'],\n","#                 theta_alpha_ratio, beta_alpha_ratio, theta_alpha_beta_ratio,theta_beta_ratio,\n","#                 theta_alpha_beta_alpha_ratio, gamma_delta_ratio, gamma_beta_delta_alpha_ratio,\n","#                 wave_coeffs_mean\n","#             ]\n","#             epoch_features.append(channel_features)\n","\n","#         features.append(epoch_features)\n","\n","#     return np.array(features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bOdfEbDDLuCO"},"outputs":[],"source":["import numpy as np\n","import pywt\n","from scipy.stats import skew, kurtosis, entropy\n","from scipy.signal import welch\n","\n","def get_wavelet_coeffs(channel_data, wavelet='db4', level=5):\n","    coeffs = pywt.wavedec(channel_data, wavelet, level=level)\n","    return coeffs\n","\n","\n","def safe_divide(numerator, denominator, default_value=0.0):\n","    \"\"\"Safely divide two numbers, returning a default value if the denominator is zero.\"\"\"\n","    if denominator == 0:\n","        return default_value\n","    else:\n","        return numerator / denominator\n","\n","\n","\n","def spectral_entropy(psd, sfreq):\n","    # Adding a small constant to avoid division by zero when normalizing\n","    psd_sum = np.sum(psd) + np.finfo(float).eps\n","    psd_norm = psd / psd_sum\n","    # Normalizing PSD to a probability distribution\n","    psd_norm = psd_norm / np.sum(psd_norm)\n","    # Ensuring the normalized PSD values are positive\n","    psd_norm[psd_norm <= 0] = np.finfo(float).eps\n","    return entropy(psd_norm)\n","\n","\n","def spectral_edge_frequency(freqs, psd, edge_percent=0.95):\n","    psd_sum = np.sum(psd) + np.finfo(float).eps  # Add a small constant to ensure non-zero denominator\n","    psd_cumsum = np.cumsum(psd) / psd_sum\n","    idx = np.where(psd_cumsum <= edge_percent)[0][-1] if len(psd_cumsum) > 0 else 0\n","    spectral_edge_freq = freqs[idx] if idx < len(freqs) else 0\n","    return spectral_edge_freq\n","\n","def extract_frequency_domain_features(epochs, sfreq, wavelet='db4', bands={'delta': (1.59, 4), 'theta': (4, 8), 'alpha': (8, 12), 'beta': (12, 30), 'sigma': (11, 16)}):\n","    features = []\n","\n","    for epoch in epochs:\n","        epoch_features = []\n","\n","        for channel_data in epoch:\n","            # Compute the Power Spectral Density (PSD)\n","            freqs, psd = welch(channel_data, sfreq, nperseg=256)\n","\n","            # Frequency domain features\n","            mean_val = np.mean(psd)\n","            median_val = np.median(psd)\n","            var_val = np.var(psd)\n","            std_dev = np.std(psd)\n","            skewness = skew(psd)\n","            kurt = kurtosis(psd)\n","\n","            # Compute wavelet coefficients\n","            wave_coeffs = get_wavelet_coeffs(channel_data, wavelet, level=5)\n","            wave_coeffs_mean = np.mean(wave_coeffs[0])\n","\n","            # Band Power Features\n","            band_powers = {}\n","            for band, freq_range in bands.items():\n","                freq_mask = (freqs >= freq_range[0]) & (freqs <= freq_range[1])\n","                band_power = np.sum(psd[freq_mask])\n","                band_powers[band] = band_power\n","\n","            # Band Power Ratios\n","            # Band Power Ratios with safe division\n","            theta_alpha_ratio = safe_divide(band_powers['theta'], band_powers['alpha'])\n","            beta_alpha_ratio = safe_divide(band_powers['beta'], band_powers['alpha'])\n","            theta_alpha_beta_ratio = safe_divide(band_powers['theta'] + band_powers['alpha'], band_powers['beta'])\n","            theta_beta_ratio = safe_divide(band_powers['theta'], band_powers['beta'])\n","            theta_alpha_beta_alpha_ratio = safe_divide(band_powers['theta'] + band_powers['alpha'], band_powers['alpha'] + band_powers['beta'])\n","\n","            # gamma_delta_ratio = band_powers['gamma'] / band_powers['delta']\n","            # gamma_beta_delta_alpha_ratio = (band_powers['gamma'] + band_powers['beta']) / (band_powers['delta'] + band_powers['alpha'])\n","\n","            # Additional Features\n","            spectral_entropy_val = spectral_entropy(psd, sfreq)\n","            spectral_edge_freq = spectral_edge_frequency(freqs, psd)\n","\n","            channel_features = [\n","                mean_val, median_val, var_val, std_dev, skewness, kurt,\n","                band_powers['delta'], band_powers['theta'], band_powers['alpha'],\n","                band_powers['beta'], band_powers['sigma'],\n","                theta_alpha_ratio, beta_alpha_ratio, theta_alpha_beta_ratio, theta_beta_ratio,\n","                theta_alpha_beta_alpha_ratio,\n","                wave_coeffs_mean, spectral_entropy_val, spectral_edge_freq\n","            ]\n","            epoch_features.append(channel_features)\n","\n","        features.append(epoch_features)\n","\n","    return np.array(features)"]},{"cell_type":"markdown","metadata":{"id":"Wr0D9sMbcn_j"},"source":["##Time-Frequency Domain"]},{"cell_type":"markdown","metadata":{"id":"NU9ozIPchKzy"},"source":["###Short-Time Fourier Transform (STFT) Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Q5qQUz0cudQ"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.signal import stft\n","\n","def extract_stft_features(epochs, fs, nperseg=256):\n","    stft_features = []\n","\n","    for epoch in epochs:\n","        epoch_features = []\n","\n","        for channel in epoch:\n","            f, t, Zxx = stft(channel, fs=fs, nperseg=nperseg)\n","            magnitude = np.abs(Zxx)\n","            power = np.square(magnitude)\n","\n","            # Basic STFT derived features\n","            mean_power = np.mean(power, axis=0)\n","            max_power = np.max(power, axis=0)\n","            min_power = np.min(power, axis=0)\n","            std_power = np.std(power, axis=0)\n","\n","            # Concatenate features\n","            aggregated_features = np.concatenate((mean_power, max_power, min_power, std_power), axis=0)\n","            epoch_features.append(aggregated_features)\n","\n","        stft_features.append(epoch_features)\n","\n","    return np.array(stft_features)"]},{"cell_type":"markdown","metadata":{"id":"B0VY2cdEhTUn"},"source":["###Continuous Wavelet Transform (CWT) Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tygY6FxKcuaj"},"outputs":[],"source":["import numpy as np\n","import pywt\n","\n","def extract_cwt_features(epochs, scales, wavelet_name='morl'):\n","    cwt_features = []\n","\n","    for epoch in epochs:\n","        epoch_features = []\n","\n","        for channel in epoch:\n","            coef, freqs = pywt.cwt(channel, scales, wavelet_name)\n","            magnitude = np.abs(coef)\n","            power = np.square(magnitude)\n","\n","            # Basic CWT derived features\n","            mean_power = np.mean(power, axis=0)\n","            max_power = np.max(power, axis=0)\n","            min_power = np.min(power, axis=0)\n","            std_power = np.std(power, axis=0)\n","\n","            # Concatenate features\n","            aggregated_features = np.concatenate((mean_power, max_power, min_power, std_power), axis=0)\n","            epoch_features.append(aggregated_features)\n","\n","        cwt_features.append(epoch_features)\n","\n","    return np.array(cwt_features)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbMr5DdLDKf4"},"outputs":[],"source":["import numpy as np\n","import pywt\n","\n","def apply_cwt_to_epoch(epoch_data, scales, waveletname):\n","    \"\"\"\n","    Apply Continuous Wavelet Transform (CWT) to each epoch of EEG data.\n","\n","    Parameters:\n","    - epoch_data: numpy array of shape (num_epochs, num_channels, epoch_length)\n","                  containing EEG data, where:\n","                  - num_epochs: number of epochs\n","                  - num_channels: number of EEG channels\n","                  - epoch_length: length of each epoch\n","    - scales: list or array of scales to use for CWT\n","    - waveletname: name of the wavelet to use for CWT\n","\n","    Returns:\n","    - cwt_coefficients: numpy array of shape (num_epochs, num_scales, num_channels, epoch_length)\n","                        containing CWT coefficients for each epoch, where:\n","                        - num_scales: number of scales used for CWT\n","    \"\"\"\n","    num_epochs, num_channels, epoch_length = epoch_data.shape\n","    num_scales = len(scales)\n","\n","    cwt_coefficients = np.zeros((num_epochs, num_scales, num_channels, epoch_length))\n","\n","    for epoch_idx in range(num_epochs):\n","        for channel_idx in range(num_channels):\n","            signal = epoch_data[epoch_idx, channel_idx, :]\n","            coeff, _ = pywt.cwt(signal, scales, waveletname, 1)\n","            cwt_coefficients[epoch_idx, :, channel_idx, :] = coeff[:, :epoch_length]\n","\n","    return cwt_coefficients\n","\n","# Example usage:\n","# Define your scales and waveletname\n","scales = range(1, 128)\n","waveletname = 'morl'\n","\n","# # Example EEG epoch data (replace this with your actual epoch data)\n","# epoch_data = np.random.rand(10, 29, 180)  # 10 epochs, 29 channels, 180 time points per epoch\n","\n","# # Apply CWT to each epoch\n","# cwt_coefficients = apply_cwt_to_epoch(epoch_data, scales, waveletname)\n","\n","# # Shape of the resulting CWT coefficients\n","# print(\"Shape of CWT coefficients:\", cwt_coefficients.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfO6m3VPEkBQ"},"outputs":[],"source":["CWT_data = apply_cwt_to_epoch(X,scales,waveletname)"]},{"cell_type":"markdown","metadata":{"id":"7cDwWkkUIxj1"},"source":["##DWT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WhqhwdEtIzuz"},"outputs":[],"source":["import numpy as np\n","import pywt\n","import scipy.stats\n","\n","def calculate_entropy(list_values):\n","    values, counts = np.unique(list_values, return_counts=True)\n","    probabilities = counts / counts.sum()\n","    entropy = scipy.stats.entropy(probabilities)\n","    return entropy\n","\n","def calculate_statistics(list_values):\n","    n5 = np.nanpercentile(list_values, 5)\n","    n25 = np.nanpercentile(list_values, 25)\n","    n75 = np.nanpercentile(list_values, 75)\n","    n95 = np.nanpercentile(list_values, 95)\n","    median = np.nanpercentile(list_values, 50)\n","    mean = np.nanmean(list_values)\n","    std = np.nanstd(list_values)\n","    var = np.nanvar(list_values)\n","    rms = np.nanmean(np.sqrt(list_values**2))\n","    return [n5, n25, n75, n95, median, mean, std, var, rms]\n","\n","def calculate_crossings(list_values):\n","    zero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n","    no_zero_crossings = len(zero_crossing_indices)\n","    mean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n","    no_mean_crossings = len(mean_crossing_indices)\n","    return [no_zero_crossings, no_mean_crossings]\n","\n","def get_dwt_features(channel_data, waveletname='db4'):\n","    list_coeff = pywt.wavedec(channel_data, waveletname)\n","    features = []\n","    for coeff in list_coeff:\n","        features += calculate_statistics(coeff)\n","        features.append(calculate_entropy(coeff))\n","    crossings = calculate_crossings(channel_data)\n","    features += crossings\n","    return features\n","\n","def extract_features_from_epochs(epochs_data, waveletname='db4'):\n","    num_epochs, num_channels, _ = epochs_data.shape\n","    # Initialize an empty list to hold the feature array for each epoch\n","    all_features = []\n","    # Iterate through each epoch\n","    for epoch_idx in range(num_epochs):\n","        # Initialize an empty list to hold the feature array for each channel in the current epoch\n","        epoch_features = []\n","        for channel_idx in range(num_channels):\n","            channel_data = epochs_data[epoch_idx, channel_idx, :]\n","            channel_features = get_dwt_features(channel_data, waveletname)\n","            epoch_features.append(channel_features)\n","        all_features.append(epoch_features)\n","    # Convert list to numpy array for easier handling\n","    features_array = np.array(all_features, dtype=object)\n","    return features_array\n","# Example usage\n","# Assuming your EEG data is in 'eeg_data' with shape (num_epochs, num_channels, epoch_length)\n","# e.g., eeg_data.shape could be (5834, 29, 180) for 5834 epochs, 29 channels, and 180 samples per epoch\n"," # Example data, replace with your actual EEG data\n","\n","# Extract DWT features\n","DWT_data = extract_features_from_epochs(X)\n","\n","# Now 'eeg_features' contains the features extracted from each epoch which can be used for classification\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1707497222762,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"nMM5jb_yIzrL","outputId":"fde24ae8-5657-46f1-c195-8d42cc41d205"},"outputs":[{"data":{"text/plain":["(5834, 29, 52)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["DWT_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J9UkBQWPxIb5"},"outputs":[],"source":["np.savez('/content/drive/MyDrive/EEG Signal /Epileptic seizure/data/DWT/DWT.npz', X=DWT_data, Y= Y, G=group)"]},{"cell_type":"markdown","metadata":{"id":"sRBLguql2mTN"},"source":["##Non-Linear"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18fXw_s62rP0"},"outputs":[],"source":["pip install hurst"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jk7xTxze2rMa"},"outputs":[],"source":["import numpy as np\n","from hurst import compute_Hc\n","\n","def calculate_hurst_exponent(time_series):\n","    # Calculate the Hurst exponent\n","    H, c, data = compute_Hc(time_series, kind='price', simplified=True)\n","    return H\n","\n","# Sample usage:\n","ts = np.random.rand(100)  # Replace with your actual time series data\n","hurst_exponent = calculate_hurst_exponent(ts)\n","print(f\"Hurst exponent: {hurst_exponent}\")\n","\n","\n","import numpy as np\n","\n","def renyi_scaling_exponent(time_series, q, scale_min, scale_max, scale_step):\n","    if q == 1:\n","        raise ValueError(\"q=1 not allowed (leads to division by zero)\")\n","\n","    segment_sizes = range(scale_min, scale_max, scale_step)\n","    moments = []\n","\n","    for size in segment_sizes:\n","        segments = [time_series[i:i + size] for i in range(0, len(time_series), size)]\n","        segment_moments = []\n","        for segment in segments:\n","            probability_distribution = np.histogram(segment, bins=100, density=True)[0]\n","            moment = np.sum(probability_distribution**q)\n","            segment_moments.append(moment)\n","        moments.append(np.mean(segment_moments))\n","\n","    # Fit a line to the log-log plot and return the slope (scaling exponent)\n","    logs = np.log(segment_sizes)\n","    log_moments = np.log(moments)\n","    polyfit = np.polyfit(logs, log_moments, 1)\n","    return polyfit[0]  # The slope of the line\n","\n","# Sample usage:\n","ts = np.random.rand(1000)  # Replace with your actual time series data\n","q = 2  # Replace with the desired moment\n","renyi_exponent = renyi_scaling_exponent(ts, q, scale_min=10, scale_max=500, scale_step=10)\n","print(f\"Rényi scaling exponent for q={q}: {renyi_exponent}\")\n","\n","\n","\n","import numpy as np\n","\n","def renyi_generalized_dimension(time_series, q, scale_min, scale_max, scale_step):\n","    if q == 1:\n","        # Special case for q=1, using Shannon entropy\n","        # ... (implementation for q=1, involving entropy)\n","        pass\n","    else:\n","        segment_sizes = range(scale_min, scale_max, scale_step)\n","        probs = []\n","\n","        for size in segment_sizes:\n","            segments = [time_series[i:i + size] for i in range(0, len(time_series), size)]\n","            segment_probs = []\n","            for segment in segments:\n","                probability_distribution = np.histogram(segment, bins=100, density=True)[0]\n","                prob = np.sum(probability_distribution**q)\n","                segment_probs.append(prob)\n","            probs.append(np.mean(segment_probs))\n","\n","        # Compute Rényi generalized dimension\n","        logs = np.log(segment_sizes)\n","        log_probs = np.log(probs) / (q - 1)\n","        polyfit = np.polyfit(logs, log_probs, 1)\n","        return polyfit[0]  # The slope of the line corresponds to the Rényi generalized dimension\n","\n","# Sample usage:\n","ts = np.random.rand(1000)  # Replace with your actual time series data\n","q = 2  # Replace with the desired moment\n","renyi_dimension = renyi_generalized_dimension(ts, q, scale_min=10, scale_max=500, scale_step=10)\n","print(f\"Rényi generalized dimension for q={q}: {renyi_dimension}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8F76ESINcuUm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRqX2u59cuR-"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SubwD3RVcuOp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1-5p_yzcuKl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9fc4LGGaWGnl"},"source":["#Apply Feature Extraction Methods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8DdP3ADWKi3"},"outputs":[],"source":["# X_time = extract_time_domain_features(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0hmbblbJbDA"},"outputs":[],"source":["X_time = extract_time_domain_features(X)\n","sfreq = 256  # Replace with the sampling frequency of your data\n","# epochs_data = [epoch.get_data() for epoch in epochs]  # Assuming epochs is a list of MNE Epochs objects\n","X_frequency = extract_frequency_domain_features(X, sfreq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eGL289PKMJx"},"outputs":[],"source":["#merge the time and frequency features\n","X_merged_features = np.concatenate((X_time ,X_frequency), axis=2)\n","\n","X_reshape = X_merged_features.reshape(X_merged_features.shape[0], -1)\n","# X_reshape = DWT_data.reshape(DWT_data.shape[0], -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqHnVTIpKK6T"},"outputs":[],"source":["from sklearn.metrics import accuracy_score,confusion_matrix, precision_score, recall_score, f1_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GroupKFold, GridSearchCV\n","\n","\n","import xgboost as xgb\n","from sklearn.ensemble import RandomForestClassifier,HistGradientBoostingClassifier,\\\n","                                StackingClassifier,VotingClassifier,IsolationForest,\\\n","                                RandomForestRegressor\n","from sklearn.svm import SVC\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB,BernoulliNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.naive_bayes import CategoricalNB\n","from xgboost import XGBClassifier\n","# import lightgbm as lgb\n","from sklearn.tree import DecisionTreeClassifier\n","\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.model_selection import cross_val_predict, RandomizedSearchCV,GridSearchCV\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, matthews_corrcoef\n","from sklearn.model_selection import KFold\n","import pandas as pd\n","\n","\n","#Scale the data\n","SS1= StandardScaler()\n","X_scaled = SS1.fit_transform(X_reshape)\n","\n","\n","models = {\n","    'XGBoost': XGBClassifier(),\n","    'RandomForest': RandomForestClassifier(),\n","    'HistGradientBoosting': HistGradientBoostingClassifier(),\n","    'SVM': SVC(),\n","    'GradientBoosting': GradientBoostingClassifier(),\n","    'KNeighbors': KNeighborsClassifier(),\n","    'MLP': MLPClassifier(),\n","    'DecisionTree': DecisionTreeClassifier(),\n","    # 'MultinomialNB': MultinomialNB(),\n","    'GaussianNB': GaussianNB(),\n","    'BernoulliNB': BernoulliNB(),\n","    'LogisticRegression': LogisticRegression(),\n","    'AdaBoost': AdaBoostClassifier(),\n","    # 'Bagging': BaggingClassifier(),\n","    'ExtraTrees': ExtraTreesClassifier(),\n","\n","}\n","\n","def evaluate_model(model, X, y, n_splits):\n","    kf = KFold(n_splits=n_splits,shuffle=True,random_state=42)\n","    accuracies, precisions, recalls,f1s,cm,specificitys, sensitivitys,roc_aucs,kappas,mccs= [], [], [],[],[],[],[],[],[],[]\n","\n","    for train_idx, test_idx in kf.split(X):\n","      # take the location of the splited data and access it by the index\n","\n","      X_train, y_train = (X.iloc[train_idx], y.iloc[train_idx]) if isinstance(X, pd.DataFrame) else (X[train_idx], y[train_idx])\n","      X_test, y_test = (X.iloc[test_idx], y.iloc[test_idx]) if isinstance(X, pd.DataFrame) else (X[test_idx], y[test_idx])\n","      # train the model\n","      model.fit(X_train, y_train)\n","      # make prediction\n","      y_pred = model.predict(X_test)\n","\n","      # evaluation scores\n","      accuracy = accuracy_score(y_test, y_pred)\n","      precision = precision_score(y_test, y_pred,zero_division=1)\n","      recall = recall_score(y_test, y_pred)\n","      f1= f1_score(y_test, y_pred)\n","      cms = confusion_matrix(y_test, y_pred)  # Calculate confusion matrix\n","      roc_auc=roc_auc_score(y_test, y_pred)\n","\n","      # calculate the specificity and sensitivity\n","      tn, fp, fn, tp = cms.ravel()\n","      specificity = tn / (tn + fp)\n","      sensitivity = tp / (tp + fn)\n","      # Calculate Cohen's Kappa\n","      kappa = cohen_kappa_score(y_test, y_pred)\n","      mcc = matthews_corrcoef(y_test, y_pred)\n","\n","\n","\n","\n","\n","      # append all the results\n","      accuracies.append(accuracy)\n","      precisions.append(precision)\n","      recalls.append(recall)\n","      f1s.append(f1)\n","      cm.append(cms)  # Add confusion matrix to list\n","      specificitys.append(specificity) #add specificity score\n","      sensitivitys.append(sensitivity) #add sensitivity scores to the list\n","      roc_aucs.append(roc_auc) #add roc_auc score to the list\n","      kappas.append(kappa)\n","      mccs.append(mcc)\n","\n","\n","    return (format(sum(accuracies)/n_splits, '.3f'), format(sum(precisions)/n_splits, '.3f'),\n","            format(sum(recalls)/n_splits, '.3f'),format(sum(f1s)/n_splits, '.3f'),format(sum(roc_aucs)/n_splits, '.3f'), sum(cm)/n_splits ,format(sum(specificitys)/n_splits, '.3f'),\n","            format(sum(sensitivitys)/n_splits, '.3f'),format(sum(kappas)/n_splits, '.3f'),format(sum(mccs)/n_splits, '.3f'))\n","\n","\n","\n","\n","def modelApplied(Model,X_train , y_train, n_splits=5,MN='model_name',model_result = pd.DataFrame()):\n","  accuracy, precision, recall,f1,roc_auc,cm,specificity,sensitivity,kappa,mcc = evaluate_model(Model,X_train , y_train, n_splits)\n","\n","  new_row=[MN,accuracy,precision,recall,f1,roc_auc,specificity, sensitivity,kappa,mcc]\n","\n","  model_result.loc[len(model_result)]=new_row\n","  print(new_row)\n","  print('DONE!!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mT4Y0ykKgB3"},"outputs":[],"source":["# alpha_result = pd.DataFrame(columns=['Models', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Roc_Auc', 'Specificity', 'Sensitivity', 'Kappa', 'MCC'])\n","\n","# print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>ALL band<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\")\n","# # Assuming Models is your dictionary of models\n","# for k, v in models.items():\n","#     modelApplied(v, X_scaled, Y, MN=k,model_result= alpha_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYpAF6VLtU8I"},"outputs":[],"source":["# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>ALL band<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n","# ['XGBoost', '0.957', '0.987', '0.926', '0.955', '0.957', '0.987', '0.926', '0.913', '0.915']\n","# DONE!!\n","# ['RandomForest', '0.937', '0.973', '0.898', '0.934', '0.937', '0.975', '0.898', '0.873', '0.876']\n","# DONE!!\n","# ['HistGradientBoosting', '0.958', '0.991', '0.925', '0.957', '0.958', '0.991', '0.925', '0.916', '0.919']\n","# DONE!!\n","# ['SVM', '0.916', '0.960', '0.868', '0.911', '0.916', '0.963', '0.868', '0.831', '0.835']\n","# DONE!!\n","# ['GradientBoosting', '0.942', '0.975', '0.907', '0.940', '0.942', '0.977', '0.907', '0.884', '0.886']\n","# DONE!!\n","# ['KNeighbors', '0.818', '0.988', '0.644', '0.778', '0.818', '0.992', '0.644', '0.636', '0.679']\n","# DONE!!\n","# ['MLP', '0.928', '0.953', '0.901', '0.926', '0.928', '0.955', '0.901', '0.856', '0.857']\n","# DONE!!\n","# ['DecisionTree', '0.891', '0.887', '0.897', '0.891', '0.891', '0.885', '0.897', '0.782', '0.782']\n","# DONE!!\n","# ['GaussianNB', '0.857', '0.985', '0.726', '0.836', '0.857', '0.989', '0.726', '0.714', '0.741']\n","# DONE!!\n","# ['BernoulliNB', '0.865', '0.933', '0.787', '0.854', '0.865', '0.943', '0.787', '0.731', '0.740']\n","# DONE!!\n","# ['LogisticRegression', '0.899', '0.911', '0.885', '0.897', '0.899', '0.913', '0.885', '0.798', '0.798']\n","# DONE!!\n","# ['AdaBoost', '0.923', '0.947', '0.897', '0.921', '0.923', '0.950', '0.897', '0.847', '0.848']\n","# DONE!!\n","# ['ExtraTrees', '0.929', '0.964', '0.892', '0.926', '0.929', '0.967', '0.892', '0.858', '0.861']\n","# DONE!!"]},{"cell_type":"markdown","metadata":{"id":"v9PMm15XGqaK"},"source":["#Optimization"]},{"cell_type":"markdown","metadata":{"id":"IVz-RFOiLzEw"},"source":["#Final Model Exicuition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tM2k_LHINXMM"},"outputs":[],"source":["X_time.shape, X_frequency.shape, X_merged_features.shape #3 different data is available for analysis"]},{"cell_type":"markdown","metadata":{"id":"vtqS4TrtM8JO"},"source":["##Model run without data normalization"]},{"cell_type":"markdown","metadata":{"id":"ZHngiihpZpTc"},"source":["###time domain data"]},{"cell_type":"markdown","metadata":{"id":"hBvyLN6wNJ9V"},"source":["data : X_time, X_frequecy, X_merged_features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1710172854185,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"es90H7UnORAS","outputId":"6dd87964-3ccc-4ee2-d8c2-116ce296cc9b"},"outputs":[{"data":{"text/plain":["((4667, 522), (1167, 522), (4667,), (1167,))"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import train_test_split\n","X_time = X_time.reshape(X_time.shape[0],-1)\n","X_train, X_test, Y_train,Y_test = train_test_split(X_time,Y, random_state= 42, shuffle = True,test_size=0.20)\n","X_train.shape, X_test.shape, Y_train.shape,Y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1710172911521,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"4a7v6BrhPKoC","outputId":"ff0ae3f9-5902-49f4-98a0-f03171266e97"},"outputs":[{"data":{"text/plain":["1    587\n","0    580\n","dtype: int64"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["pd.DataFrame(Y_test).value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":692814,"status":"ok","timestamp":1710175175488,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"XePGSOt1L2Fg","outputId":"122af3b6-aa02-4aae-db44-95aa94385ffe"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                          Classifier  Accuracy       mcc  \\\n","0  RandomForestClassifier(max_depth=9, n_estimato...  0.930065  0.864236   \n","1                DecisionTreeClassifier(max_depth=5)  0.902811  0.809774   \n","2  ExtraTreesClassifier(max_depth=7, n_estimators...  0.897326  0.797470   \n","3                             KNeighborsClassifier()  0.775283  0.556909   \n","4  AdaBoostClassifier(learning_rate=0.5, n_estima...  0.926980  0.856534   \n","5  XGBClassifier(base_score=0.5, booster=None, ca...  0.945149  0.893378   \n","\n","      Kappa  precision    recall        f1  sensitivity  specificity  \n","0  0.860130   0.976453  0.881385  0.926486     0.978745     0.881385  \n","1  0.805622   0.948131  0.852245  0.897635     0.953377     0.852245  \n","2  0.794652   0.933757  0.855331  0.892825     0.939321     0.855331  \n","3  0.550566   0.824052  0.700034  0.756997     0.850531     0.700034  \n","4  0.853960   0.962839  0.888241  0.924037     0.965718     0.888241  \n","5  0.890298   0.985421  0.903668  0.942775     0.986630     0.903668  \n"]}],"source":["# total_Metics = []\n","# total_Metics = pd.DataFrame(total_Metics)\n","# total_Metics['Classifier'] = 'Classifier'\n","# total_Metics['Accuracy'] = 'Accuracy'\n","# total_Metics['mcc'] = 'mcc'\n","# # total_Metics['auc'] = 'auc'\n","# total_Metics['Kappa'] = 'Kappa'\n","# total_Metics['precision'] = 'precision'\n","# total_Metics['recall'] = 'recall'\n","# total_Metics['f1'] = 'f1'\n","# total_Metics['sensitivity'] = 'sensitivity'\n","# total_Metics['specificity'] = 'specificity'\n","\n","# cv = KFold(n_splits=5, random_state=42, shuffle=True)\n","\n","# # create model\n","# models = [RandomForestClassifier(n_estimators = 450, max_depth = 9),\n","#           DecisionTreeClassifier(max_depth = 5),\n","#           ExtraTreesClassifier(n_estimators = 450, max_depth = 7),\n","#           KNeighborsClassifier(n_neighbors=5),\n","#           AdaBoostClassifier(n_estimators = 350, learning_rate = 0.5, random_state = 50),\n","#           XGBClassifier(n_estimators = 350,max_depth = 7, base_score = 0.5, learning_rate = 0.1),\n","#           # LGBMClassifier(learning_rate = 0.1,max_depth = 7,random_state = 50)\n","#           ]\n","# for model in models:\n","#   from sklearn.metrics import f1_score, precision_score, recall_score, log_loss, accuracy_score, matthews_corrcoef, roc_auc_score, cohen_kappa_score\n","#   # evaluate model\n","#   # scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n","#   # model.fit(xtrain, ytrain)\n","#   # pred = model.predict(xtest)\n","#   pred = cross_val_predict(model, X_time, Y, cv=cv, n_jobs=-1)\n","\n","#   # cm1 = confusion_matrix(y, y_pred)\n","#   # report performance\n","#   Accuracy = accuracy_score(Y, pred)\n","#   mcc = matthews_corrcoef(Y, pred)\n","#   cm1 = confusion_matrix(Y, pred)\n","#   kappa = cohen_kappa_score(Y, pred)\n","#   f1 = f1_score(Y, pred)\n","#   precision_score = precision_score(Y, pred)\n","#   recall_score = recall_score(Y, pred)\n","#   sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n","#   specificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n","#   # y_pred = np.argmax(y_pred, axis=0)\n","#   # auc = roc_auc_score(y, y_pred, multi_class='ovr')\n","#   total_Metics.loc[len(total_Metics.index)] = [model,Accuracy, mcc, kappa, precision_score,recall_score, f1, sensitivity,specificity]\n","\n","# print(total_Metics)\n","# # total_Metics.to_csv(\"total metrics(FT-CV).csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":807051,"status":"ok","timestamp":1710176711396,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"Phl50Z8rVwxz","outputId":"4c3e0f05-ca15-4c94-f27a-ad45736e1c69"},"outputs":[{"name":"stdout","output_type":"stream","text":["['XGBoost', '0.942', '0.980', '0.903', '0.940', '0.942', '0.981', '0.903', '0.884', '0.887']\n","DONE!!\n","['RandomForest', '0.935', '0.974', '0.895', '0.933', '0.935', '0.976', '0.895', '0.871', '0.874']\n","DONE!!\n","['HistGradientBoosting', '0.943', '0.986', '0.899', '0.940', '0.943', '0.987', '0.899', '0.886', '0.889']\n","DONE!!\n","['SVM', '0.791', '0.864', '0.691', '0.768', '0.791', '0.891', '0.691', '0.582', '0.594']\n","DONE!!\n","['GradientBoosting', '0.926', '0.962', '0.888', '0.923', '0.926', '0.964', '0.888', '0.853', '0.855']\n","DONE!!\n","['KNeighbors', '0.775', '0.825', '0.700', '0.756', '0.775', '0.851', '0.700', '0.551', '0.558']\n","DONE!!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["['MLP', '0.832', '0.843', '0.825', '0.831', '0.831', '0.837', '0.825', '0.663', '0.669']\n","DONE!!\n","['DecisionTree', '0.889', '0.888', '0.889', '0.889', '0.889', '0.888', '0.889', '0.777', '0.777']\n","DONE!!\n","['GaussianNB', '0.771', '0.877', '0.630', '0.733', '0.771', '0.912', '0.630', '0.542', '0.565']\n","DONE!!\n","['BernoulliNB', '0.578', '0.577', '0.581', '0.579', '0.578', '0.575', '0.581', '0.156', '0.156']\n","DONE!!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"name":"stdout","output_type":"stream","text":["['LogisticRegression', '0.811', '0.828', '0.787', '0.806', '0.811', '0.836', '0.787', '0.623', '0.624']\n","DONE!!\n","['AdaBoost', '0.914', '0.941', '0.883', '0.911', '0.914', '0.944', '0.883', '0.828', '0.829']\n","DONE!!\n","['ExtraTrees', '0.932', '0.968', '0.892', '0.929', '0.932', '0.971', '0.892', '0.863', '0.866']\n","DONE!!\n"]}],"source":["time_domain_result = pd.DataFrame(columns=['Models', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Roc_Auc', 'Specificity', 'Sensitivity', 'Kappa', 'MCC'])\n","for k,v in models.items():\n","  modelApplied(v,X_time , Y, n_splits=5,MN=k,model_result = time_domain_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":477},"executionInfo":{"elapsed":2077197,"status":"error","timestamp":1710183603643,"user":{"displayName":"Raihan Rabby","userId":"04881462177269385890"},"user_tz":-360},"id":"XXETc9gRab_H","outputId":"5561629e-3f78-4006-c888-3e2f120d9470"},"outputs":[{"name":"stdout","output_type":"stream","text":["['XGBoost', '0.944', '0.982', '0.905', '0.942', '0.944', '0.983', '0.905', '0.888', '0.891']\n","DONE!!\n","['RandomForest', '0.933', '0.970', '0.894', '0.930', '0.933', '0.972', '0.894', '0.866', '0.869']\n","DONE!!\n","['HistGradientBoosting', '0.943', '0.985', '0.900', '0.940', '0.943', '0.986', '0.900', '0.886', '0.890']\n","DONE!!\n","['SVM', '0.791', '0.864', '0.691', '0.768', '0.791', '0.891', '0.691', '0.582', '0.594']\n","DONE!!\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-14de47490d64>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtime_domain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Models'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Recall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'F1-Score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Roc_Auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Specificity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sensitivity'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Kappa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MCC'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mmodelApplied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_time\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_domain_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-29-52a9f675a7fc>\u001b[0m in \u001b[0;36mmodelApplied\u001b[0;34m(Model, X_train, y_train, n_splits, MN, model_result)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodelApplied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m   \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspecificity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msensitivity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0mnew_row\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_auc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspecificity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitivity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkappa\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmcc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-52a9f675a7fc>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, X, y, n_splits)\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m       \u001b[0;31m# make prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    616\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \"\"\"\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1248\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    377\u001b[0m             )\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","\n","\n","\n","\n","\n","Classifier = {\n","    'XGBoost': XGBClassifier(n_estimators=500, learning_rate=.1, max_depth=5,base_score = 0.5),\n","    'RandomForest': RandomForestClassifier(n_estimators=150,criterion = 'gini',max_features= 'sqrt'),\n","    'HistGradientBoosting': HistGradientBoostingClassifier( max_leaf_nodes=20,max_iter=300,max_depth=17),\n","    'SVM': SVC(),\n","    'GradientBoosting': GradientBoostingClassifier(learning_rate= .2,n_estimators=450,),\n","    'KNeighbors': KNeighborsClassifier(),\n","    'MLP': MLPClassifier(),\n","    'DecisionTree': DecisionTreeClassifier(),\n","    # 'MultinomialNB': MultinomialNB(),\n","    'GaussianNB': GaussianNB(),\n","    'BernoulliNB': BernoulliNB(),\n","    'LogisticRegression': LogisticRegression(),\n","    'AdaBoost': AdaBoostClassifier(estimator = RandomForestClassifier(n_estimators=150,criterion = 'gini',max_features= 'sqrt')),\n","    # 'Bagging': BaggingClassifier(),\n","    'ExtraTrees': ExtraTreesClassifier(n_estimators=150),\n","\n","}\n","\n","time_domain_result = pd.DataFrame(columns=['Models', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Roc_Auc', 'Specificity', 'Sensitivity', 'Kappa', 'MCC'])\n","for k,v in Classifier.items():\n","  modelApplied(v,X_time , Y, n_splits=5,MN=k,model_result = time_domain_result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EuGLQOfwyks"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}